# ===== Mac M4 Pro (Apple Silicon) =====
# MLX packages for local inference
mlx==0.29.2
mlx-lm==0.28.1

# ===== Linux/HPC (NVIDIA GPUs) =====
# vLLM for optimized GPU inference
# Install with: pip install vllm torch transformers
# Uncomment when deploying to Linux/HPC:
# vllm>=0.3.0
# torch>=2.1.0
# transformers>=4.36.0

# ===== Cross-Platform (All Systems) =====
# Data processing and configuration
numpy==1.22.4
pandas==1.5.3
pyyaml>=6.0

# Progress bars and UI
tqdm==4.64.1

# Visualization (for plotting scripts)
matplotlib>=3.5.0

# HTTP requests (for Ollama backend)
requests>=2.28.0

# Legacy/Evaluation (GPT-based evaluation scripts)
openai==0.27.6

# ===== Installation Notes =====
# Mac (Apple Silicon):
#   pip install -r requirements.txt
#
# Linux/HPC (NVIDIA GPUs):
#   Uncomment vLLM section above, then:
#   pip install -r requirements.txt
#
# Ollama (Any platform):
#   curl -fsSL https://ollama.com/install.sh | sh
#   ollama pull llama3.2:3b
