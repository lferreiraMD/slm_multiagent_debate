# Multiagent Debate with Small Language Models
# Requirements for all platforms

# ===== Core Dependencies (All Platforms) =====
# Data processing and configuration
numpy>=1.22.4
pandas>=1.5.3
pyyaml>=6.0

# Environment variables (for .env file support)
python-dotenv>=0.19.0

# Progress bars and CLI
tqdm>=4.64.1

# Visualization (for plotting scripts)
matplotlib>=3.5.0

# HTTP requests (for Ollama backend)
requests>=2.28.0

# Legacy/Evaluation (GPT-based evaluation scripts)
openai==0.27.6

# ===== Mac M4 Pro (Apple Silicon) =====
# MLX packages for local inference on Apple Silicon
# Only install on macOS with ARM64:
mlx==0.29.2; sys_platform == "darwin" and platform_machine == "arm64"
mlx-lm==0.28.1; sys_platform == "darwin" and platform_machine == "arm64"

# ===== Linux/HPC (NVIDIA GPUs) =====
# vLLM for optimized GPU inference
# Only install on Linux (will fail on macOS):
# Uncomment for HPC/Linux deployment or use requirements_hpc.txt
# vllm>=0.11.0
# torch>=2.6.0
# transformers>=4.55.0

# ===== Installation Instructions =====
#
# Mac (Apple Silicon):
#   pip3 install -r requirements.txt
#
# Linux/HPC (NVIDIA GPUs):
#   pip3 install -r requirements_hpc.txt
#
# Windows/Cross-platform (Ollama):
#   pip3 install -r requirements.txt
#   Install Ollama: https://ollama.com/download
#   ollama pull llama3.2:3b
#   ollama pull qwen2.5:7b
#
# ===== Verified Versions =====
# Tested on:
# - macOS 14 (M4 Pro, 48GB RAM): mlx-lm 0.28.1, mlx 0.29.2
# - Ubuntu 22.04 (2x RTX 3090, 128GB RAM): vllm 0.11.0, torch 2.8.0, transformers 4.57.1
