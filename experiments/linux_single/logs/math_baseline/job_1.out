============================================================
Math Task - Multiagent Debate (NO COMPRESSION)
============================================================
Model: vllm-qwen3-0.6b
Agents: 1
Rounds: 3
Problems: 20
Generation params: {'temperature': 1.0, 'max_tokens': None, 'top_p': 1.0, 'n': 1}
============================================================
  0%|          | 0/20 [00:00<?, ?it/s]
--- Problem 1/20, Round 1, Agent 1/1 ---
INFO 12-03 11:51:33 [__init__.py:216] Automatically detected platform cuda.
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 11:51:36 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 11:51:56 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 11:52:16 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 11:52:36 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 11:52:56 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 11:53:16 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 11:53:36 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 11:53:56 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 11:54:16 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 11:54:37 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 11:54:57 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 11:55:17 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 11:55:37 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 11:55:57 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 11:56:17 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 11:56:37 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 11:56:57 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 11:57:17 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 11:57:38 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 11:57:58 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 11:58:18 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 11:58:38 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 11:58:58 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 11:59:18 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 11:59:38 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 11:59:58 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:00:18 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:00:38 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:00:59 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:01:19 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:01:39 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:01:59 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:02:19 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:02:39 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:02:59 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:03:19 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:03:39 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:03:59 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:04:19 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:04:40 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:05:00 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:05:20 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:05:40 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:06:00 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:06:20 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:06:40 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:07:00 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:07:21 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:07:41 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:08:01 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:08:21 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:08:41 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:09:01 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:09:21 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:09:41 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:10:01 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:10:21 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:10:42 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:11:02 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:11:22 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:11:42 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:12:02 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:12:22 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:12:42 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:13:02 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:13:22 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:13:43 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:14:03 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:14:23 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:14:43 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:15:03 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:15:23 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:15:43 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:16:03 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:16:23 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:16:44 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:17:04 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:17:24 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:17:44 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:18:04 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:18:24 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:18:44 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:19:04 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:19:24 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:19:44 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:20:05 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:20:25 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:20:45 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:21:05 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:21:25 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:21:45 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:22:05 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:22:25 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:22:45 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:23:05 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:23:26 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:23:46 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:24:06 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:24:26 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:24:46 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:25:06 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:25:26 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:25:46 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:26:06 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:26:27 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:26:47 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:27:07 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:27:27 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:27:47 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:28:07 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:28:27 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:28:47 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:29:07 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:29:28 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:29:48 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:30:08 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:30:28 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:30:48 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:31:08 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:31:28 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:31:48 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:32:08 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:32:28 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:32:49 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:33:09 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:33:29 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:33:49 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:34:09 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:34:29 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:34:49 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:35:09 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:35:29 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:35:49 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:36:10 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:36:30 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:36:50 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:37:10 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:37:30 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:37:50 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:38:10 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:38:30 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:38:50 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:39:10 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:39:31 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:39:51 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:40:11 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:40:31 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:40:51 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:41:11 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:41:31 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:41:51 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:42:11 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:42:31 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:42:52 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:43:12 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:43:32 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:43:52 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:44:12 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:44:32 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:44:52 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:45:12 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:45:33 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:45:53 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:46:13 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:46:33 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:46:53 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:47:13 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:47:33 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:47:53 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:48:14 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:48:34 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:48:54 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:49:14 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:49:34 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:49:54 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:50:14 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:50:34 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:50:54 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:51:14 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:51:35 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:51:55 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:52:15 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:52:35 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:52:55 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:53:15 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:53:35 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:53:55 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:54:15 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:54:35 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:54:56 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:55:16 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:55:36 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:55:56 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:56:16 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:56:36 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:56:56 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:57:16 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:57:36 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:57:57 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:58:17 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:58:37 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:58:57 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:59:17 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:59:37 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 12:59:57 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:00:17 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:00:37 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:00:57 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:01:18 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:01:38 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:01:58 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:02:18 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:02:38 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:02:58 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:03:18 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:03:38 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:03:58 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:04:18 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:04:39 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:04:59 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:05:19 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:05:39 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:05:59 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:06:19 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:06:39 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:07:00 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:07:20 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:07:40 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:08:00 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:08:20 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:08:40 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:09:00 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:09:20 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:09:40 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:10:01 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:10:21 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:10:41 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:11:01 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:11:21 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:11:41 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:12:01 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:12:21 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:12:41 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:13:01 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:13:22 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:13:42 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:14:02 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:14:22 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:14:42 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:15:02 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:15:22 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:15:42 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:16:02 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:16:22 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:16:43 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:17:03 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:17:23 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:17:43 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:18:03 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:18:23 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:18:43 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:19:03 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:19:23 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:19:43 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:20:04 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:20:24 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:20:44 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:21:04 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:21:24 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:21:44 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:22:04 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:22:24 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:22:44 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:23:05 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:23:25 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:23:45 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:24:05 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:24:25 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:24:45 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:25:05 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:25:25 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:25:45 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:26:06 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:26:26 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:26:46 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:27:06 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:27:26 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:27:46 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:28:06 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:28:26 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:28:47 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:29:07 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:29:27 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:29:47 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:30:07 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:30:27 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:30:47 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:31:07 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:31:27 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:31:48 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:32:08 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:32:28 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:32:48 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:33:08 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:33:28 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:33:48 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:34:08 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:34:28 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:34:49 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:35:09 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:35:29 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:35:49 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:36:09 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:36:29 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:36:49 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:37:09 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:37:29 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:37:49 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:38:10 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:38:30 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:38:50 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:39:10 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:39:30 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:39:50 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:40:10 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:40:30 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:40:50 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:41:10 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:41:31 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:41:51 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:42:11 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:42:31 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:42:51 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:43:11 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:43:31 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:43:51 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:44:11 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:44:31 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:44:52 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:45:12 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:45:32 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:45:52 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:46:12 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:46:32 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:46:52 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:47:12 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:47:33 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:47:53 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:48:13 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:48:33 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:48:53 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:49:13 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:49:33 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:49:53 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:50:13 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:50:34 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:50:54 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:51:14 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:51:34 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:51:54 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:52:14 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:52:34 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:52:54 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:53:14 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:53:35 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:53:55 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:54:15 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:54:35 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:54:55 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:55:15 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:55:35 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:55:55 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:56:15 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:56:35 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:56:56 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:57:16 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:57:36 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:57:56 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:58:16 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:58:36 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:58:56 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:59:16 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:59:36 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 13:59:57 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 14:00:17 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 14:00:37 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 14:00:57 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 14:01:17 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 14:01:37 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 14:01:57 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 14:02:17 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 14:02:37 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 14:02:57 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 14:03:18 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 14:03:38 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 14:03:58 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 14:04:18 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 14:04:38 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 14:04:58 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 14:05:18 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 14:05:38 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 14:05:58 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 14:06:18 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 14:06:39 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 14:06:59 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 14:07:19 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 14:07:39 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 14:07:59 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 14:08:19 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 14:08:39 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 14:08:59 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 14:09:20 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 14:09:40 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 14:10:00 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 14:10:20 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 14:10:40 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 14:11:00 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 14:11:20 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 14:11:40 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 14:12:00 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 14:12:20 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 14:12:41 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 14:13:01 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 14:13:21 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 14:13:41 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 14:14:01 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 14:14:21 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 14:14:41 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 14:15:01 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 14:15:21 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 14:15:41 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 14:16:02 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 14:16:22 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 14:16:42 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 14:17:02 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 14:17:22 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 14:17:42 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 14:18:02 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 14:18:22 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 14:18:42 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 14:19:02 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 14:19:23 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 14:19:43 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 14:20:03 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 14:20:23 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 14:20:43 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 14:21:03 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 14:21:23 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 14:21:43 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 14:22:03 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 14:22:23 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 14:22:44 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 14:23:04 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 14:23:24 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 14:23:44 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 14:24:04 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 14:24:24 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 14:24:44 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 14:25:04 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 14:25:24 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 14:25:44 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 14:26:05 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 14:26:25 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 14:26:45 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 14:27:05 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 14:27:25 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 14:27:45 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 14:28:05 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 14:28:25 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 14:28:46 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 14:29:06 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 14:29:26 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
