============================================================
Math Task - Multiagent Debate (NO COMPRESSION)
============================================================
Model: vllm-qwen3-0.6b
Agents: 1
Rounds: 3
Problems: 20
Generation params: {'temperature': 1.0, 'max_tokens': None, 'top_p': 1.0, 'n': 1}
============================================================
  0%|          | 0/20 [00:00<?, ?it/s]
--- Problem 1/20, Round 1, Agent 1/1 ---
INFO 12-03 14:32:12 [__init__.py:216] Automatically detected platform cuda.
[ModelCache] Loading model: vllm-qwen3-0.6b (backend=vllm)
[ModelCache] Warning: Model vllm-qwen3-0.6b not found in config.yaml, using default context_length=32768
[ModelCache] Using max_model_len=32768 for vllm-qwen3-0.6b
[ModelCache] Auto-config: 2 GPU(s) detected, tensor_parallel_size=2
INFO 12-03 14:32:14 [utils.py:233] non-default args: {'max_model_len': 32768, 'tensor_parallel_size': 2, 'max_num_batched_tokens': 8192, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'enable_sleep_mode': True, 'model': 'vllm-qwen3-0.6b'}
  0%|          | 0/20 [00:11<?, ?it/s]
Error occurred: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
Retrying due to an error......
Traceback (most recent call last):
  File "/home/leonardo/.pyenv/versions/torch/lib/python3.12/site-packages/huggingface_hub/utils/_http.py", line 402, in hf_raise_for_status
    response.raise_for_status()
  File "/home/leonardo/.pyenv/versions/torch/lib/python3.12/site-packages/requests/models.py", line 1024, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/vllm-qwen3-0.6b/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/leonardo/.pyenv/versions/torch/lib/python3.12/site-packages/transformers/utils/hub.py", line 479, in cached_files
    hf_hub_download(
  File "/home/leonardo/.pyenv/versions/torch/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/leonardo/.pyenv/versions/torch/lib/python3.12/site-packages/huggingface_hub/file_download.py", line 1007, in hf_hub_download
    return _hf_hub_download_to_cache_dir(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/leonardo/.pyenv/versions/torch/lib/python3.12/site-packages/huggingface_hub/file_download.py", line 1114, in _hf_hub_download_to_cache_dir
    _raise_on_head_call_error(head_call_error, force_download, local_files_only)
  File "/home/leonardo/.pyenv/versions/torch/lib/python3.12/site-packages/huggingface_hub/file_download.py", line 1655, in _raise_on_head_call_error
    raise head_call_error
  File "/home/leonardo/.pyenv/versions/torch/lib/python3.12/site-packages/huggingface_hub/file_download.py", line 1543, in _get_metadata_or_catch_error
    metadata = get_hf_file_metadata(
               ^^^^^^^^^^^^^^^^^^^^^
  File "/home/leonardo/.pyenv/versions/torch/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/leonardo/.pyenv/versions/torch/lib/python3.12/site-packages/huggingface_hub/file_download.py", line 1460, in get_hf_file_metadata
    r = _request_wrapper(
        ^^^^^^^^^^^^^^^^^
  File "/home/leonardo/.pyenv/versions/torch/lib/python3.12/site-packages/huggingface_hub/file_download.py", line 283, in _request_wrapper
    response = _request_wrapper(
               ^^^^^^^^^^^^^^^^^
  File "/home/leonardo/.pyenv/versions/torch/lib/python3.12/site-packages/huggingface_hub/file_download.py", line 307, in _request_wrapper
    hf_raise_for_status(response)
  File "/home/leonardo/.pyenv/versions/torch/lib/python3.12/site-packages/huggingface_hub/utils/_http.py", line 452, in hf_raise_for_status
    raise _format(RepositoryNotFoundError, message, response) from e
huggingface_hub.errors.RepositoryNotFoundError: 404 Client Error. (Request ID: Root=1-6930903e-1658f38d4a60645c7a4ccbfd;a11a0adb-cbcf-4b62-abf2-b49a2de42c96)

Repository Not Found for url: https://huggingface.co/vllm-qwen3-0.6b/resolve/main/config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated. For more details, see https://huggingface.co/docs/huggingface_hub/authentication

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/leonardo/Dropbox/Harvard/Fellowship/Classes/TopicsAI/project/slm_multiagent_debate/utils/helpers.py", line 84, in generate_answer
    completion = ChatCompletion.create(
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/leonardo/Dropbox/Harvard/Fellowship/Classes/TopicsAI/project/slm_multiagent_debate/utils/llm_wrapper.py", line 111, in create
    return cls._create_vllm(model, messages, temperature, max_tokens, top_p, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/leonardo/Dropbox/Harvard/Fellowship/Classes/TopicsAI/project/slm_multiagent_debate/utils/llm_wrapper.py", line 301, in _create_vllm
    llm, tokenizer = cls._model_cache.get_or_load(model, backend="vllm")
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/leonardo/Dropbox/Harvard/Fellowship/Classes/TopicsAI/project/slm_multiagent_debate/utils/model_cache.py", line 85, in get_or_load
    model, tokenizer = self._load_vllm(model_path)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/leonardo/Dropbox/Harvard/Fellowship/Classes/TopicsAI/project/slm_multiagent_debate/utils/model_cache.py", line 135, in _load_vllm
    llm = LLM(model=model_path, **vllm_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/leonardo/.pyenv/versions/torch/lib/python3.12/site-packages/vllm/entrypoints/llm.py", line 297, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/leonardo/.pyenv/versions/torch/lib/python3.12/site-packages/vllm/v1/engine/llm_engine.py", line 169, in from_engine_args
    vllm_config = engine_args.create_engine_config(usage_context)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/leonardo/.pyenv/versions/torch/lib/python3.12/site-packages/vllm/engine/arg_utils.py", line 1135, in create_engine_config
    self.speculative_config) = maybe_override_with_speculators(
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/leonardo/.pyenv/versions/torch/lib/python3.12/site-packages/vllm/transformers_utils/config.py", line 501, in maybe_override_with_speculators
    config_dict, _ = PretrainedConfig.get_config_dict(
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/leonardo/.pyenv/versions/torch/lib/python3.12/site-packages/transformers/configuration_utils.py", line 662, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/leonardo/.pyenv/versions/torch/lib/python3.12/site-packages/transformers/configuration_utils.py", line 721, in _get_config_dict
    resolved_config_file = cached_file(
                           ^^^^^^^^^^^^
  File "/home/leonardo/.pyenv/versions/torch/lib/python3.12/site-packages/transformers/utils/hub.py", line 322, in cached_file
    file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/leonardo/.pyenv/versions/torch/lib/python3.12/site-packages/transformers/utils/hub.py", line 511, in cached_files
    raise OSError(
OSError: vllm-qwen3-0.6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/leonardo/Dropbox/Harvard/Fellowship/Classes/TopicsAI/project/slm_multiagent_debate/tasks/math/gen_math_clean.py", line 206, in <module>
    completion = generate_answer(agent_context, model_name, generation_params,
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/leonardo/Dropbox/Harvard/Fellowship/Classes/TopicsAI/project/slm_multiagent_debate/utils/helpers.py", line 92, in generate_answer
    time.sleep(20)
KeyboardInterrupt
