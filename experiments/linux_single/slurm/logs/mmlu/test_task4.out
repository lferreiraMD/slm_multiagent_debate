Using persona diversity with 3 different personas
============================================================
MMLU Task - Multiagent Debate
============================================================
Model: Qwen/Qwen3-14B
Persona diversity mode:
  Agent 1: an enigma machine operator whose primary filter is signal-to...
  Agent 2: a Zen master who communicates only through non-sequiturs, ko...
  Agent 3: a deep-sea volcanologist focused on extremes of pressure, he...
Agents: 3
Rounds: 3
Questions: 2
Dataset: /home/ch269957/projects/slm_multiagent_debate/data/mmlu
Generation params: {'temperature': 1.0, 'max_tokens': None, 'top_p': 1.0, 'n': 1}
============================================================
Found 57 MMLU test files

--- Problem 1/2, Round 1, Agent 1/3 ---
[ModelCache] Loading model: Qwen/Qwen3-14B (backend=vllm)
[ModelCache] Using max_model_len=40960 for Qwen/Qwen3-14B
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 13:12:04 [utils.py:253] non-default args: {'max_model_len': 40960, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'Qwen/Qwen3-14B'}
Using persona diversity with 3 different personas
============================================================
MMLU Task - Multiagent Debate
============================================================
Model: Qwen/Qwen3-14B
Persona diversity mode:
  Agent 1: an enigma machine operator whose primary filter is signal-to...
  Agent 2: a Zen master who communicates only through non-sequiturs, ko...
  Agent 3: a deep-sea volcanologist focused on extremes of pressure, he...
Agents: 3
Rounds: 3
Questions: 2
Dataset: /home/ch269957/projects/slm_multiagent_debate/data/mmlu
Generation params: {'temperature': 1.0, 'max_tokens': None, 'top_p': 1.0, 'n': 1}
============================================================
Found 57 MMLU test files

--- Problem 1/2, Round 1, Agent 1/3 ---
[ModelCache] Loading model: Qwen/Qwen3-14B (backend=vllm)
[ModelCache] Using max_model_len=40960 for Qwen/Qwen3-14B
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 13:12:05 [utils.py:253] non-default args: {'max_model_len': 40960, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'Qwen/Qwen3-14B'}
INFO 12-04 13:12:05 [model.py:631] Resolved architecture: Qwen3ForCausalLM
INFO 12-04 13:12:05 [model.py:1745] Using max model len 40960
INFO 12-04 13:12:05 [model.py:631] Resolved architecture: Qwen3ForCausalLM
INFO 12-04 13:12:05 [model.py:1745] Using max model len 40960
INFO 12-04 13:12:06 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
INFO 12-04 13:12:06 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-04 13:12:07 [system_utils.py:103] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: CUDA is initialized
WARNING 12-04 13:12:07 [system_utils.py:103] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: CUDA is initialized
[1;36m(EngineCore_DP0 pid=367252)[0;0m INFO 12-04 13:12:23 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='Qwen/Qwen3-14B', speculative_config=None, tokenizer='Qwen/Qwen3-14B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-14B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=367246)[0;0m INFO 12-04 13:12:23 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='Qwen/Qwen3-14B', speculative_config=None, tokenizer='Qwen/Qwen3-14B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-14B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=367246)[0;0m INFO 12-04 13:12:25 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.98:60179 backend=nccl
[1;36m(EngineCore_DP0 pid=367252)[0;0m INFO 12-04 13:12:25 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.98:36791 backend=nccl
[W1204 13:12:25.362054115 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-25-0.rc.tch.harvard.edu]:60179 (errno: 97 - Address family not supported by protocol).
[W1204 13:12:25.363229341 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-25-0.rc.tch.harvard.edu]:36791 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=367252)[0;0m INFO 12-04 13:12:25 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=367246)[0;0m INFO 12-04 13:12:25 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=367252)[0;0m INFO 12-04 13:12:26 [gpu_model_runner.py:3259] Starting to load model Qwen/Qwen3-14B...
[1;36m(EngineCore_DP0 pid=367246)[0;0m INFO 12-04 13:12:26 [gpu_model_runner.py:3259] Starting to load model Qwen/Qwen3-14B...
[1;36m(EngineCore_DP0 pid=367252)[0;0m INFO 12-04 13:12:27 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=367252)[0;0m INFO 12-04 13:12:27 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=367246)[0;0m INFO 12-04 13:12:27 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=367246)[0;0m INFO 12-04 13:12:27 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=367246)[0;0m ERROR 12-04 13:12:28 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=367246)[0;0m ERROR 12-04 13:12:28 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=367246)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=367246)[0;0m ERROR 12-04 13:12:28 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=367246)[0;0m ERROR 12-04 13:12:28 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367246)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=367246)[0;0m ERROR 12-04 13:12:28 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=367246)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=367246)[0;0m ERROR 12-04 13:12:28 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=367246)[0;0m ERROR 12-04 13:12:28 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367246)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=367246)[0;0m ERROR 12-04 13:12:28 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=367246)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[1;36m(EngineCore_DP0 pid=367246)[0;0m ERROR 12-04 13:12:28 [core.py:842]     self.driver_worker.load_model()
[1;36m(EngineCore_DP0 pid=367246)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 273, in load_model
[1;36m(EngineCore_DP0 pid=367246)[0;0m ERROR 12-04 13:12:28 [core.py:842]     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[1;36m(EngineCore_DP0 pid=367246)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3276, in load_model
[1;36m(EngineCore_DP0 pid=367246)[0;0m ERROR 12-04 13:12:28 [core.py:842]     self.model = model_loader.load_model(
[1;36m(EngineCore_DP0 pid=367246)[0;0m ERROR 12-04 13:12:28 [core.py:842]                  ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367246)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
[1;36m(EngineCore_DP0 pid=367246)[0;0m ERROR 12-04 13:12:28 [core.py:842]     model = initialize_model(
[1;36m(EngineCore_DP0 pid=367246)[0;0m ERROR 12-04 13:12:28 [core.py:842]             ^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367246)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py", line 55, in initialize_model
[1;36m(EngineCore_DP0 pid=367246)[0;0m ERROR 12-04 13:12:28 [core.py:842]     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(EngineCore_DP0 pid=367246)[0;0m ERROR 12-04 13:12:28 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367246)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 279, in __init__
[1;36m(EngineCore_DP0 pid=367246)[0;0m ERROR 12-04 13:12:28 [core.py:842]     self.model = Qwen3Model(
[1;36m(EngineCore_DP0 pid=367246)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=367246)[0;0m ERROR 12-04 13:12:28 [core.py:842]                  ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367246)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=367246)[0;0m ERROR 12-04 13:12:28 [core.py:842]     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=367246)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 253, in __init__
[1;36m(EngineCore_DP0 pid=367246)[0;0m ERROR 12-04 13:12:28 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=367246)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=367246)[0;0m ERROR 12-04 13:12:28 [core.py:842]     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=367246)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 337, in __init__
[1;36m(EngineCore_DP0 pid=367246)[0;0m ERROR 12-04 13:12:28 [core.py:842]     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(EngineCore_DP0 pid=367246)[0;0m ERROR 12-04 13:12:28 [core.py:842]                                                     ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367246)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 651, in make_layers
[1;36m(EngineCore_DP0 pid=367246)[0;0m ERROR 12-04 13:12:28 [core.py:842]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(EngineCore_DP0 pid=367246)[0;0m ERROR 12-04 13:12:28 [core.py:842]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367246)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 339, in <lambda>
[1;36m(EngineCore_DP0 pid=367246)[0;0m ERROR 12-04 13:12:28 [core.py:842]     lambda prefix: decoder_layer_type(
[1;36m(EngineCore_DP0 pid=367246)[0;0m ERROR 12-04 13:12:28 [core.py:842]                    ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367246)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 201, in __init__
[1;36m(EngineCore_DP0 pid=367246)[0;0m ERROR 12-04 13:12:28 [core.py:842]     self.mlp = Qwen3MLP(
[1;36m(EngineCore_DP0 pid=367246)[0;0m ERROR 12-04 13:12:28 [core.py:842]                ^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367246)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 84, in __init__
[1;36m(EngineCore_DP0 pid=367246)[0;0m ERROR 12-04 13:12:28 [core.py:842]     self.gate_up_proj = MergedColumnParallelLinear(
[1;36m(EngineCore_DP0 pid=367246)[0;0m ERROR 12-04 13:12:28 [core.py:842]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367246)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 631, in __init__
[1;36m(EngineCore_DP0 pid=367246)[0;0m ERROR 12-04 13:12:28 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=367246)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 484, in __init__
[1;36m(EngineCore_DP0 pid=367246)[0;0m ERROR 12-04 13:12:28 [core.py:842]     self.quant_method.create_weights(
[1;36m(EngineCore_DP0 pid=367246)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 215, in create_weights
[1;36m(EngineCore_DP0 pid=367246)[0;0m ERROR 12-04 13:12:28 [core.py:842]     data=torch.empty(
[1;36m(EngineCore_DP0 pid=367246)[0;0m ERROR 12-04 13:12:28 [core.py:842]          ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367246)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_device.py", line 103, in __torch_function__
[1;36m(EngineCore_DP0 pid=367246)[0;0m ERROR 12-04 13:12:28 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=367246)[0;0m ERROR 12-04 13:12:28 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367246)[0;0m ERROR 12-04 13:12:28 [core.py:842] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 340.00 MiB. GPU 0 has a total capacity of 44.39 GiB of which 54.00 MiB is free. Including non-PyTorch memory, this process has 21.87 GiB memory in use. Process 367252 has 22.48 GiB memory in use. Of the allocated memory 21.35 GiB is allocated by PyTorch, and 18.55 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=367246)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=367246)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=367246)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=367246)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=367246)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=367246)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=367246)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=367246)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=367246)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=367246)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367246)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=367246)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=367246)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=367246)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=367246)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367246)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=367246)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=367246)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[1;36m(EngineCore_DP0 pid=367246)[0;0m     self.driver_worker.load_model()
[1;36m(EngineCore_DP0 pid=367246)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 273, in load_model
[1;36m(EngineCore_DP0 pid=367246)[0;0m     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[1;36m(EngineCore_DP0 pid=367246)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3276, in load_model
[1;36m(EngineCore_DP0 pid=367246)[0;0m     self.model = model_loader.load_model(
[1;36m(EngineCore_DP0 pid=367246)[0;0m                  ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367246)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
[1;36m(EngineCore_DP0 pid=367246)[0;0m     model = initialize_model(
[1;36m(EngineCore_DP0 pid=367246)[0;0m             ^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367246)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py", line 55, in initialize_model
[1;36m(EngineCore_DP0 pid=367246)[0;0m     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(EngineCore_DP0 pid=367246)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367246)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 279, in __init__
[1;36m(EngineCore_DP0 pid=367246)[0;0m     self.model = Qwen3Model(
[1;36m(EngineCore_DP0 pid=367246)[0;0m                  ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367246)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=367246)[0;0m     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=367246)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 253, in __init__
[1;36m(EngineCore_DP0 pid=367246)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=367246)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=367246)[0;0m     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=367246)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 337, in __init__
[1;36m(EngineCore_DP0 pid=367246)[0;0m     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(EngineCore_DP0 pid=367246)[0;0m                                                     ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367246)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 651, in make_layers
[1;36m(EngineCore_DP0 pid=367246)[0;0m     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(EngineCore_DP0 pid=367246)[0;0m                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367246)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 339, in <lambda>
[1;36m(EngineCore_DP0 pid=367246)[0;0m     lambda prefix: decoder_layer_type(
[1;36m(EngineCore_DP0 pid=367246)[0;0m                    ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367246)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 201, in __init__
[1;36m(EngineCore_DP0 pid=367246)[0;0m     self.mlp = Qwen3MLP(
[1;36m(EngineCore_DP0 pid=367246)[0;0m                ^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367246)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 84, in __init__
[1;36m(EngineCore_DP0 pid=367246)[0;0m     self.gate_up_proj = MergedColumnParallelLinear(
[1;36m(EngineCore_DP0 pid=367246)[0;0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367246)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 631, in __init__
[1;36m(EngineCore_DP0 pid=367246)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=367246)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 484, in __init__
[1;36m(EngineCore_DP0 pid=367246)[0;0m     self.quant_method.create_weights(
[1;36m(EngineCore_DP0 pid=367246)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 215, in create_weights
[1;36m(EngineCore_DP0 pid=367246)[0;0m     data=torch.empty(
[1;36m(EngineCore_DP0 pid=367246)[0;0m          ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367246)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_device.py", line 103, in __torch_function__
[1;36m(EngineCore_DP0 pid=367246)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=367246)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367246)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 340.00 MiB. GPU 0 has a total capacity of 44.39 GiB of which 54.00 MiB is free. Including non-PyTorch memory, this process has 21.87 GiB memory in use. Process 367252 has 22.48 GiB memory in use. Of the allocated memory 21.35 GiB is allocated by PyTorch, and 18.55 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=367252)[0;0m ERROR 12-04 13:12:28 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=367252)[0;0m ERROR 12-04 13:12:28 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=367252)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=367252)[0;0m ERROR 12-04 13:12:28 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=367252)[0;0m ERROR 12-04 13:12:28 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367252)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=367252)[0;0m ERROR 12-04 13:12:28 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=367252)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=367252)[0;0m ERROR 12-04 13:12:28 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=367252)[0;0m ERROR 12-04 13:12:28 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367252)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=367252)[0;0m ERROR 12-04 13:12:28 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=367252)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[1;36m(EngineCore_DP0 pid=367252)[0;0m ERROR 12-04 13:12:28 [core.py:842]     self.driver_worker.load_model()
[1;36m(EngineCore_DP0 pid=367252)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 273, in load_model
[1;36m(EngineCore_DP0 pid=367252)[0;0m ERROR 12-04 13:12:28 [core.py:842]     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[1;36m(EngineCore_DP0 pid=367252)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3276, in load_model
[1;36m(EngineCore_DP0 pid=367252)[0;0m ERROR 12-04 13:12:28 [core.py:842]     self.model = model_loader.load_model(
[1;36m(EngineCore_DP0 pid=367252)[0;0m ERROR 12-04 13:12:28 [core.py:842]                  ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367252)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
[1;36m(EngineCore_DP0 pid=367252)[0;0m ERROR 12-04 13:12:28 [core.py:842]     model = initialize_model(
[1;36m(EngineCore_DP0 pid=367252)[0;0m ERROR 12-04 13:12:28 [core.py:842]             ^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367252)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py", line 55, in initialize_model
[1;36m(EngineCore_DP0 pid=367252)[0;0m ERROR 12-04 13:12:28 [core.py:842]     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(EngineCore_DP0 pid=367252)[0;0m ERROR 12-04 13:12:28 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367252)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 279, in __init__
[1;36m(EngineCore_DP0 pid=367252)[0;0m ERROR 12-04 13:12:28 [core.py:842]     self.model = Qwen3Model(
[1;36m(EngineCore_DP0 pid=367252)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=367252)[0;0m ERROR 12-04 13:12:28 [core.py:842]                  ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367252)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=367252)[0;0m ERROR 12-04 13:12:28 [core.py:842]     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=367252)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 253, in __init__
[1;36m(EngineCore_DP0 pid=367252)[0;0m ERROR 12-04 13:12:28 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=367252)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=367252)[0;0m ERROR 12-04 13:12:28 [core.py:842]     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=367252)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 337, in __init__
[1;36m(EngineCore_DP0 pid=367252)[0;0m ERROR 12-04 13:12:28 [core.py:842]     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(EngineCore_DP0 pid=367252)[0;0m ERROR 12-04 13:12:28 [core.py:842]                                                     ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367252)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 651, in make_layers
[1;36m(EngineCore_DP0 pid=367252)[0;0m ERROR 12-04 13:12:28 [core.py:842]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(EngineCore_DP0 pid=367252)[0;0m ERROR 12-04 13:12:28 [core.py:842]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367252)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 339, in <lambda>
[1;36m(EngineCore_DP0 pid=367252)[0;0m ERROR 12-04 13:12:28 [core.py:842]     lambda prefix: decoder_layer_type(
[1;36m(EngineCore_DP0 pid=367252)[0;0m ERROR 12-04 13:12:28 [core.py:842]                    ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367252)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 201, in __init__
[1;36m(EngineCore_DP0 pid=367252)[0;0m ERROR 12-04 13:12:28 [core.py:842]     self.mlp = Qwen3MLP(
[1;36m(EngineCore_DP0 pid=367252)[0;0m ERROR 12-04 13:12:28 [core.py:842]                ^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367252)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 84, in __init__
[1;36m(EngineCore_DP0 pid=367252)[0;0m ERROR 12-04 13:12:28 [core.py:842]     self.gate_up_proj = MergedColumnParallelLinear(
[1;36m(EngineCore_DP0 pid=367252)[0;0m ERROR 12-04 13:12:28 [core.py:842]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367252)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 631, in __init__
[1;36m(EngineCore_DP0 pid=367252)[0;0m ERROR 12-04 13:12:28 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=367252)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 484, in __init__
[1;36m(EngineCore_DP0 pid=367252)[0;0m ERROR 12-04 13:12:28 [core.py:842]     self.quant_method.create_weights(
[1;36m(EngineCore_DP0 pid=367252)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 215, in create_weights
[1;36m(EngineCore_DP0 pid=367252)[0;0m ERROR 12-04 13:12:28 [core.py:842]     data=torch.empty(
[1;36m(EngineCore_DP0 pid=367252)[0;0m ERROR 12-04 13:12:28 [core.py:842]          ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367252)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_device.py", line 103, in __torch_function__
[1;36m(EngineCore_DP0 pid=367252)[0;0m ERROR 12-04 13:12:28 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=367252)[0;0m ERROR 12-04 13:12:28 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367252)[0;0m ERROR 12-04 13:12:28 [core.py:842] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 340.00 MiB. GPU 0 has a total capacity of 44.39 GiB of which 34.00 MiB is free. Process 367246 has 21.87 GiB memory in use. Including non-PyTorch memory, this process has 22.48 GiB memory in use. Of the allocated memory 21.96 GiB is allocated by PyTorch, and 18.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=367252)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=367252)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=367252)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=367252)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=367252)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=367252)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=367252)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=367252)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=367252)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=367252)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367252)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=367252)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=367252)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=367252)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=367252)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367252)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=367252)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=367252)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[1;36m(EngineCore_DP0 pid=367252)[0;0m     self.driver_worker.load_model()
[1;36m(EngineCore_DP0 pid=367252)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 273, in load_model
[1;36m(EngineCore_DP0 pid=367252)[0;0m     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[1;36m(EngineCore_DP0 pid=367252)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3276, in load_model
[1;36m(EngineCore_DP0 pid=367252)[0;0m     self.model = model_loader.load_model(
[1;36m(EngineCore_DP0 pid=367252)[0;0m                  ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367252)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
[1;36m(EngineCore_DP0 pid=367252)[0;0m     model = initialize_model(
[1;36m(EngineCore_DP0 pid=367252)[0;0m             ^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367252)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py", line 55, in initialize_model
[1;36m(EngineCore_DP0 pid=367252)[0;0m     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(EngineCore_DP0 pid=367252)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367252)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 279, in __init__
[1;36m(EngineCore_DP0 pid=367252)[0;0m     self.model = Qwen3Model(
[1;36m(EngineCore_DP0 pid=367252)[0;0m                  ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367252)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=367252)[0;0m     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=367252)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 253, in __init__
[1;36m(EngineCore_DP0 pid=367252)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=367252)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=367252)[0;0m     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=367252)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 337, in __init__
[1;36m(EngineCore_DP0 pid=367252)[0;0m     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(EngineCore_DP0 pid=367252)[0;0m                                                     ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367252)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 651, in make_layers
[1;36m(EngineCore_DP0 pid=367252)[0;0m     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(EngineCore_DP0 pid=367252)[0;0m                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367252)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 339, in <lambda>
[1;36m(EngineCore_DP0 pid=367252)[0;0m     lambda prefix: decoder_layer_type(
[1;36m(EngineCore_DP0 pid=367252)[0;0m                    ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367252)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 201, in __init__
[1;36m(EngineCore_DP0 pid=367252)[0;0m     self.mlp = Qwen3MLP(
[1;36m(EngineCore_DP0 pid=367252)[0;0m                ^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367252)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 84, in __init__
[1;36m(EngineCore_DP0 pid=367252)[0;0m     self.gate_up_proj = MergedColumnParallelLinear(
[1;36m(EngineCore_DP0 pid=367252)[0;0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367252)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 631, in __init__
[1;36m(EngineCore_DP0 pid=367252)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=367252)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 484, in __init__
[1;36m(EngineCore_DP0 pid=367252)[0;0m     self.quant_method.create_weights(
[1;36m(EngineCore_DP0 pid=367252)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 215, in create_weights
[1;36m(EngineCore_DP0 pid=367252)[0;0m     data=torch.empty(
[1;36m(EngineCore_DP0 pid=367252)[0;0m          ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367252)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_device.py", line 103, in __torch_function__
[1;36m(EngineCore_DP0 pid=367252)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=367252)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367252)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 340.00 MiB. GPU 0 has a total capacity of 44.39 GiB of which 34.00 MiB is free. Process 367246 has 21.87 GiB memory in use. Including non-PyTorch memory, this process has 22.48 GiB memory in use. Of the allocated memory 21.96 GiB is allocated by PyTorch, and 18.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W1204 13:12:29.814011001 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank0]:[W1204 13:12:29.814011085 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: Qwen/Qwen3-14B (backend=vllm)
[ModelCache] Using max_model_len=40960 for Qwen/Qwen3-14B
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 13:12:51 [utils.py:253] non-default args: {'max_model_len': 40960, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'Qwen/Qwen3-14B'}
INFO 12-04 13:12:51 [model.py:631] Resolved architecture: Qwen3ForCausalLM
INFO 12-04 13:12:51 [model.py:1745] Using max model len 40960
INFO 12-04 13:12:51 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: Qwen/Qwen3-14B (backend=vllm)
[ModelCache] Using max_model_len=40960 for Qwen/Qwen3-14B
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 13:12:51 [utils.py:253] non-default args: {'max_model_len': 40960, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'Qwen/Qwen3-14B'}
INFO 12-04 13:12:51 [model.py:631] Resolved architecture: Qwen3ForCausalLM
INFO 12-04 13:12:51 [model.py:1745] Using max model len 40960
INFO 12-04 13:12:51 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=367770)[0;0m INFO 12-04 13:13:10 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='Qwen/Qwen3-14B', speculative_config=None, tokenizer='Qwen/Qwen3-14B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-14B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=367761)[0;0m INFO 12-04 13:13:10 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='Qwen/Qwen3-14B', speculative_config=None, tokenizer='Qwen/Qwen3-14B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-14B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=367761)[0;0m INFO 12-04 13:13:12 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.98:45515 backend=nccl
[W1204 13:13:12.345700677 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-25-0.rc.tch.harvard.edu]:45515 (errno: 97 - Address family not supported by protocol).
[1;36m(EngineCore_DP0 pid=367770)[0;0m INFO 12-04 13:13:12 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.98:38735 backend=nccl
[W1204 13:13:12.362876478 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-25-0.rc.tch.harvard.edu]:38735 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=367761)[0;0m INFO 12-04 13:13:12 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=367770)[0;0m INFO 12-04 13:13:12 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=367761)[0;0m INFO 12-04 13:13:13 [gpu_model_runner.py:3259] Starting to load model Qwen/Qwen3-14B...
[1;36m(EngineCore_DP0 pid=367770)[0;0m INFO 12-04 13:13:13 [gpu_model_runner.py:3259] Starting to load model Qwen/Qwen3-14B...
[1;36m(EngineCore_DP0 pid=367761)[0;0m INFO 12-04 13:13:14 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=367761)[0;0m INFO 12-04 13:13:14 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=367770)[0;0m INFO 12-04 13:13:14 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=367770)[0;0m INFO 12-04 13:13:14 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=367770)[0;0m ERROR 12-04 13:13:15 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=367770)[0;0m ERROR 12-04 13:13:15 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=367770)[0;0m ERROR 12-04 13:13:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=367770)[0;0m ERROR 12-04 13:13:15 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=367770)[0;0m ERROR 12-04 13:13:15 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367770)[0;0m ERROR 12-04 13:13:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=367770)[0;0m ERROR 12-04 13:13:15 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=367770)[0;0m ERROR 12-04 13:13:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=367770)[0;0m ERROR 12-04 13:13:15 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=367770)[0;0m ERROR 12-04 13:13:15 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367770)[0;0m ERROR 12-04 13:13:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=367770)[0;0m ERROR 12-04 13:13:15 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=367770)[0;0m ERROR 12-04 13:13:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[1;36m(EngineCore_DP0 pid=367770)[0;0m ERROR 12-04 13:13:15 [core.py:842]     self.driver_worker.load_model()
[1;36m(EngineCore_DP0 pid=367770)[0;0m ERROR 12-04 13:13:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 273, in load_model
[1;36m(EngineCore_DP0 pid=367770)[0;0m ERROR 12-04 13:13:15 [core.py:842]     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[1;36m(EngineCore_DP0 pid=367770)[0;0m ERROR 12-04 13:13:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3276, in load_model
[1;36m(EngineCore_DP0 pid=367770)[0;0m ERROR 12-04 13:13:15 [core.py:842]     self.model = model_loader.load_model(
[1;36m(EngineCore_DP0 pid=367770)[0;0m ERROR 12-04 13:13:15 [core.py:842]                  ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367770)[0;0m ERROR 12-04 13:13:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
[1;36m(EngineCore_DP0 pid=367770)[0;0m ERROR 12-04 13:13:15 [core.py:842]     model = initialize_model(
[1;36m(EngineCore_DP0 pid=367770)[0;0m ERROR 12-04 13:13:15 [core.py:842]             ^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367770)[0;0m ERROR 12-04 13:13:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py", line 55, in initialize_model
[1;36m(EngineCore_DP0 pid=367770)[0;0m ERROR 12-04 13:13:15 [core.py:842]     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(EngineCore_DP0 pid=367770)[0;0m ERROR 12-04 13:13:15 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367770)[0;0m ERROR 12-04 13:13:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 279, in __init__
[1;36m(EngineCore_DP0 pid=367770)[0;0m ERROR 12-04 13:13:15 [core.py:842]     self.model = Qwen3Model(
[1;36m(EngineCore_DP0 pid=367770)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=367770)[0;0m ERROR 12-04 13:13:15 [core.py:842]                  ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367770)[0;0m ERROR 12-04 13:13:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=367770)[0;0m ERROR 12-04 13:13:15 [core.py:842]     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=367770)[0;0m ERROR 12-04 13:13:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 253, in __init__
[1;36m(EngineCore_DP0 pid=367770)[0;0m ERROR 12-04 13:13:15 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=367770)[0;0m ERROR 12-04 13:13:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=367770)[0;0m ERROR 12-04 13:13:15 [core.py:842]     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=367770)[0;0m ERROR 12-04 13:13:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 337, in __init__
[1;36m(EngineCore_DP0 pid=367770)[0;0m ERROR 12-04 13:13:15 [core.py:842]     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(EngineCore_DP0 pid=367770)[0;0m ERROR 12-04 13:13:15 [core.py:842]                                                     ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367770)[0;0m ERROR 12-04 13:13:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 651, in make_layers
[1;36m(EngineCore_DP0 pid=367770)[0;0m ERROR 12-04 13:13:15 [core.py:842]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(EngineCore_DP0 pid=367770)[0;0m ERROR 12-04 13:13:15 [core.py:842]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367770)[0;0m ERROR 12-04 13:13:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 339, in <lambda>
[1;36m(EngineCore_DP0 pid=367770)[0;0m ERROR 12-04 13:13:15 [core.py:842]     lambda prefix: decoder_layer_type(
[1;36m(EngineCore_DP0 pid=367770)[0;0m ERROR 12-04 13:13:15 [core.py:842]                    ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367770)[0;0m ERROR 12-04 13:13:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 201, in __init__
[1;36m(EngineCore_DP0 pid=367770)[0;0m ERROR 12-04 13:13:15 [core.py:842]     self.mlp = Qwen3MLP(
[1;36m(EngineCore_DP0 pid=367770)[0;0m ERROR 12-04 13:13:15 [core.py:842]                ^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367770)[0;0m ERROR 12-04 13:13:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 84, in __init__
[1;36m(EngineCore_DP0 pid=367770)[0;0m ERROR 12-04 13:13:15 [core.py:842]     self.gate_up_proj = MergedColumnParallelLinear(
[1;36m(EngineCore_DP0 pid=367770)[0;0m ERROR 12-04 13:13:15 [core.py:842]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367770)[0;0m ERROR 12-04 13:13:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 631, in __init__
[1;36m(EngineCore_DP0 pid=367770)[0;0m ERROR 12-04 13:13:15 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=367770)[0;0m ERROR 12-04 13:13:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 484, in __init__
[1;36m(EngineCore_DP0 pid=367770)[0;0m ERROR 12-04 13:13:15 [core.py:842]     self.quant_method.create_weights(
[1;36m(EngineCore_DP0 pid=367770)[0;0m ERROR 12-04 13:13:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 215, in create_weights
[1;36m(EngineCore_DP0 pid=367770)[0;0m ERROR 12-04 13:13:15 [core.py:842]     data=torch.empty(
[1;36m(EngineCore_DP0 pid=367770)[0;0m ERROR 12-04 13:13:15 [core.py:842]          ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367770)[0;0m ERROR 12-04 13:13:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_device.py", line 103, in __torch_function__
[1;36m(EngineCore_DP0 pid=367770)[0;0m ERROR 12-04 13:13:15 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=367770)[0;0m ERROR 12-04 13:13:15 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367770)[0;0m ERROR 12-04 13:13:15 [core.py:842] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 340.00 MiB. GPU 0 has a total capacity of 44.39 GiB of which 124.00 MiB is free. Including non-PyTorch memory, this process has 20.02 GiB memory in use. Process 367761 has 24.36 GiB memory in use. Of the allocated memory 19.50 GiB is allocated by PyTorch, and 18.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=367770)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=367770)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=367770)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=367770)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=367770)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=367770)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=367770)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=367770)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=367770)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=367770)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367770)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=367770)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=367770)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=367770)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=367770)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367770)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=367770)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=367770)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[1;36m(EngineCore_DP0 pid=367770)[0;0m     self.driver_worker.load_model()
[1;36m(EngineCore_DP0 pid=367770)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 273, in load_model
[1;36m(EngineCore_DP0 pid=367770)[0;0m     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[1;36m(EngineCore_DP0 pid=367770)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3276, in load_model
[1;36m(EngineCore_DP0 pid=367770)[0;0m     self.model = model_loader.load_model(
[1;36m(EngineCore_DP0 pid=367770)[0;0m                  ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367770)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
[1;36m(EngineCore_DP0 pid=367770)[0;0m     model = initialize_model(
[1;36m(EngineCore_DP0 pid=367770)[0;0m             ^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367770)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py", line 55, in initialize_model
[1;36m(EngineCore_DP0 pid=367770)[0;0m     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(EngineCore_DP0 pid=367770)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367770)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 279, in __init__
[1;36m(EngineCore_DP0 pid=367770)[0;0m     self.model = Qwen3Model(
[1;36m(EngineCore_DP0 pid=367770)[0;0m                  ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367770)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=367770)[0;0m     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=367770)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 253, in __init__
[1;36m(EngineCore_DP0 pid=367770)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=367770)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=367770)[0;0m     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=367770)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 337, in __init__
[1;36m(EngineCore_DP0 pid=367770)[0;0m     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(EngineCore_DP0 pid=367770)[0;0m                                                     ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367770)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 651, in make_layers
[1;36m(EngineCore_DP0 pid=367770)[0;0m     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(EngineCore_DP0 pid=367770)[0;0m                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367770)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 339, in <lambda>
[1;36m(EngineCore_DP0 pid=367770)[0;0m     lambda prefix: decoder_layer_type(
[1;36m(EngineCore_DP0 pid=367770)[0;0m                    ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367770)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 201, in __init__
[1;36m(EngineCore_DP0 pid=367770)[0;0m     self.mlp = Qwen3MLP(
[1;36m(EngineCore_DP0 pid=367770)[0;0m                ^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367770)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 84, in __init__
[1;36m(EngineCore_DP0 pid=367770)[0;0m     self.gate_up_proj = MergedColumnParallelLinear(
[1;36m(EngineCore_DP0 pid=367770)[0;0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367770)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 631, in __init__
[1;36m(EngineCore_DP0 pid=367770)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=367770)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 484, in __init__
[1;36m(EngineCore_DP0 pid=367770)[0;0m     self.quant_method.create_weights(
[1;36m(EngineCore_DP0 pid=367770)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 215, in create_weights
[1;36m(EngineCore_DP0 pid=367770)[0;0m     data=torch.empty(
[1;36m(EngineCore_DP0 pid=367770)[0;0m          ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367770)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_device.py", line 103, in __torch_function__
[1;36m(EngineCore_DP0 pid=367770)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=367770)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367770)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 340.00 MiB. GPU 0 has a total capacity of 44.39 GiB of which 124.00 MiB is free. Including non-PyTorch memory, this process has 20.02 GiB memory in use. Process 367761 has 24.36 GiB memory in use. Of the allocated memory 19.50 GiB is allocated by PyTorch, and 18.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=367761)[0;0m ERROR 12-04 13:13:15 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=367761)[0;0m ERROR 12-04 13:13:15 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=367761)[0;0m ERROR 12-04 13:13:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=367761)[0;0m ERROR 12-04 13:13:15 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=367761)[0;0m ERROR 12-04 13:13:15 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367761)[0;0m ERROR 12-04 13:13:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=367761)[0;0m ERROR 12-04 13:13:15 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=367761)[0;0m ERROR 12-04 13:13:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=367761)[0;0m ERROR 12-04 13:13:15 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=367761)[0;0m ERROR 12-04 13:13:15 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367761)[0;0m ERROR 12-04 13:13:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=367761)[0;0m ERROR 12-04 13:13:15 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=367761)[0;0m ERROR 12-04 13:13:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[1;36m(EngineCore_DP0 pid=367761)[0;0m ERROR 12-04 13:13:15 [core.py:842]     self.driver_worker.load_model()
[1;36m(EngineCore_DP0 pid=367761)[0;0m ERROR 12-04 13:13:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 273, in load_model
[1;36m(EngineCore_DP0 pid=367761)[0;0m ERROR 12-04 13:13:15 [core.py:842]     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[1;36m(EngineCore_DP0 pid=367761)[0;0m ERROR 12-04 13:13:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3276, in load_model
[1;36m(EngineCore_DP0 pid=367761)[0;0m ERROR 12-04 13:13:15 [core.py:842]     self.model = model_loader.load_model(
[1;36m(EngineCore_DP0 pid=367761)[0;0m ERROR 12-04 13:13:15 [core.py:842]                  ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367761)[0;0m ERROR 12-04 13:13:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
[1;36m(EngineCore_DP0 pid=367761)[0;0m ERROR 12-04 13:13:15 [core.py:842]     model = initialize_model(
[1;36m(EngineCore_DP0 pid=367761)[0;0m ERROR 12-04 13:13:15 [core.py:842]             ^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367761)[0;0m ERROR 12-04 13:13:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py", line 55, in initialize_model
[1;36m(EngineCore_DP0 pid=367761)[0;0m ERROR 12-04 13:13:15 [core.py:842]     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(EngineCore_DP0 pid=367761)[0;0m ERROR 12-04 13:13:15 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367761)[0;0m ERROR 12-04 13:13:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 279, in __init__
[1;36m(EngineCore_DP0 pid=367761)[0;0m ERROR 12-04 13:13:15 [core.py:842]     self.model = Qwen3Model(
[1;36m(EngineCore_DP0 pid=367761)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=367761)[0;0m ERROR 12-04 13:13:15 [core.py:842]                  ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367761)[0;0m ERROR 12-04 13:13:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=367761)[0;0m ERROR 12-04 13:13:15 [core.py:842]     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=367761)[0;0m ERROR 12-04 13:13:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 253, in __init__
[1;36m(EngineCore_DP0 pid=367761)[0;0m ERROR 12-04 13:13:15 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=367761)[0;0m ERROR 12-04 13:13:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=367761)[0;0m ERROR 12-04 13:13:15 [core.py:842]     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=367761)[0;0m ERROR 12-04 13:13:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 337, in __init__
[1;36m(EngineCore_DP0 pid=367761)[0;0m ERROR 12-04 13:13:15 [core.py:842]     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(EngineCore_DP0 pid=367761)[0;0m ERROR 12-04 13:13:15 [core.py:842]                                                     ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367761)[0;0m ERROR 12-04 13:13:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 651, in make_layers
[1;36m(EngineCore_DP0 pid=367761)[0;0m ERROR 12-04 13:13:15 [core.py:842]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(EngineCore_DP0 pid=367761)[0;0m ERROR 12-04 13:13:15 [core.py:842]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367761)[0;0m ERROR 12-04 13:13:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 339, in <lambda>
[1;36m(EngineCore_DP0 pid=367761)[0;0m ERROR 12-04 13:13:15 [core.py:842]     lambda prefix: decoder_layer_type(
[1;36m(EngineCore_DP0 pid=367761)[0;0m ERROR 12-04 13:13:15 [core.py:842]                    ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367761)[0;0m ERROR 12-04 13:13:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 201, in __init__
[1;36m(EngineCore_DP0 pid=367761)[0;0m ERROR 12-04 13:13:15 [core.py:842]     self.mlp = Qwen3MLP(
[1;36m(EngineCore_DP0 pid=367761)[0;0m ERROR 12-04 13:13:15 [core.py:842]                ^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367761)[0;0m ERROR 12-04 13:13:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 84, in __init__
[1;36m(EngineCore_DP0 pid=367761)[0;0m ERROR 12-04 13:13:15 [core.py:842]     self.gate_up_proj = MergedColumnParallelLinear(
[1;36m(EngineCore_DP0 pid=367761)[0;0m ERROR 12-04 13:13:15 [core.py:842]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367761)[0;0m ERROR 12-04 13:13:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 631, in __init__
[1;36m(EngineCore_DP0 pid=367761)[0;0m ERROR 12-04 13:13:15 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=367761)[0;0m ERROR 12-04 13:13:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 484, in __init__
[1;36m(EngineCore_DP0 pid=367761)[0;0m ERROR 12-04 13:13:15 [core.py:842]     self.quant_method.create_weights(
[1;36m(EngineCore_DP0 pid=367761)[0;0m ERROR 12-04 13:13:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 215, in create_weights
[1;36m(EngineCore_DP0 pid=367761)[0;0m ERROR 12-04 13:13:15 [core.py:842]     data=torch.empty(
[1;36m(EngineCore_DP0 pid=367761)[0;0m ERROR 12-04 13:13:15 [core.py:842]          ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367761)[0;0m ERROR 12-04 13:13:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_device.py", line 103, in __torch_function__
[1;36m(EngineCore_DP0 pid=367761)[0;0m ERROR 12-04 13:13:15 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=367761)[0;0m ERROR 12-04 13:13:15 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367761)[0;0m ERROR 12-04 13:13:15 [core.py:842] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 340.00 MiB. GPU 0 has a total capacity of 44.39 GiB of which 34.00 MiB is free. Process 367770 has 20.02 GiB memory in use. Including non-PyTorch memory, this process has 24.33 GiB memory in use. Of the allocated memory 23.81 GiB is allocated by PyTorch, and 18.46 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=367761)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=367761)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=367761)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=367761)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=367761)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=367761)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=367761)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=367761)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=367761)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=367761)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367761)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=367761)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=367761)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=367761)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=367761)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367761)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=367761)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=367761)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[1;36m(EngineCore_DP0 pid=367761)[0;0m     self.driver_worker.load_model()
[1;36m(EngineCore_DP0 pid=367761)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 273, in load_model
[1;36m(EngineCore_DP0 pid=367761)[0;0m     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[1;36m(EngineCore_DP0 pid=367761)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3276, in load_model
[1;36m(EngineCore_DP0 pid=367761)[0;0m     self.model = model_loader.load_model(
[1;36m(EngineCore_DP0 pid=367761)[0;0m                  ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367761)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
[1;36m(EngineCore_DP0 pid=367761)[0;0m     model = initialize_model(
[1;36m(EngineCore_DP0 pid=367761)[0;0m             ^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367761)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py", line 55, in initialize_model
[1;36m(EngineCore_DP0 pid=367761)[0;0m     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(EngineCore_DP0 pid=367761)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367761)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 279, in __init__
[1;36m(EngineCore_DP0 pid=367761)[0;0m     self.model = Qwen3Model(
[1;36m(EngineCore_DP0 pid=367761)[0;0m                  ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367761)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=367761)[0;0m     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=367761)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 253, in __init__
[1;36m(EngineCore_DP0 pid=367761)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=367761)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=367761)[0;0m     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=367761)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 337, in __init__
[1;36m(EngineCore_DP0 pid=367761)[0;0m     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(EngineCore_DP0 pid=367761)[0;0m                                                     ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367761)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 651, in make_layers
[1;36m(EngineCore_DP0 pid=367761)[0;0m     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(EngineCore_DP0 pid=367761)[0;0m                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367761)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 339, in <lambda>
[1;36m(EngineCore_DP0 pid=367761)[0;0m     lambda prefix: decoder_layer_type(
[1;36m(EngineCore_DP0 pid=367761)[0;0m                    ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367761)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 201, in __init__
[1;36m(EngineCore_DP0 pid=367761)[0;0m     self.mlp = Qwen3MLP(
[1;36m(EngineCore_DP0 pid=367761)[0;0m                ^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367761)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 84, in __init__
[1;36m(EngineCore_DP0 pid=367761)[0;0m     self.gate_up_proj = MergedColumnParallelLinear(
[1;36m(EngineCore_DP0 pid=367761)[0;0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367761)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 631, in __init__
[1;36m(EngineCore_DP0 pid=367761)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=367761)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 484, in __init__
[1;36m(EngineCore_DP0 pid=367761)[0;0m     self.quant_method.create_weights(
[1;36m(EngineCore_DP0 pid=367761)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 215, in create_weights
[1;36m(EngineCore_DP0 pid=367761)[0;0m     data=torch.empty(
[1;36m(EngineCore_DP0 pid=367761)[0;0m          ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367761)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_device.py", line 103, in __torch_function__
[1;36m(EngineCore_DP0 pid=367761)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=367761)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367761)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 340.00 MiB. GPU 0 has a total capacity of 44.39 GiB of which 34.00 MiB is free. Process 367770 has 20.02 GiB memory in use. Including non-PyTorch memory, this process has 24.33 GiB memory in use. Of the allocated memory 23.81 GiB is allocated by PyTorch, and 18.46 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W1204 13:13:16.825386910 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank0]:[W1204 13:13:16.883027246 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: Qwen/Qwen3-14B (backend=vllm)
[ModelCache] Using max_model_len=40960 for Qwen/Qwen3-14B
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 13:13:37 [utils.py:253] non-default args: {'max_model_len': 40960, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'Qwen/Qwen3-14B'}
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: Qwen/Qwen3-14B (backend=vllm)
[ModelCache] Using max_model_len=40960 for Qwen/Qwen3-14B
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 13:13:37 [utils.py:253] non-default args: {'max_model_len': 40960, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'Qwen/Qwen3-14B'}
INFO 12-04 13:13:37 [model.py:631] Resolved architecture: Qwen3ForCausalLM
INFO 12-04 13:13:37 [model.py:1745] Using max model len 40960
INFO 12-04 13:13:37 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
INFO 12-04 13:13:37 [model.py:631] Resolved architecture: Qwen3ForCausalLM
INFO 12-04 13:13:37 [model.py:1745] Using max model len 40960
INFO 12-04 13:13:37 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=368265)[0;0m INFO 12-04 13:13:54 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='Qwen/Qwen3-14B', speculative_config=None, tokenizer='Qwen/Qwen3-14B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-14B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=368268)[0;0m INFO 12-04 13:13:54 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='Qwen/Qwen3-14B', speculative_config=None, tokenizer='Qwen/Qwen3-14B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-14B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=368265)[0;0m INFO 12-04 13:13:57 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.98:54813 backend=nccl
[1;36m(EngineCore_DP0 pid=368268)[0;0m INFO 12-04 13:13:57 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.98:36307 backend=nccl
[W1204 13:13:57.637281422 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-25-0.rc.tch.harvard.edu]:54813 (errno: 97 - Address family not supported by protocol).
[W1204 13:13:57.637257043 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-25-0.rc.tch.harvard.edu]:36307 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=368268)[0;0m INFO 12-04 13:13:57 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=368265)[0;0m INFO 12-04 13:13:57 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=368265)[0;0m INFO 12-04 13:13:57 [gpu_model_runner.py:3259] Starting to load model Qwen/Qwen3-14B...
[1;36m(EngineCore_DP0 pid=368268)[0;0m INFO 12-04 13:13:57 [gpu_model_runner.py:3259] Starting to load model Qwen/Qwen3-14B...
[1;36m(EngineCore_DP0 pid=368268)[0;0m INFO 12-04 13:13:58 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=368268)[0;0m INFO 12-04 13:13:58 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=368265)[0;0m INFO 12-04 13:13:58 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=368265)[0;0m INFO 12-04 13:13:58 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=368268)[0;0m ERROR 12-04 13:13:59 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=368268)[0;0m ERROR 12-04 13:13:59 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=368268)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=368268)[0;0m ERROR 12-04 13:13:59 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=368268)[0;0m ERROR 12-04 13:13:59 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368268)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=368268)[0;0m ERROR 12-04 13:13:59 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=368268)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=368268)[0;0m ERROR 12-04 13:13:59 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=368268)[0;0m ERROR 12-04 13:13:59 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368268)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=368268)[0;0m ERROR 12-04 13:13:59 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=368268)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[1;36m(EngineCore_DP0 pid=368268)[0;0m ERROR 12-04 13:13:59 [core.py:842]     self.driver_worker.load_model()
[1;36m(EngineCore_DP0 pid=368268)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 273, in load_model
[1;36m(EngineCore_DP0 pid=368268)[0;0m ERROR 12-04 13:13:59 [core.py:842]     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[1;36m(EngineCore_DP0 pid=368268)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3276, in load_model
[1;36m(EngineCore_DP0 pid=368268)[0;0m ERROR 12-04 13:13:59 [core.py:842]     self.model = model_loader.load_model(
[1;36m(EngineCore_DP0 pid=368268)[0;0m ERROR 12-04 13:13:59 [core.py:842]                  ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368268)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
[1;36m(EngineCore_DP0 pid=368268)[0;0m ERROR 12-04 13:13:59 [core.py:842]     model = initialize_model(
[1;36m(EngineCore_DP0 pid=368268)[0;0m ERROR 12-04 13:13:59 [core.py:842]             ^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368268)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py", line 55, in initialize_model
[1;36m(EngineCore_DP0 pid=368268)[0;0m ERROR 12-04 13:13:59 [core.py:842]     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(EngineCore_DP0 pid=368268)[0;0m ERROR 12-04 13:13:59 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368268)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 279, in __init__
[1;36m(EngineCore_DP0 pid=368268)[0;0m ERROR 12-04 13:13:59 [core.py:842]     self.model = Qwen3Model(
[1;36m(EngineCore_DP0 pid=368268)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=368268)[0;0m ERROR 12-04 13:13:59 [core.py:842]                  ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368268)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=368268)[0;0m ERROR 12-04 13:13:59 [core.py:842]     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=368268)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 253, in __init__
[1;36m(EngineCore_DP0 pid=368268)[0;0m ERROR 12-04 13:13:59 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=368268)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=368268)[0;0m ERROR 12-04 13:13:59 [core.py:842]     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=368268)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 337, in __init__
[1;36m(EngineCore_DP0 pid=368268)[0;0m ERROR 12-04 13:13:59 [core.py:842]     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(EngineCore_DP0 pid=368268)[0;0m ERROR 12-04 13:13:59 [core.py:842]                                                     ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368268)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 651, in make_layers
[1;36m(EngineCore_DP0 pid=368268)[0;0m ERROR 12-04 13:13:59 [core.py:842]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(EngineCore_DP0 pid=368268)[0;0m ERROR 12-04 13:13:59 [core.py:842]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368268)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 339, in <lambda>
[1;36m(EngineCore_DP0 pid=368268)[0;0m ERROR 12-04 13:13:59 [core.py:842]     lambda prefix: decoder_layer_type(
[1;36m(EngineCore_DP0 pid=368268)[0;0m ERROR 12-04 13:13:59 [core.py:842]                    ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368268)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 201, in __init__
[1;36m(EngineCore_DP0 pid=368268)[0;0m ERROR 12-04 13:13:59 [core.py:842]     self.mlp = Qwen3MLP(
[1;36m(EngineCore_DP0 pid=368268)[0;0m ERROR 12-04 13:13:59 [core.py:842]                ^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368268)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 84, in __init__
[1;36m(EngineCore_DP0 pid=368268)[0;0m ERROR 12-04 13:13:59 [core.py:842]     self.gate_up_proj = MergedColumnParallelLinear(
[1;36m(EngineCore_DP0 pid=368268)[0;0m ERROR 12-04 13:13:59 [core.py:842]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368268)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 631, in __init__
[1;36m(EngineCore_DP0 pid=368268)[0;0m ERROR 12-04 13:13:59 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=368268)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 484, in __init__
[1;36m(EngineCore_DP0 pid=368268)[0;0m ERROR 12-04 13:13:59 [core.py:842]     self.quant_method.create_weights(
[1;36m(EngineCore_DP0 pid=368268)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 215, in create_weights
[1;36m(EngineCore_DP0 pid=368268)[0;0m ERROR 12-04 13:13:59 [core.py:842]     data=torch.empty(
[1;36m(EngineCore_DP0 pid=368268)[0;0m ERROR 12-04 13:13:59 [core.py:842]          ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368268)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_device.py", line 103, in __torch_function__
[1;36m(EngineCore_DP0 pid=368268)[0;0m ERROR 12-04 13:13:59 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=368268)[0;0m ERROR 12-04 13:13:59 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368268)[0;0m ERROR 12-04 13:13:59 [core.py:842] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 340.00 MiB. GPU 0 has a total capacity of 44.39 GiB of which 34.00 MiB is free. Process 368265 has 22.48 GiB memory in use. Including non-PyTorch memory, this process has 21.87 GiB memory in use. Of the allocated memory 21.35 GiB is allocated by PyTorch, and 18.55 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=368268)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=368268)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=368268)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=368268)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=368268)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=368268)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=368268)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=368268)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=368268)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=368268)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368268)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=368268)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=368268)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=368268)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=368268)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368268)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=368268)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=368268)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[1;36m(EngineCore_DP0 pid=368268)[0;0m     self.driver_worker.load_model()
[1;36m(EngineCore_DP0 pid=368268)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 273, in load_model
[1;36m(EngineCore_DP0 pid=368268)[0;0m     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[1;36m(EngineCore_DP0 pid=368268)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3276, in load_model
[1;36m(EngineCore_DP0 pid=368268)[0;0m     self.model = model_loader.load_model(
[1;36m(EngineCore_DP0 pid=368268)[0;0m                  ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368268)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
[1;36m(EngineCore_DP0 pid=368268)[0;0m     model = initialize_model(
[1;36m(EngineCore_DP0 pid=368268)[0;0m             ^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368268)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py", line 55, in initialize_model
[1;36m(EngineCore_DP0 pid=368268)[0;0m     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(EngineCore_DP0 pid=368268)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368268)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 279, in __init__
[1;36m(EngineCore_DP0 pid=368268)[0;0m     self.model = Qwen3Model(
[1;36m(EngineCore_DP0 pid=368268)[0;0m                  ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368268)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=368268)[0;0m     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=368268)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 253, in __init__
[1;36m(EngineCore_DP0 pid=368268)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=368268)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=368268)[0;0m     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=368268)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 337, in __init__
[1;36m(EngineCore_DP0 pid=368268)[0;0m     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(EngineCore_DP0 pid=368268)[0;0m                                                     ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368268)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 651, in make_layers
[1;36m(EngineCore_DP0 pid=368268)[0;0m     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(EngineCore_DP0 pid=368268)[0;0m                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368268)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 339, in <lambda>
[1;36m(EngineCore_DP0 pid=368268)[0;0m     lambda prefix: decoder_layer_type(
[1;36m(EngineCore_DP0 pid=368268)[0;0m                    ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368268)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 201, in __init__
[1;36m(EngineCore_DP0 pid=368268)[0;0m     self.mlp = Qwen3MLP(
[1;36m(EngineCore_DP0 pid=368268)[0;0m                ^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368268)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 84, in __init__
[1;36m(EngineCore_DP0 pid=368268)[0;0m     self.gate_up_proj = MergedColumnParallelLinear(
[1;36m(EngineCore_DP0 pid=368268)[0;0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368268)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 631, in __init__
[1;36m(EngineCore_DP0 pid=368268)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=368268)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 484, in __init__
[1;36m(EngineCore_DP0 pid=368268)[0;0m     self.quant_method.create_weights(
[1;36m(EngineCore_DP0 pid=368268)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 215, in create_weights
[1;36m(EngineCore_DP0 pid=368268)[0;0m     data=torch.empty(
[1;36m(EngineCore_DP0 pid=368268)[0;0m          ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368268)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_device.py", line 103, in __torch_function__
[1;36m(EngineCore_DP0 pid=368268)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=368268)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368268)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 340.00 MiB. GPU 0 has a total capacity of 44.39 GiB of which 34.00 MiB is free. Process 368265 has 22.48 GiB memory in use. Including non-PyTorch memory, this process has 21.87 GiB memory in use. Of the allocated memory 21.35 GiB is allocated by PyTorch, and 18.55 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=368265)[0;0m ERROR 12-04 13:13:59 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=368265)[0;0m ERROR 12-04 13:13:59 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=368265)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=368265)[0;0m ERROR 12-04 13:13:59 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=368265)[0;0m ERROR 12-04 13:13:59 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368265)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=368265)[0;0m ERROR 12-04 13:13:59 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=368265)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=368265)[0;0m ERROR 12-04 13:13:59 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=368265)[0;0m ERROR 12-04 13:13:59 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368265)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=368265)[0;0m ERROR 12-04 13:13:59 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=368265)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[1;36m(EngineCore_DP0 pid=368265)[0;0m ERROR 12-04 13:13:59 [core.py:842]     self.driver_worker.load_model()
[1;36m(EngineCore_DP0 pid=368265)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 273, in load_model
[1;36m(EngineCore_DP0 pid=368265)[0;0m ERROR 12-04 13:13:59 [core.py:842]     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[1;36m(EngineCore_DP0 pid=368265)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3276, in load_model
[1;36m(EngineCore_DP0 pid=368265)[0;0m ERROR 12-04 13:13:59 [core.py:842]     self.model = model_loader.load_model(
[1;36m(EngineCore_DP0 pid=368265)[0;0m ERROR 12-04 13:13:59 [core.py:842]                  ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368265)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
[1;36m(EngineCore_DP0 pid=368265)[0;0m ERROR 12-04 13:13:59 [core.py:842]     model = initialize_model(
[1;36m(EngineCore_DP0 pid=368265)[0;0m ERROR 12-04 13:13:59 [core.py:842]             ^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368265)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py", line 55, in initialize_model
[1;36m(EngineCore_DP0 pid=368265)[0;0m ERROR 12-04 13:13:59 [core.py:842]     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(EngineCore_DP0 pid=368265)[0;0m ERROR 12-04 13:13:59 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368265)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 279, in __init__
[1;36m(EngineCore_DP0 pid=368265)[0;0m ERROR 12-04 13:13:59 [core.py:842]     self.model = Qwen3Model(
[1;36m(EngineCore_DP0 pid=368265)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=368265)[0;0m ERROR 12-04 13:13:59 [core.py:842]                  ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368265)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=368265)[0;0m ERROR 12-04 13:13:59 [core.py:842]     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=368265)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 253, in __init__
[1;36m(EngineCore_DP0 pid=368265)[0;0m ERROR 12-04 13:13:59 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=368265)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=368265)[0;0m ERROR 12-04 13:13:59 [core.py:842]     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=368265)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 337, in __init__
[1;36m(EngineCore_DP0 pid=368265)[0;0m ERROR 12-04 13:13:59 [core.py:842]     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(EngineCore_DP0 pid=368265)[0;0m ERROR 12-04 13:13:59 [core.py:842]                                                     ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368265)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 651, in make_layers
[1;36m(EngineCore_DP0 pid=368265)[0;0m ERROR 12-04 13:13:59 [core.py:842]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(EngineCore_DP0 pid=368265)[0;0m ERROR 12-04 13:13:59 [core.py:842]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368265)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 339, in <lambda>
[1;36m(EngineCore_DP0 pid=368265)[0;0m ERROR 12-04 13:13:59 [core.py:842]     lambda prefix: decoder_layer_type(
[1;36m(EngineCore_DP0 pid=368265)[0;0m ERROR 12-04 13:13:59 [core.py:842]                    ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368265)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 201, in __init__
[1;36m(EngineCore_DP0 pid=368265)[0;0m ERROR 12-04 13:13:59 [core.py:842]     self.mlp = Qwen3MLP(
[1;36m(EngineCore_DP0 pid=368265)[0;0m ERROR 12-04 13:13:59 [core.py:842]                ^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368265)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 84, in __init__
[1;36m(EngineCore_DP0 pid=368265)[0;0m ERROR 12-04 13:13:59 [core.py:842]     self.gate_up_proj = MergedColumnParallelLinear(
[1;36m(EngineCore_DP0 pid=368265)[0;0m ERROR 12-04 13:13:59 [core.py:842]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368265)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 631, in __init__
[1;36m(EngineCore_DP0 pid=368265)[0;0m ERROR 12-04 13:13:59 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=368265)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 484, in __init__
[1;36m(EngineCore_DP0 pid=368265)[0;0m ERROR 12-04 13:13:59 [core.py:842]     self.quant_method.create_weights(
[1;36m(EngineCore_DP0 pid=368265)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 215, in create_weights
[1;36m(EngineCore_DP0 pid=368265)[0;0m ERROR 12-04 13:13:59 [core.py:842]     data=torch.empty(
[1;36m(EngineCore_DP0 pid=368265)[0;0m ERROR 12-04 13:13:59 [core.py:842]          ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368265)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_device.py", line 103, in __torch_function__
[1;36m(EngineCore_DP0 pid=368265)[0;0m ERROR 12-04 13:13:59 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=368265)[0;0m ERROR 12-04 13:13:59 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368265)[0;0m ERROR 12-04 13:13:59 [core.py:842] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 340.00 MiB. GPU 0 has a total capacity of 44.39 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 22.48 GiB memory in use. Process 368268 has 21.87 GiB memory in use. Of the allocated memory 21.96 GiB is allocated by PyTorch, and 18.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=368265)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=368265)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=368265)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=368265)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=368265)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=368265)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=368265)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=368265)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=368265)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=368265)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368265)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=368265)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=368265)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=368265)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=368265)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368265)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=368265)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=368265)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[1;36m(EngineCore_DP0 pid=368265)[0;0m     self.driver_worker.load_model()
[1;36m(EngineCore_DP0 pid=368265)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 273, in load_model
[1;36m(EngineCore_DP0 pid=368265)[0;0m     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[1;36m(EngineCore_DP0 pid=368265)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3276, in load_model
[1;36m(EngineCore_DP0 pid=368265)[0;0m     self.model = model_loader.load_model(
[1;36m(EngineCore_DP0 pid=368265)[0;0m                  ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368265)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
[1;36m(EngineCore_DP0 pid=368265)[0;0m     model = initialize_model(
[1;36m(EngineCore_DP0 pid=368265)[0;0m             ^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368265)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py", line 55, in initialize_model
[1;36m(EngineCore_DP0 pid=368265)[0;0m     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(EngineCore_DP0 pid=368265)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368265)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 279, in __init__
[1;36m(EngineCore_DP0 pid=368265)[0;0m     self.model = Qwen3Model(
[1;36m(EngineCore_DP0 pid=368265)[0;0m                  ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368265)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=368265)[0;0m     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=368265)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 253, in __init__
[1;36m(EngineCore_DP0 pid=368265)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=368265)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=368265)[0;0m     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=368265)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 337, in __init__
[1;36m(EngineCore_DP0 pid=368265)[0;0m     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(EngineCore_DP0 pid=368265)[0;0m                                                     ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368265)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 651, in make_layers
[1;36m(EngineCore_DP0 pid=368265)[0;0m     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(EngineCore_DP0 pid=368265)[0;0m                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368265)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 339, in <lambda>
[1;36m(EngineCore_DP0 pid=368265)[0;0m     lambda prefix: decoder_layer_type(
[1;36m(EngineCore_DP0 pid=368265)[0;0m                    ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368265)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 201, in __init__
[1;36m(EngineCore_DP0 pid=368265)[0;0m     self.mlp = Qwen3MLP(
[1;36m(EngineCore_DP0 pid=368265)[0;0m                ^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368265)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 84, in __init__
[1;36m(EngineCore_DP0 pid=368265)[0;0m     self.gate_up_proj = MergedColumnParallelLinear(
[1;36m(EngineCore_DP0 pid=368265)[0;0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368265)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 631, in __init__
[1;36m(EngineCore_DP0 pid=368265)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=368265)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 484, in __init__
[1;36m(EngineCore_DP0 pid=368265)[0;0m     self.quant_method.create_weights(
[1;36m(EngineCore_DP0 pid=368265)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 215, in create_weights
[1;36m(EngineCore_DP0 pid=368265)[0;0m     data=torch.empty(
[1;36m(EngineCore_DP0 pid=368265)[0;0m          ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368265)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_device.py", line 103, in __torch_function__
[1;36m(EngineCore_DP0 pid=368265)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=368265)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368265)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 340.00 MiB. GPU 0 has a total capacity of 44.39 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 22.48 GiB memory in use. Process 368268 has 21.87 GiB memory in use. Of the allocated memory 21.96 GiB is allocated by PyTorch, and 18.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W1204 13:14:00.133253533 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank0]:[W1204 13:14:00.140047042 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: Qwen/Qwen3-14B (backend=vllm)
[ModelCache] Using max_model_len=40960 for Qwen/Qwen3-14B
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 13:14:21 [utils.py:253] non-default args: {'max_model_len': 40960, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'Qwen/Qwen3-14B'}
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: Qwen/Qwen3-14B (backend=vllm)
[ModelCache] Using max_model_len=40960 for Qwen/Qwen3-14B
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 13:14:21 [utils.py:253] non-default args: {'max_model_len': 40960, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'Qwen/Qwen3-14B'}
INFO 12-04 13:14:22 [model.py:631] Resolved architecture: Qwen3ForCausalLM
INFO 12-04 13:14:22 [model.py:1745] Using max model len 40960
INFO 12-04 13:14:22 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
INFO 12-04 13:14:22 [model.py:631] Resolved architecture: Qwen3ForCausalLM
INFO 12-04 13:14:22 [model.py:1745] Using max model len 40960
INFO 12-04 13:14:22 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=368860)[0;0m INFO 12-04 13:14:41 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='Qwen/Qwen3-14B', speculative_config=None, tokenizer='Qwen/Qwen3-14B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-14B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=368857)[0;0m INFO 12-04 13:14:41 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='Qwen/Qwen3-14B', speculative_config=None, tokenizer='Qwen/Qwen3-14B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-14B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=368860)[0;0m INFO 12-04 13:14:43 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.98:41803 backend=nccl
[1;36m(EngineCore_DP0 pid=368857)[0;0m INFO 12-04 13:14:43 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.98:51273 backend=nccl
[W1204 13:14:43.590211800 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-25-0.rc.tch.harvard.edu]:41803 (errno: 97 - Address family not supported by protocol).
[W1204 13:14:43.590179323 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-25-0.rc.tch.harvard.edu]:51273 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=368857)[0;0m INFO 12-04 13:14:43 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=368860)[0;0m INFO 12-04 13:14:43 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=368857)[0;0m INFO 12-04 13:14:43 [gpu_model_runner.py:3259] Starting to load model Qwen/Qwen3-14B...
[1;36m(EngineCore_DP0 pid=368860)[0;0m INFO 12-04 13:14:43 [gpu_model_runner.py:3259] Starting to load model Qwen/Qwen3-14B...
[1;36m(EngineCore_DP0 pid=368860)[0;0m INFO 12-04 13:14:44 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=368860)[0;0m INFO 12-04 13:14:44 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=368857)[0;0m INFO 12-04 13:14:44 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=368857)[0;0m INFO 12-04 13:14:44 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=368857)[0;0m ERROR 12-04 13:14:45 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=368857)[0;0m ERROR 12-04 13:14:45 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=368857)[0;0m ERROR 12-04 13:14:45 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=368857)[0;0m ERROR 12-04 13:14:45 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=368857)[0;0m ERROR 12-04 13:14:45 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368857)[0;0m ERROR 12-04 13:14:45 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=368857)[0;0m ERROR 12-04 13:14:45 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=368857)[0;0m ERROR 12-04 13:14:45 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=368857)[0;0m ERROR 12-04 13:14:45 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=368857)[0;0m ERROR 12-04 13:14:45 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368857)[0;0m ERROR 12-04 13:14:45 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=368857)[0;0m ERROR 12-04 13:14:45 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=368857)[0;0m ERROR 12-04 13:14:45 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[1;36m(EngineCore_DP0 pid=368857)[0;0m ERROR 12-04 13:14:45 [core.py:842]     self.driver_worker.load_model()
[1;36m(EngineCore_DP0 pid=368857)[0;0m ERROR 12-04 13:14:45 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 273, in load_model
[1;36m(EngineCore_DP0 pid=368857)[0;0m ERROR 12-04 13:14:45 [core.py:842]     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[1;36m(EngineCore_DP0 pid=368857)[0;0m ERROR 12-04 13:14:45 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3276, in load_model
[1;36m(EngineCore_DP0 pid=368857)[0;0m ERROR 12-04 13:14:45 [core.py:842]     self.model = model_loader.load_model(
[1;36m(EngineCore_DP0 pid=368857)[0;0m ERROR 12-04 13:14:45 [core.py:842]                  ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368857)[0;0m ERROR 12-04 13:14:45 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
[1;36m(EngineCore_DP0 pid=368857)[0;0m ERROR 12-04 13:14:45 [core.py:842]     model = initialize_model(
[1;36m(EngineCore_DP0 pid=368857)[0;0m ERROR 12-04 13:14:45 [core.py:842]             ^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368857)[0;0m ERROR 12-04 13:14:45 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py", line 55, in initialize_model
[1;36m(EngineCore_DP0 pid=368857)[0;0m ERROR 12-04 13:14:45 [core.py:842]     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(EngineCore_DP0 pid=368857)[0;0m ERROR 12-04 13:14:45 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368857)[0;0m ERROR 12-04 13:14:45 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 279, in __init__
[1;36m(EngineCore_DP0 pid=368857)[0;0m ERROR 12-04 13:14:45 [core.py:842]     self.model = Qwen3Model(
[1;36m(EngineCore_DP0 pid=368857)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=368857)[0;0m ERROR 12-04 13:14:45 [core.py:842]                  ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368857)[0;0m ERROR 12-04 13:14:45 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=368857)[0;0m ERROR 12-04 13:14:45 [core.py:842]     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=368857)[0;0m ERROR 12-04 13:14:45 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 253, in __init__
[1;36m(EngineCore_DP0 pid=368857)[0;0m ERROR 12-04 13:14:45 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=368857)[0;0m ERROR 12-04 13:14:45 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=368857)[0;0m ERROR 12-04 13:14:45 [core.py:842]     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=368857)[0;0m ERROR 12-04 13:14:45 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 337, in __init__
[1;36m(EngineCore_DP0 pid=368857)[0;0m ERROR 12-04 13:14:45 [core.py:842]     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(EngineCore_DP0 pid=368857)[0;0m ERROR 12-04 13:14:45 [core.py:842]                                                     ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368857)[0;0m ERROR 12-04 13:14:45 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 651, in make_layers
[1;36m(EngineCore_DP0 pid=368857)[0;0m ERROR 12-04 13:14:45 [core.py:842]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(EngineCore_DP0 pid=368857)[0;0m ERROR 12-04 13:14:45 [core.py:842]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368857)[0;0m ERROR 12-04 13:14:45 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 339, in <lambda>
[1;36m(EngineCore_DP0 pid=368857)[0;0m ERROR 12-04 13:14:45 [core.py:842]     lambda prefix: decoder_layer_type(
[1;36m(EngineCore_DP0 pid=368857)[0;0m ERROR 12-04 13:14:45 [core.py:842]                    ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368857)[0;0m ERROR 12-04 13:14:45 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 201, in __init__
[1;36m(EngineCore_DP0 pid=368857)[0;0m ERROR 12-04 13:14:45 [core.py:842]     self.mlp = Qwen3MLP(
[1;36m(EngineCore_DP0 pid=368857)[0;0m ERROR 12-04 13:14:45 [core.py:842]                ^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368857)[0;0m ERROR 12-04 13:14:45 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 84, in __init__
[1;36m(EngineCore_DP0 pid=368857)[0;0m ERROR 12-04 13:14:45 [core.py:842]     self.gate_up_proj = MergedColumnParallelLinear(
[1;36m(EngineCore_DP0 pid=368857)[0;0m ERROR 12-04 13:14:45 [core.py:842]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368857)[0;0m ERROR 12-04 13:14:45 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 631, in __init__
[1;36m(EngineCore_DP0 pid=368857)[0;0m ERROR 12-04 13:14:45 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=368857)[0;0m ERROR 12-04 13:14:45 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 484, in __init__
[1;36m(EngineCore_DP0 pid=368857)[0;0m ERROR 12-04 13:14:45 [core.py:842]     self.quant_method.create_weights(
[1;36m(EngineCore_DP0 pid=368857)[0;0m ERROR 12-04 13:14:45 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 215, in create_weights
[1;36m(EngineCore_DP0 pid=368857)[0;0m ERROR 12-04 13:14:45 [core.py:842]     data=torch.empty(
[1;36m(EngineCore_DP0 pid=368857)[0;0m ERROR 12-04 13:14:45 [core.py:842]          ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368857)[0;0m ERROR 12-04 13:14:45 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_device.py", line 103, in __torch_function__
[1;36m(EngineCore_DP0 pid=368857)[0;0m ERROR 12-04 13:14:45 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=368857)[0;0m ERROR 12-04 13:14:45 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368857)[0;0m ERROR 12-04 13:14:45 [core.py:842] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 340.00 MiB. GPU 0 has a total capacity of 44.39 GiB of which 34.00 MiB is free. Including non-PyTorch memory, this process has 20.63 GiB memory in use. Process 368860 has 23.71 GiB memory in use. Of the allocated memory 20.11 GiB is allocated by PyTorch, and 18.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=368857)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=368857)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=368857)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=368857)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=368857)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=368857)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=368857)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=368857)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=368857)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=368857)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368857)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=368857)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=368857)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=368857)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=368857)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368857)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=368857)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=368857)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[1;36m(EngineCore_DP0 pid=368857)[0;0m     self.driver_worker.load_model()
[1;36m(EngineCore_DP0 pid=368857)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 273, in load_model
[1;36m(EngineCore_DP0 pid=368857)[0;0m     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[1;36m(EngineCore_DP0 pid=368857)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3276, in load_model
[1;36m(EngineCore_DP0 pid=368857)[0;0m     self.model = model_loader.load_model(
[1;36m(EngineCore_DP0 pid=368857)[0;0m                  ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368857)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
[1;36m(EngineCore_DP0 pid=368857)[0;0m     model = initialize_model(
[1;36m(EngineCore_DP0 pid=368857)[0;0m             ^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368857)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py", line 55, in initialize_model
[1;36m(EngineCore_DP0 pid=368857)[0;0m     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(EngineCore_DP0 pid=368857)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368857)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 279, in __init__
[1;36m(EngineCore_DP0 pid=368857)[0;0m     self.model = Qwen3Model(
[1;36m(EngineCore_DP0 pid=368857)[0;0m                  ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368857)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=368857)[0;0m     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=368857)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 253, in __init__
[1;36m(EngineCore_DP0 pid=368857)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=368857)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=368857)[0;0m     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=368857)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 337, in __init__
[1;36m(EngineCore_DP0 pid=368857)[0;0m     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(EngineCore_DP0 pid=368857)[0;0m                                                     ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368857)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 651, in make_layers
[1;36m(EngineCore_DP0 pid=368857)[0;0m     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(EngineCore_DP0 pid=368857)[0;0m                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368857)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 339, in <lambda>
[1;36m(EngineCore_DP0 pid=368857)[0;0m     lambda prefix: decoder_layer_type(
[1;36m(EngineCore_DP0 pid=368857)[0;0m                    ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368857)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 201, in __init__
[1;36m(EngineCore_DP0 pid=368857)[0;0m     self.mlp = Qwen3MLP(
[1;36m(EngineCore_DP0 pid=368857)[0;0m                ^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368857)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 84, in __init__
[1;36m(EngineCore_DP0 pid=368857)[0;0m     self.gate_up_proj = MergedColumnParallelLinear(
[1;36m(EngineCore_DP0 pid=368857)[0;0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368857)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 631, in __init__
[1;36m(EngineCore_DP0 pid=368857)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=368857)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 484, in __init__
[1;36m(EngineCore_DP0 pid=368857)[0;0m     self.quant_method.create_weights(
[1;36m(EngineCore_DP0 pid=368857)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 215, in create_weights
[1;36m(EngineCore_DP0 pid=368857)[0;0m     data=torch.empty(
[1;36m(EngineCore_DP0 pid=368857)[0;0m          ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368857)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_device.py", line 103, in __torch_function__
[1;36m(EngineCore_DP0 pid=368857)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=368857)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368857)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 340.00 MiB. GPU 0 has a total capacity of 44.39 GiB of which 34.00 MiB is free. Including non-PyTorch memory, this process has 20.63 GiB memory in use. Process 368860 has 23.71 GiB memory in use. Of the allocated memory 20.11 GiB is allocated by PyTorch, and 18.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=368860)[0;0m ERROR 12-04 13:14:45 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=368860)[0;0m ERROR 12-04 13:14:45 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=368860)[0;0m ERROR 12-04 13:14:45 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=368860)[0;0m ERROR 12-04 13:14:45 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=368860)[0;0m ERROR 12-04 13:14:45 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368860)[0;0m ERROR 12-04 13:14:45 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=368860)[0;0m ERROR 12-04 13:14:45 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=368860)[0;0m ERROR 12-04 13:14:45 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=368860)[0;0m ERROR 12-04 13:14:45 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=368860)[0;0m ERROR 12-04 13:14:45 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368860)[0;0m ERROR 12-04 13:14:45 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=368860)[0;0m ERROR 12-04 13:14:45 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=368860)[0;0m ERROR 12-04 13:14:45 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[1;36m(EngineCore_DP0 pid=368860)[0;0m ERROR 12-04 13:14:45 [core.py:842]     self.driver_worker.load_model()
[1;36m(EngineCore_DP0 pid=368860)[0;0m ERROR 12-04 13:14:45 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 273, in load_model
[1;36m(EngineCore_DP0 pid=368860)[0;0m ERROR 12-04 13:14:45 [core.py:842]     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[1;36m(EngineCore_DP0 pid=368860)[0;0m ERROR 12-04 13:14:45 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3276, in load_model
[1;36m(EngineCore_DP0 pid=368860)[0;0m ERROR 12-04 13:14:45 [core.py:842]     self.model = model_loader.load_model(
[1;36m(EngineCore_DP0 pid=368860)[0;0m ERROR 12-04 13:14:45 [core.py:842]                  ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368860)[0;0m ERROR 12-04 13:14:45 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
[1;36m(EngineCore_DP0 pid=368860)[0;0m ERROR 12-04 13:14:45 [core.py:842]     model = initialize_model(
[1;36m(EngineCore_DP0 pid=368860)[0;0m ERROR 12-04 13:14:45 [core.py:842]             ^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368860)[0;0m ERROR 12-04 13:14:45 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py", line 55, in initialize_model
[1;36m(EngineCore_DP0 pid=368860)[0;0m ERROR 12-04 13:14:45 [core.py:842]     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(EngineCore_DP0 pid=368860)[0;0m ERROR 12-04 13:14:45 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368860)[0;0m ERROR 12-04 13:14:45 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 279, in __init__
[1;36m(EngineCore_DP0 pid=368860)[0;0m ERROR 12-04 13:14:45 [core.py:842]     self.model = Qwen3Model(
[1;36m(EngineCore_DP0 pid=368860)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=368860)[0;0m ERROR 12-04 13:14:45 [core.py:842]                  ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368860)[0;0m ERROR 12-04 13:14:45 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=368860)[0;0m ERROR 12-04 13:14:45 [core.py:842]     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=368860)[0;0m ERROR 12-04 13:14:45 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 253, in __init__
[1;36m(EngineCore_DP0 pid=368860)[0;0m ERROR 12-04 13:14:45 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=368860)[0;0m ERROR 12-04 13:14:45 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=368860)[0;0m ERROR 12-04 13:14:45 [core.py:842]     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=368860)[0;0m ERROR 12-04 13:14:45 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 337, in __init__
[1;36m(EngineCore_DP0 pid=368860)[0;0m ERROR 12-04 13:14:45 [core.py:842]     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(EngineCore_DP0 pid=368860)[0;0m ERROR 12-04 13:14:45 [core.py:842]                                                     ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368860)[0;0m ERROR 12-04 13:14:45 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 651, in make_layers
[1;36m(EngineCore_DP0 pid=368860)[0;0m ERROR 12-04 13:14:45 [core.py:842]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(EngineCore_DP0 pid=368860)[0;0m ERROR 12-04 13:14:45 [core.py:842]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368860)[0;0m ERROR 12-04 13:14:45 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 339, in <lambda>
[1;36m(EngineCore_DP0 pid=368860)[0;0m ERROR 12-04 13:14:45 [core.py:842]     lambda prefix: decoder_layer_type(
[1;36m(EngineCore_DP0 pid=368860)[0;0m ERROR 12-04 13:14:45 [core.py:842]                    ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368860)[0;0m ERROR 12-04 13:14:45 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 201, in __init__
[1;36m(EngineCore_DP0 pid=368860)[0;0m ERROR 12-04 13:14:45 [core.py:842]     self.mlp = Qwen3MLP(
[1;36m(EngineCore_DP0 pid=368860)[0;0m ERROR 12-04 13:14:45 [core.py:842]                ^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368860)[0;0m ERROR 12-04 13:14:45 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 84, in __init__
[1;36m(EngineCore_DP0 pid=368860)[0;0m ERROR 12-04 13:14:45 [core.py:842]     self.gate_up_proj = MergedColumnParallelLinear(
[1;36m(EngineCore_DP0 pid=368860)[0;0m ERROR 12-04 13:14:45 [core.py:842]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368860)[0;0m ERROR 12-04 13:14:45 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 631, in __init__
[1;36m(EngineCore_DP0 pid=368860)[0;0m ERROR 12-04 13:14:45 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=368860)[0;0m ERROR 12-04 13:14:45 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 484, in __init__
[1;36m(EngineCore_DP0 pid=368860)[0;0m ERROR 12-04 13:14:45 [core.py:842]     self.quant_method.create_weights(
[1;36m(EngineCore_DP0 pid=368860)[0;0m ERROR 12-04 13:14:45 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 215, in create_weights
[1;36m(EngineCore_DP0 pid=368860)[0;0m ERROR 12-04 13:14:45 [core.py:842]     data=torch.empty(
[1;36m(EngineCore_DP0 pid=368860)[0;0m ERROR 12-04 13:14:45 [core.py:842]          ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368860)[0;0m ERROR 12-04 13:14:45 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_device.py", line 103, in __torch_function__
[1;36m(EngineCore_DP0 pid=368860)[0;0m ERROR 12-04 13:14:45 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=368860)[0;0m ERROR 12-04 13:14:45 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368860)[0;0m ERROR 12-04 13:14:45 [core.py:842] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 340.00 MiB. GPU 0 has a total capacity of 44.39 GiB of which 34.00 MiB is free. Process 368857 has 20.63 GiB memory in use. Including non-PyTorch memory, this process has 23.71 GiB memory in use. Of the allocated memory 23.19 GiB is allocated by PyTorch, and 18.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=368860)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=368860)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=368860)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=368860)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=368860)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=368860)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=368860)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=368860)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=368860)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=368860)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368860)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=368860)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=368860)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=368860)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=368860)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368860)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=368860)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=368860)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[1;36m(EngineCore_DP0 pid=368860)[0;0m     self.driver_worker.load_model()
[1;36m(EngineCore_DP0 pid=368860)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 273, in load_model
[1;36m(EngineCore_DP0 pid=368860)[0;0m     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[1;36m(EngineCore_DP0 pid=368860)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3276, in load_model
[1;36m(EngineCore_DP0 pid=368860)[0;0m     self.model = model_loader.load_model(
[1;36m(EngineCore_DP0 pid=368860)[0;0m                  ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368860)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
[1;36m(EngineCore_DP0 pid=368860)[0;0m     model = initialize_model(
[1;36m(EngineCore_DP0 pid=368860)[0;0m             ^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368860)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py", line 55, in initialize_model
[1;36m(EngineCore_DP0 pid=368860)[0;0m     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(EngineCore_DP0 pid=368860)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368860)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 279, in __init__
[1;36m(EngineCore_DP0 pid=368860)[0;0m     self.model = Qwen3Model(
[1;36m(EngineCore_DP0 pid=368860)[0;0m                  ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368860)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=368860)[0;0m     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=368860)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 253, in __init__
[1;36m(EngineCore_DP0 pid=368860)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=368860)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=368860)[0;0m     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=368860)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 337, in __init__
[1;36m(EngineCore_DP0 pid=368860)[0;0m     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(EngineCore_DP0 pid=368860)[0;0m                                                     ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368860)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 651, in make_layers
[1;36m(EngineCore_DP0 pid=368860)[0;0m     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(EngineCore_DP0 pid=368860)[0;0m                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368860)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 339, in <lambda>
[1;36m(EngineCore_DP0 pid=368860)[0;0m     lambda prefix: decoder_layer_type(
[1;36m(EngineCore_DP0 pid=368860)[0;0m                    ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368860)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 201, in __init__
[1;36m(EngineCore_DP0 pid=368860)[0;0m     self.mlp = Qwen3MLP(
[1;36m(EngineCore_DP0 pid=368860)[0;0m                ^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368860)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 84, in __init__
[1;36m(EngineCore_DP0 pid=368860)[0;0m     self.gate_up_proj = MergedColumnParallelLinear(
[1;36m(EngineCore_DP0 pid=368860)[0;0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368860)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 631, in __init__
[1;36m(EngineCore_DP0 pid=368860)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=368860)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 484, in __init__
[1;36m(EngineCore_DP0 pid=368860)[0;0m     self.quant_method.create_weights(
[1;36m(EngineCore_DP0 pid=368860)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 215, in create_weights
[1;36m(EngineCore_DP0 pid=368860)[0;0m     data=torch.empty(
[1;36m(EngineCore_DP0 pid=368860)[0;0m          ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368860)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_device.py", line 103, in __torch_function__
[1;36m(EngineCore_DP0 pid=368860)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=368860)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368860)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 340.00 MiB. GPU 0 has a total capacity of 44.39 GiB of which 34.00 MiB is free. Process 368857 has 20.63 GiB memory in use. Including non-PyTorch memory, this process has 23.71 GiB memory in use. Of the allocated memory 23.19 GiB is allocated by PyTorch, and 18.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W1204 13:14:46.192698915 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank0]:[W1204 13:14:46.223654028 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: Qwen/Qwen3-14B (backend=vllm)
[ModelCache] Using max_model_len=40960 for Qwen/Qwen3-14B
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 13:15:07 [utils.py:253] non-default args: {'max_model_len': 40960, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'Qwen/Qwen3-14B'}
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: Qwen/Qwen3-14B (backend=vllm)
[ModelCache] Using max_model_len=40960 for Qwen/Qwen3-14B
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 13:15:07 [utils.py:253] non-default args: {'max_model_len': 40960, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'Qwen/Qwen3-14B'}
INFO 12-04 13:15:08 [model.py:631] Resolved architecture: Qwen3ForCausalLM
INFO 12-04 13:15:08 [model.py:1745] Using max model len 40960
INFO 12-04 13:15:08 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
INFO 12-04 13:15:08 [model.py:631] Resolved architecture: Qwen3ForCausalLM
INFO 12-04 13:15:08 [model.py:1745] Using max model len 40960
INFO 12-04 13:15:08 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=369376)[0;0m INFO 12-04 13:15:25 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='Qwen/Qwen3-14B', speculative_config=None, tokenizer='Qwen/Qwen3-14B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-14B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=369373)[0;0m INFO 12-04 13:15:25 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='Qwen/Qwen3-14B', speculative_config=None, tokenizer='Qwen/Qwen3-14B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-14B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=369376)[0;0m INFO 12-04 13:15:27 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.98:57477 backend=nccl
[1;36m(EngineCore_DP0 pid=369373)[0;0m INFO 12-04 13:15:27 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.98:46503 backend=nccl
[W1204 13:15:27.852267139 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-25-0.rc.tch.harvard.edu]:57477 (errno: 97 - Address family not supported by protocol).
[W1204 13:15:27.852267194 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-25-0.rc.tch.harvard.edu]:46503 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=369373)[0;0m INFO 12-04 13:15:27 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=369376)[0;0m INFO 12-04 13:15:27 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=369373)[0;0m INFO 12-04 13:15:27 [gpu_model_runner.py:3259] Starting to load model Qwen/Qwen3-14B...
[1;36m(EngineCore_DP0 pid=369376)[0;0m INFO 12-04 13:15:27 [gpu_model_runner.py:3259] Starting to load model Qwen/Qwen3-14B...
[1;36m(EngineCore_DP0 pid=369373)[0;0m INFO 12-04 13:15:28 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=369373)[0;0m INFO 12-04 13:15:28 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=369376)[0;0m INFO 12-04 13:15:29 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=369376)[0;0m INFO 12-04 13:15:29 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=369376)[0;0m ERROR 12-04 13:15:30 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=369376)[0;0m ERROR 12-04 13:15:30 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=369376)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=369376)[0;0m ERROR 12-04 13:15:30 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=369376)[0;0m ERROR 12-04 13:15:30 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369376)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=369376)[0;0m ERROR 12-04 13:15:30 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=369376)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=369376)[0;0m ERROR 12-04 13:15:30 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=369376)[0;0m ERROR 12-04 13:15:30 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369376)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=369376)[0;0m ERROR 12-04 13:15:30 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=369376)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[1;36m(EngineCore_DP0 pid=369376)[0;0m ERROR 12-04 13:15:30 [core.py:842]     self.driver_worker.load_model()
[1;36m(EngineCore_DP0 pid=369376)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 273, in load_model
[1;36m(EngineCore_DP0 pid=369376)[0;0m ERROR 12-04 13:15:30 [core.py:842]     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[1;36m(EngineCore_DP0 pid=369376)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3276, in load_model
[1;36m(EngineCore_DP0 pid=369376)[0;0m ERROR 12-04 13:15:30 [core.py:842]     self.model = model_loader.load_model(
[1;36m(EngineCore_DP0 pid=369376)[0;0m ERROR 12-04 13:15:30 [core.py:842]                  ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369376)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
[1;36m(EngineCore_DP0 pid=369376)[0;0m ERROR 12-04 13:15:30 [core.py:842]     model = initialize_model(
[1;36m(EngineCore_DP0 pid=369376)[0;0m ERROR 12-04 13:15:30 [core.py:842]             ^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369376)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py", line 55, in initialize_model
[1;36m(EngineCore_DP0 pid=369376)[0;0m ERROR 12-04 13:15:30 [core.py:842]     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(EngineCore_DP0 pid=369376)[0;0m ERROR 12-04 13:15:30 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369376)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 279, in __init__
[1;36m(EngineCore_DP0 pid=369376)[0;0m ERROR 12-04 13:15:30 [core.py:842]     self.model = Qwen3Model(
[1;36m(EngineCore_DP0 pid=369376)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=369376)[0;0m ERROR 12-04 13:15:30 [core.py:842]                  ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369376)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=369376)[0;0m ERROR 12-04 13:15:30 [core.py:842]     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=369376)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 253, in __init__
[1;36m(EngineCore_DP0 pid=369376)[0;0m ERROR 12-04 13:15:30 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=369376)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=369376)[0;0m ERROR 12-04 13:15:30 [core.py:842]     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=369376)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 337, in __init__
[1;36m(EngineCore_DP0 pid=369376)[0;0m ERROR 12-04 13:15:30 [core.py:842]     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(EngineCore_DP0 pid=369376)[0;0m ERROR 12-04 13:15:30 [core.py:842]                                                     ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369376)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 651, in make_layers
[1;36m(EngineCore_DP0 pid=369376)[0;0m ERROR 12-04 13:15:30 [core.py:842]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(EngineCore_DP0 pid=369376)[0;0m ERROR 12-04 13:15:30 [core.py:842]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369376)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 339, in <lambda>
[1;36m(EngineCore_DP0 pid=369376)[0;0m ERROR 12-04 13:15:30 [core.py:842]     lambda prefix: decoder_layer_type(
[1;36m(EngineCore_DP0 pid=369376)[0;0m ERROR 12-04 13:15:30 [core.py:842]                    ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369376)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 201, in __init__
[1;36m(EngineCore_DP0 pid=369376)[0;0m ERROR 12-04 13:15:30 [core.py:842]     self.mlp = Qwen3MLP(
[1;36m(EngineCore_DP0 pid=369376)[0;0m ERROR 12-04 13:15:30 [core.py:842]                ^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369376)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 84, in __init__
[1;36m(EngineCore_DP0 pid=369376)[0;0m ERROR 12-04 13:15:30 [core.py:842]     self.gate_up_proj = MergedColumnParallelLinear(
[1;36m(EngineCore_DP0 pid=369376)[0;0m ERROR 12-04 13:15:30 [core.py:842]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369376)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 631, in __init__
[1;36m(EngineCore_DP0 pid=369376)[0;0m ERROR 12-04 13:15:30 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=369376)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 484, in __init__
[1;36m(EngineCore_DP0 pid=369376)[0;0m ERROR 12-04 13:15:30 [core.py:842]     self.quant_method.create_weights(
[1;36m(EngineCore_DP0 pid=369376)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 215, in create_weights
[1;36m(EngineCore_DP0 pid=369376)[0;0m ERROR 12-04 13:15:30 [core.py:842]     data=torch.empty(
[1;36m(EngineCore_DP0 pid=369376)[0;0m ERROR 12-04 13:15:30 [core.py:842]          ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369376)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_device.py", line 103, in __torch_function__
[1;36m(EngineCore_DP0 pid=369376)[0;0m ERROR 12-04 13:15:30 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=369376)[0;0m ERROR 12-04 13:15:30 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369376)[0;0m ERROR 12-04 13:15:30 [core.py:842] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 340.00 MiB. GPU 0 has a total capacity of 44.39 GiB of which 34.00 MiB is free. Including non-PyTorch memory, this process has 20.02 GiB memory in use. Process 369373 has 24.33 GiB memory in use. Of the allocated memory 19.50 GiB is allocated by PyTorch, and 18.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=369376)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=369376)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=369376)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=369376)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=369376)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=369376)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=369376)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=369376)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=369376)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=369376)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369376)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=369376)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=369376)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=369376)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=369376)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369376)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=369376)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=369376)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[1;36m(EngineCore_DP0 pid=369376)[0;0m     self.driver_worker.load_model()
[1;36m(EngineCore_DP0 pid=369376)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 273, in load_model
[1;36m(EngineCore_DP0 pid=369376)[0;0m     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[1;36m(EngineCore_DP0 pid=369376)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3276, in load_model
[1;36m(EngineCore_DP0 pid=369376)[0;0m     self.model = model_loader.load_model(
[1;36m(EngineCore_DP0 pid=369376)[0;0m                  ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369376)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
[1;36m(EngineCore_DP0 pid=369376)[0;0m     model = initialize_model(
[1;36m(EngineCore_DP0 pid=369376)[0;0m             ^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369376)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py", line 55, in initialize_model
[1;36m(EngineCore_DP0 pid=369376)[0;0m     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(EngineCore_DP0 pid=369376)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369376)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 279, in __init__
[1;36m(EngineCore_DP0 pid=369376)[0;0m     self.model = Qwen3Model(
[1;36m(EngineCore_DP0 pid=369376)[0;0m                  ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369376)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=369376)[0;0m     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=369376)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 253, in __init__
[1;36m(EngineCore_DP0 pid=369376)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=369376)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=369376)[0;0m     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=369376)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 337, in __init__
[1;36m(EngineCore_DP0 pid=369376)[0;0m     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(EngineCore_DP0 pid=369376)[0;0m                                                     ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369376)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 651, in make_layers
[1;36m(EngineCore_DP0 pid=369376)[0;0m     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(EngineCore_DP0 pid=369376)[0;0m                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369376)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 339, in <lambda>
[1;36m(EngineCore_DP0 pid=369376)[0;0m     lambda prefix: decoder_layer_type(
[1;36m(EngineCore_DP0 pid=369376)[0;0m                    ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369376)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 201, in __init__
[1;36m(EngineCore_DP0 pid=369376)[0;0m     self.mlp = Qwen3MLP(
[1;36m(EngineCore_DP0 pid=369376)[0;0m                ^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369376)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 84, in __init__
[1;36m(EngineCore_DP0 pid=369376)[0;0m     self.gate_up_proj = MergedColumnParallelLinear(
[1;36m(EngineCore_DP0 pid=369376)[0;0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369376)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 631, in __init__
[1;36m(EngineCore_DP0 pid=369376)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=369376)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 484, in __init__
[1;36m(EngineCore_DP0 pid=369376)[0;0m     self.quant_method.create_weights(
[1;36m(EngineCore_DP0 pid=369376)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 215, in create_weights
[1;36m(EngineCore_DP0 pid=369376)[0;0m     data=torch.empty(
[1;36m(EngineCore_DP0 pid=369376)[0;0m          ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369376)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_device.py", line 103, in __torch_function__
[1;36m(EngineCore_DP0 pid=369376)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=369376)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369376)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 340.00 MiB. GPU 0 has a total capacity of 44.39 GiB of which 34.00 MiB is free. Including non-PyTorch memory, this process has 20.02 GiB memory in use. Process 369373 has 24.33 GiB memory in use. Of the allocated memory 19.50 GiB is allocated by PyTorch, and 18.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=369373)[0;0m ERROR 12-04 13:15:30 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=369373)[0;0m ERROR 12-04 13:15:30 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=369373)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=369373)[0;0m ERROR 12-04 13:15:30 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=369373)[0;0m ERROR 12-04 13:15:30 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369373)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=369373)[0;0m ERROR 12-04 13:15:30 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=369373)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=369373)[0;0m ERROR 12-04 13:15:30 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=369373)[0;0m ERROR 12-04 13:15:30 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369373)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=369373)[0;0m ERROR 12-04 13:15:30 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=369373)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[1;36m(EngineCore_DP0 pid=369373)[0;0m ERROR 12-04 13:15:30 [core.py:842]     self.driver_worker.load_model()
[1;36m(EngineCore_DP0 pid=369373)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 273, in load_model
[1;36m(EngineCore_DP0 pid=369373)[0;0m ERROR 12-04 13:15:30 [core.py:842]     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[1;36m(EngineCore_DP0 pid=369373)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3276, in load_model
[1;36m(EngineCore_DP0 pid=369373)[0;0m ERROR 12-04 13:15:30 [core.py:842]     self.model = model_loader.load_model(
[1;36m(EngineCore_DP0 pid=369373)[0;0m ERROR 12-04 13:15:30 [core.py:842]                  ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369373)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
[1;36m(EngineCore_DP0 pid=369373)[0;0m ERROR 12-04 13:15:30 [core.py:842]     model = initialize_model(
[1;36m(EngineCore_DP0 pid=369373)[0;0m ERROR 12-04 13:15:30 [core.py:842]             ^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369373)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py", line 55, in initialize_model
[1;36m(EngineCore_DP0 pid=369373)[0;0m ERROR 12-04 13:15:30 [core.py:842]     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(EngineCore_DP0 pid=369373)[0;0m ERROR 12-04 13:15:30 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369373)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 279, in __init__
[1;36m(EngineCore_DP0 pid=369373)[0;0m ERROR 12-04 13:15:30 [core.py:842]     self.model = Qwen3Model(
[1;36m(EngineCore_DP0 pid=369373)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=369373)[0;0m ERROR 12-04 13:15:30 [core.py:842]                  ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369373)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=369373)[0;0m ERROR 12-04 13:15:30 [core.py:842]     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=369373)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 253, in __init__
[1;36m(EngineCore_DP0 pid=369373)[0;0m ERROR 12-04 13:15:30 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=369373)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=369373)[0;0m ERROR 12-04 13:15:30 [core.py:842]     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=369373)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 337, in __init__
[1;36m(EngineCore_DP0 pid=369373)[0;0m ERROR 12-04 13:15:30 [core.py:842]     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(EngineCore_DP0 pid=369373)[0;0m ERROR 12-04 13:15:30 [core.py:842]                                                     ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369373)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 651, in make_layers
[1;36m(EngineCore_DP0 pid=369373)[0;0m ERROR 12-04 13:15:30 [core.py:842]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(EngineCore_DP0 pid=369373)[0;0m ERROR 12-04 13:15:30 [core.py:842]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369373)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 339, in <lambda>
[1;36m(EngineCore_DP0 pid=369373)[0;0m ERROR 12-04 13:15:30 [core.py:842]     lambda prefix: decoder_layer_type(
[1;36m(EngineCore_DP0 pid=369373)[0;0m ERROR 12-04 13:15:30 [core.py:842]                    ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369373)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 201, in __init__
[1;36m(EngineCore_DP0 pid=369373)[0;0m ERROR 12-04 13:15:30 [core.py:842]     self.mlp = Qwen3MLP(
[1;36m(EngineCore_DP0 pid=369373)[0;0m ERROR 12-04 13:15:30 [core.py:842]                ^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369373)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 84, in __init__
[1;36m(EngineCore_DP0 pid=369373)[0;0m ERROR 12-04 13:15:30 [core.py:842]     self.gate_up_proj = MergedColumnParallelLinear(
[1;36m(EngineCore_DP0 pid=369373)[0;0m ERROR 12-04 13:15:30 [core.py:842]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369373)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 631, in __init__
[1;36m(EngineCore_DP0 pid=369373)[0;0m ERROR 12-04 13:15:30 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=369373)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 484, in __init__
[1;36m(EngineCore_DP0 pid=369373)[0;0m ERROR 12-04 13:15:30 [core.py:842]     self.quant_method.create_weights(
[1;36m(EngineCore_DP0 pid=369373)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 215, in create_weights
[1;36m(EngineCore_DP0 pid=369373)[0;0m ERROR 12-04 13:15:30 [core.py:842]     data=torch.empty(
[1;36m(EngineCore_DP0 pid=369373)[0;0m ERROR 12-04 13:15:30 [core.py:842]          ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369373)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_device.py", line 103, in __torch_function__
[1;36m(EngineCore_DP0 pid=369373)[0;0m ERROR 12-04 13:15:30 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=369373)[0;0m ERROR 12-04 13:15:30 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369373)[0;0m ERROR 12-04 13:15:30 [core.py:842] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 340.00 MiB. GPU 0 has a total capacity of 44.39 GiB of which 4.00 MiB is free. Process 369376 has 20.02 GiB memory in use. Including non-PyTorch memory, this process has 24.33 GiB memory in use. Of the allocated memory 23.81 GiB is allocated by PyTorch, and 18.46 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=369373)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=369373)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=369373)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=369373)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=369373)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=369373)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=369373)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=369373)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=369373)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=369373)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369373)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=369373)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=369373)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=369373)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=369373)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369373)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=369373)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=369373)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[1;36m(EngineCore_DP0 pid=369373)[0;0m     self.driver_worker.load_model()
[1;36m(EngineCore_DP0 pid=369373)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 273, in load_model
[1;36m(EngineCore_DP0 pid=369373)[0;0m     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[1;36m(EngineCore_DP0 pid=369373)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3276, in load_model
[1;36m(EngineCore_DP0 pid=369373)[0;0m     self.model = model_loader.load_model(
[1;36m(EngineCore_DP0 pid=369373)[0;0m                  ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369373)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
[1;36m(EngineCore_DP0 pid=369373)[0;0m     model = initialize_model(
[1;36m(EngineCore_DP0 pid=369373)[0;0m             ^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369373)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py", line 55, in initialize_model
[1;36m(EngineCore_DP0 pid=369373)[0;0m     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(EngineCore_DP0 pid=369373)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369373)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 279, in __init__
[1;36m(EngineCore_DP0 pid=369373)[0;0m     self.model = Qwen3Model(
[1;36m(EngineCore_DP0 pid=369373)[0;0m                  ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369373)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=369373)[0;0m     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=369373)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 253, in __init__
[1;36m(EngineCore_DP0 pid=369373)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=369373)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=369373)[0;0m     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=369373)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 337, in __init__
[1;36m(EngineCore_DP0 pid=369373)[0;0m     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(EngineCore_DP0 pid=369373)[0;0m                                                     ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369373)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 651, in make_layers
[1;36m(EngineCore_DP0 pid=369373)[0;0m     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(EngineCore_DP0 pid=369373)[0;0m                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369373)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 339, in <lambda>
[1;36m(EngineCore_DP0 pid=369373)[0;0m     lambda prefix: decoder_layer_type(
[1;36m(EngineCore_DP0 pid=369373)[0;0m                    ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369373)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 201, in __init__
[1;36m(EngineCore_DP0 pid=369373)[0;0m     self.mlp = Qwen3MLP(
[1;36m(EngineCore_DP0 pid=369373)[0;0m                ^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369373)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 84, in __init__
[1;36m(EngineCore_DP0 pid=369373)[0;0m     self.gate_up_proj = MergedColumnParallelLinear(
[1;36m(EngineCore_DP0 pid=369373)[0;0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369373)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 631, in __init__
[1;36m(EngineCore_DP0 pid=369373)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=369373)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 484, in __init__
[1;36m(EngineCore_DP0 pid=369373)[0;0m     self.quant_method.create_weights(
[1;36m(EngineCore_DP0 pid=369373)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 215, in create_weights
[1;36m(EngineCore_DP0 pid=369373)[0;0m     data=torch.empty(
[1;36m(EngineCore_DP0 pid=369373)[0;0m          ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369373)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_device.py", line 103, in __torch_function__
[1;36m(EngineCore_DP0 pid=369373)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=369373)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369373)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 340.00 MiB. GPU 0 has a total capacity of 44.39 GiB of which 4.00 MiB is free. Process 369376 has 20.02 GiB memory in use. Including non-PyTorch memory, this process has 24.33 GiB memory in use. Of the allocated memory 23.81 GiB is allocated by PyTorch, and 18.46 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W1204 13:15:30.279188025 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank0]:[W1204 13:15:30.318810987 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: Qwen/Qwen3-14B (backend=vllm)
[ModelCache] Using max_model_len=40960 for Qwen/Qwen3-14B
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 13:15:51 [utils.py:253] non-default args: {'max_model_len': 40960, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'Qwen/Qwen3-14B'}
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: Qwen/Qwen3-14B (backend=vllm)
[ModelCache] Using max_model_len=40960 for Qwen/Qwen3-14B
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 13:15:52 [utils.py:253] non-default args: {'max_model_len': 40960, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'Qwen/Qwen3-14B'}
INFO 12-04 13:15:52 [model.py:631] Resolved architecture: Qwen3ForCausalLM
INFO 12-04 13:15:52 [model.py:1745] Using max model len 40960
INFO 12-04 13:15:52 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
INFO 12-04 13:15:52 [model.py:631] Resolved architecture: Qwen3ForCausalLM
INFO 12-04 13:15:52 [model.py:1745] Using max model len 40960
INFO 12-04 13:15:52 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=369880)[0;0m INFO 12-04 13:16:11 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='Qwen/Qwen3-14B', speculative_config=None, tokenizer='Qwen/Qwen3-14B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-14B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=369890)[0;0m INFO 12-04 13:16:11 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='Qwen/Qwen3-14B', speculative_config=None, tokenizer='Qwen/Qwen3-14B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-14B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=369890)[0;0m INFO 12-04 13:16:12 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.98:58555 backend=nccl
[W1204 13:16:12.500048927 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-25-0.rc.tch.harvard.edu]:58555 (errno: 97 - Address family not supported by protocol).
[1;36m(EngineCore_DP0 pid=369880)[0;0m INFO 12-04 13:16:13 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.98:41019 backend=nccl
[W1204 13:16:13.529903250 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-25-0.rc.tch.harvard.edu]:41019 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=369890)[0;0m INFO 12-04 13:16:13 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=369880)[0;0m INFO 12-04 13:16:13 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=369880)[0;0m INFO 12-04 13:16:13 [gpu_model_runner.py:3259] Starting to load model Qwen/Qwen3-14B...
[1;36m(EngineCore_DP0 pid=369890)[0;0m INFO 12-04 13:16:13 [gpu_model_runner.py:3259] Starting to load model Qwen/Qwen3-14B...
[1;36m(EngineCore_DP0 pid=369890)[0;0m INFO 12-04 13:16:14 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=369890)[0;0m INFO 12-04 13:16:14 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=369880)[0;0m INFO 12-04 13:16:14 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=369880)[0;0m INFO 12-04 13:16:14 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=369880)[0;0m ERROR 12-04 13:16:15 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=369880)[0;0m ERROR 12-04 13:16:15 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=369880)[0;0m ERROR 12-04 13:16:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=369880)[0;0m ERROR 12-04 13:16:15 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=369880)[0;0m ERROR 12-04 13:16:15 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369880)[0;0m ERROR 12-04 13:16:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=369880)[0;0m ERROR 12-04 13:16:15 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=369880)[0;0m ERROR 12-04 13:16:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=369880)[0;0m ERROR 12-04 13:16:15 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=369880)[0;0m ERROR 12-04 13:16:15 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369880)[0;0m ERROR 12-04 13:16:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=369880)[0;0m ERROR 12-04 13:16:15 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=369880)[0;0m ERROR 12-04 13:16:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[1;36m(EngineCore_DP0 pid=369880)[0;0m ERROR 12-04 13:16:15 [core.py:842]     self.driver_worker.load_model()
[1;36m(EngineCore_DP0 pid=369880)[0;0m ERROR 12-04 13:16:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 273, in load_model
[1;36m(EngineCore_DP0 pid=369880)[0;0m ERROR 12-04 13:16:15 [core.py:842]     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[1;36m(EngineCore_DP0 pid=369880)[0;0m ERROR 12-04 13:16:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3276, in load_model
[1;36m(EngineCore_DP0 pid=369880)[0;0m ERROR 12-04 13:16:15 [core.py:842]     self.model = model_loader.load_model(
[1;36m(EngineCore_DP0 pid=369880)[0;0m ERROR 12-04 13:16:15 [core.py:842]                  ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369880)[0;0m ERROR 12-04 13:16:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
[1;36m(EngineCore_DP0 pid=369880)[0;0m ERROR 12-04 13:16:15 [core.py:842]     model = initialize_model(
[1;36m(EngineCore_DP0 pid=369880)[0;0m ERROR 12-04 13:16:15 [core.py:842]             ^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369880)[0;0m ERROR 12-04 13:16:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py", line 55, in initialize_model
[1;36m(EngineCore_DP0 pid=369880)[0;0m ERROR 12-04 13:16:15 [core.py:842]     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(EngineCore_DP0 pid=369880)[0;0m ERROR 12-04 13:16:15 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369880)[0;0m ERROR 12-04 13:16:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 279, in __init__
[1;36m(EngineCore_DP0 pid=369880)[0;0m ERROR 12-04 13:16:15 [core.py:842]     self.model = Qwen3Model(
[1;36m(EngineCore_DP0 pid=369880)[0;0m ERROR 12-04 13:16:15 [core.py:842]                  ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369880)[0;0m ERROR 12-04 13:16:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=369880)[0;0m ERROR 12-04 13:16:15 [core.py:842]     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=369880)[0;0m ERROR 12-04 13:16:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 253, in __init__
[1;36m(EngineCore_DP0 pid=369880)[0;0m ERROR 12-04 13:16:15 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=369880)[0;0m ERROR 12-04 13:16:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=369880)[0;0m ERROR 12-04 13:16:15 [core.py:842]     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=369880)[0;0m ERROR 12-04 13:16:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 337, in __init__
[1;36m(EngineCore_DP0 pid=369880)[0;0m ERROR 12-04 13:16:15 [core.py:842]     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(EngineCore_DP0 pid=369880)[0;0m ERROR 12-04 13:16:15 [core.py:842]                                                     ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369880)[0;0m ERROR 12-04 13:16:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 651, in make_layers
[1;36m(EngineCore_DP0 pid=369880)[0;0m ERROR 12-04 13:16:15 [core.py:842]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(EngineCore_DP0 pid=369880)[0;0m ERROR 12-04 13:16:15 [core.py:842]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369880)[0;0m ERROR 12-04 13:16:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 339, in <lambda>
[1;36m(EngineCore_DP0 pid=369880)[0;0m ERROR 12-04 13:16:15 [core.py:842]     lambda prefix: decoder_layer_type(
[1;36m(EngineCore_DP0 pid=369880)[0;0m ERROR 12-04 13:16:15 [core.py:842]                    ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369880)[0;0m ERROR 12-04 13:16:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 201, in __init__
[1;36m(EngineCore_DP0 pid=369880)[0;0m ERROR 12-04 13:16:15 [core.py:842]     self.mlp = Qwen3MLP(
[1;36m(EngineCore_DP0 pid=369880)[0;0m ERROR 12-04 13:16:15 [core.py:842]                ^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369880)[0;0m ERROR 12-04 13:16:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 84, in __init__
[1;36m(EngineCore_DP0 pid=369880)[0;0m ERROR 12-04 13:16:15 [core.py:842]     self.gate_up_proj = MergedColumnParallelLinear(
[1;36m(EngineCore_DP0 pid=369880)[0;0m ERROR 12-04 13:16:15 [core.py:842]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369880)[0;0m ERROR 12-04 13:16:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 631, in __init__
[1;36m(EngineCore_DP0 pid=369880)[0;0m ERROR 12-04 13:16:15 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=369880)[0;0m ERROR 12-04 13:16:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 484, in __init__
[1;36m(EngineCore_DP0 pid=369880)[0;0m ERROR 12-04 13:16:15 [core.py:842]     self.quant_method.create_weights(
[1;36m(EngineCore_DP0 pid=369880)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=369880)[0;0m ERROR 12-04 13:16:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 215, in create_weights
[1;36m(EngineCore_DP0 pid=369880)[0;0m ERROR 12-04 13:16:15 [core.py:842]     data=torch.empty(
[1;36m(EngineCore_DP0 pid=369880)[0;0m ERROR 12-04 13:16:15 [core.py:842]          ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369880)[0;0m ERROR 12-04 13:16:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_device.py", line 103, in __torch_function__
[1;36m(EngineCore_DP0 pid=369880)[0;0m ERROR 12-04 13:16:15 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=369880)[0;0m ERROR 12-04 13:16:15 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369880)[0;0m ERROR 12-04 13:16:15 [core.py:842] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 340.00 MiB. GPU 0 has a total capacity of 44.39 GiB of which 34.00 MiB is free. Process 369890 has 24.33 GiB memory in use. Including non-PyTorch memory, this process has 20.02 GiB memory in use. Of the allocated memory 19.50 GiB is allocated by PyTorch, and 18.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=369880)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=369880)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=369880)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=369880)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=369880)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=369880)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=369880)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=369880)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=369880)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=369880)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369880)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=369880)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=369880)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=369880)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=369880)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369880)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=369880)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=369880)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[1;36m(EngineCore_DP0 pid=369880)[0;0m     self.driver_worker.load_model()
[1;36m(EngineCore_DP0 pid=369880)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 273, in load_model
[1;36m(EngineCore_DP0 pid=369880)[0;0m     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[1;36m(EngineCore_DP0 pid=369880)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3276, in load_model
[1;36m(EngineCore_DP0 pid=369880)[0;0m     self.model = model_loader.load_model(
[1;36m(EngineCore_DP0 pid=369880)[0;0m                  ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369880)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
[1;36m(EngineCore_DP0 pid=369880)[0;0m     model = initialize_model(
[1;36m(EngineCore_DP0 pid=369880)[0;0m             ^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369880)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py", line 55, in initialize_model
[1;36m(EngineCore_DP0 pid=369880)[0;0m     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(EngineCore_DP0 pid=369880)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369880)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 279, in __init__
[1;36m(EngineCore_DP0 pid=369880)[0;0m     self.model = Qwen3Model(
[1;36m(EngineCore_DP0 pid=369880)[0;0m                  ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369880)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=369880)[0;0m     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=369880)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 253, in __init__
[1;36m(EngineCore_DP0 pid=369880)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=369880)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=369880)[0;0m     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=369880)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 337, in __init__
[1;36m(EngineCore_DP0 pid=369880)[0;0m     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(EngineCore_DP0 pid=369880)[0;0m                                                     ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369880)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 651, in make_layers
[1;36m(EngineCore_DP0 pid=369880)[0;0m     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(EngineCore_DP0 pid=369880)[0;0m                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369880)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 339, in <lambda>
[1;36m(EngineCore_DP0 pid=369880)[0;0m     lambda prefix: decoder_layer_type(
[1;36m(EngineCore_DP0 pid=369880)[0;0m                    ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369880)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 201, in __init__
[1;36m(EngineCore_DP0 pid=369880)[0;0m     self.mlp = Qwen3MLP(
[1;36m(EngineCore_DP0 pid=369880)[0;0m                ^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369880)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 84, in __init__
[1;36m(EngineCore_DP0 pid=369880)[0;0m     self.gate_up_proj = MergedColumnParallelLinear(
[1;36m(EngineCore_DP0 pid=369880)[0;0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369880)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 631, in __init__
[1;36m(EngineCore_DP0 pid=369880)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=369880)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 484, in __init__
[1;36m(EngineCore_DP0 pid=369880)[0;0m     self.quant_method.create_weights(
[1;36m(EngineCore_DP0 pid=369880)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 215, in create_weights
[1;36m(EngineCore_DP0 pid=369880)[0;0m     data=torch.empty(
[1;36m(EngineCore_DP0 pid=369880)[0;0m          ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369880)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_device.py", line 103, in __torch_function__
[1;36m(EngineCore_DP0 pid=369880)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=369880)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369880)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 340.00 MiB. GPU 0 has a total capacity of 44.39 GiB of which 34.00 MiB is free. Process 369890 has 24.33 GiB memory in use. Including non-PyTorch memory, this process has 20.02 GiB memory in use. Of the allocated memory 19.50 GiB is allocated by PyTorch, and 18.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=369890)[0;0m ERROR 12-04 13:16:15 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=369890)[0;0m ERROR 12-04 13:16:15 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=369890)[0;0m ERROR 12-04 13:16:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=369890)[0;0m ERROR 12-04 13:16:15 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=369890)[0;0m ERROR 12-04 13:16:15 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369890)[0;0m ERROR 12-04 13:16:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=369890)[0;0m ERROR 12-04 13:16:15 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=369890)[0;0m ERROR 12-04 13:16:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=369890)[0;0m ERROR 12-04 13:16:15 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=369890)[0;0m ERROR 12-04 13:16:15 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369890)[0;0m ERROR 12-04 13:16:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=369890)[0;0m ERROR 12-04 13:16:15 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=369890)[0;0m ERROR 12-04 13:16:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[1;36m(EngineCore_DP0 pid=369890)[0;0m ERROR 12-04 13:16:15 [core.py:842]     self.driver_worker.load_model()
[1;36m(EngineCore_DP0 pid=369890)[0;0m ERROR 12-04 13:16:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 273, in load_model
[1;36m(EngineCore_DP0 pid=369890)[0;0m ERROR 12-04 13:16:15 [core.py:842]     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[1;36m(EngineCore_DP0 pid=369890)[0;0m ERROR 12-04 13:16:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3276, in load_model
[1;36m(EngineCore_DP0 pid=369890)[0;0m ERROR 12-04 13:16:15 [core.py:842]     self.model = model_loader.load_model(
[1;36m(EngineCore_DP0 pid=369890)[0;0m ERROR 12-04 13:16:15 [core.py:842]                  ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369890)[0;0m ERROR 12-04 13:16:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
[1;36m(EngineCore_DP0 pid=369890)[0;0m ERROR 12-04 13:16:15 [core.py:842]     model = initialize_model(
[1;36m(EngineCore_DP0 pid=369890)[0;0m ERROR 12-04 13:16:15 [core.py:842]             ^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369890)[0;0m ERROR 12-04 13:16:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py", line 55, in initialize_model
[1;36m(EngineCore_DP0 pid=369890)[0;0m ERROR 12-04 13:16:15 [core.py:842]     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(EngineCore_DP0 pid=369890)[0;0m ERROR 12-04 13:16:15 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369890)[0;0m ERROR 12-04 13:16:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 279, in __init__
[1;36m(EngineCore_DP0 pid=369890)[0;0m ERROR 12-04 13:16:15 [core.py:842]     self.model = Qwen3Model(
[1;36m(EngineCore_DP0 pid=369890)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=369890)[0;0m ERROR 12-04 13:16:15 [core.py:842]                  ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369890)[0;0m ERROR 12-04 13:16:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=369890)[0;0m ERROR 12-04 13:16:15 [core.py:842]     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=369890)[0;0m ERROR 12-04 13:16:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 253, in __init__
[1;36m(EngineCore_DP0 pid=369890)[0;0m ERROR 12-04 13:16:15 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=369890)[0;0m ERROR 12-04 13:16:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=369890)[0;0m ERROR 12-04 13:16:15 [core.py:842]     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=369890)[0;0m ERROR 12-04 13:16:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 337, in __init__
[1;36m(EngineCore_DP0 pid=369890)[0;0m ERROR 12-04 13:16:15 [core.py:842]     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(EngineCore_DP0 pid=369890)[0;0m ERROR 12-04 13:16:15 [core.py:842]                                                     ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369890)[0;0m ERROR 12-04 13:16:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 651, in make_layers
[1;36m(EngineCore_DP0 pid=369890)[0;0m ERROR 12-04 13:16:15 [core.py:842]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(EngineCore_DP0 pid=369890)[0;0m ERROR 12-04 13:16:15 [core.py:842]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369890)[0;0m ERROR 12-04 13:16:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 339, in <lambda>
[1;36m(EngineCore_DP0 pid=369890)[0;0m ERROR 12-04 13:16:15 [core.py:842]     lambda prefix: decoder_layer_type(
[1;36m(EngineCore_DP0 pid=369890)[0;0m ERROR 12-04 13:16:15 [core.py:842]                    ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369890)[0;0m ERROR 12-04 13:16:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 201, in __init__
[1;36m(EngineCore_DP0 pid=369890)[0;0m ERROR 12-04 13:16:15 [core.py:842]     self.mlp = Qwen3MLP(
[1;36m(EngineCore_DP0 pid=369890)[0;0m ERROR 12-04 13:16:15 [core.py:842]                ^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369890)[0;0m ERROR 12-04 13:16:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 84, in __init__
[1;36m(EngineCore_DP0 pid=369890)[0;0m ERROR 12-04 13:16:15 [core.py:842]     self.gate_up_proj = MergedColumnParallelLinear(
[1;36m(EngineCore_DP0 pid=369890)[0;0m ERROR 12-04 13:16:15 [core.py:842]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369890)[0;0m ERROR 12-04 13:16:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 631, in __init__
[1;36m(EngineCore_DP0 pid=369890)[0;0m ERROR 12-04 13:16:15 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=369890)[0;0m ERROR 12-04 13:16:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 484, in __init__
[1;36m(EngineCore_DP0 pid=369890)[0;0m ERROR 12-04 13:16:15 [core.py:842]     self.quant_method.create_weights(
[1;36m(EngineCore_DP0 pid=369890)[0;0m ERROR 12-04 13:16:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 215, in create_weights
[1;36m(EngineCore_DP0 pid=369890)[0;0m ERROR 12-04 13:16:15 [core.py:842]     data=torch.empty(
[1;36m(EngineCore_DP0 pid=369890)[0;0m ERROR 12-04 13:16:15 [core.py:842]          ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369890)[0;0m ERROR 12-04 13:16:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_device.py", line 103, in __torch_function__
[1;36m(EngineCore_DP0 pid=369890)[0;0m ERROR 12-04 13:16:15 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=369890)[0;0m ERROR 12-04 13:16:15 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369890)[0;0m ERROR 12-04 13:16:15 [core.py:842] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 340.00 MiB. GPU 0 has a total capacity of 44.39 GiB of which 34.00 MiB is free. Including non-PyTorch memory, this process has 24.33 GiB memory in use. Process 369880 has 20.02 GiB memory in use. Of the allocated memory 23.81 GiB is allocated by PyTorch, and 18.46 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=369890)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=369890)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=369890)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=369890)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=369890)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=369890)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=369890)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=369890)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=369890)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=369890)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369890)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=369890)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=369890)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=369890)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=369890)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369890)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=369890)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=369890)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[1;36m(EngineCore_DP0 pid=369890)[0;0m     self.driver_worker.load_model()
[1;36m(EngineCore_DP0 pid=369890)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 273, in load_model
[1;36m(EngineCore_DP0 pid=369890)[0;0m     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[1;36m(EngineCore_DP0 pid=369890)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3276, in load_model
[1;36m(EngineCore_DP0 pid=369890)[0;0m     self.model = model_loader.load_model(
[1;36m(EngineCore_DP0 pid=369890)[0;0m                  ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369890)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
[1;36m(EngineCore_DP0 pid=369890)[0;0m     model = initialize_model(
[1;36m(EngineCore_DP0 pid=369890)[0;0m             ^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369890)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py", line 55, in initialize_model
[1;36m(EngineCore_DP0 pid=369890)[0;0m     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(EngineCore_DP0 pid=369890)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369890)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 279, in __init__
[1;36m(EngineCore_DP0 pid=369890)[0;0m     self.model = Qwen3Model(
[1;36m(EngineCore_DP0 pid=369890)[0;0m                  ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369890)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=369890)[0;0m     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=369890)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 253, in __init__
[1;36m(EngineCore_DP0 pid=369890)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=369890)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=369890)[0;0m     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=369890)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 337, in __init__
[1;36m(EngineCore_DP0 pid=369890)[0;0m     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(EngineCore_DP0 pid=369890)[0;0m                                                     ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369890)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 651, in make_layers
[1;36m(EngineCore_DP0 pid=369890)[0;0m     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(EngineCore_DP0 pid=369890)[0;0m                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369890)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 339, in <lambda>
[1;36m(EngineCore_DP0 pid=369890)[0;0m     lambda prefix: decoder_layer_type(
[1;36m(EngineCore_DP0 pid=369890)[0;0m                    ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369890)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 201, in __init__
[1;36m(EngineCore_DP0 pid=369890)[0;0m     self.mlp = Qwen3MLP(
[1;36m(EngineCore_DP0 pid=369890)[0;0m                ^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369890)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 84, in __init__
[1;36m(EngineCore_DP0 pid=369890)[0;0m     self.gate_up_proj = MergedColumnParallelLinear(
[1;36m(EngineCore_DP0 pid=369890)[0;0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369890)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 631, in __init__
[1;36m(EngineCore_DP0 pid=369890)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=369890)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 484, in __init__
[1;36m(EngineCore_DP0 pid=369890)[0;0m     self.quant_method.create_weights(
[1;36m(EngineCore_DP0 pid=369890)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 215, in create_weights
[1;36m(EngineCore_DP0 pid=369890)[0;0m     data=torch.empty(
[1;36m(EngineCore_DP0 pid=369890)[0;0m          ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369890)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_device.py", line 103, in __torch_function__
[1;36m(EngineCore_DP0 pid=369890)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=369890)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369890)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 340.00 MiB. GPU 0 has a total capacity of 44.39 GiB of which 34.00 MiB is free. Including non-PyTorch memory, this process has 24.33 GiB memory in use. Process 369880 has 20.02 GiB memory in use. Of the allocated memory 23.81 GiB is allocated by PyTorch, and 18.46 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W1204 13:16:16.108872382 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank0]:[W1204 13:16:16.147866379 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: Qwen/Qwen3-14B (backend=vllm)
[ModelCache] Using max_model_len=40960 for Qwen/Qwen3-14B
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 13:16:37 [utils.py:253] non-default args: {'max_model_len': 40960, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'Qwen/Qwen3-14B'}
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: Qwen/Qwen3-14B (backend=vllm)
[ModelCache] Using max_model_len=40960 for Qwen/Qwen3-14B
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 13:16:37 [utils.py:253] non-default args: {'max_model_len': 40960, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'Qwen/Qwen3-14B'}
INFO 12-04 13:16:37 [model.py:631] Resolved architecture: Qwen3ForCausalLM
INFO 12-04 13:16:37 [model.py:1745] Using max model len 40960
INFO 12-04 13:16:38 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
INFO 12-04 13:16:38 [model.py:631] Resolved architecture: Qwen3ForCausalLM
INFO 12-04 13:16:38 [model.py:1745] Using max model len 40960
INFO 12-04 13:16:38 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=372289)[0;0m INFO 12-04 13:16:51 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='Qwen/Qwen3-14B', speculative_config=None, tokenizer='Qwen/Qwen3-14B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-14B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=372292)[0;0m INFO 12-04 13:16:51 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='Qwen/Qwen3-14B', speculative_config=None, tokenizer='Qwen/Qwen3-14B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-14B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=372289)[0;0m INFO 12-04 13:16:53 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.98:41107 backend=nccl
[1;36m(EngineCore_DP0 pid=372292)[0;0m INFO 12-04 13:16:53 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.98:52097 backend=nccl
[W1204 13:16:53.685224227 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-25-0.rc.tch.harvard.edu]:41107 (errno: 97 - Address family not supported by protocol).
[W1204 13:16:53.685224249 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-25-0.rc.tch.harvard.edu]:52097 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=372289)[0;0m INFO 12-04 13:16:53 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=372292)[0;0m INFO 12-04 13:16:53 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=372289)[0;0m INFO 12-04 13:16:53 [gpu_model_runner.py:3259] Starting to load model Qwen/Qwen3-14B...
[1;36m(EngineCore_DP0 pid=372292)[0;0m INFO 12-04 13:16:53 [gpu_model_runner.py:3259] Starting to load model Qwen/Qwen3-14B...
[1;36m(EngineCore_DP0 pid=372292)[0;0m INFO 12-04 13:16:54 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=372292)[0;0m INFO 12-04 13:16:54 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=372289)[0;0m INFO 12-04 13:16:54 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=372289)[0;0m INFO 12-04 13:16:54 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=372289)[0;0m ERROR 12-04 13:16:55 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=372289)[0;0m ERROR 12-04 13:16:55 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=372289)[0;0m ERROR 12-04 13:16:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=372289)[0;0m ERROR 12-04 13:16:55 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=372289)[0;0m ERROR 12-04 13:16:55 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372289)[0;0m ERROR 12-04 13:16:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=372289)[0;0m ERROR 12-04 13:16:55 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=372289)[0;0m ERROR 12-04 13:16:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=372289)[0;0m ERROR 12-04 13:16:55 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=372289)[0;0m ERROR 12-04 13:16:55 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372289)[0;0m ERROR 12-04 13:16:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=372289)[0;0m ERROR 12-04 13:16:55 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=372289)[0;0m ERROR 12-04 13:16:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[1;36m(EngineCore_DP0 pid=372289)[0;0m ERROR 12-04 13:16:55 [core.py:842]     self.driver_worker.load_model()
[1;36m(EngineCore_DP0 pid=372289)[0;0m ERROR 12-04 13:16:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 273, in load_model
[1;36m(EngineCore_DP0 pid=372289)[0;0m ERROR 12-04 13:16:55 [core.py:842]     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[1;36m(EngineCore_DP0 pid=372289)[0;0m ERROR 12-04 13:16:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3276, in load_model
[1;36m(EngineCore_DP0 pid=372289)[0;0m ERROR 12-04 13:16:55 [core.py:842]     self.model = model_loader.load_model(
[1;36m(EngineCore_DP0 pid=372289)[0;0m ERROR 12-04 13:16:55 [core.py:842]                  ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372289)[0;0m ERROR 12-04 13:16:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
[1;36m(EngineCore_DP0 pid=372289)[0;0m ERROR 12-04 13:16:55 [core.py:842]     model = initialize_model(
[1;36m(EngineCore_DP0 pid=372289)[0;0m ERROR 12-04 13:16:55 [core.py:842]             ^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372289)[0;0m ERROR 12-04 13:16:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py", line 55, in initialize_model
[1;36m(EngineCore_DP0 pid=372289)[0;0m ERROR 12-04 13:16:55 [core.py:842]     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(EngineCore_DP0 pid=372289)[0;0m ERROR 12-04 13:16:55 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372289)[0;0m ERROR 12-04 13:16:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 279, in __init__
[1;36m(EngineCore_DP0 pid=372289)[0;0m ERROR 12-04 13:16:55 [core.py:842]     self.model = Qwen3Model(
[1;36m(EngineCore_DP0 pid=372289)[0;0m ERROR 12-04 13:16:55 [core.py:842]                  ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372289)[0;0m ERROR 12-04 13:16:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=372289)[0;0m ERROR 12-04 13:16:55 [core.py:842]     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=372289)[0;0m ERROR 12-04 13:16:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 253, in __init__
[1;36m(EngineCore_DP0 pid=372289)[0;0m ERROR 12-04 13:16:55 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=372289)[0;0m ERROR 12-04 13:16:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=372289)[0;0m ERROR 12-04 13:16:55 [core.py:842]     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=372289)[0;0m ERROR 12-04 13:16:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 337, in __init__
[1;36m(EngineCore_DP0 pid=372289)[0;0m ERROR 12-04 13:16:55 [core.py:842]     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(EngineCore_DP0 pid=372289)[0;0m ERROR 12-04 13:16:55 [core.py:842]                                                     ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372289)[0;0m ERROR 12-04 13:16:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 651, in make_layers
[1;36m(EngineCore_DP0 pid=372289)[0;0m ERROR 12-04 13:16:55 [core.py:842]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(EngineCore_DP0 pid=372289)[0;0m ERROR 12-04 13:16:55 [core.py:842]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372289)[0;0m ERROR 12-04 13:16:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 339, in <lambda>
[1;36m(EngineCore_DP0 pid=372289)[0;0m ERROR 12-04 13:16:55 [core.py:842]     lambda prefix: decoder_layer_type(
[1;36m(EngineCore_DP0 pid=372289)[0;0m ERROR 12-04 13:16:55 [core.py:842]                    ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372289)[0;0m ERROR 12-04 13:16:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 201, in __init__
[1;36m(EngineCore_DP0 pid=372289)[0;0m ERROR 12-04 13:16:55 [core.py:842]     self.mlp = Qwen3MLP(
[1;36m(EngineCore_DP0 pid=372289)[0;0m ERROR 12-04 13:16:55 [core.py:842]                ^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372289)[0;0m ERROR 12-04 13:16:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 84, in __init__
[1;36m(EngineCore_DP0 pid=372289)[0;0m ERROR 12-04 13:16:55 [core.py:842]     self.gate_up_proj = MergedColumnParallelLinear(
[1;36m(EngineCore_DP0 pid=372289)[0;0m ERROR 12-04 13:16:55 [core.py:842]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372289)[0;0m ERROR 12-04 13:16:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 631, in __init__
[1;36m(EngineCore_DP0 pid=372289)[0;0m ERROR 12-04 13:16:55 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=372289)[0;0m ERROR 12-04 13:16:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 484, in __init__
[1;36m(EngineCore_DP0 pid=372289)[0;0m ERROR 12-04 13:16:55 [core.py:842]     self.quant_method.create_weights(
[1;36m(EngineCore_DP0 pid=372289)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=372289)[0;0m ERROR 12-04 13:16:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 215, in create_weights
[1;36m(EngineCore_DP0 pid=372289)[0;0m ERROR 12-04 13:16:55 [core.py:842]     data=torch.empty(
[1;36m(EngineCore_DP0 pid=372289)[0;0m ERROR 12-04 13:16:55 [core.py:842]          ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372289)[0;0m ERROR 12-04 13:16:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_device.py", line 103, in __torch_function__
[1;36m(EngineCore_DP0 pid=372289)[0;0m ERROR 12-04 13:16:55 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=372289)[0;0m ERROR 12-04 13:16:55 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372289)[0;0m ERROR 12-04 13:16:55 [core.py:842] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 340.00 MiB. GPU 0 has a total capacity of 44.39 GiB of which 4.00 MiB is free. Process 372292 has 23.71 GiB memory in use. Including non-PyTorch memory, this process has 20.63 GiB memory in use. Of the allocated memory 20.11 GiB is allocated by PyTorch, and 18.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=372289)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=372289)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=372289)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=372289)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=372289)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=372289)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=372289)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=372289)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=372289)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=372289)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372289)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=372289)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=372289)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=372289)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=372289)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372289)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=372289)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=372289)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[1;36m(EngineCore_DP0 pid=372289)[0;0m     self.driver_worker.load_model()
[1;36m(EngineCore_DP0 pid=372289)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 273, in load_model
[1;36m(EngineCore_DP0 pid=372289)[0;0m     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[1;36m(EngineCore_DP0 pid=372289)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3276, in load_model
[1;36m(EngineCore_DP0 pid=372289)[0;0m     self.model = model_loader.load_model(
[1;36m(EngineCore_DP0 pid=372289)[0;0m                  ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372289)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
[1;36m(EngineCore_DP0 pid=372289)[0;0m     model = initialize_model(
[1;36m(EngineCore_DP0 pid=372289)[0;0m             ^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372289)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py", line 55, in initialize_model
[1;36m(EngineCore_DP0 pid=372289)[0;0m     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(EngineCore_DP0 pid=372289)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372289)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 279, in __init__
[1;36m(EngineCore_DP0 pid=372289)[0;0m     self.model = Qwen3Model(
[1;36m(EngineCore_DP0 pid=372289)[0;0m                  ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372289)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=372289)[0;0m     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=372289)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 253, in __init__
[1;36m(EngineCore_DP0 pid=372289)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=372289)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=372289)[0;0m     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=372289)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 337, in __init__
[1;36m(EngineCore_DP0 pid=372289)[0;0m     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(EngineCore_DP0 pid=372289)[0;0m                                                     ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372289)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 651, in make_layers
[1;36m(EngineCore_DP0 pid=372289)[0;0m     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(EngineCore_DP0 pid=372289)[0;0m                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372289)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 339, in <lambda>
[1;36m(EngineCore_DP0 pid=372289)[0;0m     lambda prefix: decoder_layer_type(
[1;36m(EngineCore_DP0 pid=372289)[0;0m                    ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372289)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 201, in __init__
[1;36m(EngineCore_DP0 pid=372289)[0;0m     self.mlp = Qwen3MLP(
[1;36m(EngineCore_DP0 pid=372289)[0;0m                ^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372289)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 84, in __init__
[1;36m(EngineCore_DP0 pid=372289)[0;0m     self.gate_up_proj = MergedColumnParallelLinear(
[1;36m(EngineCore_DP0 pid=372289)[0;0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372289)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 631, in __init__
[1;36m(EngineCore_DP0 pid=372289)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=372289)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 484, in __init__
[1;36m(EngineCore_DP0 pid=372289)[0;0m     self.quant_method.create_weights(
[1;36m(EngineCore_DP0 pid=372289)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 215, in create_weights
[1;36m(EngineCore_DP0 pid=372289)[0;0m     data=torch.empty(
[1;36m(EngineCore_DP0 pid=372289)[0;0m          ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372289)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_device.py", line 103, in __torch_function__
[1;36m(EngineCore_DP0 pid=372289)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=372289)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372289)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 340.00 MiB. GPU 0 has a total capacity of 44.39 GiB of which 4.00 MiB is free. Process 372292 has 23.71 GiB memory in use. Including non-PyTorch memory, this process has 20.63 GiB memory in use. Of the allocated memory 20.11 GiB is allocated by PyTorch, and 18.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=372292)[0;0m ERROR 12-04 13:16:55 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=372292)[0;0m ERROR 12-04 13:16:55 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=372292)[0;0m ERROR 12-04 13:16:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=372292)[0;0m ERROR 12-04 13:16:55 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=372292)[0;0m ERROR 12-04 13:16:55 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372292)[0;0m ERROR 12-04 13:16:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=372292)[0;0m ERROR 12-04 13:16:55 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=372292)[0;0m ERROR 12-04 13:16:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=372292)[0;0m ERROR 12-04 13:16:55 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=372292)[0;0m ERROR 12-04 13:16:55 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372292)[0;0m ERROR 12-04 13:16:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=372292)[0;0m ERROR 12-04 13:16:55 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=372292)[0;0m ERROR 12-04 13:16:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[1;36m(EngineCore_DP0 pid=372292)[0;0m ERROR 12-04 13:16:55 [core.py:842]     self.driver_worker.load_model()
[1;36m(EngineCore_DP0 pid=372292)[0;0m ERROR 12-04 13:16:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 273, in load_model
[1;36m(EngineCore_DP0 pid=372292)[0;0m ERROR 12-04 13:16:55 [core.py:842]     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[1;36m(EngineCore_DP0 pid=372292)[0;0m ERROR 12-04 13:16:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3276, in load_model
[1;36m(EngineCore_DP0 pid=372292)[0;0m ERROR 12-04 13:16:55 [core.py:842]     self.model = model_loader.load_model(
[1;36m(EngineCore_DP0 pid=372292)[0;0m ERROR 12-04 13:16:55 [core.py:842]                  ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372292)[0;0m ERROR 12-04 13:16:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
[1;36m(EngineCore_DP0 pid=372292)[0;0m ERROR 12-04 13:16:55 [core.py:842]     model = initialize_model(
[1;36m(EngineCore_DP0 pid=372292)[0;0m ERROR 12-04 13:16:55 [core.py:842]             ^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372292)[0;0m ERROR 12-04 13:16:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py", line 55, in initialize_model
[1;36m(EngineCore_DP0 pid=372292)[0;0m ERROR 12-04 13:16:55 [core.py:842]     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(EngineCore_DP0 pid=372292)[0;0m ERROR 12-04 13:16:55 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372292)[0;0m ERROR 12-04 13:16:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 279, in __init__
[1;36m(EngineCore_DP0 pid=372292)[0;0m ERROR 12-04 13:16:55 [core.py:842]     self.model = Qwen3Model(
[1;36m(EngineCore_DP0 pid=372292)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=372292)[0;0m ERROR 12-04 13:16:55 [core.py:842]                  ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372292)[0;0m ERROR 12-04 13:16:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=372292)[0;0m ERROR 12-04 13:16:55 [core.py:842]     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=372292)[0;0m ERROR 12-04 13:16:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 253, in __init__
[1;36m(EngineCore_DP0 pid=372292)[0;0m ERROR 12-04 13:16:55 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=372292)[0;0m ERROR 12-04 13:16:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=372292)[0;0m ERROR 12-04 13:16:55 [core.py:842]     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=372292)[0;0m ERROR 12-04 13:16:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 337, in __init__
[1;36m(EngineCore_DP0 pid=372292)[0;0m ERROR 12-04 13:16:55 [core.py:842]     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(EngineCore_DP0 pid=372292)[0;0m ERROR 12-04 13:16:55 [core.py:842]                                                     ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372292)[0;0m ERROR 12-04 13:16:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 651, in make_layers
[1;36m(EngineCore_DP0 pid=372292)[0;0m ERROR 12-04 13:16:55 [core.py:842]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(EngineCore_DP0 pid=372292)[0;0m ERROR 12-04 13:16:55 [core.py:842]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372292)[0;0m ERROR 12-04 13:16:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 339, in <lambda>
[1;36m(EngineCore_DP0 pid=372292)[0;0m ERROR 12-04 13:16:55 [core.py:842]     lambda prefix: decoder_layer_type(
[1;36m(EngineCore_DP0 pid=372292)[0;0m ERROR 12-04 13:16:55 [core.py:842]                    ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372292)[0;0m ERROR 12-04 13:16:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 201, in __init__
[1;36m(EngineCore_DP0 pid=372292)[0;0m ERROR 12-04 13:16:55 [core.py:842]     self.mlp = Qwen3MLP(
[1;36m(EngineCore_DP0 pid=372292)[0;0m ERROR 12-04 13:16:55 [core.py:842]                ^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372292)[0;0m ERROR 12-04 13:16:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 84, in __init__
[1;36m(EngineCore_DP0 pid=372292)[0;0m ERROR 12-04 13:16:55 [core.py:842]     self.gate_up_proj = MergedColumnParallelLinear(
[1;36m(EngineCore_DP0 pid=372292)[0;0m ERROR 12-04 13:16:55 [core.py:842]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372292)[0;0m ERROR 12-04 13:16:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 631, in __init__
[1;36m(EngineCore_DP0 pid=372292)[0;0m ERROR 12-04 13:16:55 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=372292)[0;0m ERROR 12-04 13:16:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 484, in __init__
[1;36m(EngineCore_DP0 pid=372292)[0;0m ERROR 12-04 13:16:55 [core.py:842]     self.quant_method.create_weights(
[1;36m(EngineCore_DP0 pid=372292)[0;0m ERROR 12-04 13:16:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 215, in create_weights
[1;36m(EngineCore_DP0 pid=372292)[0;0m ERROR 12-04 13:16:55 [core.py:842]     data=torch.empty(
[1;36m(EngineCore_DP0 pid=372292)[0;0m ERROR 12-04 13:16:55 [core.py:842]          ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372292)[0;0m ERROR 12-04 13:16:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_device.py", line 103, in __torch_function__
[1;36m(EngineCore_DP0 pid=372292)[0;0m ERROR 12-04 13:16:55 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=372292)[0;0m ERROR 12-04 13:16:55 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372292)[0;0m ERROR 12-04 13:16:55 [core.py:842] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 340.00 MiB. GPU 0 has a total capacity of 44.39 GiB of which 34.00 MiB is free. Including non-PyTorch memory, this process has 23.71 GiB memory in use. Process 372289 has 20.63 GiB memory in use. Of the allocated memory 23.19 GiB is allocated by PyTorch, and 18.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=372292)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=372292)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=372292)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=372292)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=372292)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=372292)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=372292)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=372292)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=372292)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=372292)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372292)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=372292)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=372292)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=372292)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=372292)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372292)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=372292)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=372292)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[1;36m(EngineCore_DP0 pid=372292)[0;0m     self.driver_worker.load_model()
[1;36m(EngineCore_DP0 pid=372292)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 273, in load_model
[1;36m(EngineCore_DP0 pid=372292)[0;0m     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[1;36m(EngineCore_DP0 pid=372292)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3276, in load_model
[1;36m(EngineCore_DP0 pid=372292)[0;0m     self.model = model_loader.load_model(
[1;36m(EngineCore_DP0 pid=372292)[0;0m                  ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372292)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
[1;36m(EngineCore_DP0 pid=372292)[0;0m     model = initialize_model(
[1;36m(EngineCore_DP0 pid=372292)[0;0m             ^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372292)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py", line 55, in initialize_model
[1;36m(EngineCore_DP0 pid=372292)[0;0m     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(EngineCore_DP0 pid=372292)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372292)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 279, in __init__
[1;36m(EngineCore_DP0 pid=372292)[0;0m     self.model = Qwen3Model(
[1;36m(EngineCore_DP0 pid=372292)[0;0m                  ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372292)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=372292)[0;0m     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=372292)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 253, in __init__
[1;36m(EngineCore_DP0 pid=372292)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=372292)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=372292)[0;0m     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=372292)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 337, in __init__
[1;36m(EngineCore_DP0 pid=372292)[0;0m     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(EngineCore_DP0 pid=372292)[0;0m                                                     ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372292)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 651, in make_layers
[1;36m(EngineCore_DP0 pid=372292)[0;0m     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(EngineCore_DP0 pid=372292)[0;0m                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372292)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 339, in <lambda>
[1;36m(EngineCore_DP0 pid=372292)[0;0m     lambda prefix: decoder_layer_type(
[1;36m(EngineCore_DP0 pid=372292)[0;0m                    ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372292)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 201, in __init__
[1;36m(EngineCore_DP0 pid=372292)[0;0m     self.mlp = Qwen3MLP(
[1;36m(EngineCore_DP0 pid=372292)[0;0m                ^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372292)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 84, in __init__
[1;36m(EngineCore_DP0 pid=372292)[0;0m     self.gate_up_proj = MergedColumnParallelLinear(
[1;36m(EngineCore_DP0 pid=372292)[0;0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372292)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 631, in __init__
[1;36m(EngineCore_DP0 pid=372292)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=372292)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 484, in __init__
[1;36m(EngineCore_DP0 pid=372292)[0;0m     self.quant_method.create_weights(
[1;36m(EngineCore_DP0 pid=372292)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 215, in create_weights
[1;36m(EngineCore_DP0 pid=372292)[0;0m     data=torch.empty(
[1;36m(EngineCore_DP0 pid=372292)[0;0m          ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372292)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_device.py", line 103, in __torch_function__
[1;36m(EngineCore_DP0 pid=372292)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=372292)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372292)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 340.00 MiB. GPU 0 has a total capacity of 44.39 GiB of which 34.00 MiB is free. Including non-PyTorch memory, this process has 23.71 GiB memory in use. Process 372289 has 20.63 GiB memory in use. Of the allocated memory 23.19 GiB is allocated by PyTorch, and 18.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W1204 13:16:56.900870628 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank0]:[W1204 13:16:56.937990597 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: Qwen/Qwen3-14B (backend=vllm)
[ModelCache] Using max_model_len=40960 for Qwen/Qwen3-14B
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 13:17:17 [utils.py:253] non-default args: {'max_model_len': 40960, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'Qwen/Qwen3-14B'}
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: Qwen/Qwen3-14B (backend=vllm)
[ModelCache] Using max_model_len=40960 for Qwen/Qwen3-14B
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 13:17:17 [utils.py:253] non-default args: {'max_model_len': 40960, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'Qwen/Qwen3-14B'}
INFO 12-04 13:17:17 [model.py:631] Resolved architecture: Qwen3ForCausalLM
INFO 12-04 13:17:17 [model.py:1745] Using max model len 40960
INFO 12-04 13:17:17 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
INFO 12-04 13:17:17 [model.py:631] Resolved architecture: Qwen3ForCausalLM
INFO 12-04 13:17:17 [model.py:1745] Using max model len 40960
INFO 12-04 13:17:17 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=374398)[0;0m INFO 12-04 13:17:31 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='Qwen/Qwen3-14B', speculative_config=None, tokenizer='Qwen/Qwen3-14B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-14B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=374401)[0;0m INFO 12-04 13:17:31 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='Qwen/Qwen3-14B', speculative_config=None, tokenizer='Qwen/Qwen3-14B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-14B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=374398)[0;0m INFO 12-04 13:17:33 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.98:52103 backend=nccl
[W1204 13:17:33.580299571 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-25-0.rc.tch.harvard.edu]:52103 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=374398)[0;0m INFO 12-04 13:17:33 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=374401)[0;0m INFO 12-04 13:17:33 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.98:33783 backend=nccl
[W1204 13:17:33.937533280 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-25-0.rc.tch.harvard.edu]:33783 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=374401)[0;0m INFO 12-04 13:17:33 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=374398)[0;0m INFO 12-04 13:17:33 [gpu_model_runner.py:3259] Starting to load model Qwen/Qwen3-14B...
[1;36m(EngineCore_DP0 pid=374401)[0;0m INFO 12-04 13:17:33 [gpu_model_runner.py:3259] Starting to load model Qwen/Qwen3-14B...
[1;36m(EngineCore_DP0 pid=374398)[0;0m INFO 12-04 13:17:34 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=374398)[0;0m INFO 12-04 13:17:34 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=374401)[0;0m INFO 12-04 13:17:34 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=374401)[0;0m INFO 12-04 13:17:34 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=374398)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/8 [00:00<?, ?it/s]
[1;36m(EngineCore_DP0 pid=374401)[0;0m ERROR 12-04 13:17:35 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=374401)[0;0m ERROR 12-04 13:17:35 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=374401)[0;0m ERROR 12-04 13:17:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=374401)[0;0m ERROR 12-04 13:17:35 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=374401)[0;0m ERROR 12-04 13:17:35 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374401)[0;0m ERROR 12-04 13:17:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=374401)[0;0m ERROR 12-04 13:17:35 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=374401)[0;0m ERROR 12-04 13:17:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=374401)[0;0m ERROR 12-04 13:17:35 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=374401)[0;0m ERROR 12-04 13:17:35 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374401)[0;0m ERROR 12-04 13:17:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=374401)[0;0m ERROR 12-04 13:17:35 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=374401)[0;0m ERROR 12-04 13:17:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[1;36m(EngineCore_DP0 pid=374401)[0;0m ERROR 12-04 13:17:35 [core.py:842]     self.driver_worker.load_model()
[1;36m(EngineCore_DP0 pid=374401)[0;0m ERROR 12-04 13:17:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 273, in load_model
[1;36m(EngineCore_DP0 pid=374401)[0;0m ERROR 12-04 13:17:35 [core.py:842]     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[1;36m(EngineCore_DP0 pid=374401)[0;0m ERROR 12-04 13:17:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3276, in load_model
[1;36m(EngineCore_DP0 pid=374401)[0;0m ERROR 12-04 13:17:35 [core.py:842]     self.model = model_loader.load_model(
[1;36m(EngineCore_DP0 pid=374401)[0;0m ERROR 12-04 13:17:35 [core.py:842]                  ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374401)[0;0m ERROR 12-04 13:17:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
[1;36m(EngineCore_DP0 pid=374401)[0;0m ERROR 12-04 13:17:35 [core.py:842]     model = initialize_model(
[1;36m(EngineCore_DP0 pid=374401)[0;0m ERROR 12-04 13:17:35 [core.py:842]             ^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374401)[0;0m ERROR 12-04 13:17:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py", line 55, in initialize_model
[1;36m(EngineCore_DP0 pid=374401)[0;0m ERROR 12-04 13:17:35 [core.py:842]     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(EngineCore_DP0 pid=374401)[0;0m ERROR 12-04 13:17:35 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374401)[0;0m ERROR 12-04 13:17:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 279, in __init__
[1;36m(EngineCore_DP0 pid=374401)[0;0m ERROR 12-04 13:17:35 [core.py:842]     self.model = Qwen3Model(
[1;36m(EngineCore_DP0 pid=374401)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=374401)[0;0m ERROR 12-04 13:17:35 [core.py:842]                  ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374401)[0;0m ERROR 12-04 13:17:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=374401)[0;0m ERROR 12-04 13:17:35 [core.py:842]     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=374401)[0;0m ERROR 12-04 13:17:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 253, in __init__
[1;36m(EngineCore_DP0 pid=374401)[0;0m ERROR 12-04 13:17:35 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=374401)[0;0m ERROR 12-04 13:17:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=374401)[0;0m ERROR 12-04 13:17:35 [core.py:842]     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=374401)[0;0m ERROR 12-04 13:17:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 337, in __init__
[1;36m(EngineCore_DP0 pid=374401)[0;0m ERROR 12-04 13:17:35 [core.py:842]     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(EngineCore_DP0 pid=374401)[0;0m ERROR 12-04 13:17:35 [core.py:842]                                                     ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374401)[0;0m ERROR 12-04 13:17:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 651, in make_layers
[1;36m(EngineCore_DP0 pid=374401)[0;0m ERROR 12-04 13:17:35 [core.py:842]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(EngineCore_DP0 pid=374401)[0;0m ERROR 12-04 13:17:35 [core.py:842]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374401)[0;0m ERROR 12-04 13:17:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 339, in <lambda>
[1;36m(EngineCore_DP0 pid=374401)[0;0m ERROR 12-04 13:17:35 [core.py:842]     lambda prefix: decoder_layer_type(
[1;36m(EngineCore_DP0 pid=374401)[0;0m ERROR 12-04 13:17:35 [core.py:842]                    ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374401)[0;0m ERROR 12-04 13:17:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 185, in __init__
[1;36m(EngineCore_DP0 pid=374401)[0;0m ERROR 12-04 13:17:35 [core.py:842]     self.self_attn = Qwen3Attention(
[1;36m(EngineCore_DP0 pid=374401)[0;0m ERROR 12-04 13:17:35 [core.py:842]                      ^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374401)[0;0m ERROR 12-04 13:17:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 95, in __init__
[1;36m(EngineCore_DP0 pid=374401)[0;0m ERROR 12-04 13:17:35 [core.py:842]     self.qkv_proj = QKVParallelLinear(
[1;36m(EngineCore_DP0 pid=374401)[0;0m ERROR 12-04 13:17:35 [core.py:842]                     ^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374401)[0;0m ERROR 12-04 13:17:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 935, in __init__
[1;36m(EngineCore_DP0 pid=374401)[0;0m ERROR 12-04 13:17:35 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=374401)[0;0m ERROR 12-04 13:17:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 484, in __init__
[1;36m(EngineCore_DP0 pid=374401)[0;0m ERROR 12-04 13:17:35 [core.py:842]     self.quant_method.create_weights(
[1;36m(EngineCore_DP0 pid=374401)[0;0m ERROR 12-04 13:17:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 215, in create_weights
[1;36m(EngineCore_DP0 pid=374401)[0;0m ERROR 12-04 13:17:35 [core.py:842]     data=torch.empty(
[1;36m(EngineCore_DP0 pid=374401)[0;0m ERROR 12-04 13:17:35 [core.py:842]          ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374401)[0;0m ERROR 12-04 13:17:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_device.py", line 103, in __torch_function__
[1;36m(EngineCore_DP0 pid=374401)[0;0m ERROR 12-04 13:17:35 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=374401)[0;0m ERROR 12-04 13:17:35 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374401)[0;0m ERROR 12-04 13:17:35 [core.py:842] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 70.00 MiB. GPU 0 has a total capacity of 44.39 GiB of which 20.00 MiB is free. Process 374398 has 28.15 GiB memory in use. Including non-PyTorch memory, this process has 16.21 GiB memory in use. Of the allocated memory 15.69 GiB is allocated by PyTorch, and 18.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=374401)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=374401)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=374401)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=374401)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=374401)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=374401)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=374401)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=374401)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=374401)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=374401)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374401)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=374401)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=374401)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=374401)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=374401)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374401)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=374401)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=374401)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[1;36m(EngineCore_DP0 pid=374401)[0;0m     self.driver_worker.load_model()
[1;36m(EngineCore_DP0 pid=374401)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 273, in load_model
[1;36m(EngineCore_DP0 pid=374401)[0;0m     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[1;36m(EngineCore_DP0 pid=374401)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3276, in load_model
[1;36m(EngineCore_DP0 pid=374401)[0;0m     self.model = model_loader.load_model(
[1;36m(EngineCore_DP0 pid=374401)[0;0m                  ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374401)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
[1;36m(EngineCore_DP0 pid=374401)[0;0m     model = initialize_model(
[1;36m(EngineCore_DP0 pid=374401)[0;0m             ^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374401)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py", line 55, in initialize_model
[1;36m(EngineCore_DP0 pid=374401)[0;0m     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(EngineCore_DP0 pid=374401)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374401)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 279, in __init__
[1;36m(EngineCore_DP0 pid=374401)[0;0m     self.model = Qwen3Model(
[1;36m(EngineCore_DP0 pid=374401)[0;0m                  ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374401)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=374401)[0;0m     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=374401)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 253, in __init__
[1;36m(EngineCore_DP0 pid=374401)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=374401)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=374401)[0;0m     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=374401)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 337, in __init__
[1;36m(EngineCore_DP0 pid=374401)[0;0m     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(EngineCore_DP0 pid=374401)[0;0m                                                     ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374401)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 651, in make_layers
[1;36m(EngineCore_DP0 pid=374401)[0;0m     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(EngineCore_DP0 pid=374401)[0;0m                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374401)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 339, in <lambda>
[1;36m(EngineCore_DP0 pid=374401)[0;0m     lambda prefix: decoder_layer_type(
[1;36m(EngineCore_DP0 pid=374401)[0;0m                    ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374401)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 185, in __init__
[1;36m(EngineCore_DP0 pid=374401)[0;0m     self.self_attn = Qwen3Attention(
[1;36m(EngineCore_DP0 pid=374401)[0;0m                      ^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374401)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 95, in __init__
[1;36m(EngineCore_DP0 pid=374401)[0;0m     self.qkv_proj = QKVParallelLinear(
[1;36m(EngineCore_DP0 pid=374401)[0;0m                     ^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374401)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 935, in __init__
[1;36m(EngineCore_DP0 pid=374401)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=374401)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 484, in __init__
[1;36m(EngineCore_DP0 pid=374401)[0;0m     self.quant_method.create_weights(
[1;36m(EngineCore_DP0 pid=374401)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 215, in create_weights
[1;36m(EngineCore_DP0 pid=374401)[0;0m     data=torch.empty(
[1;36m(EngineCore_DP0 pid=374401)[0;0m          ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374401)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_device.py", line 103, in __torch_function__
[1;36m(EngineCore_DP0 pid=374401)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=374401)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374401)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 70.00 MiB. GPU 0 has a total capacity of 44.39 GiB of which 20.00 MiB is free. Process 374398 has 28.15 GiB memory in use. Including non-PyTorch memory, this process has 16.21 GiB memory in use. Of the allocated memory 15.69 GiB is allocated by PyTorch, and 18.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=374398)[0;0m Loading safetensors checkpoint shards:  12% Completed | 1/8 [00:01<00:07,  1.01s/it]
[rank0]:[W1204 13:17:36.719501815 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[1;36m(EngineCore_DP0 pid=374398)[0;0m Loading safetensors checkpoint shards:  25% Completed | 2/8 [00:02<00:06,  1.01s/it]
[1;36m(EngineCore_DP0 pid=374398)[0;0m Loading safetensors checkpoint shards:  38% Completed | 3/8 [00:02<00:04,  1.07it/s]
[1;36m(EngineCore_DP0 pid=374398)[0;0m Loading safetensors checkpoint shards:  50% Completed | 4/8 [00:03<00:03,  1.16it/s]
[1;36m(EngineCore_DP0 pid=374398)[0;0m Loading safetensors checkpoint shards:  62% Completed | 5/8 [00:04<00:02,  1.24it/s]
[1;36m(EngineCore_DP0 pid=374398)[0;0m Loading safetensors checkpoint shards:  75% Completed | 6/8 [00:04<00:01,  1.32it/s]
[1;36m(EngineCore_DP0 pid=374398)[0;0m Loading safetensors checkpoint shards:  88% Completed | 7/8 [00:05<00:00,  1.38it/s]
[1;36m(EngineCore_DP0 pid=374398)[0;0m Loading safetensors checkpoint shards: 100% Completed | 8/8 [00:06<00:00,  1.63it/s]
[1;36m(EngineCore_DP0 pid=374398)[0;0m Loading safetensors checkpoint shards: 100% Completed | 8/8 [00:06<00:00,  1.33it/s]
[1;36m(EngineCore_DP0 pid=374398)[0;0m 
[1;36m(EngineCore_DP0 pid=374398)[0;0m INFO 12-04 13:17:41 [default_loader.py:314] Loading weights took 6.06 seconds
[1;36m(EngineCore_DP0 pid=374398)[0;0m INFO 12-04 13:17:41 [gpu_model_runner.py:3338] Model loading took 27.5185 GiB memory and 7.178853 seconds
[1;36m(EngineCore_DP0 pid=374398)[0;0m INFO 12-04 13:17:49 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/76690ce19a/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=374398)[0;0m INFO 12-04 13:17:49 [backends.py:647] Dynamo bytecode transform time: 7.98 s
[1;36m(EngineCore_DP0 pid=374398)[0;0m INFO 12-04 13:17:54 [backends.py:210] Directly load the compiled graph(s) for dynamic shape from the cache, took 4.696 s
[1;36m(EngineCore_DP0 pid=374398)[0;0m INFO 12-04 13:17:56 [monitor.py:34] torch.compile takes 12.68 s in total
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: Qwen/Qwen3-14B (backend=vllm)
[ModelCache] Using max_model_len=40960 for Qwen/Qwen3-14B
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 13:17:57 [utils.py:253] non-default args: {'max_model_len': 40960, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'Qwen/Qwen3-14B'}
INFO 12-04 13:17:57 [model.py:631] Resolved architecture: Qwen3ForCausalLM
INFO 12-04 13:17:57 [model.py:1745] Using max model len 40960
INFO 12-04 13:17:57 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=374398)[0;0m INFO 12-04 13:17:58 [gpu_worker.py:359] Available KV cache memory: 11.02 GiB
[1;36m(EngineCore_DP0 pid=374398)[0;0m INFO 12-04 13:17:59 [kv_cache_utils.py:1229] GPU KV cache size: 72,240 tokens
[1;36m(EngineCore_DP0 pid=374398)[0;0m INFO 12-04 13:17:59 [kv_cache_utils.py:1234] Maximum concurrency for 40,960 tokens per request: 1.76x
[1;36m(EngineCore_DP0 pid=374398)[0;0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/51 [00:00<?, ?it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   2%|▏         | 1/51 [00:00<00:07,  7.14it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   4%|▍         | 2/51 [00:00<00:06,  7.17it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 3/51 [00:00<00:06,  7.12it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|▊         | 4/51 [00:00<00:06,  7.09it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  10%|▉         | 5/51 [00:00<00:06,  7.22it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  12%|█▏        | 6/51 [00:00<00:06,  7.23it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▎        | 7/51 [00:00<00:06,  7.24it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|█▌        | 8/51 [00:01<00:05,  7.29it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 9/51 [00:01<00:05,  7.71it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|█▉        | 10/51 [00:01<00:05,  8.16it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|██▏       | 11/51 [00:01<00:04,  8.36it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  24%|██▎       | 12/51 [00:01<00:04,  8.57it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 13/51 [00:01<00:04,  8.66it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|██▋       | 14/51 [00:01<00:04,  8.79it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▉       | 15/51 [00:01<00:04,  8.88it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███▏      | 16/51 [00:01<00:03,  9.00it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  35%|███▌      | 18/51 [00:02<00:03,  9.85it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  39%|███▉      | 20/51 [00:02<00:03, 10.29it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 22/51 [00:02<00:02, 10.65it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 24/51 [00:02<00:02, 10.81it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|█████     | 26/51 [00:02<00:02, 11.11it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▍    | 28/51 [00:03<00:02, 11.24it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  59%|█████▉    | 30/51 [00:03<00:01, 11.50it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 32/51 [00:03<00:01, 11.72it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 34/51 [00:03<00:01, 11.80it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████   | 36/51 [00:03<00:01, 11.86it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▍  | 38/51 [00:03<00:01, 11.96it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  78%|███████▊  | 40/51 [00:04<00:00, 12.11it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|████████▏ | 42/51 [00:04<00:00, 12.14it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▋ | 44/51 [00:04<00:00, 12.34it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  90%|█████████ | 46/51 [00:04<00:00, 12.47it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|█████████▍| 48/51 [00:04<00:00, 12.79it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  98%|█████████▊| 50/51 [00:04<00:00, 12.94it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:04<00:00, 10.43it/s]
[1;36m(EngineCore_DP0 pid=374398)[0;0m Capturing CUDA graphs (decode, FULL):   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graphs (decode, FULL):   6%|▌         | 2/35 [00:00<00:03, 10.79it/s]Capturing CUDA graphs (decode, FULL):  11%|█▏        | 4/35 [00:00<00:02, 11.05it/s]Capturing CUDA graphs (decode, FULL):  17%|█▋        | 6/35 [00:00<00:02, 11.42it/s]Capturing CUDA graphs (decode, FULL):  23%|██▎       | 8/35 [00:00<00:02, 11.66it/s]Capturing CUDA graphs (decode, FULL):  29%|██▊       | 10/35 [00:00<00:02, 12.07it/s]Capturing CUDA graphs (decode, FULL):  34%|███▍      | 12/35 [00:01<00:01, 12.39it/s]Capturing CUDA graphs (decode, FULL):  40%|████      | 14/35 [00:01<00:01, 12.58it/s]Capturing CUDA graphs (decode, FULL):  46%|████▌     | 16/35 [00:01<00:01, 12.75it/s]Capturing CUDA graphs (decode, FULL):  51%|█████▏    | 18/35 [00:01<00:01, 12.97it/s]Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 20/35 [00:01<00:01, 13.23it/s]Capturing CUDA graphs (decode, FULL):  63%|██████▎   | 22/35 [00:01<00:00, 13.55it/s]Capturing CUDA graphs (decode, FULL):  69%|██████▊   | 24/35 [00:01<00:00, 13.70it/s]Capturing CUDA graphs (decode, FULL):  74%|███████▍  | 26/35 [00:02<00:00, 11.20it/s]Capturing CUDA graphs (decode, FULL):  80%|████████  | 28/35 [00:02<00:00, 11.99it/s]Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 30/35 [00:02<00:00, 12.59it/s]Capturing CUDA graphs (decode, FULL):  91%|█████████▏| 32/35 [00:02<00:00, 13.30it/s]Capturing CUDA graphs (decode, FULL):  97%|█████████▋| 34/35 [00:02<00:00, 13.79it/s]Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:02<00:00, 12.75it/s]
[1;36m(EngineCore_DP0 pid=374398)[0;0m INFO 12-04 13:18:07 [gpu_model_runner.py:4244] Graph capturing finished in 8 secs, took 0.65 GiB
[1;36m(EngineCore_DP0 pid=374398)[0;0m INFO 12-04 13:18:07 [core.py:250] init engine (profile, create kv cache, warmup model) took 26.05 seconds
[1;36m(EngineCore_DP0 pid=377879)[0;0m INFO 12-04 13:18:08 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='Qwen/Qwen3-14B', speculative_config=None, tokenizer='Qwen/Qwen3-14B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-14B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
INFO 12-04 13:18:08 [llm.py:352] Supported tasks: ['generate']
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 475.06it/s]
[1;36m(EngineCore_DP0 pid=377879)[0;0m INFO 12-04 13:18:10 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.98:40079 backend=nccl
[W1204 13:18:10.575028675 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-25-0.rc.tch.harvard.edu]:40079 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=377879)[0;0m INFO 12-04 13:18:10 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=377879)[0;0m ERROR 12-04 13:18:10 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=377879)[0;0m ERROR 12-04 13:18:10 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=377879)[0;0m ERROR 12-04 13:18:10 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=377879)[0;0m ERROR 12-04 13:18:10 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=377879)[0;0m ERROR 12-04 13:18:10 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=377879)[0;0m ERROR 12-04 13:18:10 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=377879)[0;0m ERROR 12-04 13:18:10 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=377879)[0;0m ERROR 12-04 13:18:10 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=377879)[0;0m ERROR 12-04 13:18:10 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=377879)[0;0m ERROR 12-04 13:18:10 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=377879)[0;0m ERROR 12-04 13:18:10 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=377879)[0;0m ERROR 12-04 13:18:10 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=377879)[0;0m ERROR 12-04 13:18:10 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=377879)[0;0m ERROR 12-04 13:18:10 [core.py:842]     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=377879)[0;0m ERROR 12-04 13:18:10 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=377879)[0;0m ERROR 12-04 13:18:10 [core.py:842]     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=377879)[0;0m ERROR 12-04 13:18:10 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=377879)[0;0m ERROR 12-04 13:18:10 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=377879)[0;0m ERROR 12-04 13:18:10 [core.py:842]     raise ValueError(
[1;36m(EngineCore_DP0 pid=377879)[0;0m ERROR 12-04 13:18:10 [core.py:842] ValueError: Free memory on device (2.63/44.39 GiB) on startup is less than desired GPU memory utilization (0.9, 39.95 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[1;36m(EngineCore_DP0 pid=377879)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=377879)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=377879)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=377879)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=377879)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=377879)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=377879)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=377879)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=377879)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=377879)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=377879)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=377879)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=377879)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=377879)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=377879)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=377879)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=377879)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=377879)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=377879)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=377879)[0;0m     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=377879)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=377879)[0;0m     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=377879)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=377879)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=377879)[0;0m     raise ValueError(
[1;36m(EngineCore_DP0 pid=377879)[0;0m ValueError: Free memory on device (2.63/44.39 GiB) on startup is less than desired GPU memory utilization (0.9, 39.95 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W1204 13:18:10.367432451 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:20<00:00, 20.79s/it, est. speed input: 5.39 toks/s, output: 25.16 toks/s]Processed prompts: 100%|██████████| 1/1 [00:20<00:00, 20.79s/it, est. speed input: 5.39 toks/s, output: 25.16 toks/s]Processed prompts: 100%|██████████| 1/1 [00:20<00:00, 20.79s/it, est. speed input: 5.39 toks/s, output: 25.16 toks/s]
Agent 1 response: The correct answer is D) 45,X. Monosomy refers to the presence of a single copy of a chromosome inst...

--- Problem 1/2, Round 1, Agent 2/3 ---
[ModelCache] Using cached model: Qwen/Qwen3-14B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1213.63it/s]
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: Qwen/Qwen3-14B (backend=vllm)
[ModelCache] Using max_model_len=40960 for Qwen/Qwen3-14B
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 13:18:31 [utils.py:253] non-default args: {'max_model_len': 40960, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'Qwen/Qwen3-14B'}
INFO 12-04 13:18:32 [model.py:631] Resolved architecture: Qwen3ForCausalLM
INFO 12-04 13:18:32 [model.py:1745] Using max model len 40960
INFO 12-04 13:18:32 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=379882)[0;0m INFO 12-04 13:18:43 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='Qwen/Qwen3-14B', speculative_config=None, tokenizer='Qwen/Qwen3-14B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-14B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=379882)[0;0m INFO 12-04 13:18:44 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.98:46211 backend=nccl
[W1204 13:18:44.138472258 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-25-0.rc.tch.harvard.edu]:46211 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=379882)[0;0m INFO 12-04 13:18:44 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=379882)[0;0m ERROR 12-04 13:18:44 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=379882)[0;0m ERROR 12-04 13:18:44 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=379882)[0;0m ERROR 12-04 13:18:44 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=379882)[0;0m ERROR 12-04 13:18:44 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=379882)[0;0m ERROR 12-04 13:18:44 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=379882)[0;0m ERROR 12-04 13:18:44 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=379882)[0;0m ERROR 12-04 13:18:44 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=379882)[0;0m ERROR 12-04 13:18:44 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=379882)[0;0m ERROR 12-04 13:18:44 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=379882)[0;0m ERROR 12-04 13:18:44 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=379882)[0;0m ERROR 12-04 13:18:44 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=379882)[0;0m ERROR 12-04 13:18:44 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=379882)[0;0m ERROR 12-04 13:18:44 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=379882)[0;0m ERROR 12-04 13:18:44 [core.py:842]     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=379882)[0;0m ERROR 12-04 13:18:44 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=379882)[0;0m ERROR 12-04 13:18:44 [core.py:842]     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=379882)[0;0m ERROR 12-04 13:18:44 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=379882)[0;0m ERROR 12-04 13:18:44 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=379882)[0;0m ERROR 12-04 13:18:44 [core.py:842]     raise ValueError(
[1;36m(EngineCore_DP0 pid=379882)[0;0m ERROR 12-04 13:18:44 [core.py:842] ValueError: Free memory on device (2.63/44.39 GiB) on startup is less than desired GPU memory utilization (0.9, 39.95 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[1;36m(EngineCore_DP0 pid=379882)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=379882)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=379882)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=379882)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=379882)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=379882)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=379882)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=379882)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=379882)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=379882)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=379882)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=379882)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=379882)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=379882)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=379882)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=379882)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=379882)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=379882)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=379882)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=379882)[0;0m     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=379882)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=379882)[0;0m     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=379882)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=379882)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=379882)[0;0m     raise ValueError(
[1;36m(EngineCore_DP0 pid=379882)[0;0m ValueError: Free memory on device (2.63/44.39 GiB) on startup is less than desired GPU memory utilization (0.9, 39.95 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:14<00:00, 14.65s/it, est. speed input: 7.92 toks/s, output: 25.05 toks/s]Processed prompts: 100%|██████████| 1/1 [00:14<00:00, 14.65s/it, est. speed input: 7.92 toks/s, output: 25.05 toks/s]Processed prompts: 100%|██████████| 1/1 [00:14<00:00, 14.65s/it, est. speed input: 7.92 toks/s, output: 25.05 toks/s]
Agent 2 response: (D) 45,X. Monosomy refers to the presence of one chromosome instead of the usual two. 45,X indicates...

--- Problem 1/2, Round 1, Agent 3/3 ---
[ModelCache] Using cached model: Qwen/Qwen3-14B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 996.98it/s]
[rank0]:[W1204 13:18:45.904996610 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: Qwen/Qwen3-14B (backend=vllm)
[ModelCache] Using max_model_len=40960 for Qwen/Qwen3-14B
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 13:19:06 [utils.py:253] non-default args: {'max_model_len': 40960, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'Qwen/Qwen3-14B'}
INFO 12-04 13:19:06 [model.py:631] Resolved architecture: Qwen3ForCausalLM
INFO 12-04 13:19:06 [model.py:1745] Using max model len 40960
INFO 12-04 13:19:06 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:22<00:00, 22.99s/it, est. speed input: 4.92 toks/s, output: 25.19 toks/s]Processed prompts: 100%|██████████| 1/1 [00:22<00:00, 22.99s/it, est. speed input: 4.92 toks/s, output: 25.19 toks/s]Processed prompts: 100%|██████████| 1/1 [00:22<00:00, 22.99s/it, est. speed input: 4.92 toks/s, output: 25.19 toks/s]
Agent 3 response: The correct answer is D) 45,X. Monosomy refers to the presence of only one copy of a chromosome inst...

--- Problem 1/2, Round 2, Agent 1/3 ---
Agent 1 receiving other agents' responses...
[ModelCache] Using cached model: Qwen/Qwen3-14B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 390.97it/s]
[1;36m(EngineCore_DP0 pid=380895)[0;0m INFO 12-04 13:19:17 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='Qwen/Qwen3-14B', speculative_config=None, tokenizer='Qwen/Qwen3-14B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-14B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=380895)[0;0m INFO 12-04 13:19:18 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.98:54625 backend=nccl
[W1204 13:19:18.030027204 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-25-0.rc.tch.harvard.edu]:54625 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=380895)[0;0m INFO 12-04 13:19:18 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=380895)[0;0m ERROR 12-04 13:19:18 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=380895)[0;0m ERROR 12-04 13:19:18 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=380895)[0;0m ERROR 12-04 13:19:18 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=380895)[0;0m ERROR 12-04 13:19:18 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=380895)[0;0m ERROR 12-04 13:19:18 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=380895)[0;0m ERROR 12-04 13:19:18 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=380895)[0;0m ERROR 12-04 13:19:18 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=380895)[0;0m ERROR 12-04 13:19:18 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=380895)[0;0m ERROR 12-04 13:19:18 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=380895)[0;0m ERROR 12-04 13:19:18 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=380895)[0;0m ERROR 12-04 13:19:18 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=380895)[0;0m ERROR 12-04 13:19:18 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=380895)[0;0m ERROR 12-04 13:19:18 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=380895)[0;0m ERROR 12-04 13:19:18 [core.py:842]     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=380895)[0;0m ERROR 12-04 13:19:18 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=380895)[0;0m ERROR 12-04 13:19:18 [core.py:842]     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=380895)[0;0m ERROR 12-04 13:19:18 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=380895)[0;0m ERROR 12-04 13:19:18 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=380895)[0;0m ERROR 12-04 13:19:18 [core.py:842]     raise ValueError(
[1;36m(EngineCore_DP0 pid=380895)[0;0m ERROR 12-04 13:19:18 [core.py:842] ValueError: Free memory on device (2.63/44.39 GiB) on startup is less than desired GPU memory utilization (0.9, 39.95 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[1;36m(EngineCore_DP0 pid=380895)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=380895)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=380895)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=380895)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=380895)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=380895)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=380895)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=380895)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=380895)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=380895)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=380895)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=380895)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=380895)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=380895)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=380895)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=380895)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=380895)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=380895)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=380895)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=380895)[0;0m     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=380895)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=380895)[0;0m     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=380895)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=380895)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=380895)[0;0m     raise ValueError(
[1;36m(EngineCore_DP0 pid=380895)[0;0m ValueError: Free memory on device (2.63/44.39 GiB) on startup is less than desired GPU memory utilization (0.9, 39.95 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W1204 13:19:19.866953959 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:20<00:00, 20.96s/it, est. speed input: 24.43 toks/s, output: 24.90 toks/s]Processed prompts: 100%|██████████| 1/1 [00:20<00:00, 20.96s/it, est. speed input: 24.43 toks/s, output: 24.90 toks/s]Processed prompts: 100%|██████████| 1/1 [00:20<00:00, 20.96s/it, est. speed input: 24.43 toks/s, output: 24.90 toks/s]
Agent 1 response: The correct answer is D) 45,X. Monosomy is defined as the absence of one chromosome (only one copy i...

--- Problem 1/2, Round 2, Agent 2/3 ---
Agent 2 receiving other agents' responses...
[ModelCache] Using cached model: Qwen/Qwen3-14B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 638.31it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:09<00:00,  9.90s/it, est. speed input: 52.03 toks/s, output: 24.96 toks/s]Processed prompts: 100%|██████████| 1/1 [00:09<00:00,  9.90s/it, est. speed input: 52.03 toks/s, output: 24.96 toks/s]Processed prompts: 100%|██████████| 1/1 [00:09<00:00,  9.90s/it, est. speed input: 52.03 toks/s, output: 24.96 toks/s]
Agent 2 response: (D)...

--- Problem 1/2, Round 2, Agent 3/3 ---
Agent 3 receiving other agents' responses...
[ModelCache] Using cached model: Qwen/Qwen3-14B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 632.34it/s]
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: Qwen/Qwen3-14B (backend=vllm)
[ModelCache] Using max_model_len=40960 for Qwen/Qwen3-14B
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 13:19:40 [utils.py:253] non-default args: {'max_model_len': 40960, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'Qwen/Qwen3-14B'}
INFO 12-04 13:19:40 [model.py:631] Resolved architecture: Qwen3ForCausalLM
INFO 12-04 13:19:40 [model.py:1745] Using max model len 40960
INFO 12-04 13:19:40 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=382488)[0;0m INFO 12-04 13:19:51 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='Qwen/Qwen3-14B', speculative_config=None, tokenizer='Qwen/Qwen3-14B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-14B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:13<00:00, 13.72s/it, est. speed input: 37.40 toks/s, output: 24.79 toks/s]Processed prompts: 100%|██████████| 1/1 [00:13<00:00, 13.72s/it, est. speed input: 37.40 toks/s, output: 24.79 toks/s]Processed prompts: 100%|██████████| 1/1 [00:13<00:00, 13.72s/it, est. speed input: 37.40 toks/s, output: 24.79 toks/s]
Agent 3 response: The correct answer is D) 45,X. Monosomy involves the loss of one chromosome from a pair, resulting i...

--- Problem 1/2, Round 3, Agent 1/3 ---
Agent 1 receiving other agents' responses...
[ModelCache] Using cached model: Qwen/Qwen3-14B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 235.77it/s]
[1;36m(EngineCore_DP0 pid=382488)[0;0m INFO 12-04 13:19:53 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.98:49391 backend=nccl
[W1204 13:19:53.702924472 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-25-0.rc.tch.harvard.edu]:49391 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=382488)[0;0m INFO 12-04 13:19:53 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=382488)[0;0m ERROR 12-04 13:19:53 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=382488)[0;0m ERROR 12-04 13:19:53 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=382488)[0;0m ERROR 12-04 13:19:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=382488)[0;0m ERROR 12-04 13:19:53 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=382488)[0;0m ERROR 12-04 13:19:53 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=382488)[0;0m ERROR 12-04 13:19:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=382488)[0;0m ERROR 12-04 13:19:53 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=382488)[0;0m ERROR 12-04 13:19:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=382488)[0;0m ERROR 12-04 13:19:53 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=382488)[0;0m ERROR 12-04 13:19:53 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=382488)[0;0m ERROR 12-04 13:19:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=382488)[0;0m ERROR 12-04 13:19:53 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=382488)[0;0m ERROR 12-04 13:19:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=382488)[0;0m ERROR 12-04 13:19:53 [core.py:842]     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=382488)[0;0m ERROR 12-04 13:19:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=382488)[0;0m ERROR 12-04 13:19:53 [core.py:842]     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=382488)[0;0m ERROR 12-04 13:19:53 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=382488)[0;0m ERROR 12-04 13:19:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=382488)[0;0m ERROR 12-04 13:19:53 [core.py:842]     raise ValueError(
[1;36m(EngineCore_DP0 pid=382488)[0;0m ERROR 12-04 13:19:53 [core.py:842] ValueError: Free memory on device (2.63/44.39 GiB) on startup is less than desired GPU memory utilization (0.9, 39.95 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[1;36m(EngineCore_DP0 pid=382488)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=382488)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=382488)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=382488)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=382488)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=382488)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=382488)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=382488)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=382488)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=382488)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=382488)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=382488)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=382488)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=382488)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=382488)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=382488)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=382488)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=382488)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=382488)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=382488)[0;0m     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=382488)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=382488)[0;0m     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=382488)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=382488)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=382488)[0;0m     raise ValueError(
[1;36m(EngineCore_DP0 pid=382488)[0;0m ValueError: Free memory on device (2.63/44.39 GiB) on startup is less than desired GPU memory utilization (0.9, 39.95 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W1204 13:19:54.556732664 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:19<00:00, 19.49s/it, est. speed input: 42.48 toks/s, output: 25.03 toks/s]Processed prompts: 100%|██████████| 1/1 [00:19<00:00, 19.49s/it, est. speed input: 42.48 toks/s, output: 25.03 toks/s]Processed prompts: 100%|██████████| 1/1 [00:19<00:00, 19.49s/it, est. speed input: 42.48 toks/s, output: 25.03 toks/s]
Agent 1 response: The correct answer is D) 45,X. Monosomy involves the loss of one chromosome from a pair, resulting i...

--- Problem 1/2, Round 3, Agent 2/3 ---
Agent 2 receiving other agents' responses...
[ModelCache] Using cached model: Qwen/Qwen3-14B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 481.50it/s]
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: Qwen/Qwen3-14B (backend=vllm)
[ModelCache] Using max_model_len=40960 for Qwen/Qwen3-14B
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 13:20:15 [utils.py:253] non-default args: {'max_model_len': 40960, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'Qwen/Qwen3-14B'}
INFO 12-04 13:20:15 [model.py:631] Resolved architecture: Qwen3ForCausalLM
INFO 12-04 13:20:15 [model.py:1745] Using max model len 40960
INFO 12-04 13:20:15 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.52s/it, est. speed input: 110.42 toks/s, output: 24.88 toks/s]Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.52s/it, est. speed input: 110.42 toks/s, output: 24.88 toks/s]Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.52s/it, est. speed input: 110.42 toks/s, output: 24.88 toks/s]
Agent 2 response: (D)...

--- Problem 1/2, Round 3, Agent 3/3 ---
Agent 3 receiving other agents' responses...
[ModelCache] Using cached model: Qwen/Qwen3-14B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 324.26it/s]
[1;36m(EngineCore_DP0 pid=383350)[0;0m INFO 12-04 13:20:26 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='Qwen/Qwen3-14B', speculative_config=None, tokenizer='Qwen/Qwen3-14B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-14B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=383350)[0;0m INFO 12-04 13:20:28 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.98:47161 backend=nccl
[W1204 13:20:28.029124393 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-25-0.rc.tch.harvard.edu]:47161 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=383350)[0;0m INFO 12-04 13:20:28 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=383350)[0;0m ERROR 12-04 13:20:28 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=383350)[0;0m ERROR 12-04 13:20:28 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=383350)[0;0m ERROR 12-04 13:20:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=383350)[0;0m ERROR 12-04 13:20:28 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=383350)[0;0m ERROR 12-04 13:20:28 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=383350)[0;0m ERROR 12-04 13:20:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=383350)[0;0m ERROR 12-04 13:20:28 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=383350)[0;0m ERROR 12-04 13:20:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=383350)[0;0m ERROR 12-04 13:20:28 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=383350)[0;0m ERROR 12-04 13:20:28 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=383350)[0;0m ERROR 12-04 13:20:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=383350)[0;0m ERROR 12-04 13:20:28 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=383350)[0;0m ERROR 12-04 13:20:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=383350)[0;0m ERROR 12-04 13:20:28 [core.py:842]     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=383350)[0;0m ERROR 12-04 13:20:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=383350)[0;0m ERROR 12-04 13:20:28 [core.py:842]     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=383350)[0;0m ERROR 12-04 13:20:28 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=383350)[0;0m ERROR 12-04 13:20:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=383350)[0;0m ERROR 12-04 13:20:28 [core.py:842]     raise ValueError(
[1;36m(EngineCore_DP0 pid=383350)[0;0m ERROR 12-04 13:20:28 [core.py:842] ValueError: Free memory on device (2.63/44.39 GiB) on startup is less than desired GPU memory utilization (0.9, 39.95 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[1;36m(EngineCore_DP0 pid=383350)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=383350)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=383350)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=383350)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=383350)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=383350)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=383350)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=383350)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=383350)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=383350)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=383350)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=383350)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=383350)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=383350)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=383350)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=383350)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=383350)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=383350)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=383350)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=383350)[0;0m     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=383350)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=383350)[0;0m     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=383350)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=383350)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=383350)[0;0m     raise ValueError(
[1;36m(EngineCore_DP0 pid=383350)[0;0m ValueError: Free memory on device (2.63/44.39 GiB) on startup is less than desired GPU memory utilization (0.9, 39.95 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W1204 13:20:29.862372726 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:27<00:00, 27.13s/it, est. speed input: 30.55 toks/s, output: 25.02 toks/s]Processed prompts: 100%|██████████| 1/1 [00:27<00:00, 27.13s/it, est. speed input: 30.55 toks/s, output: 25.02 toks/s]Processed prompts: 100%|██████████| 1/1 [00:27<00:00, 27.13s/it, est. speed input: 30.55 toks/s, output: 25.02 toks/s]
Agent 3 response: The correct answer is D) 45,X. Monosomy is the absence of one chromosome (a single copy instead of t...

Running accuracy: 1.000 ± 0.000

--- Problem 2/2, Round 1, Agent 1/3 ---
[ModelCache] Using cached model: Qwen/Qwen3-14B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1129.32it/s]
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: Qwen/Qwen3-14B (backend=vllm)
[ModelCache] Using max_model_len=40960 for Qwen/Qwen3-14B
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 13:20:50 [utils.py:253] non-default args: {'max_model_len': 40960, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'Qwen/Qwen3-14B'}
INFO 12-04 13:20:50 [model.py:631] Resolved architecture: Qwen3ForCausalLM
INFO 12-04 13:20:50 [model.py:1745] Using max model len 40960
INFO 12-04 13:20:50 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=385771)[0;0m INFO 12-04 13:21:02 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='Qwen/Qwen3-14B', speculative_config=None, tokenizer='Qwen/Qwen3-14B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-14B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=385771)[0;0m INFO 12-04 13:21:03 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.98:35065 backend=nccl
[W1204 13:21:03.071683688 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-25-0.rc.tch.harvard.edu]:35065 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=385771)[0;0m INFO 12-04 13:21:03 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=385771)[0;0m ERROR 12-04 13:21:03 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=385771)[0;0m ERROR 12-04 13:21:03 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=385771)[0;0m ERROR 12-04 13:21:03 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=385771)[0;0m ERROR 12-04 13:21:03 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=385771)[0;0m ERROR 12-04 13:21:03 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=385771)[0;0m ERROR 12-04 13:21:03 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=385771)[0;0m ERROR 12-04 13:21:03 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=385771)[0;0m ERROR 12-04 13:21:03 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=385771)[0;0m ERROR 12-04 13:21:03 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=385771)[0;0m ERROR 12-04 13:21:03 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=385771)[0;0m ERROR 12-04 13:21:03 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=385771)[0;0m ERROR 12-04 13:21:03 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=385771)[0;0m ERROR 12-04 13:21:03 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=385771)[0;0m ERROR 12-04 13:21:03 [core.py:842]     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=385771)[0;0m ERROR 12-04 13:21:03 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=385771)[0;0m ERROR 12-04 13:21:03 [core.py:842]     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=385771)[0;0m ERROR 12-04 13:21:03 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=385771)[0;0m ERROR 12-04 13:21:03 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=385771)[0;0m ERROR 12-04 13:21:03 [core.py:842]     raise ValueError(
[1;36m(EngineCore_DP0 pid=385771)[0;0m ERROR 12-04 13:21:03 [core.py:842] ValueError: Free memory on device (2.63/44.39 GiB) on startup is less than desired GPU memory utilization (0.9, 39.95 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[1;36m(EngineCore_DP0 pid=385771)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=385771)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=385771)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=385771)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=385771)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=385771)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=385771)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=385771)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=385771)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=385771)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=385771)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=385771)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=385771)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=385771)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=385771)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=385771)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=385771)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=385771)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=385771)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=385771)[0;0m     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=385771)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=385771)[0;0m     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=385771)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=385771)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=385771)[0;0m     raise ValueError(
[1;36m(EngineCore_DP0 pid=385771)[0;0m ValueError: Free memory on device (2.63/44.39 GiB) on startup is less than desired GPU memory utilization (0.9, 39.95 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W1204 13:21:04.911339874 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:24<00:00, 24.28s/it, est. speed input: 6.88 toks/s, output: 25.04 toks/s]Processed prompts: 100%|██████████| 1/1 [00:24<00:00, 24.28s/it, est. speed input: 6.88 toks/s, output: 25.04 toks/s]Processed prompts: 100%|██████████| 1/1 [00:24<00:00, 24.28s/it, est. speed input: 6.88 toks/s, output: 25.04 toks/s]
Agent 1 response: The major proprioceptive receptors for the TMJ's position are primarily located in the capsule and l...

--- Problem 2/2, Round 1, Agent 2/3 ---
[ModelCache] Using cached model: Qwen/Qwen3-14B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1222.83it/s]
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: Qwen/Qwen3-14B (backend=vllm)
[ModelCache] Using max_model_len=40960 for Qwen/Qwen3-14B
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 13:21:25 [utils.py:253] non-default args: {'max_model_len': 40960, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'Qwen/Qwen3-14B'}
INFO 12-04 13:21:25 [model.py:631] Resolved architecture: Qwen3ForCausalLM
INFO 12-04 13:21:25 [model.py:1745] Using max model len 40960
INFO 12-04 13:21:25 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:16<00:00, 16.50s/it, est. speed input: 10.36 toks/s, output: 25.21 toks/s]Processed prompts: 100%|██████████| 1/1 [00:16<00:00, 16.50s/it, est. speed input: 10.36 toks/s, output: 25.21 toks/s]Processed prompts: 100%|██████████| 1/1 [00:16<00:00, 16.50s/it, est. speed input: 10.36 toks/s, output: 25.21 toks/s]
Agent 2 response: The moon reflects the sun's light. What is the sound of one hand clapping? (B)...

--- Problem 2/2, Round 1, Agent 3/3 ---
[ModelCache] Using cached model: Qwen/Qwen3-14B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 990.16it/s]
[1;36m(EngineCore_DP0 pid=387038)[0;0m INFO 12-04 13:21:36 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='Qwen/Qwen3-14B', speculative_config=None, tokenizer='Qwen/Qwen3-14B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-14B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=387038)[0;0m INFO 12-04 13:21:38 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.98:42543 backend=nccl
[W1204 13:21:38.792817233 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-25-0.rc.tch.harvard.edu]:42543 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=387038)[0;0m INFO 12-04 13:21:38 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=387038)[0;0m ERROR 12-04 13:21:38 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=387038)[0;0m ERROR 12-04 13:21:38 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=387038)[0;0m ERROR 12-04 13:21:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=387038)[0;0m ERROR 12-04 13:21:38 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=387038)[0;0m ERROR 12-04 13:21:38 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=387038)[0;0m ERROR 12-04 13:21:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=387038)[0;0m ERROR 12-04 13:21:38 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=387038)[0;0m ERROR 12-04 13:21:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=387038)[0;0m ERROR 12-04 13:21:38 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=387038)[0;0m ERROR 12-04 13:21:38 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=387038)[0;0m ERROR 12-04 13:21:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=387038)[0;0m ERROR 12-04 13:21:38 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=387038)[0;0m ERROR 12-04 13:21:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=387038)[0;0m ERROR 12-04 13:21:38 [core.py:842]     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=387038)[0;0m ERROR 12-04 13:21:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=387038)[0;0m ERROR 12-04 13:21:38 [core.py:842]     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=387038)[0;0m ERROR 12-04 13:21:38 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=387038)[0;0m ERROR 12-04 13:21:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=387038)[0;0m ERROR 12-04 13:21:38 [core.py:842]     raise ValueError(
[1;36m(EngineCore_DP0 pid=387038)[0;0m ERROR 12-04 13:21:38 [core.py:842] ValueError: Free memory on device (2.63/44.39 GiB) on startup is less than desired GPU memory utilization (0.9, 39.95 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[1;36m(EngineCore_DP0 pid=387038)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=387038)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=387038)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=387038)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=387038)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=387038)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=387038)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=387038)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=387038)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=387038)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=387038)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=387038)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=387038)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=387038)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=387038)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=387038)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=387038)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=387038)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=387038)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=387038)[0;0m     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=387038)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=387038)[0;0m     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=387038)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=387038)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=387038)[0;0m     raise ValueError(
[1;36m(EngineCore_DP0 pid=387038)[0;0m ValueError: Free memory on device (2.63/44.39 GiB) on startup is less than desired GPU memory utilization (0.9, 39.95 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W1204 13:21:39.639336122 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:20<00:00, 20.84s/it, est. speed input: 8.06 toks/s, output: 25.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:20<00:00, 20.84s/it, est. speed input: 8.06 toks/s, output: 25.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:20<00:00, 20.84s/it, est. speed input: 8.06 toks/s, output: 25.00 toks/s]
Agent 3 response: The major concentrations of proprioceptive receptors in the TMJ are primarily located in the capsule...

--- Problem 2/2, Round 2, Agent 1/3 ---
Agent 1 receiving other agents' responses...
[ModelCache] Using cached model: Qwen/Qwen3-14B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 553.92it/s]
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: Qwen/Qwen3-14B (backend=vllm)
[ModelCache] Using max_model_len=40960 for Qwen/Qwen3-14B
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 13:22:00 [utils.py:253] non-default args: {'max_model_len': 40960, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'Qwen/Qwen3-14B'}
INFO 12-04 13:22:00 [model.py:631] Resolved architecture: Qwen3ForCausalLM
INFO 12-04 13:22:00 [model.py:1745] Using max model len 40960
INFO 12-04 13:22:00 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=388412)[0;0m INFO 12-04 13:22:12 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='Qwen/Qwen3-14B', speculative_config=None, tokenizer='Qwen/Qwen3-14B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-14B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=388412)[0;0m INFO 12-04 13:22:13 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.98:43387 backend=nccl
[W1204 13:22:13.958432559 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-25-0.rc.tch.harvard.edu]:43387 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=388412)[0;0m INFO 12-04 13:22:13 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=388412)[0;0m ERROR 12-04 13:22:13 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=388412)[0;0m ERROR 12-04 13:22:13 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=388412)[0;0m ERROR 12-04 13:22:13 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=388412)[0;0m ERROR 12-04 13:22:13 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=388412)[0;0m ERROR 12-04 13:22:13 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=388412)[0;0m ERROR 12-04 13:22:13 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=388412)[0;0m ERROR 12-04 13:22:13 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=388412)[0;0m ERROR 12-04 13:22:13 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=388412)[0;0m ERROR 12-04 13:22:13 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=388412)[0;0m ERROR 12-04 13:22:13 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=388412)[0;0m ERROR 12-04 13:22:13 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=388412)[0;0m ERROR 12-04 13:22:13 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=388412)[0;0m ERROR 12-04 13:22:13 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=388412)[0;0m ERROR 12-04 13:22:13 [core.py:842]     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=388412)[0;0m ERROR 12-04 13:22:13 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=388412)[0;0m ERROR 12-04 13:22:13 [core.py:842]     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=388412)[0;0m ERROR 12-04 13:22:13 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=388412)[0;0m ERROR 12-04 13:22:13 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=388412)[0;0m ERROR 12-04 13:22:13 [core.py:842]     raise ValueError(
[1;36m(EngineCore_DP0 pid=388412)[0;0m ERROR 12-04 13:22:13 [core.py:842] ValueError: Free memory on device (2.63/44.39 GiB) on startup is less than desired GPU memory utilization (0.9, 39.95 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[1;36m(EngineCore_DP0 pid=388412)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=388412)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=388412)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=388412)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=388412)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=388412)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=388412)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=388412)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=388412)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=388412)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=388412)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=388412)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=388412)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=388412)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=388412)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=388412)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=388412)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=388412)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=388412)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=388412)[0;0m     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=388412)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=388412)[0;0m     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=388412)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=388412)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=388412)[0;0m     raise ValueError(
[1;36m(EngineCore_DP0 pid=388412)[0;0m ValueError: Free memory on device (2.63/44.39 GiB) on startup is less than desired GPU memory utilization (0.9, 39.95 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W1204 13:22:14.762481175 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:27<00:00, 27.99s/it, est. speed input: 17.51 toks/s, output: 25.01 toks/s]Processed prompts: 100%|██████████| 1/1 [00:27<00:00, 27.99s/it, est. speed input: 17.51 toks/s, output: 25.01 toks/s]Processed prompts: 100%|██████████| 1/1 [00:27<00:00, 27.99s/it, est. speed input: 17.51 toks/s, output: 25.01 toks/s]
Agent 1 response: The major proprioceptive receptors for the TMJ are located in the **capsule and ligaments**, which a...

--- Problem 2/2, Round 2, Agent 2/3 ---
Agent 2 receiving other agents' responses...
[ModelCache] Using cached model: Qwen/Qwen3-14B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 625.08it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:12<00:00, 12.53s/it, est. speed input: 39.44 toks/s, output: 25.15 toks/s]Processed prompts: 100%|██████████| 1/1 [00:12<00:00, 12.53s/it, est. speed input: 39.44 toks/s, output: 25.15 toks/s]Processed prompts: 100%|██████████| 1/1 [00:12<00:00, 12.53s/it, est. speed input: 39.44 toks/s, output: 25.15 toks/s]
Agent 2 response: The wind does not choose the mountain. The jaw moves, but the moon remains still. (B)...

--- Problem 2/2, Round 2, Agent 3/3 ---
Agent 3 receiving other agents' responses...
[ModelCache] Using cached model: Qwen/Qwen3-14B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 656.18it/s]
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: Qwen/Qwen3-14B (backend=vllm)
[ModelCache] Using max_model_len=40960 for Qwen/Qwen3-14B
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 13:22:35 [utils.py:253] non-default args: {'max_model_len': 40960, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'Qwen/Qwen3-14B'}
INFO 12-04 13:22:35 [model.py:631] Resolved architecture: Qwen3ForCausalLM
INFO 12-04 13:22:35 [model.py:1745] Using max model len 40960
INFO 12-04 13:22:35 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=390878)[0;0m INFO 12-04 13:22:46 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='Qwen/Qwen3-14B', speculative_config=None, tokenizer='Qwen/Qwen3-14B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-14B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:18<00:00, 18.97s/it, est. speed input: 25.89 toks/s, output: 24.94 toks/s]Processed prompts: 100%|██████████| 1/1 [00:18<00:00, 18.97s/it, est. speed input: 25.89 toks/s, output: 24.94 toks/s]Processed prompts: 100%|██████████| 1/1 [00:18<00:00, 18.97s/it, est. speed input: 25.89 toks/s, output: 24.94 toks/s]
Agent 3 response: The major proprioceptive receptors for TMJ position are in the capsule and ligaments (which contain ...

--- Problem 2/2, Round 3, Agent 1/3 ---
Agent 1 receiving other agents' responses...
[ModelCache] Using cached model: Qwen/Qwen3-14B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 228.85it/s]
[1;36m(EngineCore_DP0 pid=390878)[0;0m INFO 12-04 13:22:48 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.98:57617 backend=nccl
[W1204 13:22:48.042933117 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-25-0.rc.tch.harvard.edu]:57617 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=390878)[0;0m INFO 12-04 13:22:48 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=390878)[0;0m ERROR 12-04 13:22:48 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=390878)[0;0m ERROR 12-04 13:22:48 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=390878)[0;0m ERROR 12-04 13:22:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=390878)[0;0m ERROR 12-04 13:22:48 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=390878)[0;0m ERROR 12-04 13:22:48 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=390878)[0;0m ERROR 12-04 13:22:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=390878)[0;0m ERROR 12-04 13:22:48 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=390878)[0;0m ERROR 12-04 13:22:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=390878)[0;0m ERROR 12-04 13:22:48 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=390878)[0;0m ERROR 12-04 13:22:48 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=390878)[0;0m ERROR 12-04 13:22:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=390878)[0;0m ERROR 12-04 13:22:48 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=390878)[0;0m ERROR 12-04 13:22:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=390878)[0;0m ERROR 12-04 13:22:48 [core.py:842]     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=390878)[0;0m ERROR 12-04 13:22:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=390878)[0;0m ERROR 12-04 13:22:48 [core.py:842]     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=390878)[0;0m ERROR 12-04 13:22:48 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=390878)[0;0m ERROR 12-04 13:22:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=390878)[0;0m ERROR 12-04 13:22:48 [core.py:842]     raise ValueError(
[1;36m(EngineCore_DP0 pid=390878)[0;0m ERROR 12-04 13:22:48 [core.py:842] ValueError: Free memory on device (2.63/44.39 GiB) on startup is less than desired GPU memory utilization (0.9, 39.95 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[1;36m(EngineCore_DP0 pid=390878)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=390878)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=390878)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=390878)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=390878)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=390878)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=390878)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=390878)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=390878)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=390878)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=390878)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=390878)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=390878)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=390878)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=390878)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=390878)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=390878)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=390878)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=390878)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=390878)[0;0m     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=390878)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=390878)[0;0m     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=390878)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=390878)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=390878)[0;0m     raise ValueError(
[1;36m(EngineCore_DP0 pid=390878)[0;0m ValueError: Free memory on device (2.63/44.39 GiB) on startup is less than desired GPU memory utilization (0.9, 39.95 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W1204 13:22:49.870201360 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:17<00:00, 17.23s/it, est. speed input: 49.52 toks/s, output: 25.02 toks/s]Processed prompts: 100%|██████████| 1/1 [00:17<00:00, 17.23s/it, est. speed input: 49.52 toks/s, output: 25.02 toks/s]Processed prompts: 100%|██████████| 1/1 [00:17<00:00, 17.23s/it, est. speed input: 49.52 toks/s, output: 25.02 toks/s]
Agent 1 response: The anatomical evidence confirms that the **capsule and ligaments** of the TMJ contain rich mechanor...

--- Problem 2/2, Round 3, Agent 2/3 ---
Agent 2 receiving other agents' responses...
[ModelCache] Using cached model: Qwen/Qwen3-14B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 419.39it/s]
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: Qwen/Qwen3-14B (backend=vllm)
[ModelCache] Using max_model_len=40960 for Qwen/Qwen3-14B
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 13:23:10 [utils.py:253] non-default args: {'max_model_len': 40960, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'Qwen/Qwen3-14B'}
INFO 12-04 13:23:10 [model.py:631] Resolved architecture: Qwen3ForCausalLM
INFO 12-04 13:23:10 [model.py:1745] Using max model len 40960
INFO 12-04 13:23:10 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:10<00:00, 10.17s/it, est. speed input: 84.24 toks/s, output: 24.87 toks/s]Processed prompts: 100%|██████████| 1/1 [00:10<00:00, 10.17s/it, est. speed input: 84.24 toks/s, output: 24.87 toks/s]Processed prompts: 100%|██████████| 1/1 [00:10<00:00, 10.17s/it, est. speed input: 84.24 toks/s, output: 24.87 toks/s]
Agent 2 response: The river flows without asking the mountain. (B)...

--- Problem 2/2, Round 3, Agent 3/3 ---
Agent 3 receiving other agents' responses...
[ModelCache] Using cached model: Qwen/Qwen3-14B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 294.11it/s]
[1;36m(EngineCore_DP0 pid=392150)[0;0m INFO 12-04 13:23:22 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='Qwen/Qwen3-14B', speculative_config=None, tokenizer='Qwen/Qwen3-14B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-14B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=392150)[0;0m INFO 12-04 13:23:24 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.98:60215 backend=nccl
[W1204 13:23:24.556990096 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-25-0.rc.tch.harvard.edu]:60215 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=392150)[0;0m INFO 12-04 13:23:24 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=392150)[0;0m ERROR 12-04 13:23:24 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=392150)[0;0m ERROR 12-04 13:23:24 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=392150)[0;0m ERROR 12-04 13:23:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=392150)[0;0m ERROR 12-04 13:23:24 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=392150)[0;0m ERROR 12-04 13:23:24 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=392150)[0;0m ERROR 12-04 13:23:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=392150)[0;0m ERROR 12-04 13:23:24 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=392150)[0;0m ERROR 12-04 13:23:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=392150)[0;0m ERROR 12-04 13:23:24 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=392150)[0;0m ERROR 12-04 13:23:24 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=392150)[0;0m ERROR 12-04 13:23:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=392150)[0;0m ERROR 12-04 13:23:24 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=392150)[0;0m ERROR 12-04 13:23:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=392150)[0;0m ERROR 12-04 13:23:24 [core.py:842]     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=392150)[0;0m ERROR 12-04 13:23:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=392150)[0;0m ERROR 12-04 13:23:24 [core.py:842]     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=392150)[0;0m ERROR 12-04 13:23:24 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=392150)[0;0m ERROR 12-04 13:23:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=392150)[0;0m ERROR 12-04 13:23:24 [core.py:842]     raise ValueError(
[1;36m(EngineCore_DP0 pid=392150)[0;0m ERROR 12-04 13:23:24 [core.py:842] ValueError: Free memory on device (2.63/44.39 GiB) on startup is less than desired GPU memory utilization (0.9, 39.95 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[1;36m(EngineCore_DP0 pid=392150)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=392150)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=392150)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=392150)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=392150)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=392150)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=392150)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=392150)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=392150)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=392150)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=392150)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=392150)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=392150)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=392150)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=392150)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=392150)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=392150)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=392150)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=392150)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=392150)[0;0m     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=392150)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=392150)[0;0m     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=392150)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=392150)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=392150)[0;0m     raise ValueError(
[1;36m(EngineCore_DP0 pid=392150)[0;0m ValueError: Free memory on device (2.63/44.39 GiB) on startup is less than desired GPU memory utilization (0.9, 39.95 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W1204 13:23:24.407834226 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:22<00:00, 22.21s/it, est. speed input: 38.46 toks/s, output: 24.95 toks/s]Processed prompts: 100%|██████████| 1/1 [00:22<00:00, 22.21s/it, est. speed input: 38.46 toks/s, output: 24.95 toks/s]Processed prompts: 100%|██████████| 1/1 [00:22<00:00, 22.21s/it, est. speed input: 38.46 toks/s, output: 24.95 toks/s]
[rank0]:[W1204 13:23:37.986009150 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Agent 3 response: The major proprioceptive receptors in the TMJ are found in the **capsule and ligaments** (rich in Ru...

Running accuracy: 1.000 ± 0.000

============================================================
GENERATION & EVALUATION COMPLETE
============================================================
Results saved to: /home/ch269957/projects/slm_multiagent_debate/experiments/linux_single/results/mmlu/mmlu_Qwen3-14B_persona_enigma+zen+deep-sea_agents3_rounds3.json
Questions processed: 2
Final accuracy: 1.000 ± 0.000
============================================================
[ModelCache] Shut down vLLM model: vllm:Qwen/Qwen3-14B
[ModelCache] All models shut down
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: Qwen/Qwen3-14B (backend=vllm)
[ModelCache] Using max_model_len=40960 for Qwen/Qwen3-14B
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 13:23:45 [utils.py:253] non-default args: {'max_model_len': 40960, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'Qwen/Qwen3-14B'}
INFO 12-04 13:23:46 [model.py:631] Resolved architecture: Qwen3ForCausalLM
INFO 12-04 13:23:46 [model.py:1745] Using max model len 40960
INFO 12-04 13:23:46 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=392938)[0;0m INFO 12-04 13:23:56 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='Qwen/Qwen3-14B', speculative_config=None, tokenizer='Qwen/Qwen3-14B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-14B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=392938)[0;0m INFO 12-04 13:23:58 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.98:36501 backend=nccl
[W1204 13:23:58.164224124 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-25-0.rc.tch.harvard.edu]:36501 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=392938)[0;0m INFO 12-04 13:23:58 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=392938)[0;0m INFO 12-04 13:23:58 [gpu_model_runner.py:3259] Starting to load model Qwen/Qwen3-14B...
[1;36m(EngineCore_DP0 pid=392938)[0;0m INFO 12-04 13:23:59 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=392938)[0;0m INFO 12-04 13:23:59 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=392938)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/8 [00:00<?, ?it/s]
[1;36m(EngineCore_DP0 pid=392938)[0;0m Loading safetensors checkpoint shards:  12% Completed | 1/8 [00:00<00:04,  1.61it/s]
[1;36m(EngineCore_DP0 pid=392938)[0;0m Loading safetensors checkpoint shards:  25% Completed | 2/8 [00:01<00:04,  1.49it/s]
[1;36m(EngineCore_DP0 pid=392938)[0;0m Loading safetensors checkpoint shards:  38% Completed | 3/8 [00:02<00:03,  1.45it/s]
[1;36m(EngineCore_DP0 pid=392938)[0;0m Loading safetensors checkpoint shards:  50% Completed | 4/8 [00:02<00:02,  1.40it/s]
[1;36m(EngineCore_DP0 pid=392938)[0;0m Loading safetensors checkpoint shards:  62% Completed | 5/8 [00:03<00:02,  1.39it/s]
[1;36m(EngineCore_DP0 pid=392938)[0;0m Loading safetensors checkpoint shards:  75% Completed | 6/8 [00:04<00:01,  1.39it/s]
[1;36m(EngineCore_DP0 pid=392938)[0;0m Loading safetensors checkpoint shards:  88% Completed | 7/8 [00:04<00:00,  1.39it/s]
[1;36m(EngineCore_DP0 pid=392938)[0;0m Loading safetensors checkpoint shards: 100% Completed | 8/8 [00:05<00:00,  1.62it/s]
[1;36m(EngineCore_DP0 pid=392938)[0;0m Loading safetensors checkpoint shards: 100% Completed | 8/8 [00:05<00:00,  1.49it/s]
[1;36m(EngineCore_DP0 pid=392938)[0;0m 
[1;36m(EngineCore_DP0 pid=392938)[0;0m INFO 12-04 13:24:05 [default_loader.py:314] Loading weights took 5.41 seconds
[1;36m(EngineCore_DP0 pid=392938)[0;0m INFO 12-04 13:24:05 [gpu_model_runner.py:3338] Model loading took 27.5185 GiB memory and 6.195488 seconds
[1;36m(EngineCore_DP0 pid=392938)[0;0m INFO 12-04 13:24:14 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/76690ce19a/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=392938)[0;0m INFO 12-04 13:24:14 [backends.py:647] Dynamo bytecode transform time: 8.43 s
[1;36m(EngineCore_DP0 pid=392938)[0;0m INFO 12-04 13:24:19 [backends.py:210] Directly load the compiled graph(s) for dynamic shape from the cache, took 4.727 s
[1;36m(EngineCore_DP0 pid=392938)[0;0m INFO 12-04 13:24:21 [monitor.py:34] torch.compile takes 13.16 s in total
[1;36m(EngineCore_DP0 pid=392938)[0;0m INFO 12-04 13:24:23 [gpu_worker.py:359] Available KV cache memory: 10.93 GiB
[1;36m(EngineCore_DP0 pid=392938)[0;0m INFO 12-04 13:24:23 [kv_cache_utils.py:1229] GPU KV cache size: 71,616 tokens
[1;36m(EngineCore_DP0 pid=392938)[0;0m INFO 12-04 13:24:23 [kv_cache_utils.py:1234] Maximum concurrency for 40,960 tokens per request: 1.75x
[1;36m(EngineCore_DP0 pid=392938)[0;0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/51 [00:00<?, ?it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   2%|▏         | 1/51 [00:00<00:06,  7.49it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   4%|▍         | 2/51 [00:00<00:06,  7.43it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 3/51 [00:00<00:06,  7.38it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|▊         | 4/51 [00:00<00:06,  7.49it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  10%|▉         | 5/51 [00:00<00:06,  7.59it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  12%|█▏        | 6/51 [00:00<00:05,  7.68it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▎        | 7/51 [00:00<00:05,  7.75it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|█▌        | 8/51 [00:01<00:05,  7.79it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 9/51 [00:01<00:05,  8.23it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|█▉        | 10/51 [00:01<00:04,  8.54it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|██▏       | 11/51 [00:01<00:04,  8.79it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  24%|██▎       | 12/51 [00:01<00:04,  9.03it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 13/51 [00:01<00:04,  9.13it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|██▋       | 14/51 [00:01<00:04,  9.21it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▉       | 15/51 [00:01<00:03,  9.34it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███▏      | 16/51 [00:01<00:03,  9.45it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  35%|███▌      | 18/51 [00:02<00:03, 10.49it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  39%|███▉      | 20/51 [00:02<00:02, 11.11it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 22/51 [00:02<00:02, 11.51it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 24/51 [00:02<00:02, 11.85it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|█████     | 26/51 [00:02<00:02, 12.30it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▍    | 28/51 [00:02<00:01, 12.66it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  59%|█████▉    | 30/51 [00:02<00:01, 12.92it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 32/51 [00:03<00:01, 13.08it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 34/51 [00:03<00:01, 13.14it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████   | 36/51 [00:03<00:01, 13.36it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▍  | 38/51 [00:03<00:00, 13.67it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  78%|███████▊  | 40/51 [00:03<00:00, 13.88it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|████████▏ | 42/51 [00:03<00:00, 14.07it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▋ | 44/51 [00:03<00:00, 14.31it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  90%|█████████ | 46/51 [00:04<00:00, 14.51it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|█████████▍| 48/51 [00:04<00:00, 14.91it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  98%|█████████▊| 50/51 [00:04<00:00, 15.19it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:04<00:00, 11.51it/s]
[1;36m(EngineCore_DP0 pid=392938)[0;0m Capturing CUDA graphs (decode, FULL):   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graphs (decode, FULL):   6%|▌         | 2/35 [00:00<00:02, 12.03it/s]Capturing CUDA graphs (decode, FULL):  11%|█▏        | 4/35 [00:00<00:02, 12.30it/s]Capturing CUDA graphs (decode, FULL):  17%|█▋        | 6/35 [00:00<00:02, 12.51it/s]Capturing CUDA graphs (decode, FULL):  23%|██▎       | 8/35 [00:00<00:02, 12.60it/s]Capturing CUDA graphs (decode, FULL):  29%|██▊       | 10/35 [00:00<00:01, 13.01it/s]Capturing CUDA graphs (decode, FULL):  34%|███▍      | 12/35 [00:00<00:01, 13.26it/s]Capturing CUDA graphs (decode, FULL):  40%|████      | 14/35 [00:01<00:01, 13.43it/s]Capturing CUDA graphs (decode, FULL):  46%|████▌     | 16/35 [00:01<00:01, 13.53it/s]Capturing CUDA graphs (decode, FULL):  51%|█████▏    | 18/35 [00:01<00:01, 13.76it/s]Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 20/35 [00:01<00:01, 13.97it/s]Capturing CUDA graphs (decode, FULL):  63%|██████▎   | 22/35 [00:01<00:00, 14.27it/s]Capturing CUDA graphs (decode, FULL):  69%|██████▊   | 24/35 [00:01<00:00, 14.62it/s]Capturing CUDA graphs (decode, FULL):  74%|███████▍  | 26/35 [00:01<00:00, 15.04it/s]Capturing CUDA graphs (decode, FULL):  80%|████████  | 28/35 [00:02<00:00, 15.34it/s]Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 30/35 [00:02<00:00, 15.61it/s]Capturing CUDA graphs (decode, FULL):  91%|█████████▏| 32/35 [00:02<00:00, 16.23it/s]Capturing CUDA graphs (decode, FULL):  97%|█████████▋| 34/35 [00:02<00:00, 16.67it/s]Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:02<00:00, 14.52it/s]
[1;36m(EngineCore_DP0 pid=392938)[0;0m INFO 12-04 13:24:31 [gpu_model_runner.py:4244] Graph capturing finished in 7 secs, took 0.65 GiB
[1;36m(EngineCore_DP0 pid=392938)[0;0m INFO 12-04 13:24:31 [core.py:250] init engine (profile, create kv cache, warmup model) took 25.37 seconds
INFO 12-04 13:24:32 [llm.py:352] Supported tasks: ['generate']
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 643.69it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:20<00:00, 20.78s/it, est. speed input: 5.39 toks/s, output: 25.17 toks/s]Processed prompts: 100%|██████████| 1/1 [00:20<00:00, 20.78s/it, est. speed input: 5.39 toks/s, output: 25.17 toks/s]Processed prompts: 100%|██████████| 1/1 [00:20<00:00, 20.78s/it, est. speed input: 5.39 toks/s, output: 25.17 toks/s]
Agent 1 response: The correct answer is D) 45,X. Monosomy refers to the presence of a single copy of a chromosome inst...

--- Problem 1/2, Round 1, Agent 2/3 ---
[ModelCache] Using cached model: Qwen/Qwen3-14B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1274.48it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:14<00:00, 14.53s/it, est. speed input: 7.99 toks/s, output: 25.27 toks/s]Processed prompts: 100%|██████████| 1/1 [00:14<00:00, 14.53s/it, est. speed input: 7.99 toks/s, output: 25.27 toks/s]Processed prompts: 100%|██████████| 1/1 [00:14<00:00, 14.53s/it, est. speed input: 7.99 toks/s, output: 25.27 toks/s]
Agent 2 response: (D) 45,X. Monosomy refers to the presence of one chromosome instead of the usual two. 45,X indicates...

--- Problem 1/2, Round 1, Agent 3/3 ---
[ModelCache] Using cached model: Qwen/Qwen3-14B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1380.61it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:22<00:00, 22.92s/it, est. speed input: 4.93 toks/s, output: 25.26 toks/s]Processed prompts: 100%|██████████| 1/1 [00:22<00:00, 22.92s/it, est. speed input: 4.93 toks/s, output: 25.26 toks/s]Processed prompts: 100%|██████████| 1/1 [00:22<00:00, 22.92s/it, est. speed input: 4.93 toks/s, output: 25.26 toks/s]
Agent 3 response: The correct answer is D) 45,X. Monosomy refers to the presence of only one copy of a chromosome inst...

--- Problem 1/2, Round 2, Agent 1/3 ---
Agent 1 receiving other agents' responses...
[ModelCache] Using cached model: Qwen/Qwen3-14B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 576.62it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:20<00:00, 20.83s/it, est. speed input: 24.58 toks/s, output: 25.06 toks/s]Processed prompts: 100%|██████████| 1/1 [00:20<00:00, 20.83s/it, est. speed input: 24.58 toks/s, output: 25.06 toks/s]Processed prompts: 100%|██████████| 1/1 [00:20<00:00, 20.83s/it, est. speed input: 24.58 toks/s, output: 25.06 toks/s]
Agent 1 response: The correct answer is D) 45,X. Monosomy is defined as the absence of one chromosome (only one copy i...

--- Problem 1/2, Round 2, Agent 2/3 ---
Agent 2 receiving other agents' responses...
[ModelCache] Using cached model: Qwen/Qwen3-14B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 661.25it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:09<00:00,  9.88s/it, est. speed input: 52.12 toks/s, output: 25.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:09<00:00,  9.88s/it, est. speed input: 52.12 toks/s, output: 25.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:09<00:00,  9.88s/it, est. speed input: 52.12 toks/s, output: 25.00 toks/s]
Agent 2 response: (D)...

--- Problem 1/2, Round 2, Agent 3/3 ---
Agent 3 receiving other agents' responses...
[ModelCache] Using cached model: Qwen/Qwen3-14B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 695.11it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:13<00:00, 13.58s/it, est. speed input: 37.78 toks/s, output: 25.04 toks/s]Processed prompts: 100%|██████████| 1/1 [00:13<00:00, 13.58s/it, est. speed input: 37.78 toks/s, output: 25.04 toks/s]Processed prompts: 100%|██████████| 1/1 [00:13<00:00, 13.58s/it, est. speed input: 37.78 toks/s, output: 25.04 toks/s]
Agent 3 response: The correct answer is D) 45,X. Monosomy involves the loss of one chromosome from a pair, resulting i...

--- Problem 1/2, Round 3, Agent 1/3 ---
Agent 1 receiving other agents' responses...
[ModelCache] Using cached model: Qwen/Qwen3-14B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 471.69it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:19<00:00, 19.37s/it, est. speed input: 42.74 toks/s, output: 25.19 toks/s]Processed prompts: 100%|██████████| 1/1 [00:19<00:00, 19.37s/it, est. speed input: 42.74 toks/s, output: 25.19 toks/s]Processed prompts: 100%|██████████| 1/1 [00:19<00:00, 19.37s/it, est. speed input: 42.74 toks/s, output: 25.19 toks/s]
Agent 1 response: The correct answer is D) 45,X. Monosomy involves the loss of one chromosome from a pair, resulting i...

--- Problem 1/2, Round 3, Agent 2/3 ---
Agent 2 receiving other agents' responses...
[ModelCache] Using cached model: Qwen/Qwen3-14B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 509.45it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.45s/it, est. speed input: 111.45 toks/s, output: 25.11 toks/s]Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.45s/it, est. speed input: 111.45 toks/s, output: 25.11 toks/s]Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.45s/it, est. speed input: 111.45 toks/s, output: 25.11 toks/s]
Agent 2 response: (D)...

--- Problem 1/2, Round 3, Agent 3/3 ---
Agent 3 receiving other agents' responses...
[ModelCache] Using cached model: Qwen/Qwen3-14B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 512.44it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:27<00:00, 27.03s/it, est. speed input: 30.67 toks/s, output: 25.12 toks/s]Processed prompts: 100%|██████████| 1/1 [00:27<00:00, 27.03s/it, est. speed input: 30.67 toks/s, output: 25.12 toks/s]Processed prompts: 100%|██████████| 1/1 [00:27<00:00, 27.03s/it, est. speed input: 30.67 toks/s, output: 25.12 toks/s]
Agent 3 response: The correct answer is D) 45,X. Monosomy is the absence of one chromosome (a single copy instead of t...

Running accuracy: 1.000 ± 0.000

--- Problem 2/2, Round 1, Agent 1/3 ---
[ModelCache] Using cached model: Qwen/Qwen3-14B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1176.52it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:24<00:00, 24.17s/it, est. speed input: 6.91 toks/s, output: 25.16 toks/s]Processed prompts: 100%|██████████| 1/1 [00:24<00:00, 24.17s/it, est. speed input: 6.91 toks/s, output: 25.16 toks/s]Processed prompts: 100%|██████████| 1/1 [00:24<00:00, 24.17s/it, est. speed input: 6.91 toks/s, output: 25.16 toks/s]
Agent 1 response: The major proprioceptive receptors for the TMJ's position are primarily located in the capsule and l...

--- Problem 2/2, Round 1, Agent 2/3 ---
[ModelCache] Using cached model: Qwen/Qwen3-14B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1207.69it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:16<00:00, 16.52s/it, est. speed input: 10.35 toks/s, output: 25.18 toks/s]Processed prompts: 100%|██████████| 1/1 [00:16<00:00, 16.52s/it, est. speed input: 10.35 toks/s, output: 25.18 toks/s]Processed prompts: 100%|██████████| 1/1 [00:16<00:00, 16.52s/it, est. speed input: 10.35 toks/s, output: 25.18 toks/s]
Agent 2 response: The moon reflects the sun's light. What is the sound of one hand clapping? (B)...

--- Problem 2/2, Round 1, Agent 3/3 ---
[ModelCache] Using cached model: Qwen/Qwen3-14B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1242.39it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:20<00:00, 20.65s/it, est. speed input: 8.13 toks/s, output: 25.23 toks/s]Processed prompts: 100%|██████████| 1/1 [00:20<00:00, 20.65s/it, est. speed input: 8.13 toks/s, output: 25.23 toks/s]Processed prompts: 100%|██████████| 1/1 [00:20<00:00, 20.65s/it, est. speed input: 8.13 toks/s, output: 25.23 toks/s]
Agent 3 response: The major concentrations of proprioceptive receptors in the TMJ are primarily located in the capsule...

--- Problem 2/2, Round 2, Agent 1/3 ---
Agent 1 receiving other agents' responses...
[ModelCache] Using cached model: Qwen/Qwen3-14B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 618.17it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:27<00:00, 27.86s/it, est. speed input: 17.59 toks/s, output: 25.13 toks/s]Processed prompts: 100%|██████████| 1/1 [00:27<00:00, 27.86s/it, est. speed input: 17.59 toks/s, output: 25.13 toks/s]Processed prompts: 100%|██████████| 1/1 [00:27<00:00, 27.86s/it, est. speed input: 17.59 toks/s, output: 25.13 toks/s]
Agent 1 response: The major proprioceptive receptors for the TMJ are located in the **capsule and ligaments**, which a...

--- Problem 2/2, Round 2, Agent 2/3 ---
Agent 2 receiving other agents' responses...
[ModelCache] Using cached model: Qwen/Qwen3-14B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 664.60it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:12<00:00, 12.59s/it, est. speed input: 39.24 toks/s, output: 25.02 toks/s]Processed prompts: 100%|██████████| 1/1 [00:12<00:00, 12.59s/it, est. speed input: 39.24 toks/s, output: 25.02 toks/s]Processed prompts: 100%|██████████| 1/1 [00:12<00:00, 12.59s/it, est. speed input: 39.24 toks/s, output: 25.02 toks/s]
Agent 2 response: The wind does not choose the mountain. The jaw moves, but the moon remains still. (B)...

--- Problem 2/2, Round 2, Agent 3/3 ---
Agent 3 receiving other agents' responses...
[ModelCache] Using cached model: Qwen/Qwen3-14B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 610.79it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:18<00:00, 18.88s/it, est. speed input: 26.01 toks/s, output: 25.05 toks/s]Processed prompts: 100%|██████████| 1/1 [00:18<00:00, 18.88s/it, est. speed input: 26.01 toks/s, output: 25.05 toks/s]Processed prompts: 100%|██████████| 1/1 [00:18<00:00, 18.88s/it, est. speed input: 26.01 toks/s, output: 25.05 toks/s]
Agent 3 response: The major proprioceptive receptors for TMJ position are in the capsule and ligaments (which contain ...

--- Problem 2/2, Round 3, Agent 1/3 ---
Agent 1 receiving other agents' responses...
[ModelCache] Using cached model: Qwen/Qwen3-14B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 429.79it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:17<00:00, 17.15s/it, est. speed input: 49.73 toks/s, output: 25.13 toks/s]Processed prompts: 100%|██████████| 1/1 [00:17<00:00, 17.15s/it, est. speed input: 49.73 toks/s, output: 25.13 toks/s]Processed prompts: 100%|██████████| 1/1 [00:17<00:00, 17.15s/it, est. speed input: 49.73 toks/s, output: 25.13 toks/s]
Agent 1 response: The anatomical evidence confirms that the **capsule and ligaments** of the TMJ contain rich mechanor...

--- Problem 2/2, Round 3, Agent 2/3 ---
Agent 2 receiving other agents' responses...
[ModelCache] Using cached model: Qwen/Qwen3-14B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 441.83it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:10<00:00, 10.12s/it, est. speed input: 84.69 toks/s, output: 25.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:10<00:00, 10.12s/it, est. speed input: 84.69 toks/s, output: 25.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:10<00:00, 10.12s/it, est. speed input: 84.69 toks/s, output: 25.00 toks/s]
Agent 2 response: The river flows without asking the mountain. (B)...

--- Problem 2/2, Round 3, Agent 3/3 ---
Agent 3 receiving other agents' responses...
[ModelCache] Using cached model: Qwen/Qwen3-14B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 442.34it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:22<00:00, 22.17s/it, est. speed input: 38.52 toks/s, output: 24.99 toks/s]Processed prompts: 100%|██████████| 1/1 [00:22<00:00, 22.17s/it, est. speed input: 38.52 toks/s, output: 24.99 toks/s]Processed prompts: 100%|██████████| 1/1 [00:22<00:00, 22.17s/it, est. speed input: 38.52 toks/s, output: 24.99 toks/s]
[rank0]:[W1204 13:29:59.036486126 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Agent 3 response: The major proprioceptive receptors in the TMJ are found in the **capsule and ligaments** (rich in Ru...

Running accuracy: 1.000 ± 0.000

============================================================
GENERATION & EVALUATION COMPLETE
============================================================
Results saved to: /home/ch269957/projects/slm_multiagent_debate/experiments/linux_single/results/mmlu/mmlu_Qwen3-14B_persona_enigma+zen+deep-sea_agents3_rounds3.json
Questions processed: 2
Final accuracy: 1.000 ± 0.000
============================================================
[ModelCache] Shut down vLLM model: vllm:Qwen/Qwen3-14B
[ModelCache] All models shut down
