Using persona diversity with 3 different personas
============================================================
Math Task - Multiagent Debate (NO COMPRESSION)
============================================================
Model: Qwen/Qwen3-14B
Persona diversity mode:
  Agent 1: an enigma machine operator whose primary filter is signal-to...
  Agent 2: a Zen master who communicates only through non-sequiturs, ko...
  Agent 3: a deep-sea volcanologist focused on extremes of pressure, he...
Agents: 3
Rounds: 3
Problems: 2
Generation params: {'temperature': 1.0, 'max_tokens': None, 'top_p': 1.0, 'n': 1}
============================================================
Using persona diversity with 3 different personas
============================================================
Math Task - Multiagent Debate (NO COMPRESSION)
============================================================
Model: Qwen/Qwen3-14B
Persona diversity mode:
  Agent 1: an enigma machine operator whose primary filter is signal-to...
  Agent 2: a Zen master who communicates only through non-sequiturs, ko...
  Agent 3: a deep-sea volcanologist focused on extremes of pressure, he...
Agents: 3
Rounds: 3
Problems: 2
Generation params: {'temperature': 1.0, 'max_tokens': None, 'top_p': 1.0, 'n': 1}
============================================================

--- Problem 1/2, Round 1, Agent 1/3 ---
[ModelCache] Loading model: Qwen/Qwen3-14B (backend=vllm)
[ModelCache] Using max_model_len=40960 for Qwen/Qwen3-14B
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 13:12:05 [utils.py:253] non-default args: {'max_model_len': 40960, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'Qwen/Qwen3-14B'}

--- Problem 1/2, Round 1, Agent 1/3 ---
[ModelCache] Loading model: Qwen/Qwen3-14B (backend=vllm)
[ModelCache] Using max_model_len=40960 for Qwen/Qwen3-14B
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 13:12:05 [utils.py:253] non-default args: {'max_model_len': 40960, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'Qwen/Qwen3-14B'}
INFO 12-04 13:12:05 [model.py:631] Resolved architecture: Qwen3ForCausalLM
INFO 12-04 13:12:05 [model.py:1745] Using max model len 40960
INFO 12-04 13:12:05 [model.py:631] Resolved architecture: Qwen3ForCausalLM
INFO 12-04 13:12:05 [model.py:1745] Using max model len 40960
INFO 12-04 13:12:06 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
INFO 12-04 13:12:06 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-04 13:12:07 [system_utils.py:103] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: CUDA is initialized
WARNING 12-04 13:12:07 [system_utils.py:103] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: CUDA is initialized
[1;36m(EngineCore_DP0 pid=367247)[0;0m INFO 12-04 13:12:22 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='Qwen/Qwen3-14B', speculative_config=None, tokenizer='Qwen/Qwen3-14B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-14B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=367248)[0;0m INFO 12-04 13:12:22 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='Qwen/Qwen3-14B', speculative_config=None, tokenizer='Qwen/Qwen3-14B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-14B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=367248)[0;0m INFO 12-04 13:12:25 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.98:35693 backend=nccl
[1;36m(EngineCore_DP0 pid=367247)[0;0m INFO 12-04 13:12:25 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.98:37749 backend=nccl
  0%|          | 0/2 [00:00<?, ?it/s][W1204 13:12:25.346155523 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-25-0.rc.tch.harvard.edu]:35693 (errno: 97 - Address family not supported by protocol).
  0%|          | 0/2 [00:00<?, ?it/s][W1204 13:12:25.347115202 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-25-0.rc.tch.harvard.edu]:37749 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=367247)[0;0m INFO 12-04 13:12:25 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=367248)[0;0m INFO 12-04 13:12:25 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=367248)[0;0m INFO 12-04 13:12:26 [gpu_model_runner.py:3259] Starting to load model Qwen/Qwen3-14B...
[1;36m(EngineCore_DP0 pid=367247)[0;0m INFO 12-04 13:12:26 [gpu_model_runner.py:3259] Starting to load model Qwen/Qwen3-14B...
[1;36m(EngineCore_DP0 pid=367247)[0;0m INFO 12-04 13:12:27 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=367248)[0;0m INFO 12-04 13:12:27 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=367248)[0;0m INFO 12-04 13:12:27 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=367247)[0;0m INFO 12-04 13:12:27 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=367247)[0;0m ERROR 12-04 13:12:28 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=367247)[0;0m ERROR 12-04 13:12:28 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=367247)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=367247)[0;0m ERROR 12-04 13:12:28 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=367247)[0;0m ERROR 12-04 13:12:28 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367247)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=367247)[0;0m ERROR 12-04 13:12:28 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=367247)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=367247)[0;0m ERROR 12-04 13:12:28 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=367247)[0;0m ERROR 12-04 13:12:28 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367247)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=367247)[0;0m ERROR 12-04 13:12:28 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=367247)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[1;36m(EngineCore_DP0 pid=367247)[0;0m ERROR 12-04 13:12:28 [core.py:842]     self.driver_worker.load_model()
[1;36m(EngineCore_DP0 pid=367247)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 273, in load_model
[1;36m(EngineCore_DP0 pid=367247)[0;0m ERROR 12-04 13:12:28 [core.py:842]     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[1;36m(EngineCore_DP0 pid=367247)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3276, in load_model
[1;36m(EngineCore_DP0 pid=367247)[0;0m ERROR 12-04 13:12:28 [core.py:842]     self.model = model_loader.load_model(
[1;36m(EngineCore_DP0 pid=367247)[0;0m ERROR 12-04 13:12:28 [core.py:842]                  ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367247)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
[1;36m(EngineCore_DP0 pid=367247)[0;0m ERROR 12-04 13:12:28 [core.py:842]     model = initialize_model(
[1;36m(EngineCore_DP0 pid=367247)[0;0m ERROR 12-04 13:12:28 [core.py:842]             ^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367247)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py", line 55, in initialize_model
[1;36m(EngineCore_DP0 pid=367247)[0;0m ERROR 12-04 13:12:28 [core.py:842]     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(EngineCore_DP0 pid=367247)[0;0m ERROR 12-04 13:12:28 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367247)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 279, in __init__
[1;36m(EngineCore_DP0 pid=367247)[0;0m ERROR 12-04 13:12:28 [core.py:842]     self.model = Qwen3Model(
[1;36m(EngineCore_DP0 pid=367247)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=367247)[0;0m ERROR 12-04 13:12:28 [core.py:842]                  ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367247)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=367247)[0;0m ERROR 12-04 13:12:28 [core.py:842]     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=367247)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 253, in __init__
[1;36m(EngineCore_DP0 pid=367247)[0;0m ERROR 12-04 13:12:28 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=367247)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=367247)[0;0m ERROR 12-04 13:12:28 [core.py:842]     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=367247)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 337, in __init__
[1;36m(EngineCore_DP0 pid=367247)[0;0m ERROR 12-04 13:12:28 [core.py:842]     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(EngineCore_DP0 pid=367247)[0;0m ERROR 12-04 13:12:28 [core.py:842]                                                     ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367247)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 651, in make_layers
[1;36m(EngineCore_DP0 pid=367247)[0;0m ERROR 12-04 13:12:28 [core.py:842]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(EngineCore_DP0 pid=367247)[0;0m ERROR 12-04 13:12:28 [core.py:842]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367247)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 339, in <lambda>
[1;36m(EngineCore_DP0 pid=367247)[0;0m ERROR 12-04 13:12:28 [core.py:842]     lambda prefix: decoder_layer_type(
[1;36m(EngineCore_DP0 pid=367247)[0;0m ERROR 12-04 13:12:28 [core.py:842]                    ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367247)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 201, in __init__
[1;36m(EngineCore_DP0 pid=367247)[0;0m ERROR 12-04 13:12:28 [core.py:842]     self.mlp = Qwen3MLP(
[1;36m(EngineCore_DP0 pid=367247)[0;0m ERROR 12-04 13:12:28 [core.py:842]                ^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367247)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 84, in __init__
[1;36m(EngineCore_DP0 pid=367247)[0;0m ERROR 12-04 13:12:28 [core.py:842]     self.gate_up_proj = MergedColumnParallelLinear(
[1;36m(EngineCore_DP0 pid=367247)[0;0m ERROR 12-04 13:12:28 [core.py:842]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367247)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 631, in __init__
[1;36m(EngineCore_DP0 pid=367247)[0;0m ERROR 12-04 13:12:28 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=367247)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 484, in __init__
[1;36m(EngineCore_DP0 pid=367247)[0;0m ERROR 12-04 13:12:28 [core.py:842]     self.quant_method.create_weights(
[1;36m(EngineCore_DP0 pid=367247)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 215, in create_weights
[1;36m(EngineCore_DP0 pid=367247)[0;0m ERROR 12-04 13:12:28 [core.py:842]     data=torch.empty(
[1;36m(EngineCore_DP0 pid=367247)[0;0m ERROR 12-04 13:12:28 [core.py:842]          ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367247)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_device.py", line 103, in __torch_function__
[1;36m(EngineCore_DP0 pid=367247)[0;0m ERROR 12-04 13:12:28 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=367247)[0;0m ERROR 12-04 13:12:28 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367247)[0;0m ERROR 12-04 13:12:28 [core.py:842] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 340.00 MiB. GPU 0 has a total capacity of 44.39 GiB of which 54.00 MiB is free. Process 367248 has 22.51 GiB memory in use. Including non-PyTorch memory, this process has 21.87 GiB memory in use. Of the allocated memory 21.35 GiB is allocated by PyTorch, and 18.55 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=367247)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=367247)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=367247)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=367247)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=367247)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=367247)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=367247)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=367247)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=367247)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=367247)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367247)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=367247)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=367247)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=367247)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=367247)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367247)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=367247)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=367247)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[1;36m(EngineCore_DP0 pid=367247)[0;0m     self.driver_worker.load_model()
[1;36m(EngineCore_DP0 pid=367247)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 273, in load_model
[1;36m(EngineCore_DP0 pid=367247)[0;0m     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[1;36m(EngineCore_DP0 pid=367247)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3276, in load_model
[1;36m(EngineCore_DP0 pid=367247)[0;0m     self.model = model_loader.load_model(
[1;36m(EngineCore_DP0 pid=367247)[0;0m                  ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367247)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
[1;36m(EngineCore_DP0 pid=367247)[0;0m     model = initialize_model(
[1;36m(EngineCore_DP0 pid=367247)[0;0m             ^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367247)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py", line 55, in initialize_model
[1;36m(EngineCore_DP0 pid=367247)[0;0m     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(EngineCore_DP0 pid=367247)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367247)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 279, in __init__
[1;36m(EngineCore_DP0 pid=367247)[0;0m     self.model = Qwen3Model(
[1;36m(EngineCore_DP0 pid=367247)[0;0m                  ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367247)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=367247)[0;0m     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=367247)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 253, in __init__
[1;36m(EngineCore_DP0 pid=367247)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=367247)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=367247)[0;0m     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=367247)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 337, in __init__
[1;36m(EngineCore_DP0 pid=367247)[0;0m     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(EngineCore_DP0 pid=367247)[0;0m                                                     ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367247)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 651, in make_layers
[1;36m(EngineCore_DP0 pid=367247)[0;0m     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(EngineCore_DP0 pid=367247)[0;0m                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367247)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 339, in <lambda>
[1;36m(EngineCore_DP0 pid=367247)[0;0m     lambda prefix: decoder_layer_type(
[1;36m(EngineCore_DP0 pid=367247)[0;0m                    ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367247)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 201, in __init__
[1;36m(EngineCore_DP0 pid=367247)[0;0m     self.mlp = Qwen3MLP(
[1;36m(EngineCore_DP0 pid=367247)[0;0m                ^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367247)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 84, in __init__
[1;36m(EngineCore_DP0 pid=367247)[0;0m     self.gate_up_proj = MergedColumnParallelLinear(
[1;36m(EngineCore_DP0 pid=367247)[0;0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367247)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 631, in __init__
[1;36m(EngineCore_DP0 pid=367247)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=367247)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 484, in __init__
[1;36m(EngineCore_DP0 pid=367247)[0;0m     self.quant_method.create_weights(
[1;36m(EngineCore_DP0 pid=367247)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 215, in create_weights
[1;36m(EngineCore_DP0 pid=367247)[0;0m     data=torch.empty(
[1;36m(EngineCore_DP0 pid=367247)[0;0m          ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367247)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_device.py", line 103, in __torch_function__
[1;36m(EngineCore_DP0 pid=367247)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=367247)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367247)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 340.00 MiB. GPU 0 has a total capacity of 44.39 GiB of which 54.00 MiB is free. Process 367248 has 22.51 GiB memory in use. Including non-PyTorch memory, this process has 21.87 GiB memory in use. Of the allocated memory 21.35 GiB is allocated by PyTorch, and 18.55 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=367248)[0;0m ERROR 12-04 13:12:28 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=367248)[0;0m ERROR 12-04 13:12:28 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=367248)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=367248)[0;0m ERROR 12-04 13:12:28 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=367248)[0;0m ERROR 12-04 13:12:28 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367248)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=367248)[0;0m ERROR 12-04 13:12:28 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=367248)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=367248)[0;0m ERROR 12-04 13:12:28 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=367248)[0;0m ERROR 12-04 13:12:28 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367248)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=367248)[0;0m ERROR 12-04 13:12:28 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=367248)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[1;36m(EngineCore_DP0 pid=367248)[0;0m ERROR 12-04 13:12:28 [core.py:842]     self.driver_worker.load_model()
[1;36m(EngineCore_DP0 pid=367248)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 273, in load_model
[1;36m(EngineCore_DP0 pid=367248)[0;0m ERROR 12-04 13:12:28 [core.py:842]     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[1;36m(EngineCore_DP0 pid=367248)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3276, in load_model
[1;36m(EngineCore_DP0 pid=367248)[0;0m ERROR 12-04 13:12:28 [core.py:842]     self.model = model_loader.load_model(
[1;36m(EngineCore_DP0 pid=367248)[0;0m ERROR 12-04 13:12:28 [core.py:842]                  ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367248)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
[1;36m(EngineCore_DP0 pid=367248)[0;0m ERROR 12-04 13:12:28 [core.py:842]     model = initialize_model(
[1;36m(EngineCore_DP0 pid=367248)[0;0m ERROR 12-04 13:12:28 [core.py:842]             ^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367248)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py", line 55, in initialize_model
[1;36m(EngineCore_DP0 pid=367248)[0;0m ERROR 12-04 13:12:28 [core.py:842]     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(EngineCore_DP0 pid=367248)[0;0m ERROR 12-04 13:12:28 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367248)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 279, in __init__
[1;36m(EngineCore_DP0 pid=367248)[0;0m ERROR 12-04 13:12:28 [core.py:842]     self.model = Qwen3Model(
[1;36m(EngineCore_DP0 pid=367248)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=367248)[0;0m ERROR 12-04 13:12:28 [core.py:842]                  ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367248)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=367248)[0;0m ERROR 12-04 13:12:28 [core.py:842]     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=367248)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 253, in __init__
[1;36m(EngineCore_DP0 pid=367248)[0;0m ERROR 12-04 13:12:28 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=367248)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=367248)[0;0m ERROR 12-04 13:12:28 [core.py:842]     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=367248)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 337, in __init__
[1;36m(EngineCore_DP0 pid=367248)[0;0m ERROR 12-04 13:12:28 [core.py:842]     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(EngineCore_DP0 pid=367248)[0;0m ERROR 12-04 13:12:28 [core.py:842]                                                     ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367248)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 651, in make_layers
[1;36m(EngineCore_DP0 pid=367248)[0;0m ERROR 12-04 13:12:28 [core.py:842]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(EngineCore_DP0 pid=367248)[0;0m ERROR 12-04 13:12:28 [core.py:842]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367248)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 339, in <lambda>
[1;36m(EngineCore_DP0 pid=367248)[0;0m ERROR 12-04 13:12:28 [core.py:842]     lambda prefix: decoder_layer_type(
[1;36m(EngineCore_DP0 pid=367248)[0;0m ERROR 12-04 13:12:28 [core.py:842]                    ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367248)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 201, in __init__
[1;36m(EngineCore_DP0 pid=367248)[0;0m ERROR 12-04 13:12:28 [core.py:842]     self.mlp = Qwen3MLP(
[1;36m(EngineCore_DP0 pid=367248)[0;0m ERROR 12-04 13:12:28 [core.py:842]                ^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367248)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 84, in __init__
[1;36m(EngineCore_DP0 pid=367248)[0;0m ERROR 12-04 13:12:28 [core.py:842]     self.gate_up_proj = MergedColumnParallelLinear(
[1;36m(EngineCore_DP0 pid=367248)[0;0m ERROR 12-04 13:12:28 [core.py:842]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367248)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 631, in __init__
[1;36m(EngineCore_DP0 pid=367248)[0;0m ERROR 12-04 13:12:28 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=367248)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 484, in __init__
[1;36m(EngineCore_DP0 pid=367248)[0;0m ERROR 12-04 13:12:28 [core.py:842]     self.quant_method.create_weights(
[1;36m(EngineCore_DP0 pid=367248)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 215, in create_weights
[1;36m(EngineCore_DP0 pid=367248)[0;0m ERROR 12-04 13:12:28 [core.py:842]     data=torch.empty(
[1;36m(EngineCore_DP0 pid=367248)[0;0m ERROR 12-04 13:12:28 [core.py:842]          ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367248)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_device.py", line 103, in __torch_function__
[1;36m(EngineCore_DP0 pid=367248)[0;0m ERROR 12-04 13:12:28 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=367248)[0;0m ERROR 12-04 13:12:28 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367248)[0;0m ERROR 12-04 13:12:28 [core.py:842] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 340.00 MiB. GPU 0 has a total capacity of 44.39 GiB of which 34.00 MiB is free. Including non-PyTorch memory, this process has 22.48 GiB memory in use. Process 367247 has 21.87 GiB memory in use. Of the allocated memory 21.96 GiB is allocated by PyTorch, and 18.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=367248)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=367248)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=367248)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=367248)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=367248)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=367248)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=367248)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=367248)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=367248)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=367248)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367248)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=367248)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=367248)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=367248)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=367248)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367248)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=367248)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=367248)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[1;36m(EngineCore_DP0 pid=367248)[0;0m     self.driver_worker.load_model()
[1;36m(EngineCore_DP0 pid=367248)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 273, in load_model
[1;36m(EngineCore_DP0 pid=367248)[0;0m     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[1;36m(EngineCore_DP0 pid=367248)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3276, in load_model
[1;36m(EngineCore_DP0 pid=367248)[0;0m     self.model = model_loader.load_model(
[1;36m(EngineCore_DP0 pid=367248)[0;0m                  ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367248)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
[1;36m(EngineCore_DP0 pid=367248)[0;0m     model = initialize_model(
[1;36m(EngineCore_DP0 pid=367248)[0;0m             ^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367248)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py", line 55, in initialize_model
[1;36m(EngineCore_DP0 pid=367248)[0;0m     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(EngineCore_DP0 pid=367248)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367248)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 279, in __init__
[1;36m(EngineCore_DP0 pid=367248)[0;0m     self.model = Qwen3Model(
[1;36m(EngineCore_DP0 pid=367248)[0;0m                  ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367248)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=367248)[0;0m     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=367248)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 253, in __init__
[1;36m(EngineCore_DP0 pid=367248)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=367248)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=367248)[0;0m     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=367248)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 337, in __init__
[1;36m(EngineCore_DP0 pid=367248)[0;0m     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(EngineCore_DP0 pid=367248)[0;0m                                                     ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367248)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 651, in make_layers
[1;36m(EngineCore_DP0 pid=367248)[0;0m     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(EngineCore_DP0 pid=367248)[0;0m                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367248)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 339, in <lambda>
[1;36m(EngineCore_DP0 pid=367248)[0;0m     lambda prefix: decoder_layer_type(
[1;36m(EngineCore_DP0 pid=367248)[0;0m                    ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367248)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 201, in __init__
[1;36m(EngineCore_DP0 pid=367248)[0;0m     self.mlp = Qwen3MLP(
[1;36m(EngineCore_DP0 pid=367248)[0;0m                ^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367248)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 84, in __init__
[1;36m(EngineCore_DP0 pid=367248)[0;0m     self.gate_up_proj = MergedColumnParallelLinear(
[1;36m(EngineCore_DP0 pid=367248)[0;0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367248)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 631, in __init__
[1;36m(EngineCore_DP0 pid=367248)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=367248)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 484, in __init__
[1;36m(EngineCore_DP0 pid=367248)[0;0m     self.quant_method.create_weights(
[1;36m(EngineCore_DP0 pid=367248)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 215, in create_weights
[1;36m(EngineCore_DP0 pid=367248)[0;0m     data=torch.empty(
[1;36m(EngineCore_DP0 pid=367248)[0;0m          ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367248)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_device.py", line 103, in __torch_function__
[1;36m(EngineCore_DP0 pid=367248)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=367248)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367248)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 340.00 MiB. GPU 0 has a total capacity of 44.39 GiB of which 34.00 MiB is free. Including non-PyTorch memory, this process has 22.48 GiB memory in use. Process 367247 has 21.87 GiB memory in use. Of the allocated memory 21.96 GiB is allocated by PyTorch, and 18.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W1204 13:12:29.885974397 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank0]:[W1204 13:12:29.886462941 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: Qwen/Qwen3-14B (backend=vllm)
[ModelCache] Using max_model_len=40960 for Qwen/Qwen3-14B
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 13:12:51 [utils.py:253] non-default args: {'max_model_len': 40960, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'Qwen/Qwen3-14B'}
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: Qwen/Qwen3-14B (backend=vllm)
[ModelCache] Using max_model_len=40960 for Qwen/Qwen3-14B
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 13:12:51 [utils.py:253] non-default args: {'max_model_len': 40960, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'Qwen/Qwen3-14B'}
INFO 12-04 13:12:51 [model.py:631] Resolved architecture: Qwen3ForCausalLM
INFO 12-04 13:12:51 [model.py:1745] Using max model len 40960
INFO 12-04 13:12:51 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
INFO 12-04 13:12:51 [model.py:631] Resolved architecture: Qwen3ForCausalLM
INFO 12-04 13:12:51 [model.py:1745] Using max model len 40960
INFO 12-04 13:12:51 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=367767)[0;0m INFO 12-04 13:13:11 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='Qwen/Qwen3-14B', speculative_config=None, tokenizer='Qwen/Qwen3-14B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-14B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=367773)[0;0m INFO 12-04 13:13:11 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='Qwen/Qwen3-14B', speculative_config=None, tokenizer='Qwen/Qwen3-14B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-14B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=367773)[0;0m INFO 12-04 13:13:13 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.98:39111 backend=nccl
[1;36m(EngineCore_DP0 pid=367767)[0;0m INFO 12-04 13:13:13 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.98:59095 backend=nccl
[W1204 13:13:13.827832668 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-25-0.rc.tch.harvard.edu]:39111 (errno: 97 - Address family not supported by protocol).
[W1204 13:13:13.830442782 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-25-0.rc.tch.harvard.edu]:59095 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=367773)[0;0m INFO 12-04 13:13:13 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=367767)[0;0m INFO 12-04 13:13:13 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=367773)[0;0m INFO 12-04 13:13:13 [gpu_model_runner.py:3259] Starting to load model Qwen/Qwen3-14B...
[1;36m(EngineCore_DP0 pid=367767)[0;0m INFO 12-04 13:13:13 [gpu_model_runner.py:3259] Starting to load model Qwen/Qwen3-14B...
[1;36m(EngineCore_DP0 pid=367773)[0;0m INFO 12-04 13:13:14 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=367773)[0;0m INFO 12-04 13:13:14 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=367767)[0;0m INFO 12-04 13:13:14 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=367767)[0;0m INFO 12-04 13:13:14 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=367773)[0;0m ERROR 12-04 13:13:15 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=367773)[0;0m ERROR 12-04 13:13:15 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=367773)[0;0m ERROR 12-04 13:13:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=367773)[0;0m ERROR 12-04 13:13:15 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=367773)[0;0m ERROR 12-04 13:13:15 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367773)[0;0m ERROR 12-04 13:13:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=367773)[0;0m ERROR 12-04 13:13:15 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=367773)[0;0m ERROR 12-04 13:13:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=367773)[0;0m ERROR 12-04 13:13:15 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=367773)[0;0m ERROR 12-04 13:13:15 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367773)[0;0m ERROR 12-04 13:13:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=367773)[0;0m ERROR 12-04 13:13:15 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=367773)[0;0m ERROR 12-04 13:13:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[1;36m(EngineCore_DP0 pid=367773)[0;0m ERROR 12-04 13:13:15 [core.py:842]     self.driver_worker.load_model()
[1;36m(EngineCore_DP0 pid=367773)[0;0m ERROR 12-04 13:13:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 273, in load_model
[1;36m(EngineCore_DP0 pid=367773)[0;0m ERROR 12-04 13:13:15 [core.py:842]     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[1;36m(EngineCore_DP0 pid=367773)[0;0m ERROR 12-04 13:13:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3276, in load_model
[1;36m(EngineCore_DP0 pid=367773)[0;0m ERROR 12-04 13:13:15 [core.py:842]     self.model = model_loader.load_model(
[1;36m(EngineCore_DP0 pid=367773)[0;0m ERROR 12-04 13:13:15 [core.py:842]                  ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367773)[0;0m ERROR 12-04 13:13:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
[1;36m(EngineCore_DP0 pid=367773)[0;0m ERROR 12-04 13:13:15 [core.py:842]     model = initialize_model(
[1;36m(EngineCore_DP0 pid=367773)[0;0m ERROR 12-04 13:13:15 [core.py:842]             ^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367773)[0;0m ERROR 12-04 13:13:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py", line 55, in initialize_model
[1;36m(EngineCore_DP0 pid=367773)[0;0m ERROR 12-04 13:13:15 [core.py:842]     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(EngineCore_DP0 pid=367773)[0;0m ERROR 12-04 13:13:15 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367773)[0;0m ERROR 12-04 13:13:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 279, in __init__
[1;36m(EngineCore_DP0 pid=367773)[0;0m ERROR 12-04 13:13:15 [core.py:842]     self.model = Qwen3Model(
[1;36m(EngineCore_DP0 pid=367773)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=367773)[0;0m ERROR 12-04 13:13:15 [core.py:842]                  ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367773)[0;0m ERROR 12-04 13:13:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=367773)[0;0m ERROR 12-04 13:13:15 [core.py:842]     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=367773)[0;0m ERROR 12-04 13:13:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 253, in __init__
[1;36m(EngineCore_DP0 pid=367773)[0;0m ERROR 12-04 13:13:15 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=367773)[0;0m ERROR 12-04 13:13:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=367773)[0;0m ERROR 12-04 13:13:15 [core.py:842]     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=367773)[0;0m ERROR 12-04 13:13:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 337, in __init__
[1;36m(EngineCore_DP0 pid=367773)[0;0m ERROR 12-04 13:13:15 [core.py:842]     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(EngineCore_DP0 pid=367773)[0;0m ERROR 12-04 13:13:15 [core.py:842]                                                     ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367773)[0;0m ERROR 12-04 13:13:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 651, in make_layers
[1;36m(EngineCore_DP0 pid=367773)[0;0m ERROR 12-04 13:13:15 [core.py:842]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(EngineCore_DP0 pid=367773)[0;0m ERROR 12-04 13:13:15 [core.py:842]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367773)[0;0m ERROR 12-04 13:13:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 339, in <lambda>
[1;36m(EngineCore_DP0 pid=367773)[0;0m ERROR 12-04 13:13:15 [core.py:842]     lambda prefix: decoder_layer_type(
[1;36m(EngineCore_DP0 pid=367773)[0;0m ERROR 12-04 13:13:15 [core.py:842]                    ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367773)[0;0m ERROR 12-04 13:13:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 201, in __init__
[1;36m(EngineCore_DP0 pid=367773)[0;0m ERROR 12-04 13:13:15 [core.py:842]     self.mlp = Qwen3MLP(
[1;36m(EngineCore_DP0 pid=367773)[0;0m ERROR 12-04 13:13:15 [core.py:842]                ^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367773)[0;0m ERROR 12-04 13:13:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 84, in __init__
[1;36m(EngineCore_DP0 pid=367773)[0;0m ERROR 12-04 13:13:15 [core.py:842]     self.gate_up_proj = MergedColumnParallelLinear(
[1;36m(EngineCore_DP0 pid=367773)[0;0m ERROR 12-04 13:13:15 [core.py:842]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367773)[0;0m ERROR 12-04 13:13:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 631, in __init__
[1;36m(EngineCore_DP0 pid=367773)[0;0m ERROR 12-04 13:13:15 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=367773)[0;0m ERROR 12-04 13:13:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 484, in __init__
[1;36m(EngineCore_DP0 pid=367773)[0;0m ERROR 12-04 13:13:15 [core.py:842]     self.quant_method.create_weights(
[1;36m(EngineCore_DP0 pid=367773)[0;0m ERROR 12-04 13:13:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 215, in create_weights
[1;36m(EngineCore_DP0 pid=367773)[0;0m ERROR 12-04 13:13:15 [core.py:842]     data=torch.empty(
[1;36m(EngineCore_DP0 pid=367773)[0;0m ERROR 12-04 13:13:15 [core.py:842]          ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367773)[0;0m ERROR 12-04 13:13:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_device.py", line 103, in __torch_function__
[1;36m(EngineCore_DP0 pid=367773)[0;0m ERROR 12-04 13:13:15 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=367773)[0;0m ERROR 12-04 13:13:15 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367773)[0;0m ERROR 12-04 13:13:15 [core.py:842] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 340.00 MiB. GPU 0 has a total capacity of 44.39 GiB of which 34.00 MiB is free. Including non-PyTorch memory, this process has 21.87 GiB memory in use. Process 367767 has 22.48 GiB memory in use. Of the allocated memory 21.35 GiB is allocated by PyTorch, and 18.55 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=367773)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=367773)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=367773)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=367773)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=367773)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=367773)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=367773)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=367773)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=367773)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=367773)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367773)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=367773)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=367773)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=367773)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=367773)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367773)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=367773)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=367773)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[1;36m(EngineCore_DP0 pid=367773)[0;0m     self.driver_worker.load_model()
[1;36m(EngineCore_DP0 pid=367773)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 273, in load_model
[1;36m(EngineCore_DP0 pid=367773)[0;0m     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[1;36m(EngineCore_DP0 pid=367773)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3276, in load_model
[1;36m(EngineCore_DP0 pid=367773)[0;0m     self.model = model_loader.load_model(
[1;36m(EngineCore_DP0 pid=367773)[0;0m                  ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367773)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
[1;36m(EngineCore_DP0 pid=367773)[0;0m     model = initialize_model(
[1;36m(EngineCore_DP0 pid=367773)[0;0m             ^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367773)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py", line 55, in initialize_model
[1;36m(EngineCore_DP0 pid=367773)[0;0m     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(EngineCore_DP0 pid=367773)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367773)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 279, in __init__
[1;36m(EngineCore_DP0 pid=367773)[0;0m     self.model = Qwen3Model(
[1;36m(EngineCore_DP0 pid=367773)[0;0m                  ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367773)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=367773)[0;0m     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=367773)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 253, in __init__
[1;36m(EngineCore_DP0 pid=367773)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=367773)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=367773)[0;0m     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=367773)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 337, in __init__
[1;36m(EngineCore_DP0 pid=367773)[0;0m     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(EngineCore_DP0 pid=367773)[0;0m                                                     ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367773)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 651, in make_layers
[1;36m(EngineCore_DP0 pid=367773)[0;0m     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(EngineCore_DP0 pid=367773)[0;0m                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367773)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 339, in <lambda>
[1;36m(EngineCore_DP0 pid=367773)[0;0m     lambda prefix: decoder_layer_type(
[1;36m(EngineCore_DP0 pid=367773)[0;0m                    ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367773)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 201, in __init__
[1;36m(EngineCore_DP0 pid=367773)[0;0m     self.mlp = Qwen3MLP(
[1;36m(EngineCore_DP0 pid=367773)[0;0m                ^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367773)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 84, in __init__
[1;36m(EngineCore_DP0 pid=367773)[0;0m     self.gate_up_proj = MergedColumnParallelLinear(
[1;36m(EngineCore_DP0 pid=367773)[0;0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367773)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 631, in __init__
[1;36m(EngineCore_DP0 pid=367773)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=367773)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 484, in __init__
[1;36m(EngineCore_DP0 pid=367773)[0;0m     self.quant_method.create_weights(
[1;36m(EngineCore_DP0 pid=367773)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 215, in create_weights
[1;36m(EngineCore_DP0 pid=367773)[0;0m     data=torch.empty(
[1;36m(EngineCore_DP0 pid=367773)[0;0m          ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367773)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_device.py", line 103, in __torch_function__
[1;36m(EngineCore_DP0 pid=367773)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=367773)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367773)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 340.00 MiB. GPU 0 has a total capacity of 44.39 GiB of which 34.00 MiB is free. Including non-PyTorch memory, this process has 21.87 GiB memory in use. Process 367767 has 22.48 GiB memory in use. Of the allocated memory 21.35 GiB is allocated by PyTorch, and 18.55 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=367767)[0;0m ERROR 12-04 13:13:15 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=367767)[0;0m ERROR 12-04 13:13:15 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=367767)[0;0m ERROR 12-04 13:13:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=367767)[0;0m ERROR 12-04 13:13:15 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=367767)[0;0m ERROR 12-04 13:13:15 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367767)[0;0m ERROR 12-04 13:13:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=367767)[0;0m ERROR 12-04 13:13:15 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=367767)[0;0m ERROR 12-04 13:13:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=367767)[0;0m ERROR 12-04 13:13:15 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=367767)[0;0m ERROR 12-04 13:13:15 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367767)[0;0m ERROR 12-04 13:13:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=367767)[0;0m ERROR 12-04 13:13:15 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=367767)[0;0m ERROR 12-04 13:13:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[1;36m(EngineCore_DP0 pid=367767)[0;0m ERROR 12-04 13:13:15 [core.py:842]     self.driver_worker.load_model()
[1;36m(EngineCore_DP0 pid=367767)[0;0m ERROR 12-04 13:13:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 273, in load_model
[1;36m(EngineCore_DP0 pid=367767)[0;0m ERROR 12-04 13:13:15 [core.py:842]     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[1;36m(EngineCore_DP0 pid=367767)[0;0m ERROR 12-04 13:13:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3276, in load_model
[1;36m(EngineCore_DP0 pid=367767)[0;0m ERROR 12-04 13:13:15 [core.py:842]     self.model = model_loader.load_model(
[1;36m(EngineCore_DP0 pid=367767)[0;0m ERROR 12-04 13:13:15 [core.py:842]                  ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367767)[0;0m ERROR 12-04 13:13:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
[1;36m(EngineCore_DP0 pid=367767)[0;0m ERROR 12-04 13:13:15 [core.py:842]     model = initialize_model(
[1;36m(EngineCore_DP0 pid=367767)[0;0m ERROR 12-04 13:13:15 [core.py:842]             ^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367767)[0;0m ERROR 12-04 13:13:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py", line 55, in initialize_model
[1;36m(EngineCore_DP0 pid=367767)[0;0m ERROR 12-04 13:13:15 [core.py:842]     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(EngineCore_DP0 pid=367767)[0;0m ERROR 12-04 13:13:15 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367767)[0;0m ERROR 12-04 13:13:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 279, in __init__
[1;36m(EngineCore_DP0 pid=367767)[0;0m ERROR 12-04 13:13:15 [core.py:842]     self.model = Qwen3Model(
[1;36m(EngineCore_DP0 pid=367767)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=367767)[0;0m ERROR 12-04 13:13:15 [core.py:842]                  ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367767)[0;0m ERROR 12-04 13:13:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=367767)[0;0m ERROR 12-04 13:13:15 [core.py:842]     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=367767)[0;0m ERROR 12-04 13:13:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 253, in __init__
[1;36m(EngineCore_DP0 pid=367767)[0;0m ERROR 12-04 13:13:15 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=367767)[0;0m ERROR 12-04 13:13:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=367767)[0;0m ERROR 12-04 13:13:15 [core.py:842]     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=367767)[0;0m ERROR 12-04 13:13:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 337, in __init__
[1;36m(EngineCore_DP0 pid=367767)[0;0m ERROR 12-04 13:13:15 [core.py:842]     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(EngineCore_DP0 pid=367767)[0;0m ERROR 12-04 13:13:15 [core.py:842]                                                     ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367767)[0;0m ERROR 12-04 13:13:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 651, in make_layers
[1;36m(EngineCore_DP0 pid=367767)[0;0m ERROR 12-04 13:13:15 [core.py:842]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(EngineCore_DP0 pid=367767)[0;0m ERROR 12-04 13:13:15 [core.py:842]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367767)[0;0m ERROR 12-04 13:13:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 339, in <lambda>
[1;36m(EngineCore_DP0 pid=367767)[0;0m ERROR 12-04 13:13:15 [core.py:842]     lambda prefix: decoder_layer_type(
[1;36m(EngineCore_DP0 pid=367767)[0;0m ERROR 12-04 13:13:15 [core.py:842]                    ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367767)[0;0m ERROR 12-04 13:13:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 201, in __init__
[1;36m(EngineCore_DP0 pid=367767)[0;0m ERROR 12-04 13:13:15 [core.py:842]     self.mlp = Qwen3MLP(
[1;36m(EngineCore_DP0 pid=367767)[0;0m ERROR 12-04 13:13:15 [core.py:842]                ^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367767)[0;0m ERROR 12-04 13:13:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 84, in __init__
[1;36m(EngineCore_DP0 pid=367767)[0;0m ERROR 12-04 13:13:15 [core.py:842]     self.gate_up_proj = MergedColumnParallelLinear(
[1;36m(EngineCore_DP0 pid=367767)[0;0m ERROR 12-04 13:13:15 [core.py:842]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367767)[0;0m ERROR 12-04 13:13:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 631, in __init__
[1;36m(EngineCore_DP0 pid=367767)[0;0m ERROR 12-04 13:13:15 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=367767)[0;0m ERROR 12-04 13:13:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 484, in __init__
[1;36m(EngineCore_DP0 pid=367767)[0;0m ERROR 12-04 13:13:15 [core.py:842]     self.quant_method.create_weights(
[1;36m(EngineCore_DP0 pid=367767)[0;0m ERROR 12-04 13:13:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 215, in create_weights
[1;36m(EngineCore_DP0 pid=367767)[0;0m ERROR 12-04 13:13:15 [core.py:842]     data=torch.empty(
[1;36m(EngineCore_DP0 pid=367767)[0;0m ERROR 12-04 13:13:15 [core.py:842]          ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367767)[0;0m ERROR 12-04 13:13:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_device.py", line 103, in __torch_function__
[1;36m(EngineCore_DP0 pid=367767)[0;0m ERROR 12-04 13:13:15 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=367767)[0;0m ERROR 12-04 13:13:15 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367767)[0;0m ERROR 12-04 13:13:15 [core.py:842] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 340.00 MiB. GPU 0 has a total capacity of 44.39 GiB of which 34.00 MiB is free. Process 367773 has 21.87 GiB memory in use. Including non-PyTorch memory, this process has 22.48 GiB memory in use. Of the allocated memory 21.96 GiB is allocated by PyTorch, and 18.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=367767)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=367767)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=367767)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=367767)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=367767)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=367767)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=367767)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=367767)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=367767)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=367767)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367767)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=367767)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=367767)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=367767)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=367767)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367767)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=367767)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=367767)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[1;36m(EngineCore_DP0 pid=367767)[0;0m     self.driver_worker.load_model()
[1;36m(EngineCore_DP0 pid=367767)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 273, in load_model
[1;36m(EngineCore_DP0 pid=367767)[0;0m     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[1;36m(EngineCore_DP0 pid=367767)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3276, in load_model
[1;36m(EngineCore_DP0 pid=367767)[0;0m     self.model = model_loader.load_model(
[1;36m(EngineCore_DP0 pid=367767)[0;0m                  ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367767)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
[1;36m(EngineCore_DP0 pid=367767)[0;0m     model = initialize_model(
[1;36m(EngineCore_DP0 pid=367767)[0;0m             ^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367767)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py", line 55, in initialize_model
[1;36m(EngineCore_DP0 pid=367767)[0;0m     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(EngineCore_DP0 pid=367767)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367767)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 279, in __init__
[1;36m(EngineCore_DP0 pid=367767)[0;0m     self.model = Qwen3Model(
[1;36m(EngineCore_DP0 pid=367767)[0;0m                  ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367767)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=367767)[0;0m     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=367767)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 253, in __init__
[1;36m(EngineCore_DP0 pid=367767)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=367767)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=367767)[0;0m     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=367767)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 337, in __init__
[1;36m(EngineCore_DP0 pid=367767)[0;0m     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(EngineCore_DP0 pid=367767)[0;0m                                                     ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367767)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 651, in make_layers
[1;36m(EngineCore_DP0 pid=367767)[0;0m     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(EngineCore_DP0 pid=367767)[0;0m                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367767)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 339, in <lambda>
[1;36m(EngineCore_DP0 pid=367767)[0;0m     lambda prefix: decoder_layer_type(
[1;36m(EngineCore_DP0 pid=367767)[0;0m                    ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367767)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 201, in __init__
[1;36m(EngineCore_DP0 pid=367767)[0;0m     self.mlp = Qwen3MLP(
[1;36m(EngineCore_DP0 pid=367767)[0;0m                ^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367767)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 84, in __init__
[1;36m(EngineCore_DP0 pid=367767)[0;0m     self.gate_up_proj = MergedColumnParallelLinear(
[1;36m(EngineCore_DP0 pid=367767)[0;0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367767)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 631, in __init__
[1;36m(EngineCore_DP0 pid=367767)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=367767)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 484, in __init__
[1;36m(EngineCore_DP0 pid=367767)[0;0m     self.quant_method.create_weights(
[1;36m(EngineCore_DP0 pid=367767)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 215, in create_weights
[1;36m(EngineCore_DP0 pid=367767)[0;0m     data=torch.empty(
[1;36m(EngineCore_DP0 pid=367767)[0;0m          ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367767)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_device.py", line 103, in __torch_function__
[1;36m(EngineCore_DP0 pid=367767)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=367767)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367767)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 340.00 MiB. GPU 0 has a total capacity of 44.39 GiB of which 34.00 MiB is free. Process 367773 has 21.87 GiB memory in use. Including non-PyTorch memory, this process has 22.48 GiB memory in use. Of the allocated memory 21.96 GiB is allocated by PyTorch, and 18.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W1204 13:13:16.220969579 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank0]:[W1204 13:13:16.224205951 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: Qwen/Qwen3-14B (backend=vllm)
[ModelCache] Using max_model_len=40960 for Qwen/Qwen3-14B
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 13:13:38 [utils.py:253] non-default args: {'max_model_len': 40960, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'Qwen/Qwen3-14B'}
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: Qwen/Qwen3-14B (backend=vllm)
[ModelCache] Using max_model_len=40960 for Qwen/Qwen3-14B
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 13:13:38 [utils.py:253] non-default args: {'max_model_len': 40960, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'Qwen/Qwen3-14B'}
INFO 12-04 13:13:38 [model.py:631] Resolved architecture: Qwen3ForCausalLM
INFO 12-04 13:13:38 [model.py:1745] Using max model len 40960
INFO 12-04 13:13:38 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
INFO 12-04 13:13:38 [model.py:631] Resolved architecture: Qwen3ForCausalLM
INFO 12-04 13:13:38 [model.py:1745] Using max model len 40960
INFO 12-04 13:13:38 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=368283)[0;0m INFO 12-04 13:13:55 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='Qwen/Qwen3-14B', speculative_config=None, tokenizer='Qwen/Qwen3-14B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-14B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=368286)[0;0m INFO 12-04 13:13:55 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='Qwen/Qwen3-14B', speculative_config=None, tokenizer='Qwen/Qwen3-14B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-14B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=368283)[0;0m INFO 12-04 13:13:57 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.98:47517 backend=nccl
[1;36m(EngineCore_DP0 pid=368286)[0;0m INFO 12-04 13:13:57 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.98:36993 backend=nccl
[W1204 13:13:57.645826523 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-25-0.rc.tch.harvard.edu]:47517 (errno: 97 - Address family not supported by protocol).
[W1204 13:13:57.646348074 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-25-0.rc.tch.harvard.edu]:36993 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=368286)[0;0m INFO 12-04 13:13:57 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=368283)[0;0m INFO 12-04 13:13:57 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=368286)[0;0m INFO 12-04 13:13:57 [gpu_model_runner.py:3259] Starting to load model Qwen/Qwen3-14B...
[1;36m(EngineCore_DP0 pid=368283)[0;0m INFO 12-04 13:13:57 [gpu_model_runner.py:3259] Starting to load model Qwen/Qwen3-14B...
[1;36m(EngineCore_DP0 pid=368283)[0;0m INFO 12-04 13:13:58 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=368283)[0;0m INFO 12-04 13:13:58 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=368286)[0;0m INFO 12-04 13:13:58 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=368286)[0;0m INFO 12-04 13:13:58 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=368286)[0;0m ERROR 12-04 13:13:59 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=368286)[0;0m ERROR 12-04 13:13:59 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=368286)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=368286)[0;0m ERROR 12-04 13:13:59 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=368286)[0;0m ERROR 12-04 13:13:59 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368286)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=368286)[0;0m ERROR 12-04 13:13:59 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=368286)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=368286)[0;0m ERROR 12-04 13:13:59 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=368286)[0;0m ERROR 12-04 13:13:59 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368286)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=368286)[0;0m ERROR 12-04 13:13:59 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=368286)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[1;36m(EngineCore_DP0 pid=368286)[0;0m ERROR 12-04 13:13:59 [core.py:842]     self.driver_worker.load_model()
[1;36m(EngineCore_DP0 pid=368286)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 273, in load_model
[1;36m(EngineCore_DP0 pid=368286)[0;0m ERROR 12-04 13:13:59 [core.py:842]     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[1;36m(EngineCore_DP0 pid=368286)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3276, in load_model
[1;36m(EngineCore_DP0 pid=368286)[0;0m ERROR 12-04 13:13:59 [core.py:842]     self.model = model_loader.load_model(
[1;36m(EngineCore_DP0 pid=368286)[0;0m ERROR 12-04 13:13:59 [core.py:842]                  ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368286)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
[1;36m(EngineCore_DP0 pid=368286)[0;0m ERROR 12-04 13:13:59 [core.py:842]     model = initialize_model(
[1;36m(EngineCore_DP0 pid=368286)[0;0m ERROR 12-04 13:13:59 [core.py:842]             ^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368286)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py", line 55, in initialize_model
[1;36m(EngineCore_DP0 pid=368286)[0;0m ERROR 12-04 13:13:59 [core.py:842]     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(EngineCore_DP0 pid=368286)[0;0m ERROR 12-04 13:13:59 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368286)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 279, in __init__
[1;36m(EngineCore_DP0 pid=368286)[0;0m ERROR 12-04 13:13:59 [core.py:842]     self.model = Qwen3Model(
[1;36m(EngineCore_DP0 pid=368286)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=368286)[0;0m ERROR 12-04 13:13:59 [core.py:842]                  ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368286)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=368286)[0;0m ERROR 12-04 13:13:59 [core.py:842]     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=368286)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 253, in __init__
[1;36m(EngineCore_DP0 pid=368286)[0;0m ERROR 12-04 13:13:59 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=368286)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=368286)[0;0m ERROR 12-04 13:13:59 [core.py:842]     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=368286)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 337, in __init__
[1;36m(EngineCore_DP0 pid=368286)[0;0m ERROR 12-04 13:13:59 [core.py:842]     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(EngineCore_DP0 pid=368286)[0;0m ERROR 12-04 13:13:59 [core.py:842]                                                     ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368286)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 651, in make_layers
[1;36m(EngineCore_DP0 pid=368286)[0;0m ERROR 12-04 13:13:59 [core.py:842]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(EngineCore_DP0 pid=368286)[0;0m ERROR 12-04 13:13:59 [core.py:842]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368286)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 339, in <lambda>
[1;36m(EngineCore_DP0 pid=368286)[0;0m ERROR 12-04 13:13:59 [core.py:842]     lambda prefix: decoder_layer_type(
[1;36m(EngineCore_DP0 pid=368286)[0;0m ERROR 12-04 13:13:59 [core.py:842]                    ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368286)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 201, in __init__
[1;36m(EngineCore_DP0 pid=368286)[0;0m ERROR 12-04 13:13:59 [core.py:842]     self.mlp = Qwen3MLP(
[1;36m(EngineCore_DP0 pid=368286)[0;0m ERROR 12-04 13:13:59 [core.py:842]                ^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368286)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 84, in __init__
[1;36m(EngineCore_DP0 pid=368286)[0;0m ERROR 12-04 13:13:59 [core.py:842]     self.gate_up_proj = MergedColumnParallelLinear(
[1;36m(EngineCore_DP0 pid=368286)[0;0m ERROR 12-04 13:13:59 [core.py:842]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368286)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 631, in __init__
[1;36m(EngineCore_DP0 pid=368286)[0;0m ERROR 12-04 13:13:59 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=368286)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 484, in __init__
[1;36m(EngineCore_DP0 pid=368286)[0;0m ERROR 12-04 13:13:59 [core.py:842]     self.quant_method.create_weights(
[1;36m(EngineCore_DP0 pid=368286)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 215, in create_weights
[1;36m(EngineCore_DP0 pid=368286)[0;0m ERROR 12-04 13:13:59 [core.py:842]     data=torch.empty(
[1;36m(EngineCore_DP0 pid=368286)[0;0m ERROR 12-04 13:13:59 [core.py:842]          ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368286)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_device.py", line 103, in __torch_function__
[1;36m(EngineCore_DP0 pid=368286)[0;0m ERROR 12-04 13:13:59 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=368286)[0;0m ERROR 12-04 13:13:59 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368286)[0;0m ERROR 12-04 13:13:59 [core.py:842] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 340.00 MiB. GPU 0 has a total capacity of 44.39 GiB of which 64.00 MiB is free. Process 368283 has 23.10 GiB memory in use. Including non-PyTorch memory, this process has 21.25 GiB memory in use. Of the allocated memory 20.73 GiB is allocated by PyTorch, and 18.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=368286)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=368286)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=368286)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=368286)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=368286)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=368286)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=368286)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=368286)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=368286)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=368286)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368286)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=368286)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=368286)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=368286)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=368286)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368286)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=368286)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=368286)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[1;36m(EngineCore_DP0 pid=368286)[0;0m     self.driver_worker.load_model()
[1;36m(EngineCore_DP0 pid=368286)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 273, in load_model
[1;36m(EngineCore_DP0 pid=368286)[0;0m     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[1;36m(EngineCore_DP0 pid=368286)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3276, in load_model
[1;36m(EngineCore_DP0 pid=368286)[0;0m     self.model = model_loader.load_model(
[1;36m(EngineCore_DP0 pid=368286)[0;0m                  ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368286)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
[1;36m(EngineCore_DP0 pid=368286)[0;0m     model = initialize_model(
[1;36m(EngineCore_DP0 pid=368286)[0;0m             ^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368286)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py", line 55, in initialize_model
[1;36m(EngineCore_DP0 pid=368286)[0;0m     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(EngineCore_DP0 pid=368286)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368286)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 279, in __init__
[1;36m(EngineCore_DP0 pid=368286)[0;0m     self.model = Qwen3Model(
[1;36m(EngineCore_DP0 pid=368286)[0;0m                  ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368286)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=368286)[0;0m     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=368286)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 253, in __init__
[1;36m(EngineCore_DP0 pid=368286)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=368286)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=368286)[0;0m     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=368286)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 337, in __init__
[1;36m(EngineCore_DP0 pid=368286)[0;0m     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(EngineCore_DP0 pid=368286)[0;0m                                                     ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368286)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 651, in make_layers
[1;36m(EngineCore_DP0 pid=368286)[0;0m     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(EngineCore_DP0 pid=368286)[0;0m                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368286)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 339, in <lambda>
[1;36m(EngineCore_DP0 pid=368286)[0;0m     lambda prefix: decoder_layer_type(
[1;36m(EngineCore_DP0 pid=368286)[0;0m                    ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368286)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 201, in __init__
[1;36m(EngineCore_DP0 pid=368286)[0;0m     self.mlp = Qwen3MLP(
[1;36m(EngineCore_DP0 pid=368286)[0;0m                ^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368286)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 84, in __init__
[1;36m(EngineCore_DP0 pid=368286)[0;0m     self.gate_up_proj = MergedColumnParallelLinear(
[1;36m(EngineCore_DP0 pid=368286)[0;0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368286)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 631, in __init__
[1;36m(EngineCore_DP0 pid=368286)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=368286)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 484, in __init__
[1;36m(EngineCore_DP0 pid=368286)[0;0m     self.quant_method.create_weights(
[1;36m(EngineCore_DP0 pid=368286)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 215, in create_weights
[1;36m(EngineCore_DP0 pid=368286)[0;0m     data=torch.empty(
[1;36m(EngineCore_DP0 pid=368286)[0;0m          ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368286)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_device.py", line 103, in __torch_function__
[1;36m(EngineCore_DP0 pid=368286)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=368286)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368286)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 340.00 MiB. GPU 0 has a total capacity of 44.39 GiB of which 64.00 MiB is free. Process 368283 has 23.10 GiB memory in use. Including non-PyTorch memory, this process has 21.25 GiB memory in use. Of the allocated memory 20.73 GiB is allocated by PyTorch, and 18.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=368283)[0;0m ERROR 12-04 13:13:59 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=368283)[0;0m ERROR 12-04 13:13:59 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=368283)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=368283)[0;0m ERROR 12-04 13:13:59 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=368283)[0;0m ERROR 12-04 13:13:59 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368283)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=368283)[0;0m ERROR 12-04 13:13:59 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=368283)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=368283)[0;0m ERROR 12-04 13:13:59 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=368283)[0;0m ERROR 12-04 13:13:59 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368283)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=368283)[0;0m ERROR 12-04 13:13:59 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=368283)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[1;36m(EngineCore_DP0 pid=368283)[0;0m ERROR 12-04 13:13:59 [core.py:842]     self.driver_worker.load_model()
[1;36m(EngineCore_DP0 pid=368283)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 273, in load_model
[1;36m(EngineCore_DP0 pid=368283)[0;0m ERROR 12-04 13:13:59 [core.py:842]     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[1;36m(EngineCore_DP0 pid=368283)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3276, in load_model
[1;36m(EngineCore_DP0 pid=368283)[0;0m ERROR 12-04 13:13:59 [core.py:842]     self.model = model_loader.load_model(
[1;36m(EngineCore_DP0 pid=368283)[0;0m ERROR 12-04 13:13:59 [core.py:842]                  ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368283)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
[1;36m(EngineCore_DP0 pid=368283)[0;0m ERROR 12-04 13:13:59 [core.py:842]     model = initialize_model(
[1;36m(EngineCore_DP0 pid=368283)[0;0m ERROR 12-04 13:13:59 [core.py:842]             ^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368283)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py", line 55, in initialize_model
[1;36m(EngineCore_DP0 pid=368283)[0;0m ERROR 12-04 13:13:59 [core.py:842]     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(EngineCore_DP0 pid=368283)[0;0m ERROR 12-04 13:13:59 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368283)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 279, in __init__
[1;36m(EngineCore_DP0 pid=368283)[0;0m ERROR 12-04 13:13:59 [core.py:842]     self.model = Qwen3Model(
[1;36m(EngineCore_DP0 pid=368283)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=368283)[0;0m ERROR 12-04 13:13:59 [core.py:842]                  ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368283)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=368283)[0;0m ERROR 12-04 13:13:59 [core.py:842]     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=368283)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 253, in __init__
[1;36m(EngineCore_DP0 pid=368283)[0;0m ERROR 12-04 13:13:59 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=368283)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=368283)[0;0m ERROR 12-04 13:13:59 [core.py:842]     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=368283)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 337, in __init__
[1;36m(EngineCore_DP0 pid=368283)[0;0m ERROR 12-04 13:13:59 [core.py:842]     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(EngineCore_DP0 pid=368283)[0;0m ERROR 12-04 13:13:59 [core.py:842]                                                     ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368283)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 651, in make_layers
[1;36m(EngineCore_DP0 pid=368283)[0;0m ERROR 12-04 13:13:59 [core.py:842]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(EngineCore_DP0 pid=368283)[0;0m ERROR 12-04 13:13:59 [core.py:842]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368283)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 339, in <lambda>
[1;36m(EngineCore_DP0 pid=368283)[0;0m ERROR 12-04 13:13:59 [core.py:842]     lambda prefix: decoder_layer_type(
[1;36m(EngineCore_DP0 pid=368283)[0;0m ERROR 12-04 13:13:59 [core.py:842]                    ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368283)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 201, in __init__
[1;36m(EngineCore_DP0 pid=368283)[0;0m ERROR 12-04 13:13:59 [core.py:842]     self.mlp = Qwen3MLP(
[1;36m(EngineCore_DP0 pid=368283)[0;0m ERROR 12-04 13:13:59 [core.py:842]                ^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368283)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 84, in __init__
[1;36m(EngineCore_DP0 pid=368283)[0;0m ERROR 12-04 13:13:59 [core.py:842]     self.gate_up_proj = MergedColumnParallelLinear(
[1;36m(EngineCore_DP0 pid=368283)[0;0m ERROR 12-04 13:13:59 [core.py:842]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368283)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 631, in __init__
[1;36m(EngineCore_DP0 pid=368283)[0;0m ERROR 12-04 13:13:59 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=368283)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 484, in __init__
[1;36m(EngineCore_DP0 pid=368283)[0;0m ERROR 12-04 13:13:59 [core.py:842]     self.quant_method.create_weights(
[1;36m(EngineCore_DP0 pid=368283)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 215, in create_weights
[1;36m(EngineCore_DP0 pid=368283)[0;0m ERROR 12-04 13:13:59 [core.py:842]     data=torch.empty(
[1;36m(EngineCore_DP0 pid=368283)[0;0m ERROR 12-04 13:13:59 [core.py:842]          ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368283)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_device.py", line 103, in __torch_function__
[1;36m(EngineCore_DP0 pid=368283)[0;0m ERROR 12-04 13:13:59 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=368283)[0;0m ERROR 12-04 13:13:59 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368283)[0;0m ERROR 12-04 13:13:59 [core.py:842] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 340.00 MiB. GPU 0 has a total capacity of 44.39 GiB of which 34.00 MiB is free. Including non-PyTorch memory, this process has 23.10 GiB memory in use. Process 368286 has 21.25 GiB memory in use. Of the allocated memory 22.58 GiB is allocated by PyTorch, and 18.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=368283)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=368283)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=368283)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=368283)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=368283)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=368283)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=368283)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=368283)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=368283)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=368283)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368283)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=368283)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=368283)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=368283)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=368283)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368283)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=368283)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=368283)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[1;36m(EngineCore_DP0 pid=368283)[0;0m     self.driver_worker.load_model()
[1;36m(EngineCore_DP0 pid=368283)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 273, in load_model
[1;36m(EngineCore_DP0 pid=368283)[0;0m     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[1;36m(EngineCore_DP0 pid=368283)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3276, in load_model
[1;36m(EngineCore_DP0 pid=368283)[0;0m     self.model = model_loader.load_model(
[1;36m(EngineCore_DP0 pid=368283)[0;0m                  ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368283)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
[1;36m(EngineCore_DP0 pid=368283)[0;0m     model = initialize_model(
[1;36m(EngineCore_DP0 pid=368283)[0;0m             ^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368283)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py", line 55, in initialize_model
[1;36m(EngineCore_DP0 pid=368283)[0;0m     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(EngineCore_DP0 pid=368283)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368283)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 279, in __init__
[1;36m(EngineCore_DP0 pid=368283)[0;0m     self.model = Qwen3Model(
[1;36m(EngineCore_DP0 pid=368283)[0;0m                  ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368283)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=368283)[0;0m     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=368283)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 253, in __init__
[1;36m(EngineCore_DP0 pid=368283)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=368283)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=368283)[0;0m     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=368283)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 337, in __init__
[1;36m(EngineCore_DP0 pid=368283)[0;0m     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(EngineCore_DP0 pid=368283)[0;0m                                                     ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368283)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 651, in make_layers
[1;36m(EngineCore_DP0 pid=368283)[0;0m     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(EngineCore_DP0 pid=368283)[0;0m                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368283)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 339, in <lambda>
[1;36m(EngineCore_DP0 pid=368283)[0;0m     lambda prefix: decoder_layer_type(
[1;36m(EngineCore_DP0 pid=368283)[0;0m                    ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368283)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 201, in __init__
[1;36m(EngineCore_DP0 pid=368283)[0;0m     self.mlp = Qwen3MLP(
[1;36m(EngineCore_DP0 pid=368283)[0;0m                ^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368283)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 84, in __init__
[1;36m(EngineCore_DP0 pid=368283)[0;0m     self.gate_up_proj = MergedColumnParallelLinear(
[1;36m(EngineCore_DP0 pid=368283)[0;0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368283)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 631, in __init__
[1;36m(EngineCore_DP0 pid=368283)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=368283)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 484, in __init__
[1;36m(EngineCore_DP0 pid=368283)[0;0m     self.quant_method.create_weights(
[1;36m(EngineCore_DP0 pid=368283)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 215, in create_weights
[1;36m(EngineCore_DP0 pid=368283)[0;0m     data=torch.empty(
[1;36m(EngineCore_DP0 pid=368283)[0;0m          ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368283)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_device.py", line 103, in __torch_function__
[1;36m(EngineCore_DP0 pid=368283)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=368283)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368283)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 340.00 MiB. GPU 0 has a total capacity of 44.39 GiB of which 34.00 MiB is free. Including non-PyTorch memory, this process has 23.10 GiB memory in use. Process 368286 has 21.25 GiB memory in use. Of the allocated memory 22.58 GiB is allocated by PyTorch, and 18.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W1204 13:14:00.159393720 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank0]:[W1204 13:14:00.173269930 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: Qwen/Qwen3-14B (backend=vllm)
[ModelCache] Using max_model_len=40960 for Qwen/Qwen3-14B
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 13:14:22 [utils.py:253] non-default args: {'max_model_len': 40960, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'Qwen/Qwen3-14B'}
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: Qwen/Qwen3-14B (backend=vllm)
[ModelCache] Using max_model_len=40960 for Qwen/Qwen3-14B
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 13:14:22 [utils.py:253] non-default args: {'max_model_len': 40960, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'Qwen/Qwen3-14B'}
INFO 12-04 13:14:22 [model.py:631] Resolved architecture: Qwen3ForCausalLM
INFO 12-04 13:14:22 [model.py:1745] Using max model len 40960
INFO 12-04 13:14:22 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
INFO 12-04 13:14:22 [model.py:631] Resolved architecture: Qwen3ForCausalLM
INFO 12-04 13:14:22 [model.py:1745] Using max model len 40960
INFO 12-04 13:14:22 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=368872)[0;0m INFO 12-04 13:14:41 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='Qwen/Qwen3-14B', speculative_config=None, tokenizer='Qwen/Qwen3-14B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-14B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=368863)[0;0m INFO 12-04 13:14:41 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='Qwen/Qwen3-14B', speculative_config=None, tokenizer='Qwen/Qwen3-14B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-14B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=368863)[0;0m INFO 12-04 13:14:43 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.98:49899 backend=nccl
[1;36m(EngineCore_DP0 pid=368872)[0;0m INFO 12-04 13:14:43 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.98:35849 backend=nccl
[W1204 13:14:43.230097790 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-25-0.rc.tch.harvard.edu]:49899 (errno: 97 - Address family not supported by protocol).
[W1204 13:14:43.230045139 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-25-0.rc.tch.harvard.edu]:35849 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=368863)[0;0m INFO 12-04 13:14:43 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=368872)[0;0m INFO 12-04 13:14:43 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=368872)[0;0m INFO 12-04 13:14:44 [gpu_model_runner.py:3259] Starting to load model Qwen/Qwen3-14B...
[1;36m(EngineCore_DP0 pid=368863)[0;0m INFO 12-04 13:14:44 [gpu_model_runner.py:3259] Starting to load model Qwen/Qwen3-14B...
[1;36m(EngineCore_DP0 pid=368872)[0;0m INFO 12-04 13:14:45 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=368872)[0;0m INFO 12-04 13:14:45 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=368863)[0;0m INFO 12-04 13:14:45 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=368863)[0;0m INFO 12-04 13:14:45 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=368863)[0;0m ERROR 12-04 13:14:46 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=368863)[0;0m ERROR 12-04 13:14:46 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=368863)[0;0m ERROR 12-04 13:14:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=368863)[0;0m ERROR 12-04 13:14:46 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=368863)[0;0m ERROR 12-04 13:14:46 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368863)[0;0m ERROR 12-04 13:14:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=368863)[0;0m ERROR 12-04 13:14:46 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=368863)[0;0m ERROR 12-04 13:14:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=368863)[0;0m ERROR 12-04 13:14:46 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=368863)[0;0m ERROR 12-04 13:14:46 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368863)[0;0m ERROR 12-04 13:14:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=368863)[0;0m ERROR 12-04 13:14:46 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=368863)[0;0m ERROR 12-04 13:14:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[1;36m(EngineCore_DP0 pid=368863)[0;0m ERROR 12-04 13:14:46 [core.py:842]     self.driver_worker.load_model()
[1;36m(EngineCore_DP0 pid=368863)[0;0m ERROR 12-04 13:14:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 273, in load_model
[1;36m(EngineCore_DP0 pid=368863)[0;0m ERROR 12-04 13:14:46 [core.py:842]     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[1;36m(EngineCore_DP0 pid=368863)[0;0m ERROR 12-04 13:14:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3276, in load_model
[1;36m(EngineCore_DP0 pid=368863)[0;0m ERROR 12-04 13:14:46 [core.py:842]     self.model = model_loader.load_model(
[1;36m(EngineCore_DP0 pid=368863)[0;0m ERROR 12-04 13:14:46 [core.py:842]                  ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368863)[0;0m ERROR 12-04 13:14:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
[1;36m(EngineCore_DP0 pid=368863)[0;0m ERROR 12-04 13:14:46 [core.py:842]     model = initialize_model(
[1;36m(EngineCore_DP0 pid=368863)[0;0m ERROR 12-04 13:14:46 [core.py:842]             ^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368863)[0;0m ERROR 12-04 13:14:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py", line 55, in initialize_model
[1;36m(EngineCore_DP0 pid=368863)[0;0m ERROR 12-04 13:14:46 [core.py:842]     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(EngineCore_DP0 pid=368863)[0;0m ERROR 12-04 13:14:46 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368863)[0;0m ERROR 12-04 13:14:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 279, in __init__
[1;36m(EngineCore_DP0 pid=368863)[0;0m ERROR 12-04 13:14:46 [core.py:842]     self.model = Qwen3Model(
[1;36m(EngineCore_DP0 pid=368863)[0;0m ERROR 12-04 13:14:46 [core.py:842]                  ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368863)[0;0m ERROR 12-04 13:14:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=368863)[0;0m ERROR 12-04 13:14:46 [core.py:842]     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=368863)[0;0m ERROR 12-04 13:14:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 253, in __init__
[1;36m(EngineCore_DP0 pid=368863)[0;0m ERROR 12-04 13:14:46 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=368863)[0;0m ERROR 12-04 13:14:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=368863)[0;0m ERROR 12-04 13:14:46 [core.py:842]     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=368863)[0;0m ERROR 12-04 13:14:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 337, in __init__
[1;36m(EngineCore_DP0 pid=368863)[0;0m ERROR 12-04 13:14:46 [core.py:842]     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(EngineCore_DP0 pid=368863)[0;0m ERROR 12-04 13:14:46 [core.py:842]                                                     ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368863)[0;0m ERROR 12-04 13:14:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 651, in make_layers
[1;36m(EngineCore_DP0 pid=368863)[0;0m ERROR 12-04 13:14:46 [core.py:842]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(EngineCore_DP0 pid=368863)[0;0m ERROR 12-04 13:14:46 [core.py:842]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368863)[0;0m ERROR 12-04 13:14:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 339, in <lambda>
[1;36m(EngineCore_DP0 pid=368863)[0;0m ERROR 12-04 13:14:46 [core.py:842]     lambda prefix: decoder_layer_type(
[1;36m(EngineCore_DP0 pid=368863)[0;0m ERROR 12-04 13:14:46 [core.py:842]                    ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368863)[0;0m ERROR 12-04 13:14:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 201, in __init__
[1;36m(EngineCore_DP0 pid=368863)[0;0m ERROR 12-04 13:14:46 [core.py:842]     self.mlp = Qwen3MLP(
[1;36m(EngineCore_DP0 pid=368863)[0;0m ERROR 12-04 13:14:46 [core.py:842]                ^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368863)[0;0m ERROR 12-04 13:14:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 84, in __init__
[1;36m(EngineCore_DP0 pid=368863)[0;0m ERROR 12-04 13:14:46 [core.py:842]     self.gate_up_proj = MergedColumnParallelLinear(
[1;36m(EngineCore_DP0 pid=368863)[0;0m ERROR 12-04 13:14:46 [core.py:842]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368863)[0;0m ERROR 12-04 13:14:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 631, in __init__
[1;36m(EngineCore_DP0 pid=368863)[0;0m ERROR 12-04 13:14:46 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=368863)[0;0m ERROR 12-04 13:14:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 484, in __init__
[1;36m(EngineCore_DP0 pid=368863)[0;0m ERROR 12-04 13:14:46 [core.py:842]     self.quant_method.create_weights(
[1;36m(EngineCore_DP0 pid=368863)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=368863)[0;0m ERROR 12-04 13:14:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 215, in create_weights
[1;36m(EngineCore_DP0 pid=368863)[0;0m ERROR 12-04 13:14:46 [core.py:842]     data=torch.empty(
[1;36m(EngineCore_DP0 pid=368863)[0;0m ERROR 12-04 13:14:46 [core.py:842]          ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368863)[0;0m ERROR 12-04 13:14:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_device.py", line 103, in __torch_function__
[1;36m(EngineCore_DP0 pid=368863)[0;0m ERROR 12-04 13:14:46 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=368863)[0;0m ERROR 12-04 13:14:46 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368863)[0;0m ERROR 12-04 13:14:46 [core.py:842] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 340.00 MiB. GPU 0 has a total capacity of 44.39 GiB of which 34.00 MiB is free. Process 368872 has 23.10 GiB memory in use. Including non-PyTorch memory, this process has 21.25 GiB memory in use. Of the allocated memory 20.73 GiB is allocated by PyTorch, and 18.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=368863)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=368863)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=368863)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=368863)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=368863)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=368863)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=368863)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=368863)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=368863)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=368863)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368863)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=368863)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=368863)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=368863)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=368863)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368863)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=368863)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=368863)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[1;36m(EngineCore_DP0 pid=368863)[0;0m     self.driver_worker.load_model()
[1;36m(EngineCore_DP0 pid=368863)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 273, in load_model
[1;36m(EngineCore_DP0 pid=368863)[0;0m     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[1;36m(EngineCore_DP0 pid=368863)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3276, in load_model
[1;36m(EngineCore_DP0 pid=368863)[0;0m     self.model = model_loader.load_model(
[1;36m(EngineCore_DP0 pid=368863)[0;0m                  ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368863)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
[1;36m(EngineCore_DP0 pid=368863)[0;0m     model = initialize_model(
[1;36m(EngineCore_DP0 pid=368863)[0;0m             ^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368863)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py", line 55, in initialize_model
[1;36m(EngineCore_DP0 pid=368863)[0;0m     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(EngineCore_DP0 pid=368863)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368863)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 279, in __init__
[1;36m(EngineCore_DP0 pid=368863)[0;0m     self.model = Qwen3Model(
[1;36m(EngineCore_DP0 pid=368863)[0;0m                  ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368863)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=368863)[0;0m     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=368863)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 253, in __init__
[1;36m(EngineCore_DP0 pid=368863)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=368863)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=368863)[0;0m     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=368863)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 337, in __init__
[1;36m(EngineCore_DP0 pid=368863)[0;0m     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(EngineCore_DP0 pid=368863)[0;0m                                                     ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368863)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 651, in make_layers
[1;36m(EngineCore_DP0 pid=368863)[0;0m     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(EngineCore_DP0 pid=368863)[0;0m                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368863)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 339, in <lambda>
[1;36m(EngineCore_DP0 pid=368863)[0;0m     lambda prefix: decoder_layer_type(
[1;36m(EngineCore_DP0 pid=368863)[0;0m                    ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368863)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 201, in __init__
[1;36m(EngineCore_DP0 pid=368863)[0;0m     self.mlp = Qwen3MLP(
[1;36m(EngineCore_DP0 pid=368863)[0;0m                ^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368863)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 84, in __init__
[1;36m(EngineCore_DP0 pid=368863)[0;0m     self.gate_up_proj = MergedColumnParallelLinear(
[1;36m(EngineCore_DP0 pid=368863)[0;0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368863)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 631, in __init__
[1;36m(EngineCore_DP0 pid=368863)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=368863)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 484, in __init__
[1;36m(EngineCore_DP0 pid=368863)[0;0m     self.quant_method.create_weights(
[1;36m(EngineCore_DP0 pid=368863)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 215, in create_weights
[1;36m(EngineCore_DP0 pid=368863)[0;0m     data=torch.empty(
[1;36m(EngineCore_DP0 pid=368863)[0;0m          ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368863)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_device.py", line 103, in __torch_function__
[1;36m(EngineCore_DP0 pid=368863)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=368863)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368863)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 340.00 MiB. GPU 0 has a total capacity of 44.39 GiB of which 34.00 MiB is free. Process 368872 has 23.10 GiB memory in use. Including non-PyTorch memory, this process has 21.25 GiB memory in use. Of the allocated memory 20.73 GiB is allocated by PyTorch, and 18.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=368872)[0;0m ERROR 12-04 13:14:46 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=368872)[0;0m ERROR 12-04 13:14:46 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=368872)[0;0m ERROR 12-04 13:14:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=368872)[0;0m ERROR 12-04 13:14:46 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=368872)[0;0m ERROR 12-04 13:14:46 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368872)[0;0m ERROR 12-04 13:14:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=368872)[0;0m ERROR 12-04 13:14:46 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=368872)[0;0m ERROR 12-04 13:14:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=368872)[0;0m ERROR 12-04 13:14:46 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=368872)[0;0m ERROR 12-04 13:14:46 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368872)[0;0m ERROR 12-04 13:14:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=368872)[0;0m ERROR 12-04 13:14:46 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=368872)[0;0m ERROR 12-04 13:14:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[1;36m(EngineCore_DP0 pid=368872)[0;0m ERROR 12-04 13:14:46 [core.py:842]     self.driver_worker.load_model()
[1;36m(EngineCore_DP0 pid=368872)[0;0m ERROR 12-04 13:14:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 273, in load_model
[1;36m(EngineCore_DP0 pid=368872)[0;0m ERROR 12-04 13:14:46 [core.py:842]     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[1;36m(EngineCore_DP0 pid=368872)[0;0m ERROR 12-04 13:14:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3276, in load_model
[1;36m(EngineCore_DP0 pid=368872)[0;0m ERROR 12-04 13:14:46 [core.py:842]     self.model = model_loader.load_model(
[1;36m(EngineCore_DP0 pid=368872)[0;0m ERROR 12-04 13:14:46 [core.py:842]                  ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368872)[0;0m ERROR 12-04 13:14:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
[1;36m(EngineCore_DP0 pid=368872)[0;0m ERROR 12-04 13:14:46 [core.py:842]     model = initialize_model(
[1;36m(EngineCore_DP0 pid=368872)[0;0m ERROR 12-04 13:14:46 [core.py:842]             ^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368872)[0;0m ERROR 12-04 13:14:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py", line 55, in initialize_model
[1;36m(EngineCore_DP0 pid=368872)[0;0m ERROR 12-04 13:14:46 [core.py:842]     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(EngineCore_DP0 pid=368872)[0;0m ERROR 12-04 13:14:46 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368872)[0;0m ERROR 12-04 13:14:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 279, in __init__
[1;36m(EngineCore_DP0 pid=368872)[0;0m ERROR 12-04 13:14:46 [core.py:842]     self.model = Qwen3Model(
[1;36m(EngineCore_DP0 pid=368872)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=368872)[0;0m ERROR 12-04 13:14:46 [core.py:842]                  ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368872)[0;0m ERROR 12-04 13:14:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=368872)[0;0m ERROR 12-04 13:14:46 [core.py:842]     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=368872)[0;0m ERROR 12-04 13:14:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 253, in __init__
[1;36m(EngineCore_DP0 pid=368872)[0;0m ERROR 12-04 13:14:46 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=368872)[0;0m ERROR 12-04 13:14:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=368872)[0;0m ERROR 12-04 13:14:46 [core.py:842]     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=368872)[0;0m ERROR 12-04 13:14:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 337, in __init__
[1;36m(EngineCore_DP0 pid=368872)[0;0m ERROR 12-04 13:14:46 [core.py:842]     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(EngineCore_DP0 pid=368872)[0;0m ERROR 12-04 13:14:46 [core.py:842]                                                     ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368872)[0;0m ERROR 12-04 13:14:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 651, in make_layers
[1;36m(EngineCore_DP0 pid=368872)[0;0m ERROR 12-04 13:14:46 [core.py:842]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(EngineCore_DP0 pid=368872)[0;0m ERROR 12-04 13:14:46 [core.py:842]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368872)[0;0m ERROR 12-04 13:14:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 339, in <lambda>
[1;36m(EngineCore_DP0 pid=368872)[0;0m ERROR 12-04 13:14:46 [core.py:842]     lambda prefix: decoder_layer_type(
[1;36m(EngineCore_DP0 pid=368872)[0;0m ERROR 12-04 13:14:46 [core.py:842]                    ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368872)[0;0m ERROR 12-04 13:14:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 201, in __init__
[1;36m(EngineCore_DP0 pid=368872)[0;0m ERROR 12-04 13:14:46 [core.py:842]     self.mlp = Qwen3MLP(
[1;36m(EngineCore_DP0 pid=368872)[0;0m ERROR 12-04 13:14:46 [core.py:842]                ^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368872)[0;0m ERROR 12-04 13:14:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 84, in __init__
[1;36m(EngineCore_DP0 pid=368872)[0;0m ERROR 12-04 13:14:46 [core.py:842]     self.gate_up_proj = MergedColumnParallelLinear(
[1;36m(EngineCore_DP0 pid=368872)[0;0m ERROR 12-04 13:14:46 [core.py:842]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368872)[0;0m ERROR 12-04 13:14:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 631, in __init__
[1;36m(EngineCore_DP0 pid=368872)[0;0m ERROR 12-04 13:14:46 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=368872)[0;0m ERROR 12-04 13:14:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 484, in __init__
[1;36m(EngineCore_DP0 pid=368872)[0;0m ERROR 12-04 13:14:46 [core.py:842]     self.quant_method.create_weights(
[1;36m(EngineCore_DP0 pid=368872)[0;0m ERROR 12-04 13:14:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 215, in create_weights
[1;36m(EngineCore_DP0 pid=368872)[0;0m ERROR 12-04 13:14:46 [core.py:842]     data=torch.empty(
[1;36m(EngineCore_DP0 pid=368872)[0;0m ERROR 12-04 13:14:46 [core.py:842]          ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368872)[0;0m ERROR 12-04 13:14:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_device.py", line 103, in __torch_function__
[1;36m(EngineCore_DP0 pid=368872)[0;0m ERROR 12-04 13:14:46 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=368872)[0;0m ERROR 12-04 13:14:46 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368872)[0;0m ERROR 12-04 13:14:46 [core.py:842] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 340.00 MiB. GPU 0 has a total capacity of 44.39 GiB of which 54.00 MiB is free. Including non-PyTorch memory, this process has 23.10 GiB memory in use. Process 368863 has 21.25 GiB memory in use. Of the allocated memory 22.58 GiB is allocated by PyTorch, and 18.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=368872)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=368872)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=368872)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=368872)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=368872)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=368872)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=368872)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=368872)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=368872)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=368872)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368872)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=368872)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=368872)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=368872)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=368872)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368872)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=368872)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=368872)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[1;36m(EngineCore_DP0 pid=368872)[0;0m     self.driver_worker.load_model()
[1;36m(EngineCore_DP0 pid=368872)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 273, in load_model
[1;36m(EngineCore_DP0 pid=368872)[0;0m     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[1;36m(EngineCore_DP0 pid=368872)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3276, in load_model
[1;36m(EngineCore_DP0 pid=368872)[0;0m     self.model = model_loader.load_model(
[1;36m(EngineCore_DP0 pid=368872)[0;0m                  ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368872)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
[1;36m(EngineCore_DP0 pid=368872)[0;0m     model = initialize_model(
[1;36m(EngineCore_DP0 pid=368872)[0;0m             ^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368872)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py", line 55, in initialize_model
[1;36m(EngineCore_DP0 pid=368872)[0;0m     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(EngineCore_DP0 pid=368872)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368872)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 279, in __init__
[1;36m(EngineCore_DP0 pid=368872)[0;0m     self.model = Qwen3Model(
[1;36m(EngineCore_DP0 pid=368872)[0;0m                  ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368872)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=368872)[0;0m     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=368872)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 253, in __init__
[1;36m(EngineCore_DP0 pid=368872)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=368872)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=368872)[0;0m     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=368872)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 337, in __init__
[1;36m(EngineCore_DP0 pid=368872)[0;0m     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(EngineCore_DP0 pid=368872)[0;0m                                                     ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368872)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 651, in make_layers
[1;36m(EngineCore_DP0 pid=368872)[0;0m     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(EngineCore_DP0 pid=368872)[0;0m                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368872)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 339, in <lambda>
[1;36m(EngineCore_DP0 pid=368872)[0;0m     lambda prefix: decoder_layer_type(
[1;36m(EngineCore_DP0 pid=368872)[0;0m                    ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368872)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 201, in __init__
[1;36m(EngineCore_DP0 pid=368872)[0;0m     self.mlp = Qwen3MLP(
[1;36m(EngineCore_DP0 pid=368872)[0;0m                ^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368872)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 84, in __init__
[1;36m(EngineCore_DP0 pid=368872)[0;0m     self.gate_up_proj = MergedColumnParallelLinear(
[1;36m(EngineCore_DP0 pid=368872)[0;0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368872)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 631, in __init__
[1;36m(EngineCore_DP0 pid=368872)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=368872)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 484, in __init__
[1;36m(EngineCore_DP0 pid=368872)[0;0m     self.quant_method.create_weights(
[1;36m(EngineCore_DP0 pid=368872)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 215, in create_weights
[1;36m(EngineCore_DP0 pid=368872)[0;0m     data=torch.empty(
[1;36m(EngineCore_DP0 pid=368872)[0;0m          ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368872)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_device.py", line 103, in __torch_function__
[1;36m(EngineCore_DP0 pid=368872)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=368872)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368872)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 340.00 MiB. GPU 0 has a total capacity of 44.39 GiB of which 54.00 MiB is free. Including non-PyTorch memory, this process has 23.10 GiB memory in use. Process 368863 has 21.25 GiB memory in use. Of the allocated memory 22.58 GiB is allocated by PyTorch, and 18.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W1204 13:14:47.597571238 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank0]:[W1204 13:14:47.608970003 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: Qwen/Qwen3-14B (backend=vllm)
[ModelCache] Using max_model_len=40960 for Qwen/Qwen3-14B
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 13:15:08 [utils.py:253] non-default args: {'max_model_len': 40960, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'Qwen/Qwen3-14B'}
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: Qwen/Qwen3-14B (backend=vllm)
[ModelCache] Using max_model_len=40960 for Qwen/Qwen3-14B
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 13:15:08 [utils.py:253] non-default args: {'max_model_len': 40960, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'Qwen/Qwen3-14B'}
INFO 12-04 13:15:08 [model.py:631] Resolved architecture: Qwen3ForCausalLM
INFO 12-04 13:15:08 [model.py:1745] Using max model len 40960
INFO 12-04 13:15:08 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
INFO 12-04 13:15:08 [model.py:631] Resolved architecture: Qwen3ForCausalLM
INFO 12-04 13:15:08 [model.py:1745] Using max model len 40960
INFO 12-04 13:15:08 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=369385)[0;0m INFO 12-04 13:15:24 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='Qwen/Qwen3-14B', speculative_config=None, tokenizer='Qwen/Qwen3-14B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-14B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=369394)[0;0m INFO 12-04 13:15:25 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='Qwen/Qwen3-14B', speculative_config=None, tokenizer='Qwen/Qwen3-14B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-14B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=369394)[0;0m INFO 12-04 13:15:27 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.98:33125 backend=nccl
[1;36m(EngineCore_DP0 pid=369385)[0;0m INFO 12-04 13:15:27 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.98:42143 backend=nccl
[W1204 13:15:27.853716832 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-25-0.rc.tch.harvard.edu]:33125 (errno: 97 - Address family not supported by protocol).
[W1204 13:15:27.856100653 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-25-0.rc.tch.harvard.edu]:42143 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=369394)[0;0m INFO 12-04 13:15:27 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=369385)[0;0m INFO 12-04 13:15:27 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=369394)[0;0m INFO 12-04 13:15:27 [gpu_model_runner.py:3259] Starting to load model Qwen/Qwen3-14B...
[1;36m(EngineCore_DP0 pid=369385)[0;0m INFO 12-04 13:15:27 [gpu_model_runner.py:3259] Starting to load model Qwen/Qwen3-14B...
[1;36m(EngineCore_DP0 pid=369394)[0;0m INFO 12-04 13:15:29 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=369394)[0;0m INFO 12-04 13:15:29 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=369385)[0;0m INFO 12-04 13:15:29 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=369385)[0;0m INFO 12-04 13:15:29 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=369385)[0;0m ERROR 12-04 13:15:30 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=369385)[0;0m ERROR 12-04 13:15:30 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=369385)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=369385)[0;0m ERROR 12-04 13:15:30 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=369385)[0;0m ERROR 12-04 13:15:30 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369385)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=369385)[0;0m ERROR 12-04 13:15:30 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=369385)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=369385)[0;0m ERROR 12-04 13:15:30 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=369385)[0;0m ERROR 12-04 13:15:30 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369385)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=369385)[0;0m ERROR 12-04 13:15:30 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=369385)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[1;36m(EngineCore_DP0 pid=369385)[0;0m ERROR 12-04 13:15:30 [core.py:842]     self.driver_worker.load_model()
[1;36m(EngineCore_DP0 pid=369385)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 273, in load_model
[1;36m(EngineCore_DP0 pid=369385)[0;0m ERROR 12-04 13:15:30 [core.py:842]     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[1;36m(EngineCore_DP0 pid=369385)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3276, in load_model
[1;36m(EngineCore_DP0 pid=369385)[0;0m ERROR 12-04 13:15:30 [core.py:842]     self.model = model_loader.load_model(
[1;36m(EngineCore_DP0 pid=369385)[0;0m ERROR 12-04 13:15:30 [core.py:842]                  ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369385)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
[1;36m(EngineCore_DP0 pid=369385)[0;0m ERROR 12-04 13:15:30 [core.py:842]     model = initialize_model(
[1;36m(EngineCore_DP0 pid=369385)[0;0m ERROR 12-04 13:15:30 [core.py:842]             ^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369385)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py", line 55, in initialize_model
[1;36m(EngineCore_DP0 pid=369385)[0;0m ERROR 12-04 13:15:30 [core.py:842]     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(EngineCore_DP0 pid=369385)[0;0m ERROR 12-04 13:15:30 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369385)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 279, in __init__
[1;36m(EngineCore_DP0 pid=369385)[0;0m ERROR 12-04 13:15:30 [core.py:842]     self.model = Qwen3Model(
[1;36m(EngineCore_DP0 pid=369385)[0;0m ERROR 12-04 13:15:30 [core.py:842]                  ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369385)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=369385)[0;0m ERROR 12-04 13:15:30 [core.py:842]     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=369385)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 253, in __init__
[1;36m(EngineCore_DP0 pid=369385)[0;0m ERROR 12-04 13:15:30 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=369385)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=369385)[0;0m ERROR 12-04 13:15:30 [core.py:842]     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=369385)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 337, in __init__
[1;36m(EngineCore_DP0 pid=369385)[0;0m ERROR 12-04 13:15:30 [core.py:842]     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(EngineCore_DP0 pid=369385)[0;0m ERROR 12-04 13:15:30 [core.py:842]                                                     ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369385)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 651, in make_layers
[1;36m(EngineCore_DP0 pid=369385)[0;0m ERROR 12-04 13:15:30 [core.py:842]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(EngineCore_DP0 pid=369385)[0;0m ERROR 12-04 13:15:30 [core.py:842]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369385)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 339, in <lambda>
[1;36m(EngineCore_DP0 pid=369385)[0;0m ERROR 12-04 13:15:30 [core.py:842]     lambda prefix: decoder_layer_type(
[1;36m(EngineCore_DP0 pid=369385)[0;0m ERROR 12-04 13:15:30 [core.py:842]                    ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369385)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 201, in __init__
[1;36m(EngineCore_DP0 pid=369385)[0;0m ERROR 12-04 13:15:30 [core.py:842]     self.mlp = Qwen3MLP(
[1;36m(EngineCore_DP0 pid=369385)[0;0m ERROR 12-04 13:15:30 [core.py:842]                ^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369385)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 84, in __init__
[1;36m(EngineCore_DP0 pid=369385)[0;0m ERROR 12-04 13:15:30 [core.py:842]     self.gate_up_proj = MergedColumnParallelLinear(
[1;36m(EngineCore_DP0 pid=369385)[0;0m ERROR 12-04 13:15:30 [core.py:842]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369385)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 631, in __init__
[1;36m(EngineCore_DP0 pid=369385)[0;0m ERROR 12-04 13:15:30 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=369385)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 484, in __init__
[1;36m(EngineCore_DP0 pid=369385)[0;0m ERROR 12-04 13:15:30 [core.py:842]     self.quant_method.create_weights(
[1;36m(EngineCore_DP0 pid=369385)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=369385)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 215, in create_weights
[1;36m(EngineCore_DP0 pid=369385)[0;0m ERROR 12-04 13:15:30 [core.py:842]     data=torch.empty(
[1;36m(EngineCore_DP0 pid=369385)[0;0m ERROR 12-04 13:15:30 [core.py:842]          ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369385)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_device.py", line 103, in __torch_function__
[1;36m(EngineCore_DP0 pid=369385)[0;0m ERROR 12-04 13:15:30 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=369385)[0;0m ERROR 12-04 13:15:30 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369385)[0;0m ERROR 12-04 13:15:30 [core.py:842] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 340.00 MiB. GPU 0 has a total capacity of 44.39 GiB of which 54.00 MiB is free. Including non-PyTorch memory, this process has 20.63 GiB memory in use. Process 369394 has 23.71 GiB memory in use. Of the allocated memory 20.11 GiB is allocated by PyTorch, and 18.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=369385)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=369385)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=369385)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=369385)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=369385)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=369385)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=369385)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=369385)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=369385)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=369385)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369385)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=369385)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=369385)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=369385)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=369385)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369385)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=369385)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=369385)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[1;36m(EngineCore_DP0 pid=369385)[0;0m     self.driver_worker.load_model()
[1;36m(EngineCore_DP0 pid=369385)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 273, in load_model
[1;36m(EngineCore_DP0 pid=369385)[0;0m     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[1;36m(EngineCore_DP0 pid=369385)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3276, in load_model
[1;36m(EngineCore_DP0 pid=369385)[0;0m     self.model = model_loader.load_model(
[1;36m(EngineCore_DP0 pid=369385)[0;0m                  ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369385)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
[1;36m(EngineCore_DP0 pid=369385)[0;0m     model = initialize_model(
[1;36m(EngineCore_DP0 pid=369385)[0;0m             ^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369385)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py", line 55, in initialize_model
[1;36m(EngineCore_DP0 pid=369385)[0;0m     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(EngineCore_DP0 pid=369385)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369385)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 279, in __init__
[1;36m(EngineCore_DP0 pid=369385)[0;0m     self.model = Qwen3Model(
[1;36m(EngineCore_DP0 pid=369385)[0;0m                  ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369385)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=369385)[0;0m     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=369385)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 253, in __init__
[1;36m(EngineCore_DP0 pid=369385)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=369385)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=369385)[0;0m     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=369385)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 337, in __init__
[1;36m(EngineCore_DP0 pid=369385)[0;0m     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(EngineCore_DP0 pid=369385)[0;0m                                                     ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369385)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 651, in make_layers
[1;36m(EngineCore_DP0 pid=369385)[0;0m     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(EngineCore_DP0 pid=369385)[0;0m                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369385)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 339, in <lambda>
[1;36m(EngineCore_DP0 pid=369385)[0;0m     lambda prefix: decoder_layer_type(
[1;36m(EngineCore_DP0 pid=369385)[0;0m                    ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369385)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 201, in __init__
[1;36m(EngineCore_DP0 pid=369385)[0;0m     self.mlp = Qwen3MLP(
[1;36m(EngineCore_DP0 pid=369385)[0;0m                ^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369385)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 84, in __init__
[1;36m(EngineCore_DP0 pid=369385)[0;0m     self.gate_up_proj = MergedColumnParallelLinear(
[1;36m(EngineCore_DP0 pid=369385)[0;0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369385)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 631, in __init__
[1;36m(EngineCore_DP0 pid=369385)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=369385)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 484, in __init__
[1;36m(EngineCore_DP0 pid=369385)[0;0m     self.quant_method.create_weights(
[1;36m(EngineCore_DP0 pid=369385)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 215, in create_weights
[1;36m(EngineCore_DP0 pid=369385)[0;0m     data=torch.empty(
[1;36m(EngineCore_DP0 pid=369385)[0;0m          ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369385)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_device.py", line 103, in __torch_function__
[1;36m(EngineCore_DP0 pid=369385)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=369385)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369385)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 340.00 MiB. GPU 0 has a total capacity of 44.39 GiB of which 54.00 MiB is free. Including non-PyTorch memory, this process has 20.63 GiB memory in use. Process 369394 has 23.71 GiB memory in use. Of the allocated memory 20.11 GiB is allocated by PyTorch, and 18.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=369394)[0;0m ERROR 12-04 13:15:30 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=369394)[0;0m ERROR 12-04 13:15:30 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=369394)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=369394)[0;0m ERROR 12-04 13:15:30 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=369394)[0;0m ERROR 12-04 13:15:30 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369394)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=369394)[0;0m ERROR 12-04 13:15:30 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=369394)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=369394)[0;0m ERROR 12-04 13:15:30 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=369394)[0;0m ERROR 12-04 13:15:30 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369394)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=369394)[0;0m ERROR 12-04 13:15:30 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=369394)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[1;36m(EngineCore_DP0 pid=369394)[0;0m ERROR 12-04 13:15:30 [core.py:842]     self.driver_worker.load_model()
[1;36m(EngineCore_DP0 pid=369394)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 273, in load_model
[1;36m(EngineCore_DP0 pid=369394)[0;0m ERROR 12-04 13:15:30 [core.py:842]     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[1;36m(EngineCore_DP0 pid=369394)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3276, in load_model
[1;36m(EngineCore_DP0 pid=369394)[0;0m ERROR 12-04 13:15:30 [core.py:842]     self.model = model_loader.load_model(
[1;36m(EngineCore_DP0 pid=369394)[0;0m ERROR 12-04 13:15:30 [core.py:842]                  ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369394)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
[1;36m(EngineCore_DP0 pid=369394)[0;0m ERROR 12-04 13:15:30 [core.py:842]     model = initialize_model(
[1;36m(EngineCore_DP0 pid=369394)[0;0m ERROR 12-04 13:15:30 [core.py:842]             ^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369394)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py", line 55, in initialize_model
[1;36m(EngineCore_DP0 pid=369394)[0;0m ERROR 12-04 13:15:30 [core.py:842]     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(EngineCore_DP0 pid=369394)[0;0m ERROR 12-04 13:15:30 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369394)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 279, in __init__
[1;36m(EngineCore_DP0 pid=369394)[0;0m ERROR 12-04 13:15:30 [core.py:842]     self.model = Qwen3Model(
[1;36m(EngineCore_DP0 pid=369394)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=369394)[0;0m ERROR 12-04 13:15:30 [core.py:842]                  ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369394)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=369394)[0;0m ERROR 12-04 13:15:30 [core.py:842]     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=369394)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 253, in __init__
[1;36m(EngineCore_DP0 pid=369394)[0;0m ERROR 12-04 13:15:30 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=369394)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=369394)[0;0m ERROR 12-04 13:15:30 [core.py:842]     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=369394)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 337, in __init__
[1;36m(EngineCore_DP0 pid=369394)[0;0m ERROR 12-04 13:15:30 [core.py:842]     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(EngineCore_DP0 pid=369394)[0;0m ERROR 12-04 13:15:30 [core.py:842]                                                     ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369394)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 651, in make_layers
[1;36m(EngineCore_DP0 pid=369394)[0;0m ERROR 12-04 13:15:30 [core.py:842]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(EngineCore_DP0 pid=369394)[0;0m ERROR 12-04 13:15:30 [core.py:842]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369394)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 339, in <lambda>
[1;36m(EngineCore_DP0 pid=369394)[0;0m ERROR 12-04 13:15:30 [core.py:842]     lambda prefix: decoder_layer_type(
[1;36m(EngineCore_DP0 pid=369394)[0;0m ERROR 12-04 13:15:30 [core.py:842]                    ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369394)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 201, in __init__
[1;36m(EngineCore_DP0 pid=369394)[0;0m ERROR 12-04 13:15:30 [core.py:842]     self.mlp = Qwen3MLP(
[1;36m(EngineCore_DP0 pid=369394)[0;0m ERROR 12-04 13:15:30 [core.py:842]                ^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369394)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 84, in __init__
[1;36m(EngineCore_DP0 pid=369394)[0;0m ERROR 12-04 13:15:30 [core.py:842]     self.gate_up_proj = MergedColumnParallelLinear(
[1;36m(EngineCore_DP0 pid=369394)[0;0m ERROR 12-04 13:15:30 [core.py:842]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369394)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 631, in __init__
[1;36m(EngineCore_DP0 pid=369394)[0;0m ERROR 12-04 13:15:30 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=369394)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 484, in __init__
[1;36m(EngineCore_DP0 pid=369394)[0;0m ERROR 12-04 13:15:30 [core.py:842]     self.quant_method.create_weights(
[1;36m(EngineCore_DP0 pid=369394)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 215, in create_weights
[1;36m(EngineCore_DP0 pid=369394)[0;0m ERROR 12-04 13:15:30 [core.py:842]     data=torch.empty(
[1;36m(EngineCore_DP0 pid=369394)[0;0m ERROR 12-04 13:15:30 [core.py:842]          ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369394)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_device.py", line 103, in __torch_function__
[1;36m(EngineCore_DP0 pid=369394)[0;0m ERROR 12-04 13:15:30 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=369394)[0;0m ERROR 12-04 13:15:30 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369394)[0;0m ERROR 12-04 13:15:30 [core.py:842] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 340.00 MiB. GPU 0 has a total capacity of 44.39 GiB of which 34.00 MiB is free. Process 369385 has 20.63 GiB memory in use. Including non-PyTorch memory, this process has 23.71 GiB memory in use. Of the allocated memory 23.19 GiB is allocated by PyTorch, and 18.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=369394)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=369394)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=369394)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=369394)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=369394)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=369394)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=369394)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=369394)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=369394)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=369394)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369394)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=369394)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=369394)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=369394)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=369394)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369394)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=369394)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=369394)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[1;36m(EngineCore_DP0 pid=369394)[0;0m     self.driver_worker.load_model()
[1;36m(EngineCore_DP0 pid=369394)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 273, in load_model
[1;36m(EngineCore_DP0 pid=369394)[0;0m     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[1;36m(EngineCore_DP0 pid=369394)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3276, in load_model
[1;36m(EngineCore_DP0 pid=369394)[0;0m     self.model = model_loader.load_model(
[1;36m(EngineCore_DP0 pid=369394)[0;0m                  ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369394)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
[1;36m(EngineCore_DP0 pid=369394)[0;0m     model = initialize_model(
[1;36m(EngineCore_DP0 pid=369394)[0;0m             ^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369394)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py", line 55, in initialize_model
[1;36m(EngineCore_DP0 pid=369394)[0;0m     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(EngineCore_DP0 pid=369394)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369394)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 279, in __init__
[1;36m(EngineCore_DP0 pid=369394)[0;0m     self.model = Qwen3Model(
[1;36m(EngineCore_DP0 pid=369394)[0;0m                  ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369394)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=369394)[0;0m     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=369394)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 253, in __init__
[1;36m(EngineCore_DP0 pid=369394)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=369394)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=369394)[0;0m     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=369394)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 337, in __init__
[1;36m(EngineCore_DP0 pid=369394)[0;0m     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(EngineCore_DP0 pid=369394)[0;0m                                                     ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369394)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 651, in make_layers
[1;36m(EngineCore_DP0 pid=369394)[0;0m     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(EngineCore_DP0 pid=369394)[0;0m                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369394)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 339, in <lambda>
[1;36m(EngineCore_DP0 pid=369394)[0;0m     lambda prefix: decoder_layer_type(
[1;36m(EngineCore_DP0 pid=369394)[0;0m                    ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369394)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 201, in __init__
[1;36m(EngineCore_DP0 pid=369394)[0;0m     self.mlp = Qwen3MLP(
[1;36m(EngineCore_DP0 pid=369394)[0;0m                ^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369394)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 84, in __init__
[1;36m(EngineCore_DP0 pid=369394)[0;0m     self.gate_up_proj = MergedColumnParallelLinear(
[1;36m(EngineCore_DP0 pid=369394)[0;0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369394)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 631, in __init__
[1;36m(EngineCore_DP0 pid=369394)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=369394)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 484, in __init__
[1;36m(EngineCore_DP0 pid=369394)[0;0m     self.quant_method.create_weights(
[1;36m(EngineCore_DP0 pid=369394)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 215, in create_weights
[1;36m(EngineCore_DP0 pid=369394)[0;0m     data=torch.empty(
[1;36m(EngineCore_DP0 pid=369394)[0;0m          ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369394)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_device.py", line 103, in __torch_function__
[1;36m(EngineCore_DP0 pid=369394)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=369394)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369394)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 340.00 MiB. GPU 0 has a total capacity of 44.39 GiB of which 34.00 MiB is free. Process 369385 has 20.63 GiB memory in use. Including non-PyTorch memory, this process has 23.71 GiB memory in use. Of the allocated memory 23.19 GiB is allocated by PyTorch, and 18.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W1204 13:15:30.338247698 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank0]:[W1204 13:15:30.368065534 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: Qwen/Qwen3-14B (backend=vllm)
[ModelCache] Using max_model_len=40960 for Qwen/Qwen3-14B
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 13:15:52 [utils.py:253] non-default args: {'max_model_len': 40960, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'Qwen/Qwen3-14B'}
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: Qwen/Qwen3-14B (backend=vllm)
[ModelCache] Using max_model_len=40960 for Qwen/Qwen3-14B
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 13:15:52 [utils.py:253] non-default args: {'max_model_len': 40960, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'Qwen/Qwen3-14B'}
INFO 12-04 13:15:52 [model.py:631] Resolved architecture: Qwen3ForCausalLM
INFO 12-04 13:15:52 [model.py:1745] Using max model len 40960
INFO 12-04 13:15:52 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
INFO 12-04 13:15:52 [model.py:631] Resolved architecture: Qwen3ForCausalLM
INFO 12-04 13:15:52 [model.py:1745] Using max model len 40960
INFO 12-04 13:15:52 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=369899)[0;0m INFO 12-04 13:16:11 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='Qwen/Qwen3-14B', speculative_config=None, tokenizer='Qwen/Qwen3-14B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-14B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=369902)[0;0m INFO 12-04 13:16:11 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='Qwen/Qwen3-14B', speculative_config=None, tokenizer='Qwen/Qwen3-14B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-14B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=369899)[0;0m INFO 12-04 13:16:13 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.98:55559 backend=nccl
[1;36m(EngineCore_DP0 pid=369902)[0;0m INFO 12-04 13:16:13 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.98:58655 backend=nccl
[W1204 13:16:13.017484530 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-25-0.rc.tch.harvard.edu]:55559 (errno: 97 - Address family not supported by protocol).
[W1204 13:16:13.026538850 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-25-0.rc.tch.harvard.edu]:58655 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=369902)[0;0m INFO 12-04 13:16:13 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=369899)[0;0m INFO 12-04 13:16:13 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=369902)[0;0m INFO 12-04 13:16:14 [gpu_model_runner.py:3259] Starting to load model Qwen/Qwen3-14B...
[1;36m(EngineCore_DP0 pid=369899)[0;0m INFO 12-04 13:16:14 [gpu_model_runner.py:3259] Starting to load model Qwen/Qwen3-14B...
[1;36m(EngineCore_DP0 pid=369899)[0;0m INFO 12-04 13:16:15 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=369899)[0;0m INFO 12-04 13:16:15 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=369902)[0;0m INFO 12-04 13:16:15 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=369902)[0;0m INFO 12-04 13:16:15 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=369902)[0;0m ERROR 12-04 13:16:16 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=369902)[0;0m ERROR 12-04 13:16:16 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=369902)[0;0m ERROR 12-04 13:16:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=369902)[0;0m ERROR 12-04 13:16:16 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=369902)[0;0m ERROR 12-04 13:16:16 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369902)[0;0m ERROR 12-04 13:16:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=369902)[0;0m ERROR 12-04 13:16:16 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=369902)[0;0m ERROR 12-04 13:16:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=369902)[0;0m ERROR 12-04 13:16:16 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=369902)[0;0m ERROR 12-04 13:16:16 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369902)[0;0m ERROR 12-04 13:16:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=369902)[0;0m ERROR 12-04 13:16:16 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=369902)[0;0m ERROR 12-04 13:16:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[1;36m(EngineCore_DP0 pid=369902)[0;0m ERROR 12-04 13:16:16 [core.py:842]     self.driver_worker.load_model()
[1;36m(EngineCore_DP0 pid=369902)[0;0m ERROR 12-04 13:16:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 273, in load_model
[1;36m(EngineCore_DP0 pid=369902)[0;0m ERROR 12-04 13:16:16 [core.py:842]     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[1;36m(EngineCore_DP0 pid=369902)[0;0m ERROR 12-04 13:16:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3276, in load_model
[1;36m(EngineCore_DP0 pid=369902)[0;0m ERROR 12-04 13:16:16 [core.py:842]     self.model = model_loader.load_model(
[1;36m(EngineCore_DP0 pid=369902)[0;0m ERROR 12-04 13:16:16 [core.py:842]                  ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369902)[0;0m ERROR 12-04 13:16:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
[1;36m(EngineCore_DP0 pid=369902)[0;0m ERROR 12-04 13:16:16 [core.py:842]     model = initialize_model(
[1;36m(EngineCore_DP0 pid=369902)[0;0m ERROR 12-04 13:16:16 [core.py:842]             ^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369902)[0;0m ERROR 12-04 13:16:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py", line 55, in initialize_model
[1;36m(EngineCore_DP0 pid=369902)[0;0m ERROR 12-04 13:16:16 [core.py:842]     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(EngineCore_DP0 pid=369902)[0;0m ERROR 12-04 13:16:16 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369902)[0;0m ERROR 12-04 13:16:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 279, in __init__
[1;36m(EngineCore_DP0 pid=369902)[0;0m ERROR 12-04 13:16:16 [core.py:842]     self.model = Qwen3Model(
[1;36m(EngineCore_DP0 pid=369902)[0;0m ERROR 12-04 13:16:16 [core.py:842]                  ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369902)[0;0m ERROR 12-04 13:16:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=369902)[0;0m ERROR 12-04 13:16:16 [core.py:842]     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=369902)[0;0m ERROR 12-04 13:16:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 253, in __init__
[1;36m(EngineCore_DP0 pid=369902)[0;0m ERROR 12-04 13:16:16 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=369902)[0;0m ERROR 12-04 13:16:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=369902)[0;0m ERROR 12-04 13:16:16 [core.py:842]     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=369902)[0;0m ERROR 12-04 13:16:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 337, in __init__
[1;36m(EngineCore_DP0 pid=369902)[0;0m ERROR 12-04 13:16:16 [core.py:842]     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(EngineCore_DP0 pid=369902)[0;0m ERROR 12-04 13:16:16 [core.py:842]                                                     ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369902)[0;0m ERROR 12-04 13:16:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 651, in make_layers
[1;36m(EngineCore_DP0 pid=369902)[0;0m ERROR 12-04 13:16:16 [core.py:842]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(EngineCore_DP0 pid=369902)[0;0m ERROR 12-04 13:16:16 [core.py:842]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369902)[0;0m ERROR 12-04 13:16:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 339, in <lambda>
[1;36m(EngineCore_DP0 pid=369902)[0;0m ERROR 12-04 13:16:16 [core.py:842]     lambda prefix: decoder_layer_type(
[1;36m(EngineCore_DP0 pid=369902)[0;0m ERROR 12-04 13:16:16 [core.py:842]                    ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369902)[0;0m ERROR 12-04 13:16:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 201, in __init__
[1;36m(EngineCore_DP0 pid=369902)[0;0m ERROR 12-04 13:16:16 [core.py:842]     self.mlp = Qwen3MLP(
[1;36m(EngineCore_DP0 pid=369902)[0;0m ERROR 12-04 13:16:16 [core.py:842]                ^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369902)[0;0m ERROR 12-04 13:16:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 84, in __init__
[1;36m(EngineCore_DP0 pid=369902)[0;0m ERROR 12-04 13:16:16 [core.py:842]     self.gate_up_proj = MergedColumnParallelLinear(
[1;36m(EngineCore_DP0 pid=369902)[0;0m ERROR 12-04 13:16:16 [core.py:842]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369902)[0;0m ERROR 12-04 13:16:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 631, in __init__
[1;36m(EngineCore_DP0 pid=369902)[0;0m ERROR 12-04 13:16:16 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=369902)[0;0m ERROR 12-04 13:16:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 484, in __init__
[1;36m(EngineCore_DP0 pid=369902)[0;0m ERROR 12-04 13:16:16 [core.py:842]     self.quant_method.create_weights(
[1;36m(EngineCore_DP0 pid=369902)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=369902)[0;0m ERROR 12-04 13:16:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 215, in create_weights
[1;36m(EngineCore_DP0 pid=369902)[0;0m ERROR 12-04 13:16:16 [core.py:842]     data=torch.empty(
[1;36m(EngineCore_DP0 pid=369902)[0;0m ERROR 12-04 13:16:16 [core.py:842]          ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369902)[0;0m ERROR 12-04 13:16:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_device.py", line 103, in __torch_function__
[1;36m(EngineCore_DP0 pid=369902)[0;0m ERROR 12-04 13:16:16 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=369902)[0;0m ERROR 12-04 13:16:16 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369902)[0;0m ERROR 12-04 13:16:16 [core.py:842] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 340.00 MiB. GPU 0 has a total capacity of 44.39 GiB of which 124.00 MiB is free. Process 369899 has 24.36 GiB memory in use. Including non-PyTorch memory, this process has 20.02 GiB memory in use. Of the allocated memory 19.50 GiB is allocated by PyTorch, and 18.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=369902)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=369902)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=369902)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=369902)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=369902)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=369902)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=369902)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=369902)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=369902)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=369902)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369902)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=369902)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=369902)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=369902)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=369902)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369902)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=369902)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=369902)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[1;36m(EngineCore_DP0 pid=369902)[0;0m     self.driver_worker.load_model()
[1;36m(EngineCore_DP0 pid=369902)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 273, in load_model
[1;36m(EngineCore_DP0 pid=369902)[0;0m     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[1;36m(EngineCore_DP0 pid=369902)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3276, in load_model
[1;36m(EngineCore_DP0 pid=369902)[0;0m     self.model = model_loader.load_model(
[1;36m(EngineCore_DP0 pid=369902)[0;0m                  ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369902)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
[1;36m(EngineCore_DP0 pid=369902)[0;0m     model = initialize_model(
[1;36m(EngineCore_DP0 pid=369902)[0;0m             ^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369902)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py", line 55, in initialize_model
[1;36m(EngineCore_DP0 pid=369902)[0;0m     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(EngineCore_DP0 pid=369902)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369902)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 279, in __init__
[1;36m(EngineCore_DP0 pid=369902)[0;0m     self.model = Qwen3Model(
[1;36m(EngineCore_DP0 pid=369902)[0;0m                  ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369902)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=369902)[0;0m     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=369902)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 253, in __init__
[1;36m(EngineCore_DP0 pid=369902)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=369902)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=369902)[0;0m     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=369902)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 337, in __init__
[1;36m(EngineCore_DP0 pid=369902)[0;0m     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(EngineCore_DP0 pid=369902)[0;0m                                                     ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369902)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 651, in make_layers
[1;36m(EngineCore_DP0 pid=369902)[0;0m     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(EngineCore_DP0 pid=369902)[0;0m                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369902)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 339, in <lambda>
[1;36m(EngineCore_DP0 pid=369902)[0;0m     lambda prefix: decoder_layer_type(
[1;36m(EngineCore_DP0 pid=369902)[0;0m                    ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369902)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 201, in __init__
[1;36m(EngineCore_DP0 pid=369902)[0;0m     self.mlp = Qwen3MLP(
[1;36m(EngineCore_DP0 pid=369902)[0;0m                ^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369902)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 84, in __init__
[1;36m(EngineCore_DP0 pid=369902)[0;0m     self.gate_up_proj = MergedColumnParallelLinear(
[1;36m(EngineCore_DP0 pid=369902)[0;0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369902)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 631, in __init__
[1;36m(EngineCore_DP0 pid=369902)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=369902)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 484, in __init__
[1;36m(EngineCore_DP0 pid=369902)[0;0m     self.quant_method.create_weights(
[1;36m(EngineCore_DP0 pid=369902)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 215, in create_weights
[1;36m(EngineCore_DP0 pid=369902)[0;0m     data=torch.empty(
[1;36m(EngineCore_DP0 pid=369902)[0;0m          ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369902)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_device.py", line 103, in __torch_function__
[1;36m(EngineCore_DP0 pid=369902)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=369902)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369902)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 340.00 MiB. GPU 0 has a total capacity of 44.39 GiB of which 124.00 MiB is free. Process 369899 has 24.36 GiB memory in use. Including non-PyTorch memory, this process has 20.02 GiB memory in use. Of the allocated memory 19.50 GiB is allocated by PyTorch, and 18.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=369899)[0;0m ERROR 12-04 13:16:16 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=369899)[0;0m ERROR 12-04 13:16:16 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=369899)[0;0m ERROR 12-04 13:16:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=369899)[0;0m ERROR 12-04 13:16:16 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=369899)[0;0m ERROR 12-04 13:16:16 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369899)[0;0m ERROR 12-04 13:16:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=369899)[0;0m ERROR 12-04 13:16:16 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=369899)[0;0m ERROR 12-04 13:16:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=369899)[0;0m ERROR 12-04 13:16:16 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=369899)[0;0m ERROR 12-04 13:16:16 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369899)[0;0m ERROR 12-04 13:16:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=369899)[0;0m ERROR 12-04 13:16:16 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=369899)[0;0m ERROR 12-04 13:16:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[1;36m(EngineCore_DP0 pid=369899)[0;0m ERROR 12-04 13:16:16 [core.py:842]     self.driver_worker.load_model()
[1;36m(EngineCore_DP0 pid=369899)[0;0m ERROR 12-04 13:16:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 273, in load_model
[1;36m(EngineCore_DP0 pid=369899)[0;0m ERROR 12-04 13:16:16 [core.py:842]     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[1;36m(EngineCore_DP0 pid=369899)[0;0m ERROR 12-04 13:16:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3276, in load_model
[1;36m(EngineCore_DP0 pid=369899)[0;0m ERROR 12-04 13:16:16 [core.py:842]     self.model = model_loader.load_model(
[1;36m(EngineCore_DP0 pid=369899)[0;0m ERROR 12-04 13:16:16 [core.py:842]                  ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369899)[0;0m ERROR 12-04 13:16:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
[1;36m(EngineCore_DP0 pid=369899)[0;0m ERROR 12-04 13:16:16 [core.py:842]     model = initialize_model(
[1;36m(EngineCore_DP0 pid=369899)[0;0m ERROR 12-04 13:16:16 [core.py:842]             ^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369899)[0;0m ERROR 12-04 13:16:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py", line 55, in initialize_model
[1;36m(EngineCore_DP0 pid=369899)[0;0m ERROR 12-04 13:16:16 [core.py:842]     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(EngineCore_DP0 pid=369899)[0;0m ERROR 12-04 13:16:16 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369899)[0;0m ERROR 12-04 13:16:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 279, in __init__
[1;36m(EngineCore_DP0 pid=369899)[0;0m ERROR 12-04 13:16:16 [core.py:842]     self.model = Qwen3Model(
[1;36m(EngineCore_DP0 pid=369899)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=369899)[0;0m ERROR 12-04 13:16:16 [core.py:842]                  ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369899)[0;0m ERROR 12-04 13:16:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=369899)[0;0m ERROR 12-04 13:16:16 [core.py:842]     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=369899)[0;0m ERROR 12-04 13:16:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 253, in __init__
[1;36m(EngineCore_DP0 pid=369899)[0;0m ERROR 12-04 13:16:16 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=369899)[0;0m ERROR 12-04 13:16:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=369899)[0;0m ERROR 12-04 13:16:16 [core.py:842]     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=369899)[0;0m ERROR 12-04 13:16:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 337, in __init__
[1;36m(EngineCore_DP0 pid=369899)[0;0m ERROR 12-04 13:16:16 [core.py:842]     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(EngineCore_DP0 pid=369899)[0;0m ERROR 12-04 13:16:16 [core.py:842]                                                     ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369899)[0;0m ERROR 12-04 13:16:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 651, in make_layers
[1;36m(EngineCore_DP0 pid=369899)[0;0m ERROR 12-04 13:16:16 [core.py:842]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(EngineCore_DP0 pid=369899)[0;0m ERROR 12-04 13:16:16 [core.py:842]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369899)[0;0m ERROR 12-04 13:16:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 339, in <lambda>
[1;36m(EngineCore_DP0 pid=369899)[0;0m ERROR 12-04 13:16:16 [core.py:842]     lambda prefix: decoder_layer_type(
[1;36m(EngineCore_DP0 pid=369899)[0;0m ERROR 12-04 13:16:16 [core.py:842]                    ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369899)[0;0m ERROR 12-04 13:16:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 201, in __init__
[1;36m(EngineCore_DP0 pid=369899)[0;0m ERROR 12-04 13:16:16 [core.py:842]     self.mlp = Qwen3MLP(
[1;36m(EngineCore_DP0 pid=369899)[0;0m ERROR 12-04 13:16:16 [core.py:842]                ^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369899)[0;0m ERROR 12-04 13:16:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 84, in __init__
[1;36m(EngineCore_DP0 pid=369899)[0;0m ERROR 12-04 13:16:16 [core.py:842]     self.gate_up_proj = MergedColumnParallelLinear(
[1;36m(EngineCore_DP0 pid=369899)[0;0m ERROR 12-04 13:16:16 [core.py:842]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369899)[0;0m ERROR 12-04 13:16:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 631, in __init__
[1;36m(EngineCore_DP0 pid=369899)[0;0m ERROR 12-04 13:16:16 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=369899)[0;0m ERROR 12-04 13:16:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 484, in __init__
[1;36m(EngineCore_DP0 pid=369899)[0;0m ERROR 12-04 13:16:16 [core.py:842]     self.quant_method.create_weights(
[1;36m(EngineCore_DP0 pid=369899)[0;0m ERROR 12-04 13:16:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 215, in create_weights
[1;36m(EngineCore_DP0 pid=369899)[0;0m ERROR 12-04 13:16:16 [core.py:842]     data=torch.empty(
[1;36m(EngineCore_DP0 pid=369899)[0;0m ERROR 12-04 13:16:16 [core.py:842]          ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369899)[0;0m ERROR 12-04 13:16:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_device.py", line 103, in __torch_function__
[1;36m(EngineCore_DP0 pid=369899)[0;0m ERROR 12-04 13:16:16 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=369899)[0;0m ERROR 12-04 13:16:16 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369899)[0;0m ERROR 12-04 13:16:16 [core.py:842] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 340.00 MiB. GPU 0 has a total capacity of 44.39 GiB of which 34.00 MiB is free. Including non-PyTorch memory, this process has 24.33 GiB memory in use. Process 369902 has 20.02 GiB memory in use. Of the allocated memory 23.81 GiB is allocated by PyTorch, and 18.46 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=369899)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=369899)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=369899)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=369899)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=369899)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=369899)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=369899)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=369899)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=369899)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=369899)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369899)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=369899)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=369899)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=369899)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=369899)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369899)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=369899)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=369899)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[1;36m(EngineCore_DP0 pid=369899)[0;0m     self.driver_worker.load_model()
[1;36m(EngineCore_DP0 pid=369899)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 273, in load_model
[1;36m(EngineCore_DP0 pid=369899)[0;0m     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[1;36m(EngineCore_DP0 pid=369899)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3276, in load_model
[1;36m(EngineCore_DP0 pid=369899)[0;0m     self.model = model_loader.load_model(
[1;36m(EngineCore_DP0 pid=369899)[0;0m                  ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369899)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
[1;36m(EngineCore_DP0 pid=369899)[0;0m     model = initialize_model(
[1;36m(EngineCore_DP0 pid=369899)[0;0m             ^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369899)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py", line 55, in initialize_model
[1;36m(EngineCore_DP0 pid=369899)[0;0m     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(EngineCore_DP0 pid=369899)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369899)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 279, in __init__
[1;36m(EngineCore_DP0 pid=369899)[0;0m     self.model = Qwen3Model(
[1;36m(EngineCore_DP0 pid=369899)[0;0m                  ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369899)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=369899)[0;0m     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=369899)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 253, in __init__
[1;36m(EngineCore_DP0 pid=369899)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=369899)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=369899)[0;0m     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=369899)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 337, in __init__
[1;36m(EngineCore_DP0 pid=369899)[0;0m     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(EngineCore_DP0 pid=369899)[0;0m                                                     ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369899)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 651, in make_layers
[1;36m(EngineCore_DP0 pid=369899)[0;0m     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(EngineCore_DP0 pid=369899)[0;0m                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369899)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 339, in <lambda>
[1;36m(EngineCore_DP0 pid=369899)[0;0m     lambda prefix: decoder_layer_type(
[1;36m(EngineCore_DP0 pid=369899)[0;0m                    ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369899)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 201, in __init__
[1;36m(EngineCore_DP0 pid=369899)[0;0m     self.mlp = Qwen3MLP(
[1;36m(EngineCore_DP0 pid=369899)[0;0m                ^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369899)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 84, in __init__
[1;36m(EngineCore_DP0 pid=369899)[0;0m     self.gate_up_proj = MergedColumnParallelLinear(
[1;36m(EngineCore_DP0 pid=369899)[0;0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369899)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 631, in __init__
[1;36m(EngineCore_DP0 pid=369899)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=369899)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 484, in __init__
[1;36m(EngineCore_DP0 pid=369899)[0;0m     self.quant_method.create_weights(
[1;36m(EngineCore_DP0 pid=369899)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 215, in create_weights
[1;36m(EngineCore_DP0 pid=369899)[0;0m     data=torch.empty(
[1;36m(EngineCore_DP0 pid=369899)[0;0m          ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369899)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_device.py", line 103, in __torch_function__
[1;36m(EngineCore_DP0 pid=369899)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=369899)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369899)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 340.00 MiB. GPU 0 has a total capacity of 44.39 GiB of which 34.00 MiB is free. Including non-PyTorch memory, this process has 24.33 GiB memory in use. Process 369902 has 20.02 GiB memory in use. Of the allocated memory 23.81 GiB is allocated by PyTorch, and 18.46 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W1204 13:16:16.387791794 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank0]:[W1204 13:16:16.396948846 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: Qwen/Qwen3-14B (backend=vllm)
[ModelCache] Using max_model_len=40960 for Qwen/Qwen3-14B
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 13:16:38 [utils.py:253] non-default args: {'max_model_len': 40960, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'Qwen/Qwen3-14B'}
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: Qwen/Qwen3-14B (backend=vllm)
[ModelCache] Using max_model_len=40960 for Qwen/Qwen3-14B
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 13:16:38 [utils.py:253] non-default args: {'max_model_len': 40960, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'Qwen/Qwen3-14B'}
INFO 12-04 13:16:38 [model.py:631] Resolved architecture: Qwen3ForCausalLM
INFO 12-04 13:16:38 [model.py:1745] Using max model len 40960
INFO 12-04 13:16:38 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
INFO 12-04 13:16:38 [model.py:631] Resolved architecture: Qwen3ForCausalLM
INFO 12-04 13:16:38 [model.py:1745] Using max model len 40960
INFO 12-04 13:16:38 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=372295)[0;0m INFO 12-04 13:16:51 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='Qwen/Qwen3-14B', speculative_config=None, tokenizer='Qwen/Qwen3-14B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-14B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=372302)[0;0m INFO 12-04 13:16:51 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='Qwen/Qwen3-14B', speculative_config=None, tokenizer='Qwen/Qwen3-14B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-14B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=372295)[0;0m INFO 12-04 13:16:53 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.98:48321 backend=nccl
[W1204 13:16:53.658651772 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-25-0.rc.tch.harvard.edu]:48321 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=372295)[0;0m INFO 12-04 13:16:53 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=372302)[0;0m INFO 12-04 13:16:53 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.98:41153 backend=nccl
[W1204 13:16:53.902243964 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-25-0.rc.tch.harvard.edu]:41153 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=372302)[0;0m INFO 12-04 13:16:53 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=372295)[0;0m INFO 12-04 13:16:53 [gpu_model_runner.py:3259] Starting to load model Qwen/Qwen3-14B...
[1;36m(EngineCore_DP0 pid=372302)[0;0m INFO 12-04 13:16:53 [gpu_model_runner.py:3259] Starting to load model Qwen/Qwen3-14B...
[1;36m(EngineCore_DP0 pid=372295)[0;0m INFO 12-04 13:16:54 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=372295)[0;0m INFO 12-04 13:16:54 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=372302)[0;0m INFO 12-04 13:16:54 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=372302)[0;0m INFO 12-04 13:16:54 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=372295)[0;0m ERROR 12-04 13:16:55 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=372295)[0;0m ERROR 12-04 13:16:55 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=372295)[0;0m ERROR 12-04 13:16:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=372295)[0;0m ERROR 12-04 13:16:55 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=372295)[0;0m ERROR 12-04 13:16:55 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372295)[0;0m ERROR 12-04 13:16:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=372295)[0;0m ERROR 12-04 13:16:55 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=372295)[0;0m ERROR 12-04 13:16:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=372295)[0;0m ERROR 12-04 13:16:55 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=372295)[0;0m ERROR 12-04 13:16:55 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372295)[0;0m ERROR 12-04 13:16:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=372295)[0;0m ERROR 12-04 13:16:55 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=372295)[0;0m ERROR 12-04 13:16:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[1;36m(EngineCore_DP0 pid=372295)[0;0m ERROR 12-04 13:16:55 [core.py:842]     self.driver_worker.load_model()
[1;36m(EngineCore_DP0 pid=372295)[0;0m ERROR 12-04 13:16:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 273, in load_model
[1;36m(EngineCore_DP0 pid=372295)[0;0m ERROR 12-04 13:16:55 [core.py:842]     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[1;36m(EngineCore_DP0 pid=372295)[0;0m ERROR 12-04 13:16:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3276, in load_model
[1;36m(EngineCore_DP0 pid=372295)[0;0m ERROR 12-04 13:16:55 [core.py:842]     self.model = model_loader.load_model(
[1;36m(EngineCore_DP0 pid=372295)[0;0m ERROR 12-04 13:16:55 [core.py:842]                  ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372295)[0;0m ERROR 12-04 13:16:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
[1;36m(EngineCore_DP0 pid=372295)[0;0m ERROR 12-04 13:16:55 [core.py:842]     model = initialize_model(
[1;36m(EngineCore_DP0 pid=372295)[0;0m ERROR 12-04 13:16:55 [core.py:842]             ^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372295)[0;0m ERROR 12-04 13:16:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py", line 55, in initialize_model
[1;36m(EngineCore_DP0 pid=372295)[0;0m ERROR 12-04 13:16:55 [core.py:842]     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(EngineCore_DP0 pid=372295)[0;0m ERROR 12-04 13:16:55 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372295)[0;0m ERROR 12-04 13:16:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 287, in __init__
[1;36m(EngineCore_DP0 pid=372295)[0;0m ERROR 12-04 13:16:55 [core.py:842]     self.lm_head = ParallelLMHead(
[1;36m(EngineCore_DP0 pid=372295)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=372295)[0;0m ERROR 12-04 13:16:55 [core.py:842]                    ^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372295)[0;0m ERROR 12-04 13:16:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/vocab_parallel_embedding.py", line 523, in __init__
[1;36m(EngineCore_DP0 pid=372295)[0;0m ERROR 12-04 13:16:55 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=372295)[0;0m ERROR 12-04 13:16:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/vocab_parallel_embedding.py", line 301, in __init__
[1;36m(EngineCore_DP0 pid=372295)[0;0m ERROR 12-04 13:16:55 [core.py:842]     self.quant_method.create_weights(
[1;36m(EngineCore_DP0 pid=372295)[0;0m ERROR 12-04 13:16:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/vocab_parallel_embedding.py", line 46, in create_weights
[1;36m(EngineCore_DP0 pid=372295)[0;0m ERROR 12-04 13:16:55 [core.py:842]     torch.empty(
[1;36m(EngineCore_DP0 pid=372295)[0;0m ERROR 12-04 13:16:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_device.py", line 103, in __torch_function__
[1;36m(EngineCore_DP0 pid=372295)[0;0m ERROR 12-04 13:16:55 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=372295)[0;0m ERROR 12-04 13:16:55 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372295)[0;0m ERROR 12-04 13:16:55 [core.py:842] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.45 GiB. GPU 0 has a total capacity of 44.39 GiB of which 154.00 MiB is free. Including non-PyTorch memory, this process has 26.67 GiB memory in use. Process 372302 has 17.56 GiB memory in use. Of the allocated memory 26.15 GiB is allocated by PyTorch, and 18.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=372295)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=372295)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=372295)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=372295)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=372295)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=372295)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=372295)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=372295)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=372295)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=372295)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372295)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=372295)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=372295)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=372295)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=372295)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372295)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=372295)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=372295)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[1;36m(EngineCore_DP0 pid=372295)[0;0m     self.driver_worker.load_model()
[1;36m(EngineCore_DP0 pid=372295)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 273, in load_model
[1;36m(EngineCore_DP0 pid=372295)[0;0m     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[1;36m(EngineCore_DP0 pid=372295)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3276, in load_model
[1;36m(EngineCore_DP0 pid=372295)[0;0m     self.model = model_loader.load_model(
[1;36m(EngineCore_DP0 pid=372295)[0;0m                  ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372295)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
[1;36m(EngineCore_DP0 pid=372295)[0;0m     model = initialize_model(
[1;36m(EngineCore_DP0 pid=372295)[0;0m             ^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372295)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py", line 55, in initialize_model
[1;36m(EngineCore_DP0 pid=372295)[0;0m     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(EngineCore_DP0 pid=372295)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372295)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 287, in __init__
[1;36m(EngineCore_DP0 pid=372295)[0;0m     self.lm_head = ParallelLMHead(
[1;36m(EngineCore_DP0 pid=372295)[0;0m                    ^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372295)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/vocab_parallel_embedding.py", line 523, in __init__
[1;36m(EngineCore_DP0 pid=372295)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=372295)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/vocab_parallel_embedding.py", line 301, in __init__
[1;36m(EngineCore_DP0 pid=372295)[0;0m     self.quant_method.create_weights(
[1;36m(EngineCore_DP0 pid=372295)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/vocab_parallel_embedding.py", line 46, in create_weights
[1;36m(EngineCore_DP0 pid=372295)[0;0m     torch.empty(
[1;36m(EngineCore_DP0 pid=372295)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_device.py", line 103, in __torch_function__
[1;36m(EngineCore_DP0 pid=372295)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=372295)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372295)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.45 GiB. GPU 0 has a total capacity of 44.39 GiB of which 154.00 MiB is free. Including non-PyTorch memory, this process has 26.67 GiB memory in use. Process 372302 has 17.56 GiB memory in use. Of the allocated memory 26.15 GiB is allocated by PyTorch, and 18.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=372302)[0;0m ERROR 12-04 13:16:55 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=372302)[0;0m ERROR 12-04 13:16:55 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=372302)[0;0m ERROR 12-04 13:16:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=372302)[0;0m ERROR 12-04 13:16:55 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=372302)[0;0m ERROR 12-04 13:16:55 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372302)[0;0m ERROR 12-04 13:16:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=372302)[0;0m ERROR 12-04 13:16:55 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=372302)[0;0m ERROR 12-04 13:16:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=372302)[0;0m ERROR 12-04 13:16:55 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=372302)[0;0m ERROR 12-04 13:16:55 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372302)[0;0m ERROR 12-04 13:16:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=372302)[0;0m ERROR 12-04 13:16:55 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=372302)[0;0m ERROR 12-04 13:16:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[1;36m(EngineCore_DP0 pid=372302)[0;0m ERROR 12-04 13:16:55 [core.py:842]     self.driver_worker.load_model()
[1;36m(EngineCore_DP0 pid=372302)[0;0m ERROR 12-04 13:16:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 273, in load_model
[1;36m(EngineCore_DP0 pid=372302)[0;0m ERROR 12-04 13:16:55 [core.py:842]     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[1;36m(EngineCore_DP0 pid=372302)[0;0m ERROR 12-04 13:16:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3276, in load_model
[1;36m(EngineCore_DP0 pid=372302)[0;0m ERROR 12-04 13:16:55 [core.py:842]     self.model = model_loader.load_model(
[1;36m(EngineCore_DP0 pid=372302)[0;0m ERROR 12-04 13:16:55 [core.py:842]                  ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372302)[0;0m ERROR 12-04 13:16:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
[1;36m(EngineCore_DP0 pid=372302)[0;0m ERROR 12-04 13:16:55 [core.py:842]     model = initialize_model(
[1;36m(EngineCore_DP0 pid=372302)[0;0m ERROR 12-04 13:16:55 [core.py:842]             ^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372302)[0;0m ERROR 12-04 13:16:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py", line 55, in initialize_model
[1;36m(EngineCore_DP0 pid=372302)[0;0m ERROR 12-04 13:16:55 [core.py:842]     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(EngineCore_DP0 pid=372302)[0;0m ERROR 12-04 13:16:55 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372302)[0;0m ERROR 12-04 13:16:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 279, in __init__
[1;36m(EngineCore_DP0 pid=372302)[0;0m ERROR 12-04 13:16:55 [core.py:842]     self.model = Qwen3Model(
[1;36m(EngineCore_DP0 pid=372302)[0;0m ERROR 12-04 13:16:55 [core.py:842]                  ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372302)[0;0m ERROR 12-04 13:16:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=372302)[0;0m ERROR 12-04 13:16:55 [core.py:842]     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=372302)[0;0m ERROR 12-04 13:16:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 253, in __init__
[1;36m(EngineCore_DP0 pid=372302)[0;0m ERROR 12-04 13:16:55 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=372302)[0;0m ERROR 12-04 13:16:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=372302)[0;0m ERROR 12-04 13:16:55 [core.py:842]     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=372302)[0;0m ERROR 12-04 13:16:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 337, in __init__
[1;36m(EngineCore_DP0 pid=372302)[0;0m ERROR 12-04 13:16:55 [core.py:842]     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(EngineCore_DP0 pid=372302)[0;0m ERROR 12-04 13:16:55 [core.py:842]                                                     ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372302)[0;0m ERROR 12-04 13:16:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 651, in make_layers
[1;36m(EngineCore_DP0 pid=372302)[0;0m ERROR 12-04 13:16:55 [core.py:842]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(EngineCore_DP0 pid=372302)[0;0m ERROR 12-04 13:16:55 [core.py:842]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372302)[0;0m ERROR 12-04 13:16:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 339, in <lambda>
[1;36m(EngineCore_DP0 pid=372302)[0;0m ERROR 12-04 13:16:55 [core.py:842]     lambda prefix: decoder_layer_type(
[1;36m(EngineCore_DP0 pid=372302)[0;0m ERROR 12-04 13:16:55 [core.py:842]                    ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372302)[0;0m ERROR 12-04 13:16:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 201, in __init__
[1;36m(EngineCore_DP0 pid=372302)[0;0m ERROR 12-04 13:16:55 [core.py:842]     self.mlp = Qwen3MLP(
[1;36m(EngineCore_DP0 pid=372302)[0;0m ERROR 12-04 13:16:55 [core.py:842]                ^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372302)[0;0m ERROR 12-04 13:16:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 84, in __init__
[1;36m(EngineCore_DP0 pid=372302)[0;0m ERROR 12-04 13:16:55 [core.py:842]     self.gate_up_proj = MergedColumnParallelLinear(
[1;36m(EngineCore_DP0 pid=372302)[0;0m ERROR 12-04 13:16:55 [core.py:842]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372302)[0;0m ERROR 12-04 13:16:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 631, in __init__
[1;36m(EngineCore_DP0 pid=372302)[0;0m ERROR 12-04 13:16:55 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=372302)[0;0m ERROR 12-04 13:16:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 484, in __init__
[1;36m(EngineCore_DP0 pid=372302)[0;0m ERROR 12-04 13:16:55 [core.py:842]     self.quant_method.create_weights(
[1;36m(EngineCore_DP0 pid=372302)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=372302)[0;0m ERROR 12-04 13:16:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 215, in create_weights
[1;36m(EngineCore_DP0 pid=372302)[0;0m ERROR 12-04 13:16:55 [core.py:842]     data=torch.empty(
[1;36m(EngineCore_DP0 pid=372302)[0;0m ERROR 12-04 13:16:55 [core.py:842]          ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372302)[0;0m ERROR 12-04 13:16:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_device.py", line 103, in __torch_function__
[1;36m(EngineCore_DP0 pid=372302)[0;0m ERROR 12-04 13:16:55 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=372302)[0;0m ERROR 12-04 13:16:55 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372302)[0;0m ERROR 12-04 13:16:55 [core.py:842] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 340.00 MiB. GPU 0 has a total capacity of 44.39 GiB of which 124.00 MiB is free. Process 372295 has 26.70 GiB memory in use. Including non-PyTorch memory, this process has 17.56 GiB memory in use. Of the allocated memory 17.04 GiB is allocated by PyTorch, and 18.72 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=372302)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=372302)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=372302)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=372302)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=372302)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=372302)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=372302)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=372302)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=372302)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=372302)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372302)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=372302)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=372302)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=372302)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=372302)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372302)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=372302)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=372302)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[1;36m(EngineCore_DP0 pid=372302)[0;0m     self.driver_worker.load_model()
[1;36m(EngineCore_DP0 pid=372302)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 273, in load_model
[1;36m(EngineCore_DP0 pid=372302)[0;0m     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[1;36m(EngineCore_DP0 pid=372302)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3276, in load_model
[1;36m(EngineCore_DP0 pid=372302)[0;0m     self.model = model_loader.load_model(
[1;36m(EngineCore_DP0 pid=372302)[0;0m                  ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372302)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
[1;36m(EngineCore_DP0 pid=372302)[0;0m     model = initialize_model(
[1;36m(EngineCore_DP0 pid=372302)[0;0m             ^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372302)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py", line 55, in initialize_model
[1;36m(EngineCore_DP0 pid=372302)[0;0m     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(EngineCore_DP0 pid=372302)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372302)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 279, in __init__
[1;36m(EngineCore_DP0 pid=372302)[0;0m     self.model = Qwen3Model(
[1;36m(EngineCore_DP0 pid=372302)[0;0m                  ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372302)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=372302)[0;0m     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=372302)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 253, in __init__
[1;36m(EngineCore_DP0 pid=372302)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=372302)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=372302)[0;0m     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=372302)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 337, in __init__
[1;36m(EngineCore_DP0 pid=372302)[0;0m     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(EngineCore_DP0 pid=372302)[0;0m                                                     ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372302)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 651, in make_layers
[1;36m(EngineCore_DP0 pid=372302)[0;0m     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(EngineCore_DP0 pid=372302)[0;0m                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372302)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 339, in <lambda>
[1;36m(EngineCore_DP0 pid=372302)[0;0m     lambda prefix: decoder_layer_type(
[1;36m(EngineCore_DP0 pid=372302)[0;0m                    ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372302)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 201, in __init__
[1;36m(EngineCore_DP0 pid=372302)[0;0m     self.mlp = Qwen3MLP(
[1;36m(EngineCore_DP0 pid=372302)[0;0m                ^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372302)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 84, in __init__
[1;36m(EngineCore_DP0 pid=372302)[0;0m     self.gate_up_proj = MergedColumnParallelLinear(
[1;36m(EngineCore_DP0 pid=372302)[0;0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372302)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 631, in __init__
[1;36m(EngineCore_DP0 pid=372302)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=372302)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 484, in __init__
[1;36m(EngineCore_DP0 pid=372302)[0;0m     self.quant_method.create_weights(
[1;36m(EngineCore_DP0 pid=372302)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 215, in create_weights
[1;36m(EngineCore_DP0 pid=372302)[0;0m     data=torch.empty(
[1;36m(EngineCore_DP0 pid=372302)[0;0m          ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372302)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_device.py", line 103, in __torch_function__
[1;36m(EngineCore_DP0 pid=372302)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=372302)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372302)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 340.00 MiB. GPU 0 has a total capacity of 44.39 GiB of which 124.00 MiB is free. Process 372295 has 26.70 GiB memory in use. Including non-PyTorch memory, this process has 17.56 GiB memory in use. Of the allocated memory 17.04 GiB is allocated by PyTorch, and 18.72 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W1204 13:16:56.781302893 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank0]:[W1204 13:16:56.870552571 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: Qwen/Qwen3-14B (backend=vllm)
[ModelCache] Using max_model_len=40960 for Qwen/Qwen3-14B
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 13:17:17 [utils.py:253] non-default args: {'max_model_len': 40960, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'Qwen/Qwen3-14B'}
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: Qwen/Qwen3-14B (backend=vllm)
[ModelCache] Using max_model_len=40960 for Qwen/Qwen3-14B
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 13:17:17 [utils.py:253] non-default args: {'max_model_len': 40960, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'Qwen/Qwen3-14B'}
INFO 12-04 13:17:17 [model.py:631] Resolved architecture: Qwen3ForCausalLM
INFO 12-04 13:17:17 [model.py:1745] Using max model len 40960
INFO 12-04 13:17:17 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
INFO 12-04 13:17:17 [model.py:631] Resolved architecture: Qwen3ForCausalLM
INFO 12-04 13:17:17 [model.py:1745] Using max model len 40960
INFO 12-04 13:17:17 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=374404)[0;0m INFO 12-04 13:17:31 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='Qwen/Qwen3-14B', speculative_config=None, tokenizer='Qwen/Qwen3-14B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-14B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=374395)[0;0m INFO 12-04 13:17:31 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='Qwen/Qwen3-14B', speculative_config=None, tokenizer='Qwen/Qwen3-14B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-14B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=374395)[0;0m INFO 12-04 13:17:33 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.98:37807 backend=nccl
[1;36m(EngineCore_DP0 pid=374404)[0;0m INFO 12-04 13:17:33 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.98:42181 backend=nccl
[W1204 13:17:33.560073370 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-25-0.rc.tch.harvard.edu]:37807 (errno: 97 - Address family not supported by protocol).
[W1204 13:17:33.562919082 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-25-0.rc.tch.harvard.edu]:42181 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=374395)[0;0m INFO 12-04 13:17:33 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=374404)[0;0m INFO 12-04 13:17:33 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=374404)[0;0m INFO 12-04 13:17:33 [gpu_model_runner.py:3259] Starting to load model Qwen/Qwen3-14B...
[1;36m(EngineCore_DP0 pid=374395)[0;0m INFO 12-04 13:17:33 [gpu_model_runner.py:3259] Starting to load model Qwen/Qwen3-14B...
[1;36m(EngineCore_DP0 pid=374404)[0;0m INFO 12-04 13:17:34 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=374404)[0;0m INFO 12-04 13:17:34 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=374395)[0;0m INFO 12-04 13:17:34 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=374395)[0;0m INFO 12-04 13:17:34 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=374395)[0;0m ERROR 12-04 13:17:35 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=374395)[0;0m ERROR 12-04 13:17:35 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=374395)[0;0m ERROR 12-04 13:17:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=374395)[0;0m ERROR 12-04 13:17:35 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=374395)[0;0m ERROR 12-04 13:17:35 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374395)[0;0m ERROR 12-04 13:17:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=374395)[0;0m ERROR 12-04 13:17:35 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=374395)[0;0m ERROR 12-04 13:17:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=374395)[0;0m ERROR 12-04 13:17:35 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=374395)[0;0m ERROR 12-04 13:17:35 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374395)[0;0m ERROR 12-04 13:17:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=374395)[0;0m ERROR 12-04 13:17:35 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=374395)[0;0m ERROR 12-04 13:17:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[1;36m(EngineCore_DP0 pid=374395)[0;0m ERROR 12-04 13:17:35 [core.py:842]     self.driver_worker.load_model()
[1;36m(EngineCore_DP0 pid=374395)[0;0m ERROR 12-04 13:17:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 273, in load_model
[1;36m(EngineCore_DP0 pid=374395)[0;0m ERROR 12-04 13:17:35 [core.py:842]     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[1;36m(EngineCore_DP0 pid=374395)[0;0m ERROR 12-04 13:17:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3276, in load_model
[1;36m(EngineCore_DP0 pid=374395)[0;0m ERROR 12-04 13:17:35 [core.py:842]     self.model = model_loader.load_model(
[1;36m(EngineCore_DP0 pid=374395)[0;0m ERROR 12-04 13:17:35 [core.py:842]                  ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374395)[0;0m ERROR 12-04 13:17:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
[1;36m(EngineCore_DP0 pid=374395)[0;0m ERROR 12-04 13:17:35 [core.py:842]     model = initialize_model(
[1;36m(EngineCore_DP0 pid=374395)[0;0m ERROR 12-04 13:17:35 [core.py:842]             ^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374395)[0;0m ERROR 12-04 13:17:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py", line 55, in initialize_model
[1;36m(EngineCore_DP0 pid=374395)[0;0m ERROR 12-04 13:17:35 [core.py:842]     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(EngineCore_DP0 pid=374395)[0;0m ERROR 12-04 13:17:35 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374395)[0;0m ERROR 12-04 13:17:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 279, in __init__
[1;36m(EngineCore_DP0 pid=374395)[0;0m ERROR 12-04 13:17:35 [core.py:842]     self.model = Qwen3Model(
[1;36m(EngineCore_DP0 pid=374395)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=374395)[0;0m ERROR 12-04 13:17:35 [core.py:842]                  ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374395)[0;0m ERROR 12-04 13:17:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=374395)[0;0m ERROR 12-04 13:17:35 [core.py:842]     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=374395)[0;0m ERROR 12-04 13:17:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 253, in __init__
[1;36m(EngineCore_DP0 pid=374395)[0;0m ERROR 12-04 13:17:35 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=374395)[0;0m ERROR 12-04 13:17:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=374395)[0;0m ERROR 12-04 13:17:35 [core.py:842]     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=374395)[0;0m ERROR 12-04 13:17:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 337, in __init__
[1;36m(EngineCore_DP0 pid=374395)[0;0m ERROR 12-04 13:17:35 [core.py:842]     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(EngineCore_DP0 pid=374395)[0;0m ERROR 12-04 13:17:35 [core.py:842]                                                     ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374395)[0;0m ERROR 12-04 13:17:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 651, in make_layers
[1;36m(EngineCore_DP0 pid=374395)[0;0m ERROR 12-04 13:17:35 [core.py:842]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(EngineCore_DP0 pid=374395)[0;0m ERROR 12-04 13:17:35 [core.py:842]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374395)[0;0m ERROR 12-04 13:17:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 339, in <lambda>
[1;36m(EngineCore_DP0 pid=374395)[0;0m ERROR 12-04 13:17:35 [core.py:842]     lambda prefix: decoder_layer_type(
[1;36m(EngineCore_DP0 pid=374395)[0;0m ERROR 12-04 13:17:35 [core.py:842]                    ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374395)[0;0m ERROR 12-04 13:17:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 201, in __init__
[1;36m(EngineCore_DP0 pid=374395)[0;0m ERROR 12-04 13:17:35 [core.py:842]     self.mlp = Qwen3MLP(
[1;36m(EngineCore_DP0 pid=374395)[0;0m ERROR 12-04 13:17:35 [core.py:842]                ^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374395)[0;0m ERROR 12-04 13:17:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 84, in __init__
[1;36m(EngineCore_DP0 pid=374395)[0;0m ERROR 12-04 13:17:35 [core.py:842]     self.gate_up_proj = MergedColumnParallelLinear(
[1;36m(EngineCore_DP0 pid=374395)[0;0m ERROR 12-04 13:17:35 [core.py:842]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374395)[0;0m ERROR 12-04 13:17:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 631, in __init__
[1;36m(EngineCore_DP0 pid=374395)[0;0m ERROR 12-04 13:17:35 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=374395)[0;0m ERROR 12-04 13:17:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 484, in __init__
[1;36m(EngineCore_DP0 pid=374395)[0;0m ERROR 12-04 13:17:35 [core.py:842]     self.quant_method.create_weights(
[1;36m(EngineCore_DP0 pid=374395)[0;0m ERROR 12-04 13:17:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 215, in create_weights
[1;36m(EngineCore_DP0 pid=374395)[0;0m ERROR 12-04 13:17:35 [core.py:842]     data=torch.empty(
[1;36m(EngineCore_DP0 pid=374395)[0;0m ERROR 12-04 13:17:35 [core.py:842]          ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374395)[0;0m ERROR 12-04 13:17:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_device.py", line 103, in __torch_function__
[1;36m(EngineCore_DP0 pid=374395)[0;0m ERROR 12-04 13:17:35 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=374395)[0;0m ERROR 12-04 13:17:35 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374395)[0;0m ERROR 12-04 13:17:35 [core.py:842] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 340.00 MiB. GPU 0 has a total capacity of 44.39 GiB of which 34.00 MiB is free. Including non-PyTorch memory, this process has 21.25 GiB memory in use. Process 374404 has 23.10 GiB memory in use. Of the allocated memory 20.73 GiB is allocated by PyTorch, and 18.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=374395)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=374395)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=374395)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=374395)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=374395)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=374395)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=374395)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=374395)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=374395)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=374395)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374395)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=374395)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=374395)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=374395)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=374395)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374395)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=374395)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=374395)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[1;36m(EngineCore_DP0 pid=374395)[0;0m     self.driver_worker.load_model()
[1;36m(EngineCore_DP0 pid=374395)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 273, in load_model
[1;36m(EngineCore_DP0 pid=374395)[0;0m     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[1;36m(EngineCore_DP0 pid=374395)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3276, in load_model
[1;36m(EngineCore_DP0 pid=374395)[0;0m     self.model = model_loader.load_model(
[1;36m(EngineCore_DP0 pid=374395)[0;0m                  ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374395)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
[1;36m(EngineCore_DP0 pid=374395)[0;0m     model = initialize_model(
[1;36m(EngineCore_DP0 pid=374395)[0;0m             ^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374395)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py", line 55, in initialize_model
[1;36m(EngineCore_DP0 pid=374395)[0;0m     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(EngineCore_DP0 pid=374395)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374395)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 279, in __init__
[1;36m(EngineCore_DP0 pid=374395)[0;0m     self.model = Qwen3Model(
[1;36m(EngineCore_DP0 pid=374395)[0;0m                  ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374395)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=374395)[0;0m     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=374395)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 253, in __init__
[1;36m(EngineCore_DP0 pid=374395)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=374395)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=374395)[0;0m     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=374395)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 337, in __init__
[1;36m(EngineCore_DP0 pid=374395)[0;0m     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(EngineCore_DP0 pid=374395)[0;0m                                                     ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374395)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 651, in make_layers
[1;36m(EngineCore_DP0 pid=374395)[0;0m     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(EngineCore_DP0 pid=374395)[0;0m                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374395)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 339, in <lambda>
[1;36m(EngineCore_DP0 pid=374395)[0;0m     lambda prefix: decoder_layer_type(
[1;36m(EngineCore_DP0 pid=374395)[0;0m                    ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374395)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 201, in __init__
[1;36m(EngineCore_DP0 pid=374395)[0;0m     self.mlp = Qwen3MLP(
[1;36m(EngineCore_DP0 pid=374395)[0;0m                ^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374395)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 84, in __init__
[1;36m(EngineCore_DP0 pid=374395)[0;0m     self.gate_up_proj = MergedColumnParallelLinear(
[1;36m(EngineCore_DP0 pid=374395)[0;0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374395)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 631, in __init__
[1;36m(EngineCore_DP0 pid=374395)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=374395)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 484, in __init__
[1;36m(EngineCore_DP0 pid=374395)[0;0m     self.quant_method.create_weights(
[1;36m(EngineCore_DP0 pid=374395)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 215, in create_weights
[1;36m(EngineCore_DP0 pid=374395)[0;0m     data=torch.empty(
[1;36m(EngineCore_DP0 pid=374395)[0;0m          ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374395)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_device.py", line 103, in __torch_function__
[1;36m(EngineCore_DP0 pid=374395)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=374395)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374395)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 340.00 MiB. GPU 0 has a total capacity of 44.39 GiB of which 34.00 MiB is free. Including non-PyTorch memory, this process has 21.25 GiB memory in use. Process 374404 has 23.10 GiB memory in use. Of the allocated memory 20.73 GiB is allocated by PyTorch, and 18.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=374404)[0;0m ERROR 12-04 13:17:35 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=374404)[0;0m ERROR 12-04 13:17:35 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=374404)[0;0m ERROR 12-04 13:17:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=374404)[0;0m ERROR 12-04 13:17:35 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=374404)[0;0m ERROR 12-04 13:17:35 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374404)[0;0m ERROR 12-04 13:17:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=374404)[0;0m ERROR 12-04 13:17:35 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=374404)[0;0m ERROR 12-04 13:17:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=374404)[0;0m ERROR 12-04 13:17:35 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=374404)[0;0m ERROR 12-04 13:17:35 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374404)[0;0m ERROR 12-04 13:17:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=374404)[0;0m ERROR 12-04 13:17:35 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=374404)[0;0m ERROR 12-04 13:17:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[1;36m(EngineCore_DP0 pid=374404)[0;0m ERROR 12-04 13:17:35 [core.py:842]     self.driver_worker.load_model()
[1;36m(EngineCore_DP0 pid=374404)[0;0m ERROR 12-04 13:17:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 273, in load_model
[1;36m(EngineCore_DP0 pid=374404)[0;0m ERROR 12-04 13:17:35 [core.py:842]     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[1;36m(EngineCore_DP0 pid=374404)[0;0m ERROR 12-04 13:17:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3276, in load_model
[1;36m(EngineCore_DP0 pid=374404)[0;0m ERROR 12-04 13:17:35 [core.py:842]     self.model = model_loader.load_model(
[1;36m(EngineCore_DP0 pid=374404)[0;0m ERROR 12-04 13:17:35 [core.py:842]                  ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374404)[0;0m ERROR 12-04 13:17:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
[1;36m(EngineCore_DP0 pid=374404)[0;0m ERROR 12-04 13:17:35 [core.py:842]     model = initialize_model(
[1;36m(EngineCore_DP0 pid=374404)[0;0m ERROR 12-04 13:17:35 [core.py:842]             ^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374404)[0;0m ERROR 12-04 13:17:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py", line 55, in initialize_model
[1;36m(EngineCore_DP0 pid=374404)[0;0m ERROR 12-04 13:17:35 [core.py:842]     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(EngineCore_DP0 pid=374404)[0;0m ERROR 12-04 13:17:35 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374404)[0;0m ERROR 12-04 13:17:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 279, in __init__
[1;36m(EngineCore_DP0 pid=374404)[0;0m ERROR 12-04 13:17:35 [core.py:842]     self.model = Qwen3Model(
[1;36m(EngineCore_DP0 pid=374404)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=374404)[0;0m ERROR 12-04 13:17:35 [core.py:842]                  ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374404)[0;0m ERROR 12-04 13:17:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=374404)[0;0m ERROR 12-04 13:17:35 [core.py:842]     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=374404)[0;0m ERROR 12-04 13:17:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 253, in __init__
[1;36m(EngineCore_DP0 pid=374404)[0;0m ERROR 12-04 13:17:35 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=374404)[0;0m ERROR 12-04 13:17:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=374404)[0;0m ERROR 12-04 13:17:35 [core.py:842]     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=374404)[0;0m ERROR 12-04 13:17:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 337, in __init__
[1;36m(EngineCore_DP0 pid=374404)[0;0m ERROR 12-04 13:17:35 [core.py:842]     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(EngineCore_DP0 pid=374404)[0;0m ERROR 12-04 13:17:35 [core.py:842]                                                     ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374404)[0;0m ERROR 12-04 13:17:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 651, in make_layers
[1;36m(EngineCore_DP0 pid=374404)[0;0m ERROR 12-04 13:17:35 [core.py:842]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(EngineCore_DP0 pid=374404)[0;0m ERROR 12-04 13:17:35 [core.py:842]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374404)[0;0m ERROR 12-04 13:17:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 339, in <lambda>
[1;36m(EngineCore_DP0 pid=374404)[0;0m ERROR 12-04 13:17:35 [core.py:842]     lambda prefix: decoder_layer_type(
[1;36m(EngineCore_DP0 pid=374404)[0;0m ERROR 12-04 13:17:35 [core.py:842]                    ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374404)[0;0m ERROR 12-04 13:17:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 201, in __init__
[1;36m(EngineCore_DP0 pid=374404)[0;0m ERROR 12-04 13:17:35 [core.py:842]     self.mlp = Qwen3MLP(
[1;36m(EngineCore_DP0 pid=374404)[0;0m ERROR 12-04 13:17:35 [core.py:842]                ^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374404)[0;0m ERROR 12-04 13:17:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 84, in __init__
[1;36m(EngineCore_DP0 pid=374404)[0;0m ERROR 12-04 13:17:35 [core.py:842]     self.gate_up_proj = MergedColumnParallelLinear(
[1;36m(EngineCore_DP0 pid=374404)[0;0m ERROR 12-04 13:17:35 [core.py:842]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374404)[0;0m ERROR 12-04 13:17:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 631, in __init__
[1;36m(EngineCore_DP0 pid=374404)[0;0m ERROR 12-04 13:17:35 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=374404)[0;0m ERROR 12-04 13:17:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 484, in __init__
[1;36m(EngineCore_DP0 pid=374404)[0;0m ERROR 12-04 13:17:35 [core.py:842]     self.quant_method.create_weights(
[1;36m(EngineCore_DP0 pid=374404)[0;0m ERROR 12-04 13:17:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 215, in create_weights
[1;36m(EngineCore_DP0 pid=374404)[0;0m ERROR 12-04 13:17:35 [core.py:842]     data=torch.empty(
[1;36m(EngineCore_DP0 pid=374404)[0;0m ERROR 12-04 13:17:35 [core.py:842]          ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374404)[0;0m ERROR 12-04 13:17:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_device.py", line 103, in __torch_function__
[1;36m(EngineCore_DP0 pid=374404)[0;0m ERROR 12-04 13:17:35 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=374404)[0;0m ERROR 12-04 13:17:35 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374404)[0;0m ERROR 12-04 13:17:35 [core.py:842] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 340.00 MiB. GPU 0 has a total capacity of 44.39 GiB of which 54.00 MiB is free. Process 374395 has 21.27 GiB memory in use. Including non-PyTorch memory, this process has 23.10 GiB memory in use. Of the allocated memory 22.58 GiB is allocated by PyTorch, and 18.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=374404)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=374404)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=374404)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=374404)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=374404)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=374404)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=374404)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=374404)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=374404)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=374404)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374404)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=374404)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=374404)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=374404)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=374404)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374404)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=374404)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=374404)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[1;36m(EngineCore_DP0 pid=374404)[0;0m     self.driver_worker.load_model()
[1;36m(EngineCore_DP0 pid=374404)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 273, in load_model
[1;36m(EngineCore_DP0 pid=374404)[0;0m     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[1;36m(EngineCore_DP0 pid=374404)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3276, in load_model
[1;36m(EngineCore_DP0 pid=374404)[0;0m     self.model = model_loader.load_model(
[1;36m(EngineCore_DP0 pid=374404)[0;0m                  ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374404)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
[1;36m(EngineCore_DP0 pid=374404)[0;0m     model = initialize_model(
[1;36m(EngineCore_DP0 pid=374404)[0;0m             ^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374404)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py", line 55, in initialize_model
[1;36m(EngineCore_DP0 pid=374404)[0;0m     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(EngineCore_DP0 pid=374404)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374404)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 279, in __init__
[1;36m(EngineCore_DP0 pid=374404)[0;0m     self.model = Qwen3Model(
[1;36m(EngineCore_DP0 pid=374404)[0;0m                  ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374404)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=374404)[0;0m     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=374404)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 253, in __init__
[1;36m(EngineCore_DP0 pid=374404)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=374404)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=374404)[0;0m     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=374404)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 337, in __init__
[1;36m(EngineCore_DP0 pid=374404)[0;0m     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(EngineCore_DP0 pid=374404)[0;0m                                                     ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374404)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 651, in make_layers
[1;36m(EngineCore_DP0 pid=374404)[0;0m     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(EngineCore_DP0 pid=374404)[0;0m                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374404)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 339, in <lambda>
[1;36m(EngineCore_DP0 pid=374404)[0;0m     lambda prefix: decoder_layer_type(
[1;36m(EngineCore_DP0 pid=374404)[0;0m                    ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374404)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 201, in __init__
[1;36m(EngineCore_DP0 pid=374404)[0;0m     self.mlp = Qwen3MLP(
[1;36m(EngineCore_DP0 pid=374404)[0;0m                ^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374404)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 84, in __init__
[1;36m(EngineCore_DP0 pid=374404)[0;0m     self.gate_up_proj = MergedColumnParallelLinear(
[1;36m(EngineCore_DP0 pid=374404)[0;0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374404)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 631, in __init__
[1;36m(EngineCore_DP0 pid=374404)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=374404)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 484, in __init__
[1;36m(EngineCore_DP0 pid=374404)[0;0m     self.quant_method.create_weights(
[1;36m(EngineCore_DP0 pid=374404)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 215, in create_weights
[1;36m(EngineCore_DP0 pid=374404)[0;0m     data=torch.empty(
[1;36m(EngineCore_DP0 pid=374404)[0;0m          ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374404)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_device.py", line 103, in __torch_function__
[1;36m(EngineCore_DP0 pid=374404)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=374404)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374404)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 340.00 MiB. GPU 0 has a total capacity of 44.39 GiB of which 54.00 MiB is free. Process 374395 has 21.27 GiB memory in use. Including non-PyTorch memory, this process has 23.10 GiB memory in use. Of the allocated memory 22.58 GiB is allocated by PyTorch, and 18.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W1204 13:17:36.519025763 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank0]:[W1204 13:17:36.527863708 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: Qwen/Qwen3-14B (backend=vllm)
[ModelCache] Using max_model_len=40960 for Qwen/Qwen3-14B
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 13:17:57 [utils.py:253] non-default args: {'max_model_len': 40960, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'Qwen/Qwen3-14B'}
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: Qwen/Qwen3-14B (backend=vllm)
[ModelCache] Using max_model_len=40960 for Qwen/Qwen3-14B
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 13:17:57 [utils.py:253] non-default args: {'max_model_len': 40960, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'Qwen/Qwen3-14B'}
INFO 12-04 13:17:57 [model.py:631] Resolved architecture: Qwen3ForCausalLM
INFO 12-04 13:17:57 [model.py:1745] Using max model len 40960
INFO 12-04 13:17:57 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
INFO 12-04 13:17:57 [model.py:631] Resolved architecture: Qwen3ForCausalLM
INFO 12-04 13:17:57 [model.py:1745] Using max model len 40960
INFO 12-04 13:17:57 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=377868)[0;0m INFO 12-04 13:18:10 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='Qwen/Qwen3-14B', speculative_config=None, tokenizer='Qwen/Qwen3-14B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-14B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=377882)[0;0m INFO 12-04 13:18:10 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='Qwen/Qwen3-14B', speculative_config=None, tokenizer='Qwen/Qwen3-14B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-14B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=377868)[0;0m INFO 12-04 13:18:12 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.98:52295 backend=nccl
[1;36m(EngineCore_DP0 pid=377882)[0;0m INFO 12-04 13:18:12 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.98:37083 backend=nccl
[W1204 13:18:12.518407734 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-25-0.rc.tch.harvard.edu]:52295 (errno: 97 - Address family not supported by protocol).
[W1204 13:18:12.520784809 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-25-0.rc.tch.harvard.edu]:37083 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=377868)[0;0m INFO 12-04 13:18:12 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=377882)[0;0m INFO 12-04 13:18:12 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=377868)[0;0m INFO 12-04 13:18:12 [gpu_model_runner.py:3259] Starting to load model Qwen/Qwen3-14B...
[1;36m(EngineCore_DP0 pid=377882)[0;0m INFO 12-04 13:18:12 [gpu_model_runner.py:3259] Starting to load model Qwen/Qwen3-14B...
[1;36m(EngineCore_DP0 pid=377868)[0;0m INFO 12-04 13:18:13 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=377868)[0;0m INFO 12-04 13:18:13 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=377882)[0;0m INFO 12-04 13:18:13 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=377882)[0;0m INFO 12-04 13:18:13 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=377882)[0;0m ERROR 12-04 13:18:14 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=377882)[0;0m ERROR 12-04 13:18:14 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=377882)[0;0m ERROR 12-04 13:18:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=377882)[0;0m ERROR 12-04 13:18:14 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=377882)[0;0m ERROR 12-04 13:18:14 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=377882)[0;0m ERROR 12-04 13:18:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=377882)[0;0m ERROR 12-04 13:18:14 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=377882)[0;0m ERROR 12-04 13:18:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=377882)[0;0m ERROR 12-04 13:18:14 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=377882)[0;0m ERROR 12-04 13:18:14 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=377882)[0;0m ERROR 12-04 13:18:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=377882)[0;0m ERROR 12-04 13:18:14 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=377882)[0;0m ERROR 12-04 13:18:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[1;36m(EngineCore_DP0 pid=377882)[0;0m ERROR 12-04 13:18:14 [core.py:842]     self.driver_worker.load_model()
[1;36m(EngineCore_DP0 pid=377882)[0;0m ERROR 12-04 13:18:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 273, in load_model
[1;36m(EngineCore_DP0 pid=377882)[0;0m ERROR 12-04 13:18:14 [core.py:842]     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[1;36m(EngineCore_DP0 pid=377882)[0;0m ERROR 12-04 13:18:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3276, in load_model
[1;36m(EngineCore_DP0 pid=377882)[0;0m ERROR 12-04 13:18:14 [core.py:842]     self.model = model_loader.load_model(
[1;36m(EngineCore_DP0 pid=377882)[0;0m ERROR 12-04 13:18:14 [core.py:842]                  ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=377882)[0;0m ERROR 12-04 13:18:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
[1;36m(EngineCore_DP0 pid=377882)[0;0m ERROR 12-04 13:18:14 [core.py:842]     model = initialize_model(
[1;36m(EngineCore_DP0 pid=377882)[0;0m ERROR 12-04 13:18:14 [core.py:842]             ^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=377882)[0;0m ERROR 12-04 13:18:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py", line 55, in initialize_model
[1;36m(EngineCore_DP0 pid=377882)[0;0m ERROR 12-04 13:18:14 [core.py:842]     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(EngineCore_DP0 pid=377882)[0;0m ERROR 12-04 13:18:14 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=377882)[0;0m ERROR 12-04 13:18:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 279, in __init__
[1;36m(EngineCore_DP0 pid=377882)[0;0m ERROR 12-04 13:18:14 [core.py:842]     self.model = Qwen3Model(
[1;36m(EngineCore_DP0 pid=377882)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=377882)[0;0m ERROR 12-04 13:18:14 [core.py:842]                  ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=377882)[0;0m ERROR 12-04 13:18:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=377882)[0;0m ERROR 12-04 13:18:14 [core.py:842]     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=377882)[0;0m ERROR 12-04 13:18:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 253, in __init__
[1;36m(EngineCore_DP0 pid=377882)[0;0m ERROR 12-04 13:18:14 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=377882)[0;0m ERROR 12-04 13:18:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=377882)[0;0m ERROR 12-04 13:18:14 [core.py:842]     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=377882)[0;0m ERROR 12-04 13:18:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 337, in __init__
[1;36m(EngineCore_DP0 pid=377882)[0;0m ERROR 12-04 13:18:14 [core.py:842]     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(EngineCore_DP0 pid=377882)[0;0m ERROR 12-04 13:18:14 [core.py:842]                                                     ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=377882)[0;0m ERROR 12-04 13:18:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 651, in make_layers
[1;36m(EngineCore_DP0 pid=377882)[0;0m ERROR 12-04 13:18:14 [core.py:842]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(EngineCore_DP0 pid=377882)[0;0m ERROR 12-04 13:18:14 [core.py:842]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=377882)[0;0m ERROR 12-04 13:18:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 339, in <lambda>
[1;36m(EngineCore_DP0 pid=377882)[0;0m ERROR 12-04 13:18:14 [core.py:842]     lambda prefix: decoder_layer_type(
[1;36m(EngineCore_DP0 pid=377882)[0;0m ERROR 12-04 13:18:14 [core.py:842]                    ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=377882)[0;0m ERROR 12-04 13:18:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 201, in __init__
[1;36m(EngineCore_DP0 pid=377882)[0;0m ERROR 12-04 13:18:14 [core.py:842]     self.mlp = Qwen3MLP(
[1;36m(EngineCore_DP0 pid=377882)[0;0m ERROR 12-04 13:18:14 [core.py:842]                ^^^^^^^^^
[1;36m(EngineCore_DP0 pid=377882)[0;0m ERROR 12-04 13:18:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 84, in __init__
[1;36m(EngineCore_DP0 pid=377882)[0;0m ERROR 12-04 13:18:14 [core.py:842]     self.gate_up_proj = MergedColumnParallelLinear(
[1;36m(EngineCore_DP0 pid=377882)[0;0m ERROR 12-04 13:18:14 [core.py:842]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=377882)[0;0m ERROR 12-04 13:18:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 631, in __init__
[1;36m(EngineCore_DP0 pid=377882)[0;0m ERROR 12-04 13:18:14 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=377882)[0;0m ERROR 12-04 13:18:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 484, in __init__
[1;36m(EngineCore_DP0 pid=377882)[0;0m ERROR 12-04 13:18:14 [core.py:842]     self.quant_method.create_weights(
[1;36m(EngineCore_DP0 pid=377882)[0;0m ERROR 12-04 13:18:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 215, in create_weights
[1;36m(EngineCore_DP0 pid=377882)[0;0m ERROR 12-04 13:18:14 [core.py:842]     data=torch.empty(
[1;36m(EngineCore_DP0 pid=377882)[0;0m ERROR 12-04 13:18:14 [core.py:842]          ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=377882)[0;0m ERROR 12-04 13:18:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_device.py", line 103, in __torch_function__
[1;36m(EngineCore_DP0 pid=377882)[0;0m ERROR 12-04 13:18:14 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=377882)[0;0m ERROR 12-04 13:18:14 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=377882)[0;0m ERROR 12-04 13:18:14 [core.py:842] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 340.00 MiB. GPU 0 has a total capacity of 44.39 GiB of which 34.00 MiB is free. Process 377868 has 23.10 GiB memory in use. Including non-PyTorch memory, this process has 21.25 GiB memory in use. Of the allocated memory 20.73 GiB is allocated by PyTorch, and 18.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=377882)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=377882)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=377882)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=377882)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=377882)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=377882)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=377882)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=377882)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=377882)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=377882)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=377882)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=377882)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=377882)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=377882)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=377882)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=377882)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=377882)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=377882)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[1;36m(EngineCore_DP0 pid=377882)[0;0m     self.driver_worker.load_model()
[1;36m(EngineCore_DP0 pid=377882)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 273, in load_model
[1;36m(EngineCore_DP0 pid=377882)[0;0m     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[1;36m(EngineCore_DP0 pid=377882)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3276, in load_model
[1;36m(EngineCore_DP0 pid=377882)[0;0m     self.model = model_loader.load_model(
[1;36m(EngineCore_DP0 pid=377882)[0;0m                  ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=377882)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
[1;36m(EngineCore_DP0 pid=377882)[0;0m     model = initialize_model(
[1;36m(EngineCore_DP0 pid=377882)[0;0m             ^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=377882)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py", line 55, in initialize_model
[1;36m(EngineCore_DP0 pid=377882)[0;0m     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(EngineCore_DP0 pid=377882)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=377882)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 279, in __init__
[1;36m(EngineCore_DP0 pid=377882)[0;0m     self.model = Qwen3Model(
[1;36m(EngineCore_DP0 pid=377882)[0;0m                  ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=377882)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=377882)[0;0m     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=377882)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 253, in __init__
[1;36m(EngineCore_DP0 pid=377882)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=377882)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=377882)[0;0m     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=377882)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 337, in __init__
[1;36m(EngineCore_DP0 pid=377882)[0;0m     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(EngineCore_DP0 pid=377882)[0;0m                                                     ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=377882)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 651, in make_layers
[1;36m(EngineCore_DP0 pid=377882)[0;0m     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(EngineCore_DP0 pid=377882)[0;0m                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=377882)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 339, in <lambda>
[1;36m(EngineCore_DP0 pid=377882)[0;0m     lambda prefix: decoder_layer_type(
[1;36m(EngineCore_DP0 pid=377882)[0;0m                    ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=377882)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 201, in __init__
[1;36m(EngineCore_DP0 pid=377882)[0;0m     self.mlp = Qwen3MLP(
[1;36m(EngineCore_DP0 pid=377882)[0;0m                ^^^^^^^^^
[1;36m(EngineCore_DP0 pid=377882)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 84, in __init__
[1;36m(EngineCore_DP0 pid=377882)[0;0m     self.gate_up_proj = MergedColumnParallelLinear(
[1;36m(EngineCore_DP0 pid=377882)[0;0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=377882)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 631, in __init__
[1;36m(EngineCore_DP0 pid=377882)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=377882)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 484, in __init__
[1;36m(EngineCore_DP0 pid=377882)[0;0m     self.quant_method.create_weights(
[1;36m(EngineCore_DP0 pid=377882)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 215, in create_weights
[1;36m(EngineCore_DP0 pid=377882)[0;0m     data=torch.empty(
[1;36m(EngineCore_DP0 pid=377882)[0;0m          ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=377882)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_device.py", line 103, in __torch_function__
[1;36m(EngineCore_DP0 pid=377882)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=377882)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=377882)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 340.00 MiB. GPU 0 has a total capacity of 44.39 GiB of which 34.00 MiB is free. Process 377868 has 23.10 GiB memory in use. Including non-PyTorch memory, this process has 21.25 GiB memory in use. Of the allocated memory 20.73 GiB is allocated by PyTorch, and 18.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=377868)[0;0m ERROR 12-04 13:18:14 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=377868)[0;0m ERROR 12-04 13:18:14 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=377868)[0;0m ERROR 12-04 13:18:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=377868)[0;0m ERROR 12-04 13:18:14 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=377868)[0;0m ERROR 12-04 13:18:14 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=377868)[0;0m ERROR 12-04 13:18:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=377868)[0;0m ERROR 12-04 13:18:14 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=377868)[0;0m ERROR 12-04 13:18:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=377868)[0;0m ERROR 12-04 13:18:14 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=377868)[0;0m ERROR 12-04 13:18:14 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=377868)[0;0m ERROR 12-04 13:18:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=377868)[0;0m ERROR 12-04 13:18:14 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=377868)[0;0m ERROR 12-04 13:18:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[1;36m(EngineCore_DP0 pid=377868)[0;0m ERROR 12-04 13:18:14 [core.py:842]     self.driver_worker.load_model()
[1;36m(EngineCore_DP0 pid=377868)[0;0m ERROR 12-04 13:18:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 273, in load_model
[1;36m(EngineCore_DP0 pid=377868)[0;0m ERROR 12-04 13:18:14 [core.py:842]     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[1;36m(EngineCore_DP0 pid=377868)[0;0m ERROR 12-04 13:18:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3276, in load_model
[1;36m(EngineCore_DP0 pid=377868)[0;0m ERROR 12-04 13:18:14 [core.py:842]     self.model = model_loader.load_model(
[1;36m(EngineCore_DP0 pid=377868)[0;0m ERROR 12-04 13:18:14 [core.py:842]                  ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=377868)[0;0m ERROR 12-04 13:18:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
[1;36m(EngineCore_DP0 pid=377868)[0;0m ERROR 12-04 13:18:14 [core.py:842]     model = initialize_model(
[1;36m(EngineCore_DP0 pid=377868)[0;0m ERROR 12-04 13:18:14 [core.py:842]             ^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=377868)[0;0m ERROR 12-04 13:18:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py", line 55, in initialize_model
[1;36m(EngineCore_DP0 pid=377868)[0;0m ERROR 12-04 13:18:14 [core.py:842]     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(EngineCore_DP0 pid=377868)[0;0m ERROR 12-04 13:18:14 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=377868)[0;0m ERROR 12-04 13:18:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 279, in __init__
[1;36m(EngineCore_DP0 pid=377868)[0;0m ERROR 12-04 13:18:14 [core.py:842]     self.model = Qwen3Model(
[1;36m(EngineCore_DP0 pid=377868)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=377868)[0;0m ERROR 12-04 13:18:14 [core.py:842]                  ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=377868)[0;0m ERROR 12-04 13:18:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=377868)[0;0m ERROR 12-04 13:18:14 [core.py:842]     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=377868)[0;0m ERROR 12-04 13:18:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 253, in __init__
[1;36m(EngineCore_DP0 pid=377868)[0;0m ERROR 12-04 13:18:14 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=377868)[0;0m ERROR 12-04 13:18:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=377868)[0;0m ERROR 12-04 13:18:14 [core.py:842]     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=377868)[0;0m ERROR 12-04 13:18:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 337, in __init__
[1;36m(EngineCore_DP0 pid=377868)[0;0m ERROR 12-04 13:18:14 [core.py:842]     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(EngineCore_DP0 pid=377868)[0;0m ERROR 12-04 13:18:14 [core.py:842]                                                     ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=377868)[0;0m ERROR 12-04 13:18:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 651, in make_layers
[1;36m(EngineCore_DP0 pid=377868)[0;0m ERROR 12-04 13:18:14 [core.py:842]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(EngineCore_DP0 pid=377868)[0;0m ERROR 12-04 13:18:14 [core.py:842]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=377868)[0;0m ERROR 12-04 13:18:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 339, in <lambda>
[1;36m(EngineCore_DP0 pid=377868)[0;0m ERROR 12-04 13:18:14 [core.py:842]     lambda prefix: decoder_layer_type(
[1;36m(EngineCore_DP0 pid=377868)[0;0m ERROR 12-04 13:18:14 [core.py:842]                    ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=377868)[0;0m ERROR 12-04 13:18:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 201, in __init__
[1;36m(EngineCore_DP0 pid=377868)[0;0m ERROR 12-04 13:18:14 [core.py:842]     self.mlp = Qwen3MLP(
[1;36m(EngineCore_DP0 pid=377868)[0;0m ERROR 12-04 13:18:14 [core.py:842]                ^^^^^^^^^
[1;36m(EngineCore_DP0 pid=377868)[0;0m ERROR 12-04 13:18:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 84, in __init__
[1;36m(EngineCore_DP0 pid=377868)[0;0m ERROR 12-04 13:18:14 [core.py:842]     self.gate_up_proj = MergedColumnParallelLinear(
[1;36m(EngineCore_DP0 pid=377868)[0;0m ERROR 12-04 13:18:14 [core.py:842]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=377868)[0;0m ERROR 12-04 13:18:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 631, in __init__
[1;36m(EngineCore_DP0 pid=377868)[0;0m ERROR 12-04 13:18:14 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=377868)[0;0m ERROR 12-04 13:18:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 484, in __init__
[1;36m(EngineCore_DP0 pid=377868)[0;0m ERROR 12-04 13:18:14 [core.py:842]     self.quant_method.create_weights(
[1;36m(EngineCore_DP0 pid=377868)[0;0m ERROR 12-04 13:18:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 215, in create_weights
[1;36m(EngineCore_DP0 pid=377868)[0;0m ERROR 12-04 13:18:14 [core.py:842]     data=torch.empty(
[1;36m(EngineCore_DP0 pid=377868)[0;0m ERROR 12-04 13:18:14 [core.py:842]          ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=377868)[0;0m ERROR 12-04 13:18:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_device.py", line 103, in __torch_function__
[1;36m(EngineCore_DP0 pid=377868)[0;0m ERROR 12-04 13:18:14 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=377868)[0;0m ERROR 12-04 13:18:14 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=377868)[0;0m ERROR 12-04 13:18:14 [core.py:842] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 340.00 MiB. GPU 0 has a total capacity of 44.39 GiB of which 34.00 MiB is free. Including non-PyTorch memory, this process has 23.10 GiB memory in use. Process 377882 has 21.25 GiB memory in use. Of the allocated memory 22.58 GiB is allocated by PyTorch, and 18.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=377868)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=377868)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=377868)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=377868)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=377868)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=377868)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=377868)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=377868)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=377868)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=377868)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=377868)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=377868)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=377868)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=377868)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=377868)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=377868)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=377868)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=377868)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[1;36m(EngineCore_DP0 pid=377868)[0;0m     self.driver_worker.load_model()
[1;36m(EngineCore_DP0 pid=377868)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 273, in load_model
[1;36m(EngineCore_DP0 pid=377868)[0;0m     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[1;36m(EngineCore_DP0 pid=377868)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3276, in load_model
[1;36m(EngineCore_DP0 pid=377868)[0;0m     self.model = model_loader.load_model(
[1;36m(EngineCore_DP0 pid=377868)[0;0m                  ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=377868)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
[1;36m(EngineCore_DP0 pid=377868)[0;0m     model = initialize_model(
[1;36m(EngineCore_DP0 pid=377868)[0;0m             ^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=377868)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py", line 55, in initialize_model
[1;36m(EngineCore_DP0 pid=377868)[0;0m     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(EngineCore_DP0 pid=377868)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=377868)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 279, in __init__
[1;36m(EngineCore_DP0 pid=377868)[0;0m     self.model = Qwen3Model(
[1;36m(EngineCore_DP0 pid=377868)[0;0m                  ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=377868)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=377868)[0;0m     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=377868)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 253, in __init__
[1;36m(EngineCore_DP0 pid=377868)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=377868)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=377868)[0;0m     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=377868)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 337, in __init__
[1;36m(EngineCore_DP0 pid=377868)[0;0m     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(EngineCore_DP0 pid=377868)[0;0m                                                     ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=377868)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 651, in make_layers
[1;36m(EngineCore_DP0 pid=377868)[0;0m     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(EngineCore_DP0 pid=377868)[0;0m                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=377868)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 339, in <lambda>
[1;36m(EngineCore_DP0 pid=377868)[0;0m     lambda prefix: decoder_layer_type(
[1;36m(EngineCore_DP0 pid=377868)[0;0m                    ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=377868)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 201, in __init__
[1;36m(EngineCore_DP0 pid=377868)[0;0m     self.mlp = Qwen3MLP(
[1;36m(EngineCore_DP0 pid=377868)[0;0m                ^^^^^^^^^
[1;36m(EngineCore_DP0 pid=377868)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 84, in __init__
[1;36m(EngineCore_DP0 pid=377868)[0;0m     self.gate_up_proj = MergedColumnParallelLinear(
[1;36m(EngineCore_DP0 pid=377868)[0;0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=377868)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 631, in __init__
[1;36m(EngineCore_DP0 pid=377868)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=377868)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 484, in __init__
[1;36m(EngineCore_DP0 pid=377868)[0;0m     self.quant_method.create_weights(
[1;36m(EngineCore_DP0 pid=377868)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 215, in create_weights
[1;36m(EngineCore_DP0 pid=377868)[0;0m     data=torch.empty(
[1;36m(EngineCore_DP0 pid=377868)[0;0m          ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=377868)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_device.py", line 103, in __torch_function__
[1;36m(EngineCore_DP0 pid=377868)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=377868)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=377868)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 340.00 MiB. GPU 0 has a total capacity of 44.39 GiB of which 34.00 MiB is free. Including non-PyTorch memory, this process has 23.10 GiB memory in use. Process 377882 has 21.25 GiB memory in use. Of the allocated memory 22.58 GiB is allocated by PyTorch, and 18.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W1204 13:18:14.489639377 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank0]:[W1204 13:18:14.500482352 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: Qwen/Qwen3-14B (backend=vllm)
[ModelCache] Using max_model_len=40960 for Qwen/Qwen3-14B
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 13:18:36 [utils.py:253] non-default args: {'max_model_len': 40960, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'Qwen/Qwen3-14B'}
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: Qwen/Qwen3-14B (backend=vllm)
[ModelCache] Using max_model_len=40960 for Qwen/Qwen3-14B
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 13:18:36 [utils.py:253] non-default args: {'max_model_len': 40960, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'Qwen/Qwen3-14B'}
INFO 12-04 13:18:36 [model.py:631] Resolved architecture: Qwen3ForCausalLM
INFO 12-04 13:18:36 [model.py:1745] Using max model len 40960
INFO 12-04 13:18:36 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
INFO 12-04 13:18:36 [model.py:631] Resolved architecture: Qwen3ForCausalLM
INFO 12-04 13:18:36 [model.py:1745] Using max model len 40960
INFO 12-04 13:18:36 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=379895)[0;0m INFO 12-04 13:18:50 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='Qwen/Qwen3-14B', speculative_config=None, tokenizer='Qwen/Qwen3-14B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-14B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=379901)[0;0m INFO 12-04 13:18:50 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='Qwen/Qwen3-14B', speculative_config=None, tokenizer='Qwen/Qwen3-14B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-14B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=379895)[0;0m INFO 12-04 13:18:51 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.98:49021 backend=nccl
[W1204 13:18:51.853160041 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-25-0.rc.tch.harvard.edu]:49021 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=379895)[0;0m INFO 12-04 13:18:51 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=379901)[0;0m INFO 12-04 13:18:51 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.98:51079 backend=nccl
[W1204 13:18:51.186401946 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-25-0.rc.tch.harvard.edu]:51079 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=379901)[0;0m INFO 12-04 13:18:51 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=379895)[0;0m INFO 12-04 13:18:51 [gpu_model_runner.py:3259] Starting to load model Qwen/Qwen3-14B...
[1;36m(EngineCore_DP0 pid=379901)[0;0m INFO 12-04 13:18:52 [gpu_model_runner.py:3259] Starting to load model Qwen/Qwen3-14B...
[1;36m(EngineCore_DP0 pid=379895)[0;0m INFO 12-04 13:18:52 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=379895)[0;0m INFO 12-04 13:18:52 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=379901)[0;0m INFO 12-04 13:18:52 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=379901)[0;0m INFO 12-04 13:18:52 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=379895)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/8 [00:00<?, ?it/s]
[1;36m(EngineCore_DP0 pid=379901)[0;0m ERROR 12-04 13:18:53 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=379901)[0;0m ERROR 12-04 13:18:53 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=379901)[0;0m ERROR 12-04 13:18:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=379901)[0;0m ERROR 12-04 13:18:53 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=379901)[0;0m ERROR 12-04 13:18:53 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=379901)[0;0m ERROR 12-04 13:18:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=379901)[0;0m ERROR 12-04 13:18:53 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=379901)[0;0m ERROR 12-04 13:18:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=379901)[0;0m ERROR 12-04 13:18:53 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=379901)[0;0m ERROR 12-04 13:18:53 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=379901)[0;0m ERROR 12-04 13:18:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=379901)[0;0m ERROR 12-04 13:18:53 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=379901)[0;0m ERROR 12-04 13:18:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[1;36m(EngineCore_DP0 pid=379901)[0;0m ERROR 12-04 13:18:53 [core.py:842]     self.driver_worker.load_model()
[1;36m(EngineCore_DP0 pid=379901)[0;0m ERROR 12-04 13:18:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 273, in load_model
[1;36m(EngineCore_DP0 pid=379901)[0;0m ERROR 12-04 13:18:53 [core.py:842]     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[1;36m(EngineCore_DP0 pid=379901)[0;0m ERROR 12-04 13:18:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3276, in load_model
[1;36m(EngineCore_DP0 pid=379901)[0;0m ERROR 12-04 13:18:53 [core.py:842]     self.model = model_loader.load_model(
[1;36m(EngineCore_DP0 pid=379901)[0;0m ERROR 12-04 13:18:53 [core.py:842]                  ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=379901)[0;0m ERROR 12-04 13:18:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
[1;36m(EngineCore_DP0 pid=379901)[0;0m ERROR 12-04 13:18:53 [core.py:842]     model = initialize_model(
[1;36m(EngineCore_DP0 pid=379901)[0;0m ERROR 12-04 13:18:53 [core.py:842]             ^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=379901)[0;0m ERROR 12-04 13:18:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py", line 55, in initialize_model
[1;36m(EngineCore_DP0 pid=379901)[0;0m ERROR 12-04 13:18:53 [core.py:842]     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(EngineCore_DP0 pid=379901)[0;0m ERROR 12-04 13:18:53 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=379901)[0;0m ERROR 12-04 13:18:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 279, in __init__
[1;36m(EngineCore_DP0 pid=379901)[0;0m ERROR 12-04 13:18:53 [core.py:842]     self.model = Qwen3Model(
[1;36m(EngineCore_DP0 pid=379901)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=379901)[0;0m ERROR 12-04 13:18:53 [core.py:842]                  ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=379901)[0;0m ERROR 12-04 13:18:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=379901)[0;0m ERROR 12-04 13:18:53 [core.py:842]     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=379901)[0;0m ERROR 12-04 13:18:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 253, in __init__
[1;36m(EngineCore_DP0 pid=379901)[0;0m ERROR 12-04 13:18:53 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=379901)[0;0m ERROR 12-04 13:18:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=379901)[0;0m ERROR 12-04 13:18:53 [core.py:842]     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=379901)[0;0m ERROR 12-04 13:18:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 337, in __init__
[1;36m(EngineCore_DP0 pid=379901)[0;0m ERROR 12-04 13:18:53 [core.py:842]     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(EngineCore_DP0 pid=379901)[0;0m ERROR 12-04 13:18:53 [core.py:842]                                                     ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=379901)[0;0m ERROR 12-04 13:18:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 651, in make_layers
[1;36m(EngineCore_DP0 pid=379901)[0;0m ERROR 12-04 13:18:53 [core.py:842]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(EngineCore_DP0 pid=379901)[0;0m ERROR 12-04 13:18:53 [core.py:842]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=379901)[0;0m ERROR 12-04 13:18:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 339, in <lambda>
[1;36m(EngineCore_DP0 pid=379901)[0;0m ERROR 12-04 13:18:53 [core.py:842]     lambda prefix: decoder_layer_type(
[1;36m(EngineCore_DP0 pid=379901)[0;0m ERROR 12-04 13:18:53 [core.py:842]                    ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=379901)[0;0m ERROR 12-04 13:18:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 185, in __init__
[1;36m(EngineCore_DP0 pid=379901)[0;0m ERROR 12-04 13:18:53 [core.py:842]     self.self_attn = Qwen3Attention(
[1;36m(EngineCore_DP0 pid=379901)[0;0m ERROR 12-04 13:18:53 [core.py:842]                      ^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=379901)[0;0m ERROR 12-04 13:18:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 95, in __init__
[1;36m(EngineCore_DP0 pid=379901)[0;0m ERROR 12-04 13:18:53 [core.py:842]     self.qkv_proj = QKVParallelLinear(
[1;36m(EngineCore_DP0 pid=379901)[0;0m ERROR 12-04 13:18:53 [core.py:842]                     ^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=379901)[0;0m ERROR 12-04 13:18:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 935, in __init__
[1;36m(EngineCore_DP0 pid=379901)[0;0m ERROR 12-04 13:18:53 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=379901)[0;0m ERROR 12-04 13:18:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 484, in __init__
[1;36m(EngineCore_DP0 pid=379901)[0;0m ERROR 12-04 13:18:53 [core.py:842]     self.quant_method.create_weights(
[1;36m(EngineCore_DP0 pid=379901)[0;0m ERROR 12-04 13:18:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 215, in create_weights
[1;36m(EngineCore_DP0 pid=379901)[0;0m ERROR 12-04 13:18:53 [core.py:842]     data=torch.empty(
[1;36m(EngineCore_DP0 pid=379901)[0;0m ERROR 12-04 13:18:53 [core.py:842]          ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=379901)[0;0m ERROR 12-04 13:18:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_device.py", line 103, in __torch_function__
[1;36m(EngineCore_DP0 pid=379901)[0;0m ERROR 12-04 13:18:53 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=379901)[0;0m ERROR 12-04 13:18:53 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=379901)[0;0m ERROR 12-04 13:18:53 [core.py:842] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 70.00 MiB. GPU 0 has a total capacity of 44.39 GiB of which 20.00 MiB is free. Process 379895 has 28.15 GiB memory in use. Including non-PyTorch memory, this process has 16.21 GiB memory in use. Of the allocated memory 15.69 GiB is allocated by PyTorch, and 18.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=379901)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=379901)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=379901)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=379901)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=379901)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=379901)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=379901)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=379901)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=379901)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=379901)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=379901)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=379901)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=379901)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=379901)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=379901)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=379901)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=379901)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=379901)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[1;36m(EngineCore_DP0 pid=379901)[0;0m     self.driver_worker.load_model()
[1;36m(EngineCore_DP0 pid=379901)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 273, in load_model
[1;36m(EngineCore_DP0 pid=379901)[0;0m     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[1;36m(EngineCore_DP0 pid=379901)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3276, in load_model
[1;36m(EngineCore_DP0 pid=379901)[0;0m     self.model = model_loader.load_model(
[1;36m(EngineCore_DP0 pid=379901)[0;0m                  ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=379901)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
[1;36m(EngineCore_DP0 pid=379901)[0;0m     model = initialize_model(
[1;36m(EngineCore_DP0 pid=379901)[0;0m             ^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=379901)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py", line 55, in initialize_model
[1;36m(EngineCore_DP0 pid=379901)[0;0m     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(EngineCore_DP0 pid=379901)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=379901)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 279, in __init__
[1;36m(EngineCore_DP0 pid=379901)[0;0m     self.model = Qwen3Model(
[1;36m(EngineCore_DP0 pid=379901)[0;0m                  ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=379901)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=379901)[0;0m     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=379901)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 253, in __init__
[1;36m(EngineCore_DP0 pid=379901)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=379901)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=379901)[0;0m     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=379901)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 337, in __init__
[1;36m(EngineCore_DP0 pid=379901)[0;0m     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(EngineCore_DP0 pid=379901)[0;0m                                                     ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=379901)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 651, in make_layers
[1;36m(EngineCore_DP0 pid=379901)[0;0m     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(EngineCore_DP0 pid=379901)[0;0m                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=379901)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 339, in <lambda>
[1;36m(EngineCore_DP0 pid=379901)[0;0m     lambda prefix: decoder_layer_type(
[1;36m(EngineCore_DP0 pid=379901)[0;0m                    ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=379901)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 185, in __init__
[1;36m(EngineCore_DP0 pid=379901)[0;0m     self.self_attn = Qwen3Attention(
[1;36m(EngineCore_DP0 pid=379901)[0;0m                      ^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=379901)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 95, in __init__
[1;36m(EngineCore_DP0 pid=379901)[0;0m     self.qkv_proj = QKVParallelLinear(
[1;36m(EngineCore_DP0 pid=379901)[0;0m                     ^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=379901)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 935, in __init__
[1;36m(EngineCore_DP0 pid=379901)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=379901)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 484, in __init__
[1;36m(EngineCore_DP0 pid=379901)[0;0m     self.quant_method.create_weights(
[1;36m(EngineCore_DP0 pid=379901)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 215, in create_weights
[1;36m(EngineCore_DP0 pid=379901)[0;0m     data=torch.empty(
[1;36m(EngineCore_DP0 pid=379901)[0;0m          ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=379901)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_device.py", line 103, in __torch_function__
[1;36m(EngineCore_DP0 pid=379901)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=379901)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=379901)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 70.00 MiB. GPU 0 has a total capacity of 44.39 GiB of which 20.00 MiB is free. Process 379895 has 28.15 GiB memory in use. Including non-PyTorch memory, this process has 16.21 GiB memory in use. Of the allocated memory 15.69 GiB is allocated by PyTorch, and 18.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=379895)[0;0m Loading safetensors checkpoint shards:  12% Completed | 1/8 [00:00<00:06,  1.02it/s]
[rank0]:[W1204 13:18:54.876042653 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[1;36m(EngineCore_DP0 pid=379895)[0;0m Loading safetensors checkpoint shards:  25% Completed | 2/8 [00:01<00:05,  1.04it/s]
[1;36m(EngineCore_DP0 pid=379895)[0;0m Loading safetensors checkpoint shards:  38% Completed | 3/8 [00:02<00:04,  1.12it/s]
[1;36m(EngineCore_DP0 pid=379895)[0;0m Loading safetensors checkpoint shards:  50% Completed | 4/8 [00:03<00:03,  1.21it/s]
[1;36m(EngineCore_DP0 pid=379895)[0;0m Loading safetensors checkpoint shards:  62% Completed | 5/8 [00:04<00:02,  1.27it/s]
[1;36m(EngineCore_DP0 pid=379895)[0;0m Loading safetensors checkpoint shards:  75% Completed | 6/8 [00:04<00:01,  1.31it/s]
[1;36m(EngineCore_DP0 pid=379895)[0;0m Loading safetensors checkpoint shards:  88% Completed | 7/8 [00:05<00:00,  1.35it/s]
[1;36m(EngineCore_DP0 pid=379895)[0;0m Loading safetensors checkpoint shards: 100% Completed | 8/8 [00:05<00:00,  1.61it/s]
[1;36m(EngineCore_DP0 pid=379895)[0;0m Loading safetensors checkpoint shards: 100% Completed | 8/8 [00:05<00:00,  1.34it/s]
[1;36m(EngineCore_DP0 pid=379895)[0;0m 
[1;36m(EngineCore_DP0 pid=379895)[0;0m INFO 12-04 13:18:59 [default_loader.py:314] Loading weights took 6.01 seconds
[1;36m(EngineCore_DP0 pid=379895)[0;0m INFO 12-04 13:18:59 [gpu_model_runner.py:3338] Model loading took 27.5185 GiB memory and 7.131347 seconds
[1;36m(EngineCore_DP0 pid=379895)[0;0m INFO 12-04 13:19:08 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/76690ce19a/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=379895)[0;0m INFO 12-04 13:19:08 [backends.py:647] Dynamo bytecode transform time: 8.76 s
[1;36m(EngineCore_DP0 pid=379895)[0;0m INFO 12-04 13:19:13 [backends.py:210] Directly load the compiled graph(s) for dynamic shape from the cache, took 4.758 s
[1;36m(EngineCore_DP0 pid=379895)[0;0m INFO 12-04 13:19:15 [monitor.py:34] torch.compile takes 13.52 s in total
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: Qwen/Qwen3-14B (backend=vllm)
[ModelCache] Using max_model_len=40960 for Qwen/Qwen3-14B
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 13:19:15 [utils.py:253] non-default args: {'max_model_len': 40960, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'Qwen/Qwen3-14B'}
INFO 12-04 13:19:15 [model.py:631] Resolved architecture: Qwen3ForCausalLM
INFO 12-04 13:19:15 [model.py:1745] Using max model len 40960
INFO 12-04 13:19:15 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=379895)[0;0m INFO 12-04 13:19:18 [gpu_worker.py:359] Available KV cache memory: 10.96 GiB
[1;36m(EngineCore_DP0 pid=379895)[0;0m INFO 12-04 13:19:18 [kv_cache_utils.py:1229] GPU KV cache size: 71,824 tokens
[1;36m(EngineCore_DP0 pid=379895)[0;0m INFO 12-04 13:19:18 [kv_cache_utils.py:1234] Maximum concurrency for 40,960 tokens per request: 1.75x
[1;36m(EngineCore_DP0 pid=379895)[0;0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/51 [00:00<?, ?it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   2%|▏         | 1/51 [00:00<00:06,  7.19it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   4%|▍         | 2/51 [00:00<00:06,  7.15it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 3/51 [00:00<00:06,  7.03it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|▊         | 4/51 [00:00<00:06,  7.10it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  10%|▉         | 5/51 [00:00<00:06,  6.99it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  12%|█▏        | 6/51 [00:00<00:06,  6.47it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▎        | 7/51 [00:01<00:06,  6.78it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|█▌        | 8/51 [00:01<00:06,  6.99it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 9/51 [00:01<00:05,  7.44it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|█▉        | 10/51 [00:01<00:05,  6.95it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|██▏       | 11/51 [00:01<00:06,  6.30it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  24%|██▎       | 12/51 [00:01<00:05,  7.04it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 13/51 [00:01<00:05,  7.52it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|██▋       | 14/51 [00:01<00:04,  7.89it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▉       | 15/51 [00:02<00:04,  8.20it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███▏      | 16/51 [00:02<00:04,  8.46it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  35%|███▌      | 18/51 [00:02<00:03,  9.39it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  39%|███▉      | 20/51 [00:02<00:03,  9.88it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 22/51 [00:02<00:02, 10.29it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 24/51 [00:02<00:02, 10.58it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|█████     | 26/51 [00:03<00:02, 10.94it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▍    | 28/51 [00:03<00:02, 11.19it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  59%|█████▉    | 30/51 [00:03<00:01, 11.38it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 32/51 [00:03<00:01, 11.55it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 34/51 [00:03<00:01, 11.59it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████   | 36/51 [00:03<00:01, 11.89it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▍  | 38/51 [00:04<00:01, 12.25it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  78%|███████▊  | 40/51 [00:04<00:00, 12.48it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|████████▏ | 42/51 [00:04<00:00, 12.65it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▋ | 44/51 [00:04<00:00, 12.85it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  90%|█████████ | 46/51 [00:04<00:00, 13.00it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|█████████▍| 48/51 [00:04<00:00, 13.34it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  98%|█████████▊| 50/51 [00:04<00:00, 13.57it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:05<00:00, 10.14it/s]
[1;36m(EngineCore_DP0 pid=379895)[0;0m Capturing CUDA graphs (decode, FULL):   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graphs (decode, FULL):   6%|▌         | 2/35 [00:00<00:02, 11.33it/s]Capturing CUDA graphs (decode, FULL):  11%|█▏        | 4/35 [00:00<00:02, 11.58it/s]Capturing CUDA graphs (decode, FULL):  17%|█▋        | 6/35 [00:00<00:02, 11.75it/s]Capturing CUDA graphs (decode, FULL):  23%|██▎       | 8/35 [00:00<00:02, 11.64it/s]Capturing CUDA graphs (decode, FULL):  29%|██▊       | 10/35 [00:00<00:02, 11.82it/s]Capturing CUDA graphs (decode, FULL):  34%|███▍      | 12/35 [00:01<00:01, 12.01it/s]Capturing CUDA graphs (decode, FULL):  40%|████      | 14/35 [00:01<00:01, 12.15it/s]Capturing CUDA graphs (decode, FULL):  46%|████▌     | 16/35 [00:01<00:01, 12.19it/s]Capturing CUDA graphs (decode, FULL):  51%|█████▏    | 18/35 [00:01<00:01, 12.36it/s]Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 20/35 [00:01<00:01, 12.50it/s]Capturing CUDA graphs (decode, FULL):  63%|██████▎   | 22/35 [00:01<00:01, 12.71it/s]Capturing CUDA graphs (decode, FULL):  69%|██████▊   | 24/35 [00:01<00:00, 12.83it/s]Capturing CUDA graphs (decode, FULL):  74%|███████▍  | 26/35 [00:02<00:00, 13.09it/s]Capturing CUDA graphs (decode, FULL):  80%|████████  | 28/35 [00:02<00:00, 13.36it/s]Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 30/35 [00:02<00:00, 13.52it/s]Capturing CUDA graphs (decode, FULL):  91%|█████████▏| 32/35 [00:02<00:00, 13.92it/s]Capturing CUDA graphs (decode, FULL):  97%|█████████▋| 34/35 [00:02<00:00, 14.22it/s]Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:02<00:00, 12.88it/s]
[1;36m(EngineCore_DP0 pid=379895)[0;0m INFO 12-04 13:19:26 [gpu_model_runner.py:4244] Graph capturing finished in 8 secs, took 0.65 GiB
[1;36m(EngineCore_DP0 pid=379895)[0;0m INFO 12-04 13:19:26 [core.py:250] init engine (profile, create kv cache, warmup model) took 27.24 seconds
[1;36m(EngineCore_DP0 pid=381206)[0;0m INFO 12-04 13:19:27 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='Qwen/Qwen3-14B', speculative_config=None, tokenizer='Qwen/Qwen3-14B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-14B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=381206)[0;0m INFO 12-04 13:19:28 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.98:49515 backend=nccl
[W1204 13:19:28.690267805 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-25-0.rc.tch.harvard.edu]:49515 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
INFO 12-04 13:19:28 [llm.py:352] Supported tasks: ['generate']
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=381206)[0;0m INFO 12-04 13:19:28 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=381206)[0;0m ERROR 12-04 13:19:28 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=381206)[0;0m ERROR 12-04 13:19:28 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=381206)[0;0m ERROR 12-04 13:19:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=381206)[0;0m ERROR 12-04 13:19:28 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=381206)[0;0m ERROR 12-04 13:19:28 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=381206)[0;0m ERROR 12-04 13:19:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=381206)[0;0m ERROR 12-04 13:19:28 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=381206)[0;0m ERROR 12-04 13:19:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=381206)[0;0m ERROR 12-04 13:19:28 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=381206)[0;0m ERROR 12-04 13:19:28 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=381206)[0;0m ERROR 12-04 13:19:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=381206)[0;0m ERROR 12-04 13:19:28 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=381206)[0;0m ERROR 12-04 13:19:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=381206)[0;0m ERROR 12-04 13:19:28 [core.py:842]     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=381206)[0;0m ERROR 12-04 13:19:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=381206)[0;0m ERROR 12-04 13:19:28 [core.py:842]     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=381206)[0;0m ERROR 12-04 13:19:28 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=381206)[0;0m ERROR 12-04 13:19:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=381206)[0;0m ERROR 12-04 13:19:28 [core.py:842]     raise ValueError(
[1;36m(EngineCore_DP0 pid=381206)[0;0m ERROR 12-04 13:19:28 [core.py:842] ValueError: Free memory on device (2.71/44.39 GiB) on startup is less than desired GPU memory utilization (0.9, 39.95 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[1;36m(EngineCore_DP0 pid=381206)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=381206)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=381206)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=381206)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=381206)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=381206)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=381206)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=381206)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=381206)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=381206)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=381206)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=381206)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=381206)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=381206)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=381206)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=381206)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=381206)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=381206)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=381206)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=381206)[0;0m     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=381206)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=381206)[0;0m     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=381206)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=381206)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=381206)[0;0m     raise ValueError(
[1;36m(EngineCore_DP0 pid=381206)[0;0m ValueError: Free memory on device (2.71/44.39 GiB) on startup is less than desired GPU memory utilization (0.9, 39.95 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.

Adding requests:   0%|          | 0/1 [00:00<?, ?it/s][AAdding requests: 100%|██████████| 1/1 [00:00<00:00, 457.84it/s]

[rank0]:[W1204 13:19:29.525486815 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts: 100%|██████████| 1/1 [00:18<00:00, 18.01s/it, est. speed input: 4.11 toks/s, output: 25.15 toks/s][A
Processed prompts: 100%|██████████| 1/1 [00:18<00:00, 18.01s/it, est. speed input: 4.11 toks/s, output: 25.15 toks/s][AProcessed prompts: 100%|██████████| 1/1 [00:18<00:00, 18.01s/it, est. speed input: 4.11 toks/s, output: 25.15 toks/s]
Agent 1 response: The result of the expression is 482....

--- Problem 1/2, Round 1, Agent 2/3 ---
[ModelCache] Using cached model: Qwen/Qwen3-14B

Adding requests:   0%|          | 0/1 [00:00<?, ?it/s][AAdding requests: 100%|██████████| 1/1 [00:00<00:00, 1337.90it/s]

Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: Qwen/Qwen3-14B (backend=vllm)
[ModelCache] Using max_model_len=40960 for Qwen/Qwen3-14B
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 13:19:49 [utils.py:253] non-default args: {'max_model_len': 40960, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'Qwen/Qwen3-14B'}
INFO 12-04 13:19:50 [model.py:631] Resolved architecture: Qwen3ForCausalLM
INFO 12-04 13:19:50 [model.py:1745] Using max model len 40960
INFO 12-04 13:19:50 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=382519)[0;0m INFO 12-04 13:20:01 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='Qwen/Qwen3-14B', speculative_config=None, tokenizer='Qwen/Qwen3-14B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-14B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=382519)[0;0m INFO 12-04 13:20:02 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.98:42685 backend=nccl
[W1204 13:20:02.166446380 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-25-0.rc.tch.harvard.edu]:42685 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=382519)[0;0m INFO 12-04 13:20:02 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=382519)[0;0m ERROR 12-04 13:20:02 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=382519)[0;0m ERROR 12-04 13:20:02 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=382519)[0;0m ERROR 12-04 13:20:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=382519)[0;0m ERROR 12-04 13:20:02 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=382519)[0;0m ERROR 12-04 13:20:02 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=382519)[0;0m ERROR 12-04 13:20:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=382519)[0;0m ERROR 12-04 13:20:02 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=382519)[0;0m ERROR 12-04 13:20:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=382519)[0;0m ERROR 12-04 13:20:02 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=382519)[0;0m ERROR 12-04 13:20:02 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=382519)[0;0m ERROR 12-04 13:20:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=382519)[0;0m ERROR 12-04 13:20:02 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=382519)[0;0m ERROR 12-04 13:20:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=382519)[0;0m ERROR 12-04 13:20:02 [core.py:842]     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=382519)[0;0m ERROR 12-04 13:20:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=382519)[0;0m ERROR 12-04 13:20:02 [core.py:842]     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=382519)[0;0m ERROR 12-04 13:20:02 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=382519)[0;0m ERROR 12-04 13:20:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=382519)[0;0m ERROR 12-04 13:20:02 [core.py:842]     raise ValueError(
[1;36m(EngineCore_DP0 pid=382519)[0;0m ERROR 12-04 13:20:02 [core.py:842] ValueError: Free memory on device (2.71/44.39 GiB) on startup is less than desired GPU memory utilization (0.9, 39.95 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[1;36m(EngineCore_DP0 pid=382519)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=382519)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=382519)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=382519)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=382519)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=382519)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=382519)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=382519)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=382519)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=382519)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=382519)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=382519)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=382519)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=382519)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=382519)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=382519)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=382519)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=382519)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=382519)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=382519)[0;0m     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=382519)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=382519)[0;0m     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=382519)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=382519)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=382519)[0;0m     raise ValueError(
[1;36m(EngineCore_DP0 pid=382519)[0;0m ValueError: Free memory on device (2.71/44.39 GiB) on startup is less than desired GPU memory utilization (0.9, 39.95 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W1204 13:20:03.062817678 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts: 100%|██████████| 1/1 [00:18<00:00, 18.19s/it, est. speed input: 4.29 toks/s, output: 25.01 toks/s][A
Processed prompts: 100%|██████████| 1/1 [00:18<00:00, 18.19s/it, est. speed input: 4.29 toks/s, output: 25.01 toks/s][AProcessed prompts: 100%|██████████| 1/1 [00:18<00:00, 18.19s/it, est. speed input: 4.29 toks/s, output: 25.01 toks/s]
Agent 2 response: The moon reflects the sun's silence.  
A stone cannot sing.  
Breathe into the hollow of the questio...

--- Problem 1/2, Round 1, Agent 3/3 ---
[ModelCache] Using cached model: Qwen/Qwen3-14B

Adding requests:   0%|          | 0/1 [00:00<?, ?it/s][AAdding requests: 100%|██████████| 1/1 [00:00<00:00, 1420.35it/s]

Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts: 100%|██████████| 1/1 [00:13<00:00, 13.65s/it, est. speed input: 5.50 toks/s, output: 25.28 toks/s][A
Processed prompts: 100%|██████████| 1/1 [00:13<00:00, 13.65s/it, est. speed input: 5.50 toks/s, output: 25.28 toks/s][AProcessed prompts: 100%|██████████| 1/1 [00:13<00:00, 13.65s/it, est. speed input: 5.50 toks/s, output: 25.28 toks/s]
Agent 3 response: The result of 6 + 19*28 + 14 - 10*7 is 482....

--- Problem 1/2, Round 2, Agent 1/3 ---
Agent 1 receiving other agents' responses...
[ModelCache] Using cached model: Qwen/Qwen3-14B

Adding requests:   0%|          | 0/1 [00:00<?, ?it/s][AAdding requests: 100%|██████████| 1/1 [00:00<00:00, 913.99it/s]

Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: Qwen/Qwen3-14B (backend=vllm)
[ModelCache] Using max_model_len=40960 for Qwen/Qwen3-14B
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 13:20:24 [utils.py:253] non-default args: {'max_model_len': 40960, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'Qwen/Qwen3-14B'}
INFO 12-04 13:20:24 [model.py:631] Resolved architecture: Qwen3ForCausalLM
INFO 12-04 13:20:24 [model.py:1745] Using max model len 40960
INFO 12-04 13:20:24 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts: 100%|██████████| 1/1 [00:11<00:00, 11.85s/it, est. speed input: 18.56 toks/s, output: 25.06 toks/s][A
Processed prompts: 100%|██████████| 1/1 [00:11<00:00, 11.85s/it, est. speed input: 18.56 toks/s, output: 25.06 toks/s][AProcessed prompts: 100%|██████████| 1/1 [00:11<00:00, 11.85s/it, est. speed input: 18.56 toks/s, output: 25.06 toks/s]
Agent 1 response: The calculation follows standard order of operations: multiplication before addition/subtraction. 19...

--- Problem 1/2, Round 2, Agent 2/3 ---
Agent 2 receiving other agents' responses...
[ModelCache] Using cached model: Qwen/Qwen3-14B

Adding requests:   0%|          | 0/1 [00:00<?, ?it/s][AAdding requests: 100%|██████████| 1/1 [00:00<00:00, 747.78it/s]

[1;36m(EngineCore_DP0 pid=383398)[0;0m INFO 12-04 13:20:35 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='Qwen/Qwen3-14B', speculative_config=None, tokenizer='Qwen/Qwen3-14B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-14B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=383398)[0;0m INFO 12-04 13:20:37 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.98:46287 backend=nccl
[W1204 13:20:37.707278374 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-25-0.rc.tch.harvard.edu]:46287 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=383398)[0;0m INFO 12-04 13:20:37 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=383398)[0;0m ERROR 12-04 13:20:37 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=383398)[0;0m ERROR 12-04 13:20:37 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=383398)[0;0m ERROR 12-04 13:20:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=383398)[0;0m ERROR 12-04 13:20:37 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=383398)[0;0m ERROR 12-04 13:20:37 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=383398)[0;0m ERROR 12-04 13:20:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=383398)[0;0m ERROR 12-04 13:20:37 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=383398)[0;0m ERROR 12-04 13:20:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=383398)[0;0m ERROR 12-04 13:20:37 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=383398)[0;0m ERROR 12-04 13:20:37 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=383398)[0;0m ERROR 12-04 13:20:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=383398)[0;0m ERROR 12-04 13:20:37 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=383398)[0;0m ERROR 12-04 13:20:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=383398)[0;0m ERROR 12-04 13:20:37 [core.py:842]     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=383398)[0;0m ERROR 12-04 13:20:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=383398)[0;0m ERROR 12-04 13:20:37 [core.py:842]     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=383398)[0;0m ERROR 12-04 13:20:37 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=383398)[0;0m ERROR 12-04 13:20:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=383398)[0;0m ERROR 12-04 13:20:37 [core.py:842]     raise ValueError(
[1;36m(EngineCore_DP0 pid=383398)[0;0m ERROR 12-04 13:20:37 [core.py:842] ValueError: Free memory on device (2.71/44.39 GiB) on startup is less than desired GPU memory utilization (0.9, 39.95 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[1;36m(EngineCore_DP0 pid=383398)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=383398)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=383398)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=383398)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=383398)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=383398)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=383398)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=383398)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=383398)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=383398)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=383398)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=383398)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=383398)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=383398)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=383398)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=383398)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=383398)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=383398)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=383398)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=383398)[0;0m     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=383398)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=383398)[0;0m     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=383398)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=383398)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=383398)[0;0m     raise ValueError(
[1;36m(EngineCore_DP0 pid=383398)[0;0m ValueError: Free memory on device (2.71/44.39 GiB) on startup is less than desired GPU memory utilization (0.9, 39.95 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W1204 13:20:37.488429127 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts: 100%|██████████| 1/1 [00:14<00:00, 14.06s/it, est. speed input: 15.93 toks/s, output: 25.18 toks/s][A
Processed prompts: 100%|██████████| 1/1 [00:14<00:00, 14.06s/it, est. speed input: 15.93 toks/s, output: 25.18 toks/s][AProcessed prompts: 100%|██████████| 1/1 [00:14<00:00, 14.06s/it, est. speed input: 15.93 toks/s, output: 25.18 toks/s]
Agent 2 response: The wind does not count the stars.  
A river forgets its source.  
The answer is 482....

--- Problem 1/2, Round 2, Agent 3/3 ---
Agent 3 receiving other agents' responses...
[ModelCache] Using cached model: Qwen/Qwen3-14B

Adding requests:   0%|          | 0/1 [00:00<?, ?it/s][AAdding requests: 100%|██████████| 1/1 [00:00<00:00, 1101.73it/s]

Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts: 100%|██████████| 1/1 [00:12<00:00, 12.27s/it, est. speed input: 18.01 toks/s, output: 25.18 toks/s][A
Processed prompts: 100%|██████████| 1/1 [00:12<00:00, 12.27s/it, est. speed input: 18.01 toks/s, output: 25.18 toks/s][AProcessed prompts: 100%|██████████| 1/1 [00:12<00:00, 12.27s/it, est. speed input: 18.01 toks/s, output: 25.18 toks/s]
Agent 3 response: The result of the expression 6 + 19*28 + 14 - 10*7 is confirmed as 482 through standard arithmetic o...

--- Problem 1/2, Round 3, Agent 1/3 ---
Agent 1 receiving other agents' responses...
[ModelCache] Using cached model: Qwen/Qwen3-14B

Adding requests:   0%|          | 0/1 [00:00<?, ?it/s][AAdding requests: 100%|██████████| 1/1 [00:00<00:00, 690.65it/s]

Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: Qwen/Qwen3-14B (backend=vllm)
[ModelCache] Using max_model_len=40960 for Qwen/Qwen3-14B
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 13:20:58 [utils.py:253] non-default args: {'max_model_len': 40960, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'Qwen/Qwen3-14B'}
INFO 12-04 13:20:59 [model.py:631] Resolved architecture: Qwen3ForCausalLM
INFO 12-04 13:20:59 [model.py:1745] Using max model len 40960
INFO 12-04 13:20:59 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=385782)[0;0m INFO 12-04 13:21:09 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='Qwen/Qwen3-14B', speculative_config=None, tokenizer='Qwen/Qwen3-14B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-14B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=385782)[0;0m INFO 12-04 13:21:11 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.98:47249 backend=nccl
[W1204 13:21:11.749367108 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-25-0.rc.tch.harvard.edu]:47249 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=385782)[0;0m INFO 12-04 13:21:11 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=385782)[0;0m ERROR 12-04 13:21:11 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=385782)[0;0m ERROR 12-04 13:21:11 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=385782)[0;0m ERROR 12-04 13:21:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=385782)[0;0m ERROR 12-04 13:21:11 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=385782)[0;0m ERROR 12-04 13:21:11 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=385782)[0;0m ERROR 12-04 13:21:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=385782)[0;0m ERROR 12-04 13:21:11 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=385782)[0;0m ERROR 12-04 13:21:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=385782)[0;0m ERROR 12-04 13:21:11 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=385782)[0;0m ERROR 12-04 13:21:11 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=385782)[0;0m ERROR 12-04 13:21:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=385782)[0;0m ERROR 12-04 13:21:11 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=385782)[0;0m ERROR 12-04 13:21:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=385782)[0;0m ERROR 12-04 13:21:11 [core.py:842]     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=385782)[0;0m ERROR 12-04 13:21:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=385782)[0;0m ERROR 12-04 13:21:11 [core.py:842]     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=385782)[0;0m ERROR 12-04 13:21:11 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=385782)[0;0m ERROR 12-04 13:21:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=385782)[0;0m ERROR 12-04 13:21:11 [core.py:842]     raise ValueError(
[1;36m(EngineCore_DP0 pid=385782)[0;0m ERROR 12-04 13:21:11 [core.py:842] ValueError: Free memory on device (2.71/44.39 GiB) on startup is less than desired GPU memory utilization (0.9, 39.95 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[1;36m(EngineCore_DP0 pid=385782)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=385782)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=385782)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=385782)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=385782)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=385782)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=385782)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=385782)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=385782)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=385782)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=385782)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=385782)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=385782)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=385782)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=385782)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=385782)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=385782)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=385782)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=385782)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=385782)[0;0m     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=385782)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=385782)[0;0m     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=385782)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=385782)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=385782)[0;0m     raise ValueError(
[1;36m(EngineCore_DP0 pid=385782)[0;0m ValueError: Free memory on device (2.71/44.39 GiB) on startup is less than desired GPU memory utilization (0.9, 39.95 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W1204 13:21:12.578642181 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts: 100%|██████████| 1/1 [00:15<00:00, 15.89s/it, est. speed input: 27.25 toks/s, output: 24.98 toks/s][A
Processed prompts: 100%|██████████| 1/1 [00:15<00:00, 15.89s/it, est. speed input: 27.25 toks/s, output: 24.98 toks/s][AProcessed prompts: 100%|██████████| 1/1 [00:15<00:00, 15.89s/it, est. speed input: 27.25 toks/s, output: 24.98 toks/s]
Agent 1 response: The expression adheres to standard arithmetic rules: multiplication precedes addition/subtraction. C...

--- Problem 1/2, Round 3, Agent 2/3 ---
Agent 2 receiving other agents' responses...
[ModelCache] Using cached model: Qwen/Qwen3-14B

Adding requests:   0%|          | 0/1 [00:00<?, ?it/s][AAdding requests: 100%|██████████| 1/1 [00:00<00:00, 538.63it/s]

Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts: 100%|██████████| 1/1 [00:12<00:00, 12.31s/it, est. speed input: 35.51 toks/s, output: 25.19 toks/s][A
Processed prompts: 100%|██████████| 1/1 [00:12<00:00, 12.31s/it, est. speed input: 35.51 toks/s, output: 25.19 toks/s][AProcessed prompts: 100%|██████████| 1/1 [00:12<00:00, 12.31s/it, est. speed input: 35.51 toks/s, output: 25.19 toks/s]
Agent 2 response: The silence between notes is louder than the music.  
A shadow does not seek the light.  
The answer...

--- Problem 1/2, Round 3, Agent 3/3 ---
Agent 3 receiving other agents' responses...
[ModelCache] Using cached model: Qwen/Qwen3-14B

Adding requests:   0%|          | 0/1 [00:00<?, ?it/s][AAdding requests: 100%|██████████| 1/1 [00:00<00:00, 779.61it/s]

Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: Qwen/Qwen3-14B (backend=vllm)
[ModelCache] Using max_model_len=40960 for Qwen/Qwen3-14B
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 13:21:32 [utils.py:253] non-default args: {'max_model_len': 40960, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'Qwen/Qwen3-14B'}
INFO 12-04 13:21:33 [model.py:631] Resolved architecture: Qwen3ForCausalLM
INFO 12-04 13:21:33 [model.py:1745] Using max model len 40960
INFO 12-04 13:21:33 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts: 100%|██████████| 1/1 [00:15<00:00, 15.87s/it, est. speed input: 27.34 toks/s, output: 25.01 toks/s][A
Processed prompts: 100%|██████████| 1/1 [00:15<00:00, 15.87s/it, est. speed input: 27.34 toks/s, output: 25.01 toks/s][AProcessed prompts: 100%|██████████| 1/1 [00:15<00:00, 15.87s/it, est. speed input: 27.34 toks/s, output: 25.01 toks/s]
Agent 3 response: The calculation adheres to the standard order of operations: multiplication (19*28 = 532 and 10*7 = ...
performance: 1.0 0.0

--- Problem 2/2, Round 1, Agent 1/3 ---
[ModelCache] Using cached model: Qwen/Qwen3-14B
 50%|█████     | 1/2 [10:00<10:00, 600.05s/it]
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s][AAdding requests: 100%|██████████| 1/1 [00:00<00:00, 1244.97it/s]

[1;36m(EngineCore_DP0 pid=387581)[0;0m INFO 12-04 13:21:44 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='Qwen/Qwen3-14B', speculative_config=None, tokenizer='Qwen/Qwen3-14B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-14B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=387581)[0;0m INFO 12-04 13:21:45 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.98:33865 backend=nccl
[W1204 13:21:45.896829892 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-25-0.rc.tch.harvard.edu]:33865 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=387581)[0;0m INFO 12-04 13:21:45 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=387581)[0;0m ERROR 12-04 13:21:45 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=387581)[0;0m ERROR 12-04 13:21:45 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=387581)[0;0m ERROR 12-04 13:21:45 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=387581)[0;0m ERROR 12-04 13:21:45 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=387581)[0;0m ERROR 12-04 13:21:45 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=387581)[0;0m ERROR 12-04 13:21:45 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=387581)[0;0m ERROR 12-04 13:21:45 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=387581)[0;0m ERROR 12-04 13:21:45 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=387581)[0;0m ERROR 12-04 13:21:45 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=387581)[0;0m ERROR 12-04 13:21:45 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=387581)[0;0m ERROR 12-04 13:21:45 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=387581)[0;0m ERROR 12-04 13:21:45 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=387581)[0;0m ERROR 12-04 13:21:45 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=387581)[0;0m ERROR 12-04 13:21:45 [core.py:842]     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=387581)[0;0m ERROR 12-04 13:21:45 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=387581)[0;0m ERROR 12-04 13:21:45 [core.py:842]     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=387581)[0;0m ERROR 12-04 13:21:45 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=387581)[0;0m ERROR 12-04 13:21:45 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=387581)[0;0m ERROR 12-04 13:21:45 [core.py:842]     raise ValueError(
[1;36m(EngineCore_DP0 pid=387581)[0;0m ERROR 12-04 13:21:45 [core.py:842] ValueError: Free memory on device (2.71/44.39 GiB) on startup is less than desired GPU memory utilization (0.9, 39.95 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[1;36m(EngineCore_DP0 pid=387581)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=387581)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=387581)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=387581)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=387581)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=387581)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=387581)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=387581)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=387581)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=387581)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=387581)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=387581)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=387581)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=387581)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=387581)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=387581)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=387581)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=387581)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=387581)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=387581)[0;0m     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=387581)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=387581)[0;0m     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=387581)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=387581)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=387581)[0;0m     raise ValueError(
[1;36m(EngineCore_DP0 pid=387581)[0;0m ValueError: Free memory on device (2.71/44.39 GiB) on startup is less than desired GPU memory utilization (0.9, 39.95 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W1204 13:21:46.715606006 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts: 100%|██████████| 1/1 [00:25<00:00, 25.68s/it, est. speed input: 2.92 toks/s, output: 25.16 toks/s][A
Processed prompts: 100%|██████████| 1/1 [00:25<00:00, 25.68s/it, est. speed input: 2.92 toks/s, output: 25.16 toks/s][AProcessed prompts: 100%|██████████| 1/1 [00:25<00:00, 25.68s/it, est. speed input: 2.92 toks/s, output: 25.16 toks/s]
Agent 1 response: Following the order of operations (multiplication before addition/subtraction):

1. Calculate the mu...

--- Problem 2/2, Round 1, Agent 2/3 ---
[ModelCache] Using cached model: Qwen/Qwen3-14B

Adding requests:   0%|          | 0/1 [00:00<?, ?it/s][AAdding requests: 100%|██████████| 1/1 [00:00<00:00, 1501.18it/s]

Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: Qwen/Qwen3-14B (backend=vllm)
[ModelCache] Using max_model_len=40960 for Qwen/Qwen3-14B
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 13:22:07 [utils.py:253] non-default args: {'max_model_len': 40960, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'Qwen/Qwen3-14B'}
INFO 12-04 13:22:07 [model.py:631] Resolved architecture: Qwen3ForCausalLM
INFO 12-04 13:22:07 [model.py:1745] Using max model len 40960
INFO 12-04 13:22:07 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=388941)[0;0m INFO 12-04 13:22:18 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='Qwen/Qwen3-14B', speculative_config=None, tokenizer='Qwen/Qwen3-14B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-14B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=388941)[0;0m INFO 12-04 13:22:19 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.98:56777 backend=nccl
[W1204 13:22:19.360505065 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-25-0.rc.tch.harvard.edu]:56777 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=388941)[0;0m INFO 12-04 13:22:19 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=388941)[0;0m ERROR 12-04 13:22:20 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=388941)[0;0m ERROR 12-04 13:22:20 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=388941)[0;0m ERROR 12-04 13:22:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=388941)[0;0m ERROR 12-04 13:22:20 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=388941)[0;0m ERROR 12-04 13:22:20 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=388941)[0;0m ERROR 12-04 13:22:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=388941)[0;0m ERROR 12-04 13:22:20 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=388941)[0;0m ERROR 12-04 13:22:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=388941)[0;0m ERROR 12-04 13:22:20 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=388941)[0;0m ERROR 12-04 13:22:20 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=388941)[0;0m ERROR 12-04 13:22:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=388941)[0;0m ERROR 12-04 13:22:20 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=388941)[0;0m ERROR 12-04 13:22:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=388941)[0;0m ERROR 12-04 13:22:20 [core.py:842]     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=388941)[0;0m ERROR 12-04 13:22:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=388941)[0;0m ERROR 12-04 13:22:20 [core.py:842]     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=388941)[0;0m ERROR 12-04 13:22:20 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=388941)[0;0m ERROR 12-04 13:22:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=388941)[0;0m ERROR 12-04 13:22:20 [core.py:842]     raise ValueError(
[1;36m(EngineCore_DP0 pid=388941)[0;0m ERROR 12-04 13:22:20 [core.py:842] ValueError: Free memory on device (2.71/44.39 GiB) on startup is less than desired GPU memory utilization (0.9, 39.95 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[1;36m(EngineCore_DP0 pid=388941)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=388941)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=388941)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=388941)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=388941)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=388941)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=388941)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=388941)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=388941)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=388941)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=388941)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=388941)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=388941)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=388941)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=388941)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=388941)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=388941)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=388941)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=388941)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=388941)[0;0m     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=388941)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=388941)[0;0m     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=388941)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=388941)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=388941)[0;0m     raise ValueError(
[1;36m(EngineCore_DP0 pid=388941)[0;0m ValueError: Free memory on device (2.71/44.39 GiB) on startup is less than desired GPU memory utilization (0.9, 39.95 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W1204 13:22:20.136386292 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts: 100%|██████████| 1/1 [00:19<00:00, 19.33s/it, est. speed input: 4.09 toks/s, output: 25.09 toks/s][A
Processed prompts: 100%|██████████| 1/1 [00:19<00:00, 19.33s/it, est. speed input: 4.09 toks/s, output: 25.09 toks/s][AProcessed prompts: 100%|██████████| 1/1 [00:19<00:00, 19.33s/it, est. speed input: 4.09 toks/s, output: 25.09 toks/s]
Agent 2 response: The moon cradles the tide.  
A stone falls from a mountain.  
The wind whispers to the flame.  
The ...

--- Problem 2/2, Round 1, Agent 3/3 ---
[ModelCache] Using cached model: Qwen/Qwen3-14B

Adding requests:   0%|          | 0/1 [00:00<?, ?it/s][AAdding requests: 100%|██████████| 1/1 [00:00<00:00, 1608.25it/s]

Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: Qwen/Qwen3-14B (backend=vllm)
[ModelCache] Using max_model_len=40960 for Qwen/Qwen3-14B
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 13:22:41 [utils.py:253] non-default args: {'max_model_len': 40960, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'Qwen/Qwen3-14B'}
INFO 12-04 13:22:41 [model.py:631] Resolved architecture: Qwen3ForCausalLM
INFO 12-04 13:22:41 [model.py:1745] Using max model len 40960
INFO 12-04 13:22:41 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=390894)[0;0m INFO 12-04 13:22:52 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='Qwen/Qwen3-14B', speculative_config=None, tokenizer='Qwen/Qwen3-14B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-14B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=390894)[0;0m INFO 12-04 13:22:54 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.98:39221 backend=nccl
[W1204 13:22:54.544346979 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-25-0.rc.tch.harvard.edu]:39221 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=390894)[0;0m INFO 12-04 13:22:54 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=390894)[0;0m ERROR 12-04 13:22:54 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=390894)[0;0m ERROR 12-04 13:22:54 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=390894)[0;0m ERROR 12-04 13:22:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=390894)[0;0m ERROR 12-04 13:22:54 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=390894)[0;0m ERROR 12-04 13:22:54 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=390894)[0;0m ERROR 12-04 13:22:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=390894)[0;0m ERROR 12-04 13:22:54 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=390894)[0;0m ERROR 12-04 13:22:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=390894)[0;0m ERROR 12-04 13:22:54 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=390894)[0;0m ERROR 12-04 13:22:54 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=390894)[0;0m ERROR 12-04 13:22:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=390894)[0;0m ERROR 12-04 13:22:54 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=390894)[0;0m ERROR 12-04 13:22:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=390894)[0;0m ERROR 12-04 13:22:54 [core.py:842]     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=390894)[0;0m ERROR 12-04 13:22:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=390894)[0;0m ERROR 12-04 13:22:54 [core.py:842]     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=390894)[0;0m ERROR 12-04 13:22:54 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=390894)[0;0m ERROR 12-04 13:22:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=390894)[0;0m ERROR 12-04 13:22:54 [core.py:842]     raise ValueError(
[1;36m(EngineCore_DP0 pid=390894)[0;0m ERROR 12-04 13:22:54 [core.py:842] ValueError: Free memory on device (2.71/44.39 GiB) on startup is less than desired GPU memory utilization (0.9, 39.95 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[1;36m(EngineCore_DP0 pid=390894)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=390894)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=390894)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=390894)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=390894)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=390894)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=390894)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=390894)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=390894)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=390894)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=390894)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=390894)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=390894)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=390894)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=390894)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=390894)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=390894)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=390894)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=390894)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=390894)[0;0m     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=390894)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=390894)[0;0m     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=390894)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=390894)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=390894)[0;0m     raise ValueError(
[1;36m(EngineCore_DP0 pid=390894)[0;0m ValueError: Free memory on device (2.71/44.39 GiB) on startup is less than desired GPU memory utilization (0.9, 39.95 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W1204 13:22:54.308686293 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts: 100%|██████████| 1/1 [00:31<00:00, 31.64s/it, est. speed input: 2.40 toks/s, output: 25.09 toks/s][A
Processed prompts: 100%|██████████| 1/1 [00:31<00:00, 31.64s/it, est. speed input: 2.40 toks/s, output: 25.09 toks/s][AProcessed prompts: 100%|██████████| 1/1 [00:31<00:00, 31.64s/it, est. speed input: 2.40 toks/s, output: 25.09 toks/s]
Agent 3 response: The result of 28 + 20*6 + 25 - 18*22 is calculated by first performing the multiplications: 20*6 = 1...

--- Problem 2/2, Round 2, Agent 1/3 ---
Agent 1 receiving other agents' responses...
[ModelCache] Using cached model: Qwen/Qwen3-14B

Adding requests:   0%|          | 0/1 [00:00<?, ?it/s][AAdding requests: 100%|██████████| 1/1 [00:00<00:00, 708.98it/s]

Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts: 100%|██████████| 1/1 [00:11<00:00, 11.60s/it, est. speed input: 39.93 toks/s, output: 25.10 toks/s][A
Processed prompts: 100%|██████████| 1/1 [00:11<00:00, 11.60s/it, est. speed input: 39.93 toks/s, output: 25.10 toks/s][AProcessed prompts: 100%|██████████| 1/1 [00:11<00:00, 11.60s/it, est. speed input: 39.93 toks/s, output: 25.10 toks/s]
Agent 1 response: The calculation adheres strictly to standard arithmetic rules, prioritizing multiplication before ad...

--- Problem 2/2, Round 2, Agent 2/3 ---
Agent 2 receiving other agents' responses...
[ModelCache] Using cached model: Qwen/Qwen3-14B

Adding requests:   0%|          | 0/1 [00:00<?, ?it/s][AAdding requests: 100%|██████████| 1/1 [00:00<00:00, 724.78it/s]

Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: Qwen/Qwen3-14B (backend=vllm)
[ModelCache] Using max_model_len=40960 for Qwen/Qwen3-14B
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 13:23:15 [utils.py:253] non-default args: {'max_model_len': 40960, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'Qwen/Qwen3-14B'}
INFO 12-04 13:23:15 [model.py:631] Resolved architecture: Qwen3ForCausalLM
INFO 12-04 13:23:15 [model.py:1745] Using max model len 40960
INFO 12-04 13:23:15 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts: 100%|██████████| 1/1 [00:13<00:00, 13.06s/it, est. speed input: 35.75 toks/s, output: 24.88 toks/s][A
Processed prompts: 100%|██████████| 1/1 [00:13<00:00, 13.06s/it, est. speed input: 35.75 toks/s, output: 24.88 toks/s][AProcessed prompts: 100%|██████████| 1/1 [00:13<00:00, 13.06s/it, est. speed input: 35.75 toks/s, output: 24.88 toks/s]
Agent 2 response: The echo of a bell fades.  
A butterfly dissolves into the sky.  
The answer is -223....

--- Problem 2/2, Round 2, Agent 3/3 ---
Agent 3 receiving other agents' responses...
[ModelCache] Using cached model: Qwen/Qwen3-14B

Adding requests:   0%|          | 0/1 [00:00<?, ?it/s][AAdding requests: 100%|██████████| 1/1 [00:00<00:00, 514.32it/s]

[1;36m(EngineCore_DP0 pid=392181)[0;0m INFO 12-04 13:23:27 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='Qwen/Qwen3-14B', speculative_config=None, tokenizer='Qwen/Qwen3-14B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-14B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=392181)[0;0m INFO 12-04 13:23:28 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.98:40843 backend=nccl
[W1204 13:23:28.848662222 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-25-0.rc.tch.harvard.edu]:40843 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=392181)[0;0m INFO 12-04 13:23:28 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=392181)[0;0m ERROR 12-04 13:23:28 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=392181)[0;0m ERROR 12-04 13:23:28 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=392181)[0;0m ERROR 12-04 13:23:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=392181)[0;0m ERROR 12-04 13:23:28 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=392181)[0;0m ERROR 12-04 13:23:28 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=392181)[0;0m ERROR 12-04 13:23:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=392181)[0;0m ERROR 12-04 13:23:28 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=392181)[0;0m ERROR 12-04 13:23:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=392181)[0;0m ERROR 12-04 13:23:28 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=392181)[0;0m ERROR 12-04 13:23:28 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=392181)[0;0m ERROR 12-04 13:23:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=392181)[0;0m ERROR 12-04 13:23:28 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=392181)[0;0m ERROR 12-04 13:23:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=392181)[0;0m ERROR 12-04 13:23:28 [core.py:842]     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=392181)[0;0m ERROR 12-04 13:23:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=392181)[0;0m ERROR 12-04 13:23:28 [core.py:842]     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=392181)[0;0m ERROR 12-04 13:23:28 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=392181)[0;0m ERROR 12-04 13:23:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=392181)[0;0m ERROR 12-04 13:23:28 [core.py:842]     raise ValueError(
[1;36m(EngineCore_DP0 pid=392181)[0;0m ERROR 12-04 13:23:28 [core.py:842] ValueError: Free memory on device (2.71/44.39 GiB) on startup is less than desired GPU memory utilization (0.9, 39.95 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[1;36m(EngineCore_DP0 pid=392181)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=392181)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=392181)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=392181)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=392181)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=392181)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=392181)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=392181)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=392181)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=392181)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=392181)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=392181)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=392181)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=392181)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=392181)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=392181)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=392181)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=392181)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=392181)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=392181)[0;0m     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=392181)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=392181)[0;0m     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=392181)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=392181)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=392181)[0;0m     raise ValueError(
[1;36m(EngineCore_DP0 pid=392181)[0;0m ValueError: Free memory on device (2.71/44.39 GiB) on startup is less than desired GPU memory utilization (0.9, 39.95 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W1204 13:23:29.676386842 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts: 100%|██████████| 1/1 [00:20<00:00, 20.38s/it, est. speed input: 22.77 toks/s, output: 25.02 toks/s][A
Processed prompts: 100%|██████████| 1/1 [00:20<00:00, 20.38s/it, est. speed input: 22.77 toks/s, output: 25.02 toks/s][AProcessed prompts: 100%|██████████| 1/1 [00:20<00:00, 20.38s/it, est. speed input: 22.77 toks/s, output: 25.02 toks/s]
Agent 3 response: The calculation follows the order of operations:  
1. Multiplication: $20 \times 6 = 120$, $18 \time...

--- Problem 2/2, Round 3, Agent 1/3 ---
Agent 1 receiving other agents' responses...
[ModelCache] Using cached model: Qwen/Qwen3-14B

Adding requests:   0%|          | 0/1 [00:00<?, ?it/s][AAdding requests: 100%|██████████| 1/1 [00:00<00:00, 551.16it/s]

Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: Qwen/Qwen3-14B (backend=vllm)
[ModelCache] Using max_model_len=40960 for Qwen/Qwen3-14B
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 13:23:50 [utils.py:253] non-default args: {'max_model_len': 40960, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'Qwen/Qwen3-14B'}
INFO 12-04 13:23:50 [model.py:631] Resolved architecture: Qwen3ForCausalLM
INFO 12-04 13:23:50 [model.py:1745] Using max model len 40960
INFO 12-04 13:23:50 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts: 100%|██████████| 1/1 [00:15<00:00, 15.11s/it, est. speed input: 47.52 toks/s, output: 24.95 toks/s][A
Processed prompts: 100%|██████████| 1/1 [00:15<00:00, 15.11s/it, est. speed input: 47.52 toks/s, output: 24.95 toks/s][AProcessed prompts: 100%|██████████| 1/1 [00:15<00:00, 15.11s/it, est. speed input: 47.52 toks/s, output: 24.95 toks/s]
Agent 1 response: The calculation remains consistent with established arithmetic principles and prior agent confirmati...

--- Problem 2/2, Round 3, Agent 2/3 ---
Agent 2 receiving other agents' responses...
[ModelCache] Using cached model: Qwen/Qwen3-14B

Adding requests:   0%|          | 0/1 [00:00<?, ?it/s][AAdding requests: 100%|██████████| 1/1 [00:00<00:00, 460.51it/s]

[1;36m(EngineCore_DP0 pid=393179)[0;0m INFO 12-04 13:24:01 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='Qwen/Qwen3-14B', speculative_config=None, tokenizer='Qwen/Qwen3-14B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-14B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=393179)[0;0m INFO 12-04 13:24:02 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.98:38037 backend=nccl
[W1204 13:24:02.033928835 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-25-0.rc.tch.harvard.edu]:38037 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=393179)[0;0m INFO 12-04 13:24:02 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=393179)[0;0m ERROR 12-04 13:24:02 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=393179)[0;0m ERROR 12-04 13:24:02 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=393179)[0;0m ERROR 12-04 13:24:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=393179)[0;0m ERROR 12-04 13:24:02 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=393179)[0;0m ERROR 12-04 13:24:02 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=393179)[0;0m ERROR 12-04 13:24:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=393179)[0;0m ERROR 12-04 13:24:02 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=393179)[0;0m ERROR 12-04 13:24:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=393179)[0;0m ERROR 12-04 13:24:02 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=393179)[0;0m ERROR 12-04 13:24:02 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=393179)[0;0m ERROR 12-04 13:24:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=393179)[0;0m ERROR 12-04 13:24:02 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=393179)[0;0m ERROR 12-04 13:24:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=393179)[0;0m ERROR 12-04 13:24:02 [core.py:842]     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=393179)[0;0m ERROR 12-04 13:24:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=393179)[0;0m ERROR 12-04 13:24:02 [core.py:842]     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=393179)[0;0m ERROR 12-04 13:24:02 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=393179)[0;0m ERROR 12-04 13:24:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=393179)[0;0m ERROR 12-04 13:24:02 [core.py:842]     raise ValueError(
[1;36m(EngineCore_DP0 pid=393179)[0;0m ERROR 12-04 13:24:02 [core.py:842] ValueError: Free memory on device (2.71/44.39 GiB) on startup is less than desired GPU memory utilization (0.9, 39.95 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[1;36m(EngineCore_DP0 pid=393179)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=393179)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=393179)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=393179)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=393179)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=393179)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=393179)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=393179)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=393179)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=393179)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=393179)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=393179)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=393179)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=393179)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=393179)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=393179)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=393179)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=393179)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=393179)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=393179)[0;0m     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=393179)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=393179)[0;0m     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=393179)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=393179)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=393179)[0;0m     raise ValueError(
[1;36m(EngineCore_DP0 pid=393179)[0;0m ValueError: Free memory on device (2.71/44.39 GiB) on startup is less than desired GPU memory utilization (0.9, 39.95 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W1204 13:24:03.905653292 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts: 100%|██████████| 1/1 [00:08<00:00,  8.04s/it, est. speed input: 89.80 toks/s, output: 24.75 toks/s][A
Processed prompts: 100%|██████████| 1/1 [00:08<00:00,  8.04s/it, est. speed input: 89.80 toks/s, output: 24.75 toks/s][AProcessed prompts: 100%|██████████| 1/1 [00:08<00:00,  8.04s/it, est. speed input: 89.80 toks/s, output: 24.75 toks/s]
Agent 2 response: The river flows without memory.  
A shadow merges with the earth.  
The answer is -223....

--- Problem 2/2, Round 3, Agent 3/3 ---
Agent 3 receiving other agents' responses...
[ModelCache] Using cached model: Qwen/Qwen3-14B

Adding requests:   0%|          | 0/1 [00:00<?, ?it/s][AAdding requests: 100%|██████████| 1/1 [00:00<00:00, 542.67it/s]

Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts: 100%|██████████| 1/1 [00:11<00:00, 11.37s/it, est. speed input: 63.24 toks/s, output: 25.07 toks/s][A
Processed prompts: 100%|██████████| 1/1 [00:11<00:00, 11.37s/it, est. speed input: 63.24 toks/s, output: 25.07 toks/s][AProcessed prompts: 100%|██████████| 1/1 [00:11<00:00, 11.37s/it, est. speed input: 63.24 toks/s, output: 25.07 toks/s]
100%|██████████| 2/2 [12:36<00:00, 338.98s/it]100%|██████████| 2/2 [12:36<00:00, 378.14s/it]
[rank0]:[W1204 13:24:17.914502487 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Agent 3 response: The calculation follows standard arithmetic rules, confirming the result remains **-223**. The answe...
performance: 1.0 0.0
============================================================
Results saved to: /home/ch269957/projects/slm_multiagent_debate/experiments/linux_single/results/math/math_Qwen3-14B_persona_enigma+zen+deep-sea_agents3_rounds3.p
Final performance: 1.000 ± 0.000
============================================================
[ModelCache] Shut down vLLM model: vllm:Qwen/Qwen3-14B
[ModelCache] All models shut down
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: Qwen/Qwen3-14B (backend=vllm)
[ModelCache] Using max_model_len=40960 for Qwen/Qwen3-14B
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 13:24:24 [utils.py:253] non-default args: {'max_model_len': 40960, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'Qwen/Qwen3-14B'}
INFO 12-04 13:24:24 [model.py:631] Resolved architecture: Qwen3ForCausalLM
INFO 12-04 13:24:24 [model.py:1745] Using max model len 40960
INFO 12-04 13:24:24 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=394123)[0;0m INFO 12-04 13:24:36 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='Qwen/Qwen3-14B', speculative_config=None, tokenizer='Qwen/Qwen3-14B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-14B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=394123)[0;0m INFO 12-04 13:24:37 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.98:52271 backend=nccl
[W1204 13:24:37.682795158 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-25-0.rc.tch.harvard.edu]:52271 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=394123)[0;0m INFO 12-04 13:24:37 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=394123)[0;0m INFO 12-04 13:24:37 [gpu_model_runner.py:3259] Starting to load model Qwen/Qwen3-14B...
[1;36m(EngineCore_DP0 pid=394123)[0;0m INFO 12-04 13:24:38 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=394123)[0;0m INFO 12-04 13:24:38 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=394123)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/8 [00:00<?, ?it/s]
[1;36m(EngineCore_DP0 pid=394123)[0;0m Loading safetensors checkpoint shards:  12% Completed | 1/8 [00:00<00:04,  1.63it/s]
[1;36m(EngineCore_DP0 pid=394123)[0;0m Loading safetensors checkpoint shards:  25% Completed | 2/8 [00:01<00:04,  1.49it/s]
[1;36m(EngineCore_DP0 pid=394123)[0;0m Loading safetensors checkpoint shards:  38% Completed | 3/8 [00:02<00:03,  1.45it/s]
[1;36m(EngineCore_DP0 pid=394123)[0;0m Loading safetensors checkpoint shards:  50% Completed | 4/8 [00:02<00:02,  1.40it/s]
[1;36m(EngineCore_DP0 pid=394123)[0;0m Loading safetensors checkpoint shards:  62% Completed | 5/8 [00:03<00:02,  1.39it/s]
[1;36m(EngineCore_DP0 pid=394123)[0;0m Loading safetensors checkpoint shards:  75% Completed | 6/8 [00:04<00:01,  1.40it/s]
[1;36m(EngineCore_DP0 pid=394123)[0;0m Loading safetensors checkpoint shards:  88% Completed | 7/8 [00:04<00:00,  1.40it/s]
[1;36m(EngineCore_DP0 pid=394123)[0;0m Loading safetensors checkpoint shards: 100% Completed | 8/8 [00:05<00:00,  1.63it/s]
[1;36m(EngineCore_DP0 pid=394123)[0;0m Loading safetensors checkpoint shards: 100% Completed | 8/8 [00:05<00:00,  1.50it/s]
[1;36m(EngineCore_DP0 pid=394123)[0;0m 
[1;36m(EngineCore_DP0 pid=394123)[0;0m INFO 12-04 13:24:44 [default_loader.py:314] Loading weights took 5.39 seconds
[1;36m(EngineCore_DP0 pid=394123)[0;0m INFO 12-04 13:24:44 [gpu_model_runner.py:3338] Model loading took 27.5185 GiB memory and 6.321971 seconds
[1;36m(EngineCore_DP0 pid=394123)[0;0m INFO 12-04 13:24:53 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/76690ce19a/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=394123)[0;0m INFO 12-04 13:24:53 [backends.py:647] Dynamo bytecode transform time: 8.42 s
[1;36m(EngineCore_DP0 pid=394123)[0;0m INFO 12-04 13:24:58 [backends.py:210] Directly load the compiled graph(s) for dynamic shape from the cache, took 4.796 s
[1;36m(EngineCore_DP0 pid=394123)[0;0m INFO 12-04 13:24:59 [monitor.py:34] torch.compile takes 13.22 s in total
[1;36m(EngineCore_DP0 pid=394123)[0;0m INFO 12-04 13:25:02 [gpu_worker.py:359] Available KV cache memory: 10.93 GiB
[1;36m(EngineCore_DP0 pid=394123)[0;0m INFO 12-04 13:25:02 [kv_cache_utils.py:1229] GPU KV cache size: 71,616 tokens
[1;36m(EngineCore_DP0 pid=394123)[0;0m INFO 12-04 13:25:02 [kv_cache_utils.py:1234] Maximum concurrency for 40,960 tokens per request: 1.75x
[1;36m(EngineCore_DP0 pid=394123)[0;0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/51 [00:00<?, ?it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   2%|▏         | 1/51 [00:00<00:06,  7.54it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   4%|▍         | 2/51 [00:00<00:06,  7.45it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 3/51 [00:00<00:06,  7.40it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|▊         | 4/51 [00:00<00:06,  7.43it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  10%|▉         | 5/51 [00:00<00:06,  7.61it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  12%|█▏        | 6/51 [00:00<00:05,  7.72it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▎        | 7/51 [00:00<00:05,  7.79it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|█▌        | 8/51 [00:01<00:05,  7.85it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 9/51 [00:01<00:05,  8.30it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|██▏       | 11/51 [00:01<00:04,  8.97it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  24%|██▎       | 12/51 [00:01<00:04,  9.17it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 13/51 [00:01<00:04,  9.28it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|██▋       | 14/51 [00:01<00:03,  9.41it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███▏      | 16/51 [00:01<00:03,  9.69it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  35%|███▌      | 18/51 [00:02<00:03, 10.58it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  39%|███▉      | 20/51 [00:02<00:02, 11.20it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 22/51 [00:02<00:02, 11.64it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 24/51 [00:02<00:02, 11.97it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|█████     | 26/51 [00:02<00:02, 12.47it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▍    | 28/51 [00:02<00:01, 12.82it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  59%|█████▉    | 30/51 [00:02<00:01, 13.11it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 32/51 [00:03<00:01, 13.36it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 34/51 [00:03<00:01, 13.43it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████   | 36/51 [00:03<00:01, 13.61it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▍  | 38/51 [00:03<00:00, 13.93it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  78%|███████▊  | 40/51 [00:03<00:00, 14.16it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|████████▏ | 42/51 [00:03<00:00, 14.22it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▋ | 44/51 [00:03<00:00, 14.31it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  90%|█████████ | 46/51 [00:04<00:00, 14.31it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|█████████▍| 48/51 [00:04<00:00, 14.64it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  98%|█████████▊| 50/51 [00:04<00:00, 14.88it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:04<00:00, 11.65it/s]
[1;36m(EngineCore_DP0 pid=394123)[0;0m Capturing CUDA graphs (decode, FULL):   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graphs (decode, FULL):   6%|▌         | 2/35 [00:00<00:02, 12.14it/s]Capturing CUDA graphs (decode, FULL):  11%|█▏        | 4/35 [00:00<00:02, 12.42it/s]Capturing CUDA graphs (decode, FULL):  17%|█▋        | 6/35 [00:00<00:02, 12.63it/s]Capturing CUDA graphs (decode, FULL):  23%|██▎       | 8/35 [00:00<00:02, 12.78it/s]Capturing CUDA graphs (decode, FULL):  29%|██▊       | 10/35 [00:00<00:01, 13.17it/s]Capturing CUDA graphs (decode, FULL):  34%|███▍      | 12/35 [00:00<00:01, 13.45it/s]Capturing CUDA graphs (decode, FULL):  40%|████      | 14/35 [00:01<00:01, 13.59it/s]Capturing CUDA graphs (decode, FULL):  46%|████▌     | 16/35 [00:01<00:01, 13.75it/s]Capturing CUDA graphs (decode, FULL):  51%|█████▏    | 18/35 [00:01<00:01, 14.10it/s]Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 20/35 [00:01<00:01, 14.39it/s]Capturing CUDA graphs (decode, FULL):  63%|██████▎   | 22/35 [00:01<00:00, 14.75it/s]Capturing CUDA graphs (decode, FULL):  69%|██████▊   | 24/35 [00:01<00:00, 15.07it/s]Capturing CUDA graphs (decode, FULL):  74%|███████▍  | 26/35 [00:01<00:00, 15.38it/s]Capturing CUDA graphs (decode, FULL):  80%|████████  | 28/35 [00:01<00:00, 15.51it/s]Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 30/35 [00:02<00:00, 15.69it/s]Capturing CUDA graphs (decode, FULL):  91%|█████████▏| 32/35 [00:02<00:00, 16.11it/s]Capturing CUDA graphs (decode, FULL):  97%|█████████▋| 34/35 [00:02<00:00, 16.56it/s]Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:02<00:00, 14.70it/s]
[1;36m(EngineCore_DP0 pid=394123)[0;0m INFO 12-04 13:25:09 [gpu_model_runner.py:4244] Graph capturing finished in 7 secs, took 0.65 GiB
[1;36m(EngineCore_DP0 pid=394123)[0;0m INFO 12-04 13:25:09 [core.py:250] init engine (profile, create kv cache, warmup model) took 25.37 seconds
INFO 12-04 13:25:10 [llm.py:352] Supported tasks: ['generate']

Adding requests:   0%|          | 0/1 [00:00<?, ?it/s][AAdding requests: 100%|██████████| 1/1 [00:00<00:00, 612.22it/s]

Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts: 100%|██████████| 1/1 [00:17<00:00, 17.95s/it, est. speed input: 4.12 toks/s, output: 25.24 toks/s][A
Processed prompts: 100%|██████████| 1/1 [00:17<00:00, 17.95s/it, est. speed input: 4.12 toks/s, output: 25.24 toks/s][AProcessed prompts: 100%|██████████| 1/1 [00:17<00:00, 17.95s/it, est. speed input: 4.12 toks/s, output: 25.24 toks/s]
Agent 1 response: The result of the expression is 482....

--- Problem 1/2, Round 1, Agent 2/3 ---
[ModelCache] Using cached model: Qwen/Qwen3-14B

Adding requests:   0%|          | 0/1 [00:00<?, ?it/s][AAdding requests: 100%|██████████| 1/1 [00:00<00:00, 1418.91it/s]

Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts: 100%|██████████| 1/1 [00:18<00:00, 18.03s/it, est. speed input: 4.33 toks/s, output: 25.24 toks/s][A
Processed prompts: 100%|██████████| 1/1 [00:18<00:00, 18.03s/it, est. speed input: 4.33 toks/s, output: 25.24 toks/s][AProcessed prompts: 100%|██████████| 1/1 [00:18<00:00, 18.03s/it, est. speed input: 4.33 toks/s, output: 25.24 toks/s]
Agent 2 response: The moon reflects the sun's silence.  
A stone cannot sing.  
Breathe into the hollow of the questio...

--- Problem 1/2, Round 1, Agent 3/3 ---
[ModelCache] Using cached model: Qwen/Qwen3-14B

Adding requests:   0%|          | 0/1 [00:00<?, ?it/s][AAdding requests: 100%|██████████| 1/1 [00:00<00:00, 1410.32it/s]

Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts: 100%|██████████| 1/1 [00:13<00:00, 13.70s/it, est. speed input: 5.47 toks/s, output: 25.18 toks/s][A
Processed prompts: 100%|██████████| 1/1 [00:13<00:00, 13.70s/it, est. speed input: 5.47 toks/s, output: 25.18 toks/s][AProcessed prompts: 100%|██████████| 1/1 [00:13<00:00, 13.70s/it, est. speed input: 5.47 toks/s, output: 25.18 toks/s]
Agent 3 response: The result of 6 + 19*28 + 14 - 10*7 is 482....

--- Problem 1/2, Round 2, Agent 1/3 ---
Agent 1 receiving other agents' responses...
[ModelCache] Using cached model: Qwen/Qwen3-14B

Adding requests:   0%|          | 0/1 [00:00<?, ?it/s][AAdding requests: 100%|██████████| 1/1 [00:00<00:00, 938.11it/s]

Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts: 100%|██████████| 1/1 [00:11<00:00, 11.78s/it, est. speed input: 18.68 toks/s, output: 25.21 toks/s][A
Processed prompts: 100%|██████████| 1/1 [00:11<00:00, 11.78s/it, est. speed input: 18.68 toks/s, output: 25.21 toks/s][AProcessed prompts: 100%|██████████| 1/1 [00:11<00:00, 11.78s/it, est. speed input: 18.68 toks/s, output: 25.21 toks/s]
Agent 1 response: The calculation follows standard order of operations: multiplication before addition/subtraction. 19...

--- Problem 1/2, Round 2, Agent 2/3 ---
Agent 2 receiving other agents' responses...
[ModelCache] Using cached model: Qwen/Qwen3-14B

Adding requests:   0%|          | 0/1 [00:00<?, ?it/s][AAdding requests: 100%|██████████| 1/1 [00:00<00:00, 1064.54it/s]

Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts: 100%|██████████| 1/1 [00:13<00:00, 14.00s/it, est. speed input: 16.00 toks/s, output: 25.29 toks/s][A
Processed prompts: 100%|██████████| 1/1 [00:13<00:00, 14.00s/it, est. speed input: 16.00 toks/s, output: 25.29 toks/s][AProcessed prompts: 100%|██████████| 1/1 [00:14<00:00, 14.00s/it, est. speed input: 16.00 toks/s, output: 25.29 toks/s]
Agent 2 response: The wind does not count the stars.  
A river forgets its source.  
The answer is 482....

--- Problem 1/2, Round 2, Agent 3/3 ---
Agent 3 receiving other agents' responses...
[ModelCache] Using cached model: Qwen/Qwen3-14B

Adding requests:   0%|          | 0/1 [00:00<?, ?it/s][AAdding requests: 100%|██████████| 1/1 [00:00<00:00, 1136.36it/s]

Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts: 100%|██████████| 1/1 [00:12<00:00, 12.19s/it, est. speed input: 18.13 toks/s, output: 25.35 toks/s][A
Processed prompts: 100%|██████████| 1/1 [00:12<00:00, 12.19s/it, est. speed input: 18.13 toks/s, output: 25.35 toks/s][AProcessed prompts: 100%|██████████| 1/1 [00:12<00:00, 12.19s/it, est. speed input: 18.13 toks/s, output: 25.35 toks/s]
Agent 3 response: The result of the expression 6 + 19*28 + 14 - 10*7 is confirmed as 482 through standard arithmetic o...

--- Problem 1/2, Round 3, Agent 1/3 ---
Agent 1 receiving other agents' responses...
[ModelCache] Using cached model: Qwen/Qwen3-14B

Adding requests:   0%|          | 0/1 [00:00<?, ?it/s][AAdding requests: 100%|██████████| 1/1 [00:00<00:00, 707.90it/s]

Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts: 100%|██████████| 1/1 [00:15<00:00, 15.75s/it, est. speed input: 27.49 toks/s, output: 25.21 toks/s][A
Processed prompts: 100%|██████████| 1/1 [00:15<00:00, 15.75s/it, est. speed input: 27.49 toks/s, output: 25.21 toks/s][AProcessed prompts: 100%|██████████| 1/1 [00:15<00:00, 15.75s/it, est. speed input: 27.49 toks/s, output: 25.21 toks/s]
Agent 1 response: The expression adheres to standard arithmetic rules: multiplication precedes addition/subtraction. C...

--- Problem 1/2, Round 3, Agent 2/3 ---
Agent 2 receiving other agents' responses...
[ModelCache] Using cached model: Qwen/Qwen3-14B

Adding requests:   0%|          | 0/1 [00:00<?, ?it/s][AAdding requests: 100%|██████████| 1/1 [00:00<00:00, 787.07it/s]

Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts: 100%|██████████| 1/1 [00:12<00:00, 12.31s/it, est. speed input: 35.51 toks/s, output: 25.19 toks/s][A
Processed prompts: 100%|██████████| 1/1 [00:12<00:00, 12.31s/it, est. speed input: 35.51 toks/s, output: 25.19 toks/s][AProcessed prompts: 100%|██████████| 1/1 [00:12<00:00, 12.31s/it, est. speed input: 35.51 toks/s, output: 25.19 toks/s]
Agent 2 response: The silence between notes is louder than the music.  
A shadow does not seek the light.  
The answer...

--- Problem 1/2, Round 3, Agent 3/3 ---
Agent 3 receiving other agents' responses...
[ModelCache] Using cached model: Qwen/Qwen3-14B

Adding requests:   0%|          | 0/1 [00:00<?, ?it/s][AAdding requests: 100%|██████████| 1/1 [00:00<00:00, 763.43it/s]

Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts: 100%|██████████| 1/1 [00:15<00:00, 15.79s/it, est. speed input: 27.48 toks/s, output: 25.14 toks/s][A
Processed prompts: 100%|██████████| 1/1 [00:15<00:00, 15.79s/it, est. speed input: 27.48 toks/s, output: 25.14 toks/s][AProcessed prompts: 100%|██████████| 1/1 [00:15<00:00, 15.79s/it, est. speed input: 27.48 toks/s, output: 25.14 toks/s]
Agent 3 response: The calculation adheres to the standard order of operations: multiplication (19*28 = 532 and 10*7 = ...
performance: 1.0 0.0

--- Problem 2/2, Round 1, Agent 1/3 ---
[ModelCache] Using cached model: Qwen/Qwen3-14B
 50%|█████     | 1/2 [15:42<15:42, 942.05s/it]
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s][AAdding requests: 100%|██████████| 1/1 [00:00<00:00, 1499.04it/s]

Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts: 100%|██████████| 1/1 [00:25<00:00, 25.67s/it, est. speed input: 2.92 toks/s, output: 25.16 toks/s][A
Processed prompts: 100%|██████████| 1/1 [00:25<00:00, 25.67s/it, est. speed input: 2.92 toks/s, output: 25.16 toks/s][AProcessed prompts: 100%|██████████| 1/1 [00:25<00:00, 25.67s/it, est. speed input: 2.92 toks/s, output: 25.16 toks/s]
Agent 1 response: Following the order of operations (multiplication before addition/subtraction):

1. Calculate the mu...

--- Problem 2/2, Round 1, Agent 2/3 ---
[ModelCache] Using cached model: Qwen/Qwen3-14B

Adding requests:   0%|          | 0/1 [00:00<?, ?it/s][AAdding requests: 100%|██████████| 1/1 [00:00<00:00, 1682.43it/s]

Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts: 100%|██████████| 1/1 [00:19<00:00, 19.22s/it, est. speed input: 4.11 toks/s, output: 25.23 toks/s][A
Processed prompts: 100%|██████████| 1/1 [00:19<00:00, 19.22s/it, est. speed input: 4.11 toks/s, output: 25.23 toks/s][AProcessed prompts: 100%|██████████| 1/1 [00:19<00:00, 19.22s/it, est. speed input: 4.11 toks/s, output: 25.23 toks/s]
Agent 2 response: The moon cradles the tide.  
A stone falls from a mountain.  
The wind whispers to the flame.  
The ...

--- Problem 2/2, Round 1, Agent 3/3 ---
[ModelCache] Using cached model: Qwen/Qwen3-14B

Adding requests:   0%|          | 0/1 [00:00<?, ?it/s][AAdding requests: 100%|██████████| 1/1 [00:00<00:00, 1461.94it/s]

Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts: 100%|██████████| 1/1 [00:31<00:00, 31.47s/it, est. speed input: 2.42 toks/s, output: 25.23 toks/s][A
Processed prompts: 100%|██████████| 1/1 [00:31<00:00, 31.47s/it, est. speed input: 2.42 toks/s, output: 25.23 toks/s][AProcessed prompts: 100%|██████████| 1/1 [00:31<00:00, 31.47s/it, est. speed input: 2.42 toks/s, output: 25.23 toks/s]
Agent 3 response: The result of 28 + 20*6 + 25 - 18*22 is calculated by first performing the multiplications: 20*6 = 1...

--- Problem 2/2, Round 2, Agent 1/3 ---
Agent 1 receiving other agents' responses...
[ModelCache] Using cached model: Qwen/Qwen3-14B

Adding requests:   0%|          | 0/1 [00:00<?, ?it/s][AAdding requests: 100%|██████████| 1/1 [00:00<00:00, 704.45it/s]

Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts: 100%|██████████| 1/1 [00:11<00:00, 11.67s/it, est. speed input: 39.66 toks/s, output: 24.93 toks/s][A
Processed prompts: 100%|██████████| 1/1 [00:11<00:00, 11.67s/it, est. speed input: 39.66 toks/s, output: 24.93 toks/s][AProcessed prompts: 100%|██████████| 1/1 [00:11<00:00, 11.67s/it, est. speed input: 39.66 toks/s, output: 24.93 toks/s]
Agent 1 response: The calculation adheres strictly to standard arithmetic rules, prioritizing multiplication before ad...

--- Problem 2/2, Round 2, Agent 2/3 ---
Agent 2 receiving other agents' responses...
[ModelCache] Using cached model: Qwen/Qwen3-14B

Adding requests:   0%|          | 0/1 [00:00<?, ?it/s][AAdding requests: 100%|██████████| 1/1 [00:00<00:00, 728.56it/s]

Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts: 100%|██████████| 1/1 [00:13<00:00, 13.01s/it, est. speed input: 35.88 toks/s, output: 24.97 toks/s][A
Processed prompts: 100%|██████████| 1/1 [00:13<00:00, 13.01s/it, est. speed input: 35.88 toks/s, output: 24.97 toks/s][AProcessed prompts: 100%|██████████| 1/1 [00:13<00:00, 13.01s/it, est. speed input: 35.88 toks/s, output: 24.97 toks/s]
Agent 2 response: The echo of a bell fades.  
A butterfly dissolves into the sky.  
The answer is -223....

--- Problem 2/2, Round 2, Agent 3/3 ---
Agent 3 receiving other agents' responses...
[ModelCache] Using cached model: Qwen/Qwen3-14B

Adding requests:   0%|          | 0/1 [00:00<?, ?it/s][AAdding requests: 100%|██████████| 1/1 [00:00<00:00, 780.34it/s]

Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts: 100%|██████████| 1/1 [00:20<00:00, 20.28s/it, est. speed input: 22.88 toks/s, output: 25.15 toks/s][A
Processed prompts: 100%|██████████| 1/1 [00:20<00:00, 20.28s/it, est. speed input: 22.88 toks/s, output: 25.15 toks/s][AProcessed prompts: 100%|██████████| 1/1 [00:20<00:00, 20.28s/it, est. speed input: 22.88 toks/s, output: 25.15 toks/s]
Agent 3 response: The calculation follows the order of operations:  
1. Multiplication: $20 \times 6 = 120$, $18 \time...

--- Problem 2/2, Round 3, Agent 1/3 ---
Agent 1 receiving other agents' responses...
[ModelCache] Using cached model: Qwen/Qwen3-14B

Adding requests:   0%|          | 0/1 [00:00<?, ?it/s][AAdding requests: 100%|██████████| 1/1 [00:00<00:00, 542.88it/s]

Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts: 100%|██████████| 1/1 [00:15<00:00, 15.03s/it, est. speed input: 47.79 toks/s, output: 25.09 toks/s][A
Processed prompts: 100%|██████████| 1/1 [00:15<00:00, 15.03s/it, est. speed input: 47.79 toks/s, output: 25.09 toks/s][AProcessed prompts: 100%|██████████| 1/1 [00:15<00:00, 15.03s/it, est. speed input: 47.79 toks/s, output: 25.09 toks/s]
Agent 1 response: The calculation remains consistent with established arithmetic principles and prior agent confirmati...

--- Problem 2/2, Round 3, Agent 2/3 ---
Agent 2 receiving other agents' responses...
[ModelCache] Using cached model: Qwen/Qwen3-14B

Adding requests:   0%|          | 0/1 [00:00<?, ?it/s][AAdding requests: 100%|██████████| 1/1 [00:00<00:00, 561.26it/s]

Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.99s/it, est. speed input: 90.42 toks/s, output: 24.92 toks/s][A
Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.99s/it, est. speed input: 90.42 toks/s, output: 24.92 toks/s][AProcessed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.99s/it, est. speed input: 90.42 toks/s, output: 24.92 toks/s]
Agent 2 response: The river flows without memory.  
A shadow merges with the earth.  
The answer is -223....

--- Problem 2/2, Round 3, Agent 3/3 ---
Agent 3 receiving other agents' responses...
[ModelCache] Using cached model: Qwen/Qwen3-14B

Adding requests:   0%|          | 0/1 [00:00<?, ?it/s][AAdding requests: 100%|██████████| 1/1 [00:00<00:00, 551.08it/s]

Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts: 100%|██████████| 1/1 [00:11<00:00, 11.39s/it, est. speed input: 63.11 toks/s, output: 25.02 toks/s][A
Processed prompts: 100%|██████████| 1/1 [00:11<00:00, 11.39s/it, est. speed input: 63.11 toks/s, output: 25.02 toks/s][AProcessed prompts: 100%|██████████| 1/1 [00:11<00:00, 11.39s/it, est. speed input: 63.11 toks/s, output: 25.02 toks/s]
100%|██████████| 2/2 [18:17<00:00, 479.52s/it]100%|██████████| 2/2 [18:17<00:00, 548.90s/it]
[rank0]:[W1204 13:29:58.452625429 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Agent 3 response: The calculation follows standard arithmetic rules, confirming the result remains **-223**. The answe...
performance: 1.0 0.0
============================================================
Results saved to: /home/ch269957/projects/slm_multiagent_debate/experiments/linux_single/results/math/math_Qwen3-14B_persona_enigma+zen+deep-sea_agents3_rounds3.p
Final performance: 1.000 ± 0.000
============================================================
[ModelCache] Shut down vLLM model: vllm:Qwen/Qwen3-14B
[ModelCache] All models shut down
