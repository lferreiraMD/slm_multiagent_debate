==============================================
Job Array Task: 12
Job ID: 12
Model: vllm-llama32-3b
Agents: 7
Rounds: 3
Task: biography
Num People: 20
==============================================
Auto-enabled temperature diversity (no model/persona diversity detected)
Using 7 different temperatures: ['0.50', '0.58', '0.67', '0.75', '0.83', '0.92', '1.00']
============================================================
Biography Task - Multiagent Debate
============================================================
Model: meta-llama/Llama-3.2-3B-Instruct
Temperature diversity mode:
  Agent 1: temp=0.5
  Agent 2: temp=0.5833316666666667
  Agent 3: temp=0.6666633333333334
  Agent 4: temp=0.749995
  Agent 5: temp=0.8333266666666668
  Agent 6: temp=0.9166583333333334
  Agent 7: temp=0.99999
Agents: 7
Rounds: 3
People: 20
Dataset: /home/ch269957/projects/slm_multiagent_debate/data/biography/article.json
Generation params: {'temperature': 1.0, 'max_tokens': None, 'top_p': 1.0, 'n': 1}
============================================================

--- Problem 1/20, Round 1, Agent 1/7, Person: Jill Zimmerman ---
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 14:16:13 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 14:16:14 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 14:16:14 [model.py:1745] Using max model len 131072
INFO 12-05 14:16:14 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-05 14:16:15 [system_utils.py:103] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: CUDA is initialized
[1;36m(EngineCore_DP0 pid=4047835)[0;0m INFO 12-05 14:16:22 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4047835)[0;0m INFO 12-05 14:16:24 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:45337 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4047835)[0;0m INFO 12-05 14:16:24 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4047835)[0;0m INFO 12-05 14:16:25 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4047835)[0;0m INFO 12-05 14:16:26 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4047835)[0;0m INFO 12-05 14:16:26 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4047835)[0;0m INFO 12-05 14:16:27 [default_loader.py:314] Loading weights took 1.04 seconds
[1;36m(EngineCore_DP0 pid=4047835)[0;0m INFO 12-05 14:16:28 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 2.066374 seconds
[1;36m(EngineCore_DP0 pid=4047835)[0;0m INFO 12-05 14:16:35 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4047835)[0;0m INFO 12-05 14:16:35 [backends.py:647] Dynamo bytecode transform time: 7.07 s
[1;36m(EngineCore_DP0 pid=4047835)[0;0m ERROR 12-05 14:16:35 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4047835)[0;0m ERROR 12-05 14:16:35 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4047835)[0;0m ERROR 12-05 14:16:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4047835)[0;0m ERROR 12-05 14:16:35 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4047835)[0;0m ERROR 12-05 14:16:35 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4047835)[0;0m ERROR 12-05 14:16:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4047835)[0;0m ERROR 12-05 14:16:35 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4047835)[0;0m ERROR 12-05 14:16:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4047835)[0;0m ERROR 12-05 14:16:35 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4047835)[0;0m ERROR 12-05 14:16:35 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4047835)[0;0m ERROR 12-05 14:16:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4047835)[0;0m ERROR 12-05 14:16:35 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4047835)[0;0m ERROR 12-05 14:16:35 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4047835)[0;0m ERROR 12-05 14:16:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4047835)[0;0m ERROR 12-05 14:16:35 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4047835)[0;0m ERROR 12-05 14:16:35 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4047835)[0;0m ERROR 12-05 14:16:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4047835)[0;0m ERROR 12-05 14:16:35 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4047835)[0;0m ERROR 12-05 14:16:35 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4047835)[0;0m ERROR 12-05 14:16:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4047835)[0;0m ERROR 12-05 14:16:35 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4047835)[0;0m ERROR 12-05 14:16:35 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4047835)[0;0m ERROR 12-05 14:16:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4047835)[0;0m ERROR 12-05 14:16:35 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4047835)[0;0m ERROR 12-05 14:16:35 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4047835)[0;0m ERROR 12-05 14:16:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4047835)[0;0m ERROR 12-05 14:16:35 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4047835)[0;0m ERROR 12-05 14:16:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4047835)[0;0m ERROR 12-05 14:16:35 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4047835)[0;0m ERROR 12-05 14:16:35 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4047835)[0;0m ERROR 12-05 14:16:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4047835)[0;0m ERROR 12-05 14:16:35 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4047835)[0;0m ERROR 12-05 14:16:35 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4047835)[0;0m ERROR 12-05 14:16:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4047835)[0;0m ERROR 12-05 14:16:35 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4047835)[0;0m ERROR 12-05 14:16:35 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4047835)[0;0m ERROR 12-05 14:16:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4047835)[0;0m ERROR 12-05 14:16:35 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4047835)[0;0m ERROR 12-05 14:16:35 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4047835)[0;0m ERROR 12-05 14:16:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4047835)[0;0m ERROR 12-05 14:16:35 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4047835)[0;0m ERROR 12-05 14:16:35 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4047835)[0;0m ERROR 12-05 14:16:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4047835)[0;0m ERROR 12-05 14:16:35 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4047835)[0;0m ERROR 12-05 14:16:35 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4047835)[0;0m ERROR 12-05 14:16:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4047835)[0;0m ERROR 12-05 14:16:35 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4047835)[0;0m ERROR 12-05 14:16:35 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4047835)[0;0m ERROR 12-05 14:16:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4047835)[0;0m ERROR 12-05 14:16:35 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4047835)[0;0m ERROR 12-05 14:16:35 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4047835)[0;0m ERROR 12-05 14:16:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4047835)[0;0m ERROR 12-05 14:16:35 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4047835)[0;0m ERROR 12-05 14:16:35 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4047835)[0;0m ERROR 12-05 14:16:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4047835)[0;0m ERROR 12-05 14:16:35 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4047835)[0;0m ERROR 12-05 14:16:35 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4047835)[0;0m ERROR 12-05 14:16:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4047835)[0;0m ERROR 12-05 14:16:35 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4047835)[0;0m ERROR 12-05 14:16:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4047835)[0;0m ERROR 12-05 14:16:35 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4047835)[0;0m ERROR 12-05 14:16:35 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4047835)[0;0m ERROR 12-05 14:16:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4047835)[0;0m ERROR 12-05 14:16:35 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4047835)[0;0m ERROR 12-05 14:16:35 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4047835)[0;0m ERROR 12-05 14:16:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4047835)[0;0m ERROR 12-05 14:16:35 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4047835)[0;0m ERROR 12-05 14:16:35 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4047835)[0;0m ERROR 12-05 14:16:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4047835)[0;0m ERROR 12-05 14:16:35 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4047835)[0;0m ERROR 12-05 14:16:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4047835)[0;0m ERROR 12-05 14:16:35 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4047835)[0;0m ERROR 12-05 14:16:35 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4047835)[0;0m ERROR 12-05 14:16:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4047835)[0;0m ERROR 12-05 14:16:35 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4047835)[0;0m ERROR 12-05 14:16:35 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4047835)[0;0m ERROR 12-05 14:16:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4047835)[0;0m ERROR 12-05 14:16:35 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4047835)[0;0m ERROR 12-05 14:16:35 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4047835)[0;0m ERROR 12-05 14:16:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4047835)[0;0m ERROR 12-05 14:16:35 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4047835)[0;0m ERROR 12-05 14:16:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4047835)[0;0m ERROR 12-05 14:16:35 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4047835)[0;0m ERROR 12-05 14:16:35 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4047835)[0;0m ERROR 12-05 14:16:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4047835)[0;0m ERROR 12-05 14:16:35 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4047835)[0;0m ERROR 12-05 14:16:35 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4047835)[0;0m ERROR 12-05 14:16:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4047835)[0;0m ERROR 12-05 14:16:35 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4047835)[0;0m ERROR 12-05 14:16:35 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4047835)[0;0m ERROR 12-05 14:16:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4047835)[0;0m ERROR 12-05 14:16:35 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4047835)[0;0m ERROR 12-05 14:16:35 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4047835)[0;0m ERROR 12-05 14:16:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4047835)[0;0m ERROR 12-05 14:16:35 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4047835)[0;0m ERROR 12-05 14:16:35 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7fe5ae1971d0>' raised:
[1;36m(EngineCore_DP0 pid=4047835)[0;0m ERROR 12-05 14:16:35 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4047835)[0;0m ERROR 12-05 14:16:35 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4047835)[0;0m ERROR 12-05 14:16:35 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4047835)[0;0m ERROR 12-05 14:16:35 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 14:17:00 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 14:17:00 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 14:17:00 [model.py:1745] Using max model len 131072
INFO 12-05 14:17:00 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4048165)[0;0m INFO 12-05 14:17:08 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4048165)[0;0m INFO 12-05 14:17:09 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:40771 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4048165)[0;0m INFO 12-05 14:17:09 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4048165)[0;0m INFO 12-05 14:17:09 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4048165)[0;0m INFO 12-05 14:17:10 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4048165)[0;0m INFO 12-05 14:17:10 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4048165)[0;0m INFO 12-05 14:17:11 [default_loader.py:314] Loading weights took 0.88 seconds
[1;36m(EngineCore_DP0 pid=4048165)[0;0m INFO 12-05 14:17:11 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.400157 seconds
[1;36m(EngineCore_DP0 pid=4048165)[0;0m INFO 12-05 14:17:15 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4048165)[0;0m INFO 12-05 14:17:15 [backends.py:647] Dynamo bytecode transform time: 3.82 s
[1;36m(EngineCore_DP0 pid=4048165)[0;0m ERROR 12-05 14:17:16 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4048165)[0;0m ERROR 12-05 14:17:16 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4048165)[0;0m ERROR 12-05 14:17:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4048165)[0;0m ERROR 12-05 14:17:16 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4048165)[0;0m ERROR 12-05 14:17:16 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048165)[0;0m ERROR 12-05 14:17:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4048165)[0;0m ERROR 12-05 14:17:16 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4048165)[0;0m ERROR 12-05 14:17:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4048165)[0;0m ERROR 12-05 14:17:16 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4048165)[0;0m ERROR 12-05 14:17:16 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048165)[0;0m ERROR 12-05 14:17:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4048165)[0;0m ERROR 12-05 14:17:16 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4048165)[0;0m ERROR 12-05 14:17:16 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048165)[0;0m ERROR 12-05 14:17:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4048165)[0;0m ERROR 12-05 14:17:16 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4048165)[0;0m ERROR 12-05 14:17:16 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048165)[0;0m ERROR 12-05 14:17:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4048165)[0;0m ERROR 12-05 14:17:16 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4048165)[0;0m ERROR 12-05 14:17:16 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048165)[0;0m ERROR 12-05 14:17:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4048165)[0;0m ERROR 12-05 14:17:16 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4048165)[0;0m ERROR 12-05 14:17:16 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048165)[0;0m ERROR 12-05 14:17:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4048165)[0;0m ERROR 12-05 14:17:16 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4048165)[0;0m ERROR 12-05 14:17:16 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048165)[0;0m ERROR 12-05 14:17:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4048165)[0;0m ERROR 12-05 14:17:16 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4048165)[0;0m ERROR 12-05 14:17:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4048165)[0;0m ERROR 12-05 14:17:16 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4048165)[0;0m ERROR 12-05 14:17:16 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048165)[0;0m ERROR 12-05 14:17:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4048165)[0;0m ERROR 12-05 14:17:16 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4048165)[0;0m ERROR 12-05 14:17:16 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048165)[0;0m ERROR 12-05 14:17:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4048165)[0;0m ERROR 12-05 14:17:16 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4048165)[0;0m ERROR 12-05 14:17:16 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048165)[0;0m ERROR 12-05 14:17:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4048165)[0;0m ERROR 12-05 14:17:16 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4048165)[0;0m ERROR 12-05 14:17:16 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048165)[0;0m ERROR 12-05 14:17:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4048165)[0;0m ERROR 12-05 14:17:16 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4048165)[0;0m ERROR 12-05 14:17:16 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048165)[0;0m ERROR 12-05 14:17:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4048165)[0;0m ERROR 12-05 14:17:16 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4048165)[0;0m ERROR 12-05 14:17:16 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048165)[0;0m ERROR 12-05 14:17:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4048165)[0;0m ERROR 12-05 14:17:16 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4048165)[0;0m ERROR 12-05 14:17:16 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048165)[0;0m ERROR 12-05 14:17:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4048165)[0;0m ERROR 12-05 14:17:16 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4048165)[0;0m ERROR 12-05 14:17:16 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048165)[0;0m ERROR 12-05 14:17:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4048165)[0;0m ERROR 12-05 14:17:16 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4048165)[0;0m ERROR 12-05 14:17:16 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048165)[0;0m ERROR 12-05 14:17:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4048165)[0;0m ERROR 12-05 14:17:16 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4048165)[0;0m ERROR 12-05 14:17:16 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048165)[0;0m ERROR 12-05 14:17:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4048165)[0;0m ERROR 12-05 14:17:16 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4048165)[0;0m ERROR 12-05 14:17:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4048165)[0;0m ERROR 12-05 14:17:16 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4048165)[0;0m ERROR 12-05 14:17:16 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048165)[0;0m ERROR 12-05 14:17:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4048165)[0;0m ERROR 12-05 14:17:16 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4048165)[0;0m ERROR 12-05 14:17:16 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048165)[0;0m ERROR 12-05 14:17:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4048165)[0;0m ERROR 12-05 14:17:16 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4048165)[0;0m ERROR 12-05 14:17:16 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048165)[0;0m ERROR 12-05 14:17:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4048165)[0;0m ERROR 12-05 14:17:16 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4048165)[0;0m ERROR 12-05 14:17:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4048165)[0;0m ERROR 12-05 14:17:16 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4048165)[0;0m ERROR 12-05 14:17:16 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048165)[0;0m ERROR 12-05 14:17:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4048165)[0;0m ERROR 12-05 14:17:16 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4048165)[0;0m ERROR 12-05 14:17:16 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048165)[0;0m ERROR 12-05 14:17:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4048165)[0;0m ERROR 12-05 14:17:16 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4048165)[0;0m ERROR 12-05 14:17:16 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048165)[0;0m ERROR 12-05 14:17:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4048165)[0;0m ERROR 12-05 14:17:16 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4048165)[0;0m ERROR 12-05 14:17:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4048165)[0;0m ERROR 12-05 14:17:16 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4048165)[0;0m ERROR 12-05 14:17:16 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048165)[0;0m ERROR 12-05 14:17:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4048165)[0;0m ERROR 12-05 14:17:16 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4048165)[0;0m ERROR 12-05 14:17:16 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048165)[0;0m ERROR 12-05 14:17:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4048165)[0;0m ERROR 12-05 14:17:16 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4048165)[0;0m ERROR 12-05 14:17:16 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048165)[0;0m ERROR 12-05 14:17:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4048165)[0;0m ERROR 12-05 14:17:16 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4048165)[0;0m ERROR 12-05 14:17:16 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048165)[0;0m ERROR 12-05 14:17:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4048165)[0;0m ERROR 12-05 14:17:16 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4048165)[0;0m ERROR 12-05 14:17:16 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7f1d8014cdd0>' raised:
[1;36m(EngineCore_DP0 pid=4048165)[0;0m ERROR 12-05 14:17:16 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4048165)[0;0m ERROR 12-05 14:17:16 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4048165)[0;0m ERROR 12-05 14:17:16 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4048165)[0;0m ERROR 12-05 14:17:16 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 14:17:37 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 14:17:37 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 14:17:37 [model.py:1745] Using max model len 131072
INFO 12-05 14:17:37 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4048247)[0;0m INFO 12-05 14:17:45 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4048247)[0;0m INFO 12-05 14:17:46 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:50663 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4048247)[0;0m INFO 12-05 14:17:46 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4048247)[0;0m INFO 12-05 14:17:46 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4048247)[0;0m INFO 12-05 14:17:47 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4048247)[0;0m INFO 12-05 14:17:47 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4048247)[0;0m INFO 12-05 14:17:48 [default_loader.py:314] Loading weights took 0.87 seconds
[1;36m(EngineCore_DP0 pid=4048247)[0;0m INFO 12-05 14:17:48 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.400923 seconds
[1;36m(EngineCore_DP0 pid=4048247)[0;0m INFO 12-05 14:17:52 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4048247)[0;0m INFO 12-05 14:17:52 [backends.py:647] Dynamo bytecode transform time: 3.85 s
[1;36m(EngineCore_DP0 pid=4048247)[0;0m ERROR 12-05 14:17:53 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4048247)[0;0m ERROR 12-05 14:17:53 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4048247)[0;0m ERROR 12-05 14:17:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4048247)[0;0m ERROR 12-05 14:17:53 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4048247)[0;0m ERROR 12-05 14:17:53 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048247)[0;0m ERROR 12-05 14:17:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4048247)[0;0m ERROR 12-05 14:17:53 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4048247)[0;0m ERROR 12-05 14:17:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4048247)[0;0m ERROR 12-05 14:17:53 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4048247)[0;0m ERROR 12-05 14:17:53 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048247)[0;0m ERROR 12-05 14:17:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4048247)[0;0m ERROR 12-05 14:17:53 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4048247)[0;0m ERROR 12-05 14:17:53 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048247)[0;0m ERROR 12-05 14:17:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4048247)[0;0m ERROR 12-05 14:17:53 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4048247)[0;0m ERROR 12-05 14:17:53 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048247)[0;0m ERROR 12-05 14:17:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4048247)[0;0m ERROR 12-05 14:17:53 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4048247)[0;0m ERROR 12-05 14:17:53 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048247)[0;0m ERROR 12-05 14:17:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4048247)[0;0m ERROR 12-05 14:17:53 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4048247)[0;0m ERROR 12-05 14:17:53 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048247)[0;0m ERROR 12-05 14:17:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4048247)[0;0m ERROR 12-05 14:17:53 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4048247)[0;0m ERROR 12-05 14:17:53 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048247)[0;0m ERROR 12-05 14:17:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4048247)[0;0m ERROR 12-05 14:17:53 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4048247)[0;0m ERROR 12-05 14:17:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4048247)[0;0m ERROR 12-05 14:17:53 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4048247)[0;0m ERROR 12-05 14:17:53 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048247)[0;0m ERROR 12-05 14:17:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4048247)[0;0m ERROR 12-05 14:17:53 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4048247)[0;0m ERROR 12-05 14:17:53 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048247)[0;0m ERROR 12-05 14:17:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4048247)[0;0m ERROR 12-05 14:17:53 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4048247)[0;0m ERROR 12-05 14:17:53 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048247)[0;0m ERROR 12-05 14:17:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4048247)[0;0m ERROR 12-05 14:17:53 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4048247)[0;0m ERROR 12-05 14:17:53 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048247)[0;0m ERROR 12-05 14:17:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4048247)[0;0m ERROR 12-05 14:17:53 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4048247)[0;0m ERROR 12-05 14:17:53 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048247)[0;0m ERROR 12-05 14:17:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4048247)[0;0m ERROR 12-05 14:17:53 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4048247)[0;0m ERROR 12-05 14:17:53 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048247)[0;0m ERROR 12-05 14:17:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4048247)[0;0m ERROR 12-05 14:17:53 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4048247)[0;0m ERROR 12-05 14:17:53 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048247)[0;0m ERROR 12-05 14:17:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4048247)[0;0m ERROR 12-05 14:17:53 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4048247)[0;0m ERROR 12-05 14:17:53 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048247)[0;0m ERROR 12-05 14:17:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4048247)[0;0m ERROR 12-05 14:17:53 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4048247)[0;0m ERROR 12-05 14:17:53 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048247)[0;0m ERROR 12-05 14:17:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4048247)[0;0m ERROR 12-05 14:17:53 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4048247)[0;0m ERROR 12-05 14:17:53 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048247)[0;0m ERROR 12-05 14:17:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4048247)[0;0m ERROR 12-05 14:17:53 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4048247)[0;0m ERROR 12-05 14:17:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4048247)[0;0m ERROR 12-05 14:17:53 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4048247)[0;0m ERROR 12-05 14:17:53 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048247)[0;0m ERROR 12-05 14:17:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4048247)[0;0m ERROR 12-05 14:17:53 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4048247)[0;0m ERROR 12-05 14:17:53 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048247)[0;0m ERROR 12-05 14:17:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4048247)[0;0m ERROR 12-05 14:17:53 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4048247)[0;0m ERROR 12-05 14:17:53 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048247)[0;0m ERROR 12-05 14:17:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4048247)[0;0m ERROR 12-05 14:17:53 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4048247)[0;0m ERROR 12-05 14:17:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4048247)[0;0m ERROR 12-05 14:17:53 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4048247)[0;0m ERROR 12-05 14:17:53 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048247)[0;0m ERROR 12-05 14:17:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4048247)[0;0m ERROR 12-05 14:17:53 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4048247)[0;0m ERROR 12-05 14:17:53 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048247)[0;0m ERROR 12-05 14:17:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4048247)[0;0m ERROR 12-05 14:17:53 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4048247)[0;0m ERROR 12-05 14:17:53 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048247)[0;0m ERROR 12-05 14:17:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4048247)[0;0m ERROR 12-05 14:17:53 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4048247)[0;0m ERROR 12-05 14:17:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4048247)[0;0m ERROR 12-05 14:17:53 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4048247)[0;0m ERROR 12-05 14:17:53 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048247)[0;0m ERROR 12-05 14:17:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4048247)[0;0m ERROR 12-05 14:17:53 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4048247)[0;0m ERROR 12-05 14:17:53 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048247)[0;0m ERROR 12-05 14:17:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4048247)[0;0m ERROR 12-05 14:17:53 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4048247)[0;0m ERROR 12-05 14:17:53 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048247)[0;0m ERROR 12-05 14:17:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4048247)[0;0m ERROR 12-05 14:17:53 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4048247)[0;0m ERROR 12-05 14:17:53 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048247)[0;0m ERROR 12-05 14:17:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4048247)[0;0m ERROR 12-05 14:17:53 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4048247)[0;0m ERROR 12-05 14:17:53 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7f947810c230>' raised:
[1;36m(EngineCore_DP0 pid=4048247)[0;0m ERROR 12-05 14:17:53 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4048247)[0;0m ERROR 12-05 14:17:53 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4048247)[0;0m ERROR 12-05 14:17:53 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4048247)[0;0m ERROR 12-05 14:17:53 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 14:18:14 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 14:18:14 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 14:18:14 [model.py:1745] Using max model len 131072
INFO 12-05 14:18:14 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4048332)[0;0m INFO 12-05 14:18:22 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4048332)[0;0m INFO 12-05 14:18:23 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:50621 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4048332)[0;0m INFO 12-05 14:18:23 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4048332)[0;0m INFO 12-05 14:18:23 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4048332)[0;0m INFO 12-05 14:18:24 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4048332)[0;0m INFO 12-05 14:18:24 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4048332)[0;0m INFO 12-05 14:18:25 [default_loader.py:314] Loading weights took 0.87 seconds
[1;36m(EngineCore_DP0 pid=4048332)[0;0m INFO 12-05 14:18:26 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 2.060706 seconds
[1;36m(EngineCore_DP0 pid=4048332)[0;0m INFO 12-05 14:18:30 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4048332)[0;0m INFO 12-05 14:18:30 [backends.py:647] Dynamo bytecode transform time: 3.77 s
[1;36m(EngineCore_DP0 pid=4048332)[0;0m ERROR 12-05 14:18:30 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4048332)[0;0m ERROR 12-05 14:18:30 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4048332)[0;0m ERROR 12-05 14:18:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4048332)[0;0m ERROR 12-05 14:18:30 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4048332)[0;0m ERROR 12-05 14:18:30 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048332)[0;0m ERROR 12-05 14:18:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4048332)[0;0m ERROR 12-05 14:18:30 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4048332)[0;0m ERROR 12-05 14:18:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4048332)[0;0m ERROR 12-05 14:18:30 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4048332)[0;0m ERROR 12-05 14:18:30 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048332)[0;0m ERROR 12-05 14:18:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4048332)[0;0m ERROR 12-05 14:18:30 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4048332)[0;0m ERROR 12-05 14:18:30 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048332)[0;0m ERROR 12-05 14:18:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4048332)[0;0m ERROR 12-05 14:18:30 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4048332)[0;0m ERROR 12-05 14:18:30 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048332)[0;0m ERROR 12-05 14:18:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4048332)[0;0m ERROR 12-05 14:18:30 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4048332)[0;0m ERROR 12-05 14:18:30 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048332)[0;0m ERROR 12-05 14:18:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4048332)[0;0m ERROR 12-05 14:18:30 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4048332)[0;0m ERROR 12-05 14:18:30 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048332)[0;0m ERROR 12-05 14:18:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4048332)[0;0m ERROR 12-05 14:18:30 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4048332)[0;0m ERROR 12-05 14:18:30 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048332)[0;0m ERROR 12-05 14:18:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4048332)[0;0m ERROR 12-05 14:18:30 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4048332)[0;0m ERROR 12-05 14:18:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4048332)[0;0m ERROR 12-05 14:18:30 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4048332)[0;0m ERROR 12-05 14:18:30 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048332)[0;0m ERROR 12-05 14:18:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4048332)[0;0m ERROR 12-05 14:18:30 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4048332)[0;0m ERROR 12-05 14:18:30 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048332)[0;0m ERROR 12-05 14:18:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4048332)[0;0m ERROR 12-05 14:18:30 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4048332)[0;0m ERROR 12-05 14:18:30 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048332)[0;0m ERROR 12-05 14:18:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4048332)[0;0m ERROR 12-05 14:18:30 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4048332)[0;0m ERROR 12-05 14:18:30 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048332)[0;0m ERROR 12-05 14:18:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4048332)[0;0m ERROR 12-05 14:18:30 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4048332)[0;0m ERROR 12-05 14:18:30 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048332)[0;0m ERROR 12-05 14:18:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4048332)[0;0m ERROR 12-05 14:18:30 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4048332)[0;0m ERROR 12-05 14:18:30 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048332)[0;0m ERROR 12-05 14:18:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4048332)[0;0m ERROR 12-05 14:18:30 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4048332)[0;0m ERROR 12-05 14:18:30 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048332)[0;0m ERROR 12-05 14:18:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4048332)[0;0m ERROR 12-05 14:18:30 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4048332)[0;0m ERROR 12-05 14:18:30 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048332)[0;0m ERROR 12-05 14:18:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4048332)[0;0m ERROR 12-05 14:18:30 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4048332)[0;0m ERROR 12-05 14:18:30 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048332)[0;0m ERROR 12-05 14:18:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4048332)[0;0m ERROR 12-05 14:18:30 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4048332)[0;0m ERROR 12-05 14:18:30 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048332)[0;0m ERROR 12-05 14:18:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4048332)[0;0m ERROR 12-05 14:18:30 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4048332)[0;0m ERROR 12-05 14:18:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4048332)[0;0m ERROR 12-05 14:18:30 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4048332)[0;0m ERROR 12-05 14:18:30 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048332)[0;0m ERROR 12-05 14:18:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4048332)[0;0m ERROR 12-05 14:18:30 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4048332)[0;0m ERROR 12-05 14:18:30 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048332)[0;0m ERROR 12-05 14:18:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4048332)[0;0m ERROR 12-05 14:18:30 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4048332)[0;0m ERROR 12-05 14:18:30 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048332)[0;0m ERROR 12-05 14:18:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4048332)[0;0m ERROR 12-05 14:18:30 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4048332)[0;0m ERROR 12-05 14:18:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4048332)[0;0m ERROR 12-05 14:18:30 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4048332)[0;0m ERROR 12-05 14:18:30 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048332)[0;0m ERROR 12-05 14:18:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4048332)[0;0m ERROR 12-05 14:18:30 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4048332)[0;0m ERROR 12-05 14:18:30 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048332)[0;0m ERROR 12-05 14:18:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4048332)[0;0m ERROR 12-05 14:18:30 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4048332)[0;0m ERROR 12-05 14:18:30 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048332)[0;0m ERROR 12-05 14:18:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4048332)[0;0m ERROR 12-05 14:18:30 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4048332)[0;0m ERROR 12-05 14:18:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4048332)[0;0m ERROR 12-05 14:18:30 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4048332)[0;0m ERROR 12-05 14:18:30 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048332)[0;0m ERROR 12-05 14:18:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4048332)[0;0m ERROR 12-05 14:18:30 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4048332)[0;0m ERROR 12-05 14:18:30 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048332)[0;0m ERROR 12-05 14:18:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4048332)[0;0m ERROR 12-05 14:18:30 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4048332)[0;0m ERROR 12-05 14:18:30 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048332)[0;0m ERROR 12-05 14:18:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4048332)[0;0m ERROR 12-05 14:18:30 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4048332)[0;0m ERROR 12-05 14:18:30 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048332)[0;0m ERROR 12-05 14:18:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4048332)[0;0m ERROR 12-05 14:18:30 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4048332)[0;0m ERROR 12-05 14:18:30 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7fb648258f50>' raised:
[1;36m(EngineCore_DP0 pid=4048332)[0;0m ERROR 12-05 14:18:30 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4048332)[0;0m ERROR 12-05 14:18:30 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4048332)[0;0m ERROR 12-05 14:18:30 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4048332)[0;0m ERROR 12-05 14:18:30 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 14:18:52 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 14:18:52 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 14:18:52 [model.py:1745] Using max model len 131072
INFO 12-05 14:18:52 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4048420)[0;0m INFO 12-05 14:18:59 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4048420)[0;0m INFO 12-05 14:19:00 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:44339 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4048420)[0;0m INFO 12-05 14:19:00 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4048420)[0;0m INFO 12-05 14:19:00 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4048420)[0;0m INFO 12-05 14:19:01 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4048420)[0;0m INFO 12-05 14:19:01 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4048420)[0;0m INFO 12-05 14:19:02 [default_loader.py:314] Loading weights took 0.88 seconds
[1;36m(EngineCore_DP0 pid=4048420)[0;0m INFO 12-05 14:19:02 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.417335 seconds
[1;36m(EngineCore_DP0 pid=4048420)[0;0m INFO 12-05 14:19:06 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4048420)[0;0m INFO 12-05 14:19:06 [backends.py:647] Dynamo bytecode transform time: 3.82 s
[1;36m(EngineCore_DP0 pid=4048420)[0;0m ERROR 12-05 14:19:07 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4048420)[0;0m ERROR 12-05 14:19:07 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4048420)[0;0m ERROR 12-05 14:19:07 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4048420)[0;0m ERROR 12-05 14:19:07 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4048420)[0;0m ERROR 12-05 14:19:07 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048420)[0;0m ERROR 12-05 14:19:07 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4048420)[0;0m ERROR 12-05 14:19:07 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4048420)[0;0m ERROR 12-05 14:19:07 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4048420)[0;0m ERROR 12-05 14:19:07 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4048420)[0;0m ERROR 12-05 14:19:07 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048420)[0;0m ERROR 12-05 14:19:07 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4048420)[0;0m ERROR 12-05 14:19:07 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4048420)[0;0m ERROR 12-05 14:19:07 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048420)[0;0m ERROR 12-05 14:19:07 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4048420)[0;0m ERROR 12-05 14:19:07 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4048420)[0;0m ERROR 12-05 14:19:07 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048420)[0;0m ERROR 12-05 14:19:07 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4048420)[0;0m ERROR 12-05 14:19:07 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4048420)[0;0m ERROR 12-05 14:19:07 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048420)[0;0m ERROR 12-05 14:19:07 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4048420)[0;0m ERROR 12-05 14:19:07 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4048420)[0;0m ERROR 12-05 14:19:07 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048420)[0;0m ERROR 12-05 14:19:07 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4048420)[0;0m ERROR 12-05 14:19:07 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4048420)[0;0m ERROR 12-05 14:19:07 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048420)[0;0m ERROR 12-05 14:19:07 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4048420)[0;0m ERROR 12-05 14:19:07 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4048420)[0;0m ERROR 12-05 14:19:07 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4048420)[0;0m ERROR 12-05 14:19:07 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4048420)[0;0m ERROR 12-05 14:19:07 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048420)[0;0m ERROR 12-05 14:19:07 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4048420)[0;0m ERROR 12-05 14:19:07 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4048420)[0;0m ERROR 12-05 14:19:07 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048420)[0;0m ERROR 12-05 14:19:07 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4048420)[0;0m ERROR 12-05 14:19:07 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4048420)[0;0m ERROR 12-05 14:19:07 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048420)[0;0m ERROR 12-05 14:19:07 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4048420)[0;0m ERROR 12-05 14:19:07 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4048420)[0;0m ERROR 12-05 14:19:07 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048420)[0;0m ERROR 12-05 14:19:07 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4048420)[0;0m ERROR 12-05 14:19:07 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4048420)[0;0m ERROR 12-05 14:19:07 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048420)[0;0m ERROR 12-05 14:19:07 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4048420)[0;0m ERROR 12-05 14:19:07 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4048420)[0;0m ERROR 12-05 14:19:07 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048420)[0;0m ERROR 12-05 14:19:07 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4048420)[0;0m ERROR 12-05 14:19:07 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4048420)[0;0m ERROR 12-05 14:19:07 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048420)[0;0m ERROR 12-05 14:19:07 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4048420)[0;0m ERROR 12-05 14:19:07 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4048420)[0;0m ERROR 12-05 14:19:07 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048420)[0;0m ERROR 12-05 14:19:07 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4048420)[0;0m ERROR 12-05 14:19:07 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4048420)[0;0m ERROR 12-05 14:19:07 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048420)[0;0m ERROR 12-05 14:19:07 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4048420)[0;0m ERROR 12-05 14:19:07 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4048420)[0;0m ERROR 12-05 14:19:07 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048420)[0;0m ERROR 12-05 14:19:07 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4048420)[0;0m ERROR 12-05 14:19:07 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4048420)[0;0m ERROR 12-05 14:19:07 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4048420)[0;0m ERROR 12-05 14:19:07 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4048420)[0;0m ERROR 12-05 14:19:07 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048420)[0;0m ERROR 12-05 14:19:07 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4048420)[0;0m ERROR 12-05 14:19:07 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4048420)[0;0m ERROR 12-05 14:19:07 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048420)[0;0m ERROR 12-05 14:19:07 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4048420)[0;0m ERROR 12-05 14:19:07 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4048420)[0;0m ERROR 12-05 14:19:07 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048420)[0;0m ERROR 12-05 14:19:07 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4048420)[0;0m ERROR 12-05 14:19:07 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4048420)[0;0m ERROR 12-05 14:19:07 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4048420)[0;0m ERROR 12-05 14:19:07 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4048420)[0;0m ERROR 12-05 14:19:07 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048420)[0;0m ERROR 12-05 14:19:07 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4048420)[0;0m ERROR 12-05 14:19:07 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4048420)[0;0m ERROR 12-05 14:19:07 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048420)[0;0m ERROR 12-05 14:19:07 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4048420)[0;0m ERROR 12-05 14:19:07 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4048420)[0;0m ERROR 12-05 14:19:07 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048420)[0;0m ERROR 12-05 14:19:07 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4048420)[0;0m ERROR 12-05 14:19:07 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4048420)[0;0m ERROR 12-05 14:19:07 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4048420)[0;0m ERROR 12-05 14:19:07 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4048420)[0;0m ERROR 12-05 14:19:07 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048420)[0;0m ERROR 12-05 14:19:07 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4048420)[0;0m ERROR 12-05 14:19:07 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4048420)[0;0m ERROR 12-05 14:19:07 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048420)[0;0m ERROR 12-05 14:19:07 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4048420)[0;0m ERROR 12-05 14:19:07 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4048420)[0;0m ERROR 12-05 14:19:07 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048420)[0;0m ERROR 12-05 14:19:07 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4048420)[0;0m ERROR 12-05 14:19:07 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4048420)[0;0m ERROR 12-05 14:19:07 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048420)[0;0m ERROR 12-05 14:19:07 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4048420)[0;0m ERROR 12-05 14:19:07 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4048420)[0;0m ERROR 12-05 14:19:07 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7f9200aafe60>' raised:
[1;36m(EngineCore_DP0 pid=4048420)[0;0m ERROR 12-05 14:19:07 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4048420)[0;0m ERROR 12-05 14:19:07 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4048420)[0;0m ERROR 12-05 14:19:07 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4048420)[0;0m ERROR 12-05 14:19:07 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 14:19:28 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 14:19:28 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 14:19:28 [model.py:1745] Using max model len 131072
INFO 12-05 14:19:28 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4048503)[0;0m INFO 12-05 14:19:36 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4048503)[0;0m INFO 12-05 14:19:37 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:52317 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4048503)[0;0m INFO 12-05 14:19:37 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4048503)[0;0m INFO 12-05 14:19:37 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4048503)[0;0m INFO 12-05 14:19:38 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4048503)[0;0m INFO 12-05 14:19:38 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4048503)[0;0m INFO 12-05 14:19:39 [default_loader.py:314] Loading weights took 0.88 seconds
[1;36m(EngineCore_DP0 pid=4048503)[0;0m INFO 12-05 14:19:39 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.401687 seconds
[1;36m(EngineCore_DP0 pid=4048503)[0;0m INFO 12-05 14:19:43 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4048503)[0;0m INFO 12-05 14:19:43 [backends.py:647] Dynamo bytecode transform time: 3.65 s
[1;36m(EngineCore_DP0 pid=4048503)[0;0m ERROR 12-05 14:19:43 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4048503)[0;0m ERROR 12-05 14:19:43 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4048503)[0;0m ERROR 12-05 14:19:43 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4048503)[0;0m ERROR 12-05 14:19:43 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4048503)[0;0m ERROR 12-05 14:19:43 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048503)[0;0m ERROR 12-05 14:19:43 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4048503)[0;0m ERROR 12-05 14:19:43 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4048503)[0;0m ERROR 12-05 14:19:43 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4048503)[0;0m ERROR 12-05 14:19:43 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4048503)[0;0m ERROR 12-05 14:19:43 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048503)[0;0m ERROR 12-05 14:19:43 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4048503)[0;0m ERROR 12-05 14:19:43 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4048503)[0;0m ERROR 12-05 14:19:43 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048503)[0;0m ERROR 12-05 14:19:43 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4048503)[0;0m ERROR 12-05 14:19:43 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4048503)[0;0m ERROR 12-05 14:19:43 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048503)[0;0m ERROR 12-05 14:19:43 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4048503)[0;0m ERROR 12-05 14:19:43 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4048503)[0;0m ERROR 12-05 14:19:43 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048503)[0;0m ERROR 12-05 14:19:43 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4048503)[0;0m ERROR 12-05 14:19:43 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4048503)[0;0m ERROR 12-05 14:19:43 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048503)[0;0m ERROR 12-05 14:19:43 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4048503)[0;0m ERROR 12-05 14:19:43 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4048503)[0;0m ERROR 12-05 14:19:43 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048503)[0;0m ERROR 12-05 14:19:43 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4048503)[0;0m ERROR 12-05 14:19:43 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4048503)[0;0m ERROR 12-05 14:19:43 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4048503)[0;0m ERROR 12-05 14:19:43 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4048503)[0;0m ERROR 12-05 14:19:43 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048503)[0;0m ERROR 12-05 14:19:43 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4048503)[0;0m ERROR 12-05 14:19:43 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4048503)[0;0m ERROR 12-05 14:19:43 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048503)[0;0m ERROR 12-05 14:19:43 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4048503)[0;0m ERROR 12-05 14:19:43 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4048503)[0;0m ERROR 12-05 14:19:43 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048503)[0;0m ERROR 12-05 14:19:43 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4048503)[0;0m ERROR 12-05 14:19:43 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4048503)[0;0m ERROR 12-05 14:19:43 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048503)[0;0m ERROR 12-05 14:19:43 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4048503)[0;0m ERROR 12-05 14:19:43 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4048503)[0;0m ERROR 12-05 14:19:43 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048503)[0;0m ERROR 12-05 14:19:43 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4048503)[0;0m ERROR 12-05 14:19:43 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4048503)[0;0m ERROR 12-05 14:19:43 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048503)[0;0m ERROR 12-05 14:19:43 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4048503)[0;0m ERROR 12-05 14:19:43 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4048503)[0;0m ERROR 12-05 14:19:43 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048503)[0;0m ERROR 12-05 14:19:43 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4048503)[0;0m ERROR 12-05 14:19:43 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4048503)[0;0m ERROR 12-05 14:19:43 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048503)[0;0m ERROR 12-05 14:19:43 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4048503)[0;0m ERROR 12-05 14:19:43 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4048503)[0;0m ERROR 12-05 14:19:43 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048503)[0;0m ERROR 12-05 14:19:43 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4048503)[0;0m ERROR 12-05 14:19:43 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4048503)[0;0m ERROR 12-05 14:19:43 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048503)[0;0m ERROR 12-05 14:19:43 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4048503)[0;0m ERROR 12-05 14:19:43 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4048503)[0;0m ERROR 12-05 14:19:43 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4048503)[0;0m ERROR 12-05 14:19:43 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4048503)[0;0m ERROR 12-05 14:19:43 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048503)[0;0m ERROR 12-05 14:19:43 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4048503)[0;0m ERROR 12-05 14:19:43 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4048503)[0;0m ERROR 12-05 14:19:43 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048503)[0;0m ERROR 12-05 14:19:43 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4048503)[0;0m ERROR 12-05 14:19:43 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4048503)[0;0m ERROR 12-05 14:19:43 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048503)[0;0m ERROR 12-05 14:19:43 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4048503)[0;0m ERROR 12-05 14:19:43 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4048503)[0;0m ERROR 12-05 14:19:43 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4048503)[0;0m ERROR 12-05 14:19:43 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4048503)[0;0m ERROR 12-05 14:19:43 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048503)[0;0m ERROR 12-05 14:19:43 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4048503)[0;0m ERROR 12-05 14:19:43 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4048503)[0;0m ERROR 12-05 14:19:43 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048503)[0;0m ERROR 12-05 14:19:43 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4048503)[0;0m ERROR 12-05 14:19:43 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4048503)[0;0m ERROR 12-05 14:19:43 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048503)[0;0m ERROR 12-05 14:19:43 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4048503)[0;0m ERROR 12-05 14:19:43 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4048503)[0;0m ERROR 12-05 14:19:43 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4048503)[0;0m ERROR 12-05 14:19:43 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4048503)[0;0m ERROR 12-05 14:19:43 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048503)[0;0m ERROR 12-05 14:19:43 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4048503)[0;0m ERROR 12-05 14:19:43 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4048503)[0;0m ERROR 12-05 14:19:43 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048503)[0;0m ERROR 12-05 14:19:43 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4048503)[0;0m ERROR 12-05 14:19:43 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4048503)[0;0m ERROR 12-05 14:19:43 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048503)[0;0m ERROR 12-05 14:19:43 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4048503)[0;0m ERROR 12-05 14:19:43 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4048503)[0;0m ERROR 12-05 14:19:43 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048503)[0;0m ERROR 12-05 14:19:43 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4048503)[0;0m ERROR 12-05 14:19:43 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4048503)[0;0m ERROR 12-05 14:19:43 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7f4c982d9370>' raised:
[1;36m(EngineCore_DP0 pid=4048503)[0;0m ERROR 12-05 14:19:43 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4048503)[0;0m ERROR 12-05 14:19:43 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4048503)[0;0m ERROR 12-05 14:19:43 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4048503)[0;0m ERROR 12-05 14:19:43 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 14:20:05 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 14:20:05 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 14:20:05 [model.py:1745] Using max model len 131072
INFO 12-05 14:20:05 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4048593)[0;0m INFO 12-05 14:20:12 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4048593)[0;0m INFO 12-05 14:20:13 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:50823 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4048593)[0;0m INFO 12-05 14:20:13 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4048593)[0;0m INFO 12-05 14:20:13 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4048593)[0;0m INFO 12-05 14:20:14 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4048593)[0;0m INFO 12-05 14:20:14 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4048593)[0;0m INFO 12-05 14:20:15 [default_loader.py:314] Loading weights took 0.87 seconds
[1;36m(EngineCore_DP0 pid=4048593)[0;0m INFO 12-05 14:20:16 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.400003 seconds
[1;36m(EngineCore_DP0 pid=4048593)[0;0m INFO 12-05 14:20:19 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4048593)[0;0m INFO 12-05 14:20:19 [backends.py:647] Dynamo bytecode transform time: 3.68 s
[1;36m(EngineCore_DP0 pid=4048593)[0;0m ERROR 12-05 14:20:20 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4048593)[0;0m ERROR 12-05 14:20:20 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4048593)[0;0m ERROR 12-05 14:20:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4048593)[0;0m ERROR 12-05 14:20:20 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4048593)[0;0m ERROR 12-05 14:20:20 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048593)[0;0m ERROR 12-05 14:20:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4048593)[0;0m ERROR 12-05 14:20:20 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4048593)[0;0m ERROR 12-05 14:20:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4048593)[0;0m ERROR 12-05 14:20:20 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4048593)[0;0m ERROR 12-05 14:20:20 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048593)[0;0m ERROR 12-05 14:20:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4048593)[0;0m ERROR 12-05 14:20:20 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4048593)[0;0m ERROR 12-05 14:20:20 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048593)[0;0m ERROR 12-05 14:20:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4048593)[0;0m ERROR 12-05 14:20:20 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4048593)[0;0m ERROR 12-05 14:20:20 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048593)[0;0m ERROR 12-05 14:20:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4048593)[0;0m ERROR 12-05 14:20:20 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4048593)[0;0m ERROR 12-05 14:20:20 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048593)[0;0m ERROR 12-05 14:20:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4048593)[0;0m ERROR 12-05 14:20:20 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4048593)[0;0m ERROR 12-05 14:20:20 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048593)[0;0m ERROR 12-05 14:20:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4048593)[0;0m ERROR 12-05 14:20:20 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4048593)[0;0m ERROR 12-05 14:20:20 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048593)[0;0m ERROR 12-05 14:20:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4048593)[0;0m ERROR 12-05 14:20:20 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4048593)[0;0m ERROR 12-05 14:20:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4048593)[0;0m ERROR 12-05 14:20:20 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4048593)[0;0m ERROR 12-05 14:20:20 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048593)[0;0m ERROR 12-05 14:20:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4048593)[0;0m ERROR 12-05 14:20:20 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4048593)[0;0m ERROR 12-05 14:20:20 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048593)[0;0m ERROR 12-05 14:20:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4048593)[0;0m ERROR 12-05 14:20:20 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4048593)[0;0m ERROR 12-05 14:20:20 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048593)[0;0m ERROR 12-05 14:20:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4048593)[0;0m ERROR 12-05 14:20:20 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4048593)[0;0m ERROR 12-05 14:20:20 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048593)[0;0m ERROR 12-05 14:20:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4048593)[0;0m ERROR 12-05 14:20:20 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4048593)[0;0m ERROR 12-05 14:20:20 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048593)[0;0m ERROR 12-05 14:20:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4048593)[0;0m ERROR 12-05 14:20:20 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4048593)[0;0m ERROR 12-05 14:20:20 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048593)[0;0m ERROR 12-05 14:20:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4048593)[0;0m ERROR 12-05 14:20:20 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4048593)[0;0m ERROR 12-05 14:20:20 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048593)[0;0m ERROR 12-05 14:20:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4048593)[0;0m ERROR 12-05 14:20:20 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4048593)[0;0m ERROR 12-05 14:20:20 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048593)[0;0m ERROR 12-05 14:20:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4048593)[0;0m ERROR 12-05 14:20:20 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4048593)[0;0m ERROR 12-05 14:20:20 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048593)[0;0m ERROR 12-05 14:20:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4048593)[0;0m ERROR 12-05 14:20:20 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4048593)[0;0m ERROR 12-05 14:20:20 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048593)[0;0m ERROR 12-05 14:20:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4048593)[0;0m ERROR 12-05 14:20:20 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4048593)[0;0m ERROR 12-05 14:20:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4048593)[0;0m ERROR 12-05 14:20:20 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4048593)[0;0m ERROR 12-05 14:20:20 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048593)[0;0m ERROR 12-05 14:20:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4048593)[0;0m ERROR 12-05 14:20:20 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4048593)[0;0m ERROR 12-05 14:20:20 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048593)[0;0m ERROR 12-05 14:20:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4048593)[0;0m ERROR 12-05 14:20:20 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4048593)[0;0m ERROR 12-05 14:20:20 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048593)[0;0m ERROR 12-05 14:20:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4048593)[0;0m ERROR 12-05 14:20:20 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4048593)[0;0m ERROR 12-05 14:20:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4048593)[0;0m ERROR 12-05 14:20:20 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4048593)[0;0m ERROR 12-05 14:20:20 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048593)[0;0m ERROR 12-05 14:20:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4048593)[0;0m ERROR 12-05 14:20:20 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4048593)[0;0m ERROR 12-05 14:20:20 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048593)[0;0m ERROR 12-05 14:20:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4048593)[0;0m ERROR 12-05 14:20:20 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4048593)[0;0m ERROR 12-05 14:20:20 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048593)[0;0m ERROR 12-05 14:20:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4048593)[0;0m ERROR 12-05 14:20:20 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4048593)[0;0m ERROR 12-05 14:20:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4048593)[0;0m ERROR 12-05 14:20:20 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4048593)[0;0m ERROR 12-05 14:20:20 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048593)[0;0m ERROR 12-05 14:20:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4048593)[0;0m ERROR 12-05 14:20:20 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4048593)[0;0m ERROR 12-05 14:20:20 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048593)[0;0m ERROR 12-05 14:20:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4048593)[0;0m ERROR 12-05 14:20:20 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4048593)[0;0m ERROR 12-05 14:20:20 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048593)[0;0m ERROR 12-05 14:20:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4048593)[0;0m ERROR 12-05 14:20:20 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4048593)[0;0m ERROR 12-05 14:20:20 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048593)[0;0m ERROR 12-05 14:20:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4048593)[0;0m ERROR 12-05 14:20:20 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4048593)[0;0m ERROR 12-05 14:20:20 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7f9f9c0fd790>' raised:
[1;36m(EngineCore_DP0 pid=4048593)[0;0m ERROR 12-05 14:20:20 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4048593)[0;0m ERROR 12-05 14:20:20 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4048593)[0;0m ERROR 12-05 14:20:20 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4048593)[0;0m ERROR 12-05 14:20:20 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 14:20:41 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 14:20:41 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 14:20:41 [model.py:1745] Using max model len 131072
INFO 12-05 14:20:41 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4048673)[0;0m INFO 12-05 14:20:50 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4048673)[0;0m INFO 12-05 14:20:51 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:53883 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4048673)[0;0m INFO 12-05 14:20:51 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4048673)[0;0m INFO 12-05 14:20:51 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4048673)[0;0m INFO 12-05 14:20:52 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4048673)[0;0m INFO 12-05 14:20:52 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4048673)[0;0m INFO 12-05 14:20:53 [default_loader.py:314] Loading weights took 0.88 seconds
[1;36m(EngineCore_DP0 pid=4048673)[0;0m INFO 12-05 14:20:54 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.438028 seconds
[1;36m(EngineCore_DP0 pid=4048673)[0;0m INFO 12-05 14:20:58 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4048673)[0;0m INFO 12-05 14:20:58 [backends.py:647] Dynamo bytecode transform time: 3.66 s
[1;36m(EngineCore_DP0 pid=4048673)[0;0m ERROR 12-05 14:20:58 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4048673)[0;0m ERROR 12-05 14:20:58 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4048673)[0;0m ERROR 12-05 14:20:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4048673)[0;0m ERROR 12-05 14:20:58 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4048673)[0;0m ERROR 12-05 14:20:58 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048673)[0;0m ERROR 12-05 14:20:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4048673)[0;0m ERROR 12-05 14:20:58 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4048673)[0;0m ERROR 12-05 14:20:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4048673)[0;0m ERROR 12-05 14:20:58 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4048673)[0;0m ERROR 12-05 14:20:58 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048673)[0;0m ERROR 12-05 14:20:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4048673)[0;0m ERROR 12-05 14:20:58 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4048673)[0;0m ERROR 12-05 14:20:58 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048673)[0;0m ERROR 12-05 14:20:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4048673)[0;0m ERROR 12-05 14:20:58 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4048673)[0;0m ERROR 12-05 14:20:58 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048673)[0;0m ERROR 12-05 14:20:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4048673)[0;0m ERROR 12-05 14:20:58 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4048673)[0;0m ERROR 12-05 14:20:58 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048673)[0;0m ERROR 12-05 14:20:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4048673)[0;0m ERROR 12-05 14:20:58 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4048673)[0;0m ERROR 12-05 14:20:58 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048673)[0;0m ERROR 12-05 14:20:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4048673)[0;0m ERROR 12-05 14:20:58 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4048673)[0;0m ERROR 12-05 14:20:58 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048673)[0;0m ERROR 12-05 14:20:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4048673)[0;0m ERROR 12-05 14:20:58 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4048673)[0;0m ERROR 12-05 14:20:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4048673)[0;0m ERROR 12-05 14:20:58 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4048673)[0;0m ERROR 12-05 14:20:58 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048673)[0;0m ERROR 12-05 14:20:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4048673)[0;0m ERROR 12-05 14:20:58 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4048673)[0;0m ERROR 12-05 14:20:58 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048673)[0;0m ERROR 12-05 14:20:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4048673)[0;0m ERROR 12-05 14:20:58 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4048673)[0;0m ERROR 12-05 14:20:58 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048673)[0;0m ERROR 12-05 14:20:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4048673)[0;0m ERROR 12-05 14:20:58 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4048673)[0;0m ERROR 12-05 14:20:58 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048673)[0;0m ERROR 12-05 14:20:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4048673)[0;0m ERROR 12-05 14:20:58 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4048673)[0;0m ERROR 12-05 14:20:58 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048673)[0;0m ERROR 12-05 14:20:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4048673)[0;0m ERROR 12-05 14:20:58 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4048673)[0;0m ERROR 12-05 14:20:58 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048673)[0;0m ERROR 12-05 14:20:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4048673)[0;0m ERROR 12-05 14:20:58 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4048673)[0;0m ERROR 12-05 14:20:58 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048673)[0;0m ERROR 12-05 14:20:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4048673)[0;0m ERROR 12-05 14:20:58 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4048673)[0;0m ERROR 12-05 14:20:58 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048673)[0;0m ERROR 12-05 14:20:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4048673)[0;0m ERROR 12-05 14:20:58 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4048673)[0;0m ERROR 12-05 14:20:58 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048673)[0;0m ERROR 12-05 14:20:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4048673)[0;0m ERROR 12-05 14:20:58 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4048673)[0;0m ERROR 12-05 14:20:58 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048673)[0;0m ERROR 12-05 14:20:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4048673)[0;0m ERROR 12-05 14:20:58 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4048673)[0;0m ERROR 12-05 14:20:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4048673)[0;0m ERROR 12-05 14:20:58 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4048673)[0;0m ERROR 12-05 14:20:58 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048673)[0;0m ERROR 12-05 14:20:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4048673)[0;0m ERROR 12-05 14:20:58 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4048673)[0;0m ERROR 12-05 14:20:58 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048673)[0;0m ERROR 12-05 14:20:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4048673)[0;0m ERROR 12-05 14:20:58 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4048673)[0;0m ERROR 12-05 14:20:58 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048673)[0;0m ERROR 12-05 14:20:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4048673)[0;0m ERROR 12-05 14:20:58 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4048673)[0;0m ERROR 12-05 14:20:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4048673)[0;0m ERROR 12-05 14:20:58 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4048673)[0;0m ERROR 12-05 14:20:58 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048673)[0;0m ERROR 12-05 14:20:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4048673)[0;0m ERROR 12-05 14:20:58 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4048673)[0;0m ERROR 12-05 14:20:58 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048673)[0;0m ERROR 12-05 14:20:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4048673)[0;0m ERROR 12-05 14:20:58 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4048673)[0;0m ERROR 12-05 14:20:58 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048673)[0;0m ERROR 12-05 14:20:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4048673)[0;0m ERROR 12-05 14:20:58 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4048673)[0;0m ERROR 12-05 14:20:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4048673)[0;0m ERROR 12-05 14:20:58 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4048673)[0;0m ERROR 12-05 14:20:58 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048673)[0;0m ERROR 12-05 14:20:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4048673)[0;0m ERROR 12-05 14:20:58 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4048673)[0;0m ERROR 12-05 14:20:58 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048673)[0;0m ERROR 12-05 14:20:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4048673)[0;0m ERROR 12-05 14:20:58 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4048673)[0;0m ERROR 12-05 14:20:58 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048673)[0;0m ERROR 12-05 14:20:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4048673)[0;0m ERROR 12-05 14:20:58 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4048673)[0;0m ERROR 12-05 14:20:58 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048673)[0;0m ERROR 12-05 14:20:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4048673)[0;0m ERROR 12-05 14:20:58 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4048673)[0;0m ERROR 12-05 14:20:58 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7fa8100e7890>' raised:
[1;36m(EngineCore_DP0 pid=4048673)[0;0m ERROR 12-05 14:20:58 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4048673)[0;0m ERROR 12-05 14:20:58 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4048673)[0;0m ERROR 12-05 14:20:58 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4048673)[0;0m ERROR 12-05 14:20:58 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 14:21:19 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 14:21:19 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 14:21:19 [model.py:1745] Using max model len 131072
INFO 12-05 14:21:19 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4048854)[0;0m INFO 12-05 14:21:27 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4048854)[0;0m INFO 12-05 14:21:28 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:47433 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4048854)[0;0m INFO 12-05 14:21:28 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4048854)[0;0m INFO 12-05 14:21:28 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4048854)[0;0m INFO 12-05 14:21:28 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4048854)[0;0m INFO 12-05 14:21:28 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4048854)[0;0m INFO 12-05 14:21:30 [default_loader.py:314] Loading weights took 0.88 seconds
[1;36m(EngineCore_DP0 pid=4048854)[0;0m INFO 12-05 14:21:30 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.394060 seconds
[1;36m(EngineCore_DP0 pid=4048854)[0;0m INFO 12-05 14:21:34 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4048854)[0;0m INFO 12-05 14:21:34 [backends.py:647] Dynamo bytecode transform time: 3.68 s
[1;36m(EngineCore_DP0 pid=4048854)[0;0m ERROR 12-05 14:21:34 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4048854)[0;0m ERROR 12-05 14:21:34 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4048854)[0;0m ERROR 12-05 14:21:34 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4048854)[0;0m ERROR 12-05 14:21:34 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4048854)[0;0m ERROR 12-05 14:21:34 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048854)[0;0m ERROR 12-05 14:21:34 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4048854)[0;0m ERROR 12-05 14:21:34 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4048854)[0;0m ERROR 12-05 14:21:34 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4048854)[0;0m ERROR 12-05 14:21:34 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4048854)[0;0m ERROR 12-05 14:21:34 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048854)[0;0m ERROR 12-05 14:21:34 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4048854)[0;0m ERROR 12-05 14:21:34 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4048854)[0;0m ERROR 12-05 14:21:34 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048854)[0;0m ERROR 12-05 14:21:34 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4048854)[0;0m ERROR 12-05 14:21:34 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4048854)[0;0m ERROR 12-05 14:21:34 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048854)[0;0m ERROR 12-05 14:21:34 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4048854)[0;0m ERROR 12-05 14:21:34 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4048854)[0;0m ERROR 12-05 14:21:34 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048854)[0;0m ERROR 12-05 14:21:34 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4048854)[0;0m ERROR 12-05 14:21:34 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4048854)[0;0m ERROR 12-05 14:21:34 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048854)[0;0m ERROR 12-05 14:21:34 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4048854)[0;0m ERROR 12-05 14:21:34 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4048854)[0;0m ERROR 12-05 14:21:34 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048854)[0;0m ERROR 12-05 14:21:34 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4048854)[0;0m ERROR 12-05 14:21:34 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4048854)[0;0m ERROR 12-05 14:21:34 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4048854)[0;0m ERROR 12-05 14:21:34 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4048854)[0;0m ERROR 12-05 14:21:34 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048854)[0;0m ERROR 12-05 14:21:34 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4048854)[0;0m ERROR 12-05 14:21:34 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4048854)[0;0m ERROR 12-05 14:21:34 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048854)[0;0m ERROR 12-05 14:21:34 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4048854)[0;0m ERROR 12-05 14:21:34 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4048854)[0;0m ERROR 12-05 14:21:34 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048854)[0;0m ERROR 12-05 14:21:34 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4048854)[0;0m ERROR 12-05 14:21:34 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4048854)[0;0m ERROR 12-05 14:21:34 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048854)[0;0m ERROR 12-05 14:21:34 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4048854)[0;0m ERROR 12-05 14:21:34 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4048854)[0;0m ERROR 12-05 14:21:34 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048854)[0;0m ERROR 12-05 14:21:34 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4048854)[0;0m ERROR 12-05 14:21:34 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4048854)[0;0m ERROR 12-05 14:21:34 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048854)[0;0m ERROR 12-05 14:21:34 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4048854)[0;0m ERROR 12-05 14:21:34 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4048854)[0;0m ERROR 12-05 14:21:34 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048854)[0;0m ERROR 12-05 14:21:34 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4048854)[0;0m ERROR 12-05 14:21:34 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4048854)[0;0m ERROR 12-05 14:21:34 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048854)[0;0m ERROR 12-05 14:21:34 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4048854)[0;0m ERROR 12-05 14:21:34 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4048854)[0;0m ERROR 12-05 14:21:34 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048854)[0;0m ERROR 12-05 14:21:34 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4048854)[0;0m ERROR 12-05 14:21:34 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4048854)[0;0m ERROR 12-05 14:21:34 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048854)[0;0m ERROR 12-05 14:21:34 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4048854)[0;0m ERROR 12-05 14:21:34 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4048854)[0;0m ERROR 12-05 14:21:34 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4048854)[0;0m ERROR 12-05 14:21:34 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4048854)[0;0m ERROR 12-05 14:21:34 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048854)[0;0m ERROR 12-05 14:21:34 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4048854)[0;0m ERROR 12-05 14:21:34 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4048854)[0;0m ERROR 12-05 14:21:34 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048854)[0;0m ERROR 12-05 14:21:34 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4048854)[0;0m ERROR 12-05 14:21:34 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4048854)[0;0m ERROR 12-05 14:21:34 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048854)[0;0m ERROR 12-05 14:21:34 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4048854)[0;0m ERROR 12-05 14:21:34 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4048854)[0;0m ERROR 12-05 14:21:34 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4048854)[0;0m ERROR 12-05 14:21:34 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4048854)[0;0m ERROR 12-05 14:21:34 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048854)[0;0m ERROR 12-05 14:21:34 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4048854)[0;0m ERROR 12-05 14:21:34 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4048854)[0;0m ERROR 12-05 14:21:34 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048854)[0;0m ERROR 12-05 14:21:34 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4048854)[0;0m ERROR 12-05 14:21:34 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4048854)[0;0m ERROR 12-05 14:21:34 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048854)[0;0m ERROR 12-05 14:21:34 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4048854)[0;0m ERROR 12-05 14:21:34 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4048854)[0;0m ERROR 12-05 14:21:34 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4048854)[0;0m ERROR 12-05 14:21:34 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4048854)[0;0m ERROR 12-05 14:21:34 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048854)[0;0m ERROR 12-05 14:21:34 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4048854)[0;0m ERROR 12-05 14:21:34 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4048854)[0;0m ERROR 12-05 14:21:34 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048854)[0;0m ERROR 12-05 14:21:34 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4048854)[0;0m ERROR 12-05 14:21:34 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4048854)[0;0m ERROR 12-05 14:21:34 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048854)[0;0m ERROR 12-05 14:21:34 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4048854)[0;0m ERROR 12-05 14:21:34 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4048854)[0;0m ERROR 12-05 14:21:34 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048854)[0;0m ERROR 12-05 14:21:34 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4048854)[0;0m ERROR 12-05 14:21:34 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4048854)[0;0m ERROR 12-05 14:21:34 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7f5cf42e4620>' raised:
[1;36m(EngineCore_DP0 pid=4048854)[0;0m ERROR 12-05 14:21:34 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4048854)[0;0m ERROR 12-05 14:21:34 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4048854)[0;0m ERROR 12-05 14:21:34 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4048854)[0;0m ERROR 12-05 14:21:34 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 14:21:56 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 14:21:56 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 14:21:56 [model.py:1745] Using max model len 131072
INFO 12-05 14:21:56 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4048941)[0;0m INFO 12-05 14:22:03 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4048941)[0;0m INFO 12-05 14:22:04 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:54003 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4048941)[0;0m INFO 12-05 14:22:04 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4048941)[0;0m INFO 12-05 14:22:04 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4048941)[0;0m INFO 12-05 14:22:05 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4048941)[0;0m INFO 12-05 14:22:05 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4048941)[0;0m INFO 12-05 14:22:06 [default_loader.py:314] Loading weights took 0.88 seconds
[1;36m(EngineCore_DP0 pid=4048941)[0;0m INFO 12-05 14:22:07 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.404993 seconds
[1;36m(EngineCore_DP0 pid=4048941)[0;0m INFO 12-05 14:22:10 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4048941)[0;0m INFO 12-05 14:22:10 [backends.py:647] Dynamo bytecode transform time: 3.63 s
[1;36m(EngineCore_DP0 pid=4048941)[0;0m ERROR 12-05 14:22:11 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4048941)[0;0m ERROR 12-05 14:22:11 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4048941)[0;0m ERROR 12-05 14:22:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4048941)[0;0m ERROR 12-05 14:22:11 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4048941)[0;0m ERROR 12-05 14:22:11 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048941)[0;0m ERROR 12-05 14:22:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4048941)[0;0m ERROR 12-05 14:22:11 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4048941)[0;0m ERROR 12-05 14:22:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4048941)[0;0m ERROR 12-05 14:22:11 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4048941)[0;0m ERROR 12-05 14:22:11 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048941)[0;0m ERROR 12-05 14:22:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4048941)[0;0m ERROR 12-05 14:22:11 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4048941)[0;0m ERROR 12-05 14:22:11 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048941)[0;0m ERROR 12-05 14:22:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4048941)[0;0m ERROR 12-05 14:22:11 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4048941)[0;0m ERROR 12-05 14:22:11 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048941)[0;0m ERROR 12-05 14:22:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4048941)[0;0m ERROR 12-05 14:22:11 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4048941)[0;0m ERROR 12-05 14:22:11 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048941)[0;0m ERROR 12-05 14:22:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4048941)[0;0m ERROR 12-05 14:22:11 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4048941)[0;0m ERROR 12-05 14:22:11 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048941)[0;0m ERROR 12-05 14:22:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4048941)[0;0m ERROR 12-05 14:22:11 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4048941)[0;0m ERROR 12-05 14:22:11 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048941)[0;0m ERROR 12-05 14:22:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4048941)[0;0m ERROR 12-05 14:22:11 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4048941)[0;0m ERROR 12-05 14:22:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4048941)[0;0m ERROR 12-05 14:22:11 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4048941)[0;0m ERROR 12-05 14:22:11 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048941)[0;0m ERROR 12-05 14:22:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4048941)[0;0m ERROR 12-05 14:22:11 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4048941)[0;0m ERROR 12-05 14:22:11 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048941)[0;0m ERROR 12-05 14:22:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4048941)[0;0m ERROR 12-05 14:22:11 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4048941)[0;0m ERROR 12-05 14:22:11 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048941)[0;0m ERROR 12-05 14:22:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4048941)[0;0m ERROR 12-05 14:22:11 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4048941)[0;0m ERROR 12-05 14:22:11 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048941)[0;0m ERROR 12-05 14:22:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4048941)[0;0m ERROR 12-05 14:22:11 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4048941)[0;0m ERROR 12-05 14:22:11 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048941)[0;0m ERROR 12-05 14:22:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4048941)[0;0m ERROR 12-05 14:22:11 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4048941)[0;0m ERROR 12-05 14:22:11 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048941)[0;0m ERROR 12-05 14:22:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4048941)[0;0m ERROR 12-05 14:22:11 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4048941)[0;0m ERROR 12-05 14:22:11 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048941)[0;0m ERROR 12-05 14:22:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4048941)[0;0m ERROR 12-05 14:22:11 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4048941)[0;0m ERROR 12-05 14:22:11 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048941)[0;0m ERROR 12-05 14:22:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4048941)[0;0m ERROR 12-05 14:22:11 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4048941)[0;0m ERROR 12-05 14:22:11 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048941)[0;0m ERROR 12-05 14:22:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4048941)[0;0m ERROR 12-05 14:22:11 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4048941)[0;0m ERROR 12-05 14:22:11 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048941)[0;0m ERROR 12-05 14:22:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4048941)[0;0m ERROR 12-05 14:22:11 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4048941)[0;0m ERROR 12-05 14:22:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4048941)[0;0m ERROR 12-05 14:22:11 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4048941)[0;0m ERROR 12-05 14:22:11 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048941)[0;0m ERROR 12-05 14:22:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4048941)[0;0m ERROR 12-05 14:22:11 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4048941)[0;0m ERROR 12-05 14:22:11 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048941)[0;0m ERROR 12-05 14:22:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4048941)[0;0m ERROR 12-05 14:22:11 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4048941)[0;0m ERROR 12-05 14:22:11 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048941)[0;0m ERROR 12-05 14:22:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4048941)[0;0m ERROR 12-05 14:22:11 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4048941)[0;0m ERROR 12-05 14:22:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4048941)[0;0m ERROR 12-05 14:22:11 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4048941)[0;0m ERROR 12-05 14:22:11 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048941)[0;0m ERROR 12-05 14:22:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4048941)[0;0m ERROR 12-05 14:22:11 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4048941)[0;0m ERROR 12-05 14:22:11 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048941)[0;0m ERROR 12-05 14:22:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4048941)[0;0m ERROR 12-05 14:22:11 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4048941)[0;0m ERROR 12-05 14:22:11 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048941)[0;0m ERROR 12-05 14:22:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4048941)[0;0m ERROR 12-05 14:22:11 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4048941)[0;0m ERROR 12-05 14:22:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4048941)[0;0m ERROR 12-05 14:22:11 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4048941)[0;0m ERROR 12-05 14:22:11 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048941)[0;0m ERROR 12-05 14:22:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4048941)[0;0m ERROR 12-05 14:22:11 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4048941)[0;0m ERROR 12-05 14:22:11 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048941)[0;0m ERROR 12-05 14:22:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4048941)[0;0m ERROR 12-05 14:22:11 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4048941)[0;0m ERROR 12-05 14:22:11 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048941)[0;0m ERROR 12-05 14:22:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4048941)[0;0m ERROR 12-05 14:22:11 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4048941)[0;0m ERROR 12-05 14:22:11 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4048941)[0;0m ERROR 12-05 14:22:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4048941)[0;0m ERROR 12-05 14:22:11 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4048941)[0;0m ERROR 12-05 14:22:11 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7f4c1c0eb860>' raised:
[1;36m(EngineCore_DP0 pid=4048941)[0;0m ERROR 12-05 14:22:11 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4048941)[0;0m ERROR 12-05 14:22:11 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4048941)[0;0m ERROR 12-05 14:22:11 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4048941)[0;0m ERROR 12-05 14:22:11 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 14:22:32 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 14:22:32 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 14:22:32 [model.py:1745] Using max model len 131072
INFO 12-05 14:22:32 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4049022)[0;0m INFO 12-05 14:22:40 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4049022)[0;0m INFO 12-05 14:22:40 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:43483 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4049022)[0;0m INFO 12-05 14:22:41 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4049022)[0;0m INFO 12-05 14:22:41 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4049022)[0;0m INFO 12-05 14:22:41 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4049022)[0;0m INFO 12-05 14:22:41 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4049022)[0;0m INFO 12-05 14:22:42 [default_loader.py:314] Loading weights took 0.88 seconds
[1;36m(EngineCore_DP0 pid=4049022)[0;0m INFO 12-05 14:22:43 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.424288 seconds
[1;36m(EngineCore_DP0 pid=4049022)[0;0m INFO 12-05 14:22:47 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4049022)[0;0m INFO 12-05 14:22:47 [backends.py:647] Dynamo bytecode transform time: 3.71 s
[1;36m(EngineCore_DP0 pid=4049022)[0;0m ERROR 12-05 14:22:47 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4049022)[0;0m ERROR 12-05 14:22:47 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4049022)[0;0m ERROR 12-05 14:22:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4049022)[0;0m ERROR 12-05 14:22:47 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4049022)[0;0m ERROR 12-05 14:22:47 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049022)[0;0m ERROR 12-05 14:22:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4049022)[0;0m ERROR 12-05 14:22:47 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4049022)[0;0m ERROR 12-05 14:22:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4049022)[0;0m ERROR 12-05 14:22:47 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4049022)[0;0m ERROR 12-05 14:22:47 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049022)[0;0m ERROR 12-05 14:22:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4049022)[0;0m ERROR 12-05 14:22:47 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4049022)[0;0m ERROR 12-05 14:22:47 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049022)[0;0m ERROR 12-05 14:22:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4049022)[0;0m ERROR 12-05 14:22:47 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4049022)[0;0m ERROR 12-05 14:22:47 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049022)[0;0m ERROR 12-05 14:22:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4049022)[0;0m ERROR 12-05 14:22:47 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4049022)[0;0m ERROR 12-05 14:22:47 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049022)[0;0m ERROR 12-05 14:22:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4049022)[0;0m ERROR 12-05 14:22:47 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4049022)[0;0m ERROR 12-05 14:22:47 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049022)[0;0m ERROR 12-05 14:22:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4049022)[0;0m ERROR 12-05 14:22:47 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4049022)[0;0m ERROR 12-05 14:22:47 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049022)[0;0m ERROR 12-05 14:22:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4049022)[0;0m ERROR 12-05 14:22:47 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4049022)[0;0m ERROR 12-05 14:22:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4049022)[0;0m ERROR 12-05 14:22:47 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4049022)[0;0m ERROR 12-05 14:22:47 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049022)[0;0m ERROR 12-05 14:22:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4049022)[0;0m ERROR 12-05 14:22:47 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4049022)[0;0m ERROR 12-05 14:22:47 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049022)[0;0m ERROR 12-05 14:22:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4049022)[0;0m ERROR 12-05 14:22:47 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4049022)[0;0m ERROR 12-05 14:22:47 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049022)[0;0m ERROR 12-05 14:22:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4049022)[0;0m ERROR 12-05 14:22:47 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4049022)[0;0m ERROR 12-05 14:22:47 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049022)[0;0m ERROR 12-05 14:22:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4049022)[0;0m ERROR 12-05 14:22:47 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4049022)[0;0m ERROR 12-05 14:22:47 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049022)[0;0m ERROR 12-05 14:22:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4049022)[0;0m ERROR 12-05 14:22:47 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4049022)[0;0m ERROR 12-05 14:22:47 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049022)[0;0m ERROR 12-05 14:22:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4049022)[0;0m ERROR 12-05 14:22:47 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4049022)[0;0m ERROR 12-05 14:22:47 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049022)[0;0m ERROR 12-05 14:22:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4049022)[0;0m ERROR 12-05 14:22:47 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4049022)[0;0m ERROR 12-05 14:22:47 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049022)[0;0m ERROR 12-05 14:22:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4049022)[0;0m ERROR 12-05 14:22:47 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4049022)[0;0m ERROR 12-05 14:22:47 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049022)[0;0m ERROR 12-05 14:22:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4049022)[0;0m ERROR 12-05 14:22:47 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4049022)[0;0m ERROR 12-05 14:22:47 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049022)[0;0m ERROR 12-05 14:22:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4049022)[0;0m ERROR 12-05 14:22:47 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4049022)[0;0m ERROR 12-05 14:22:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4049022)[0;0m ERROR 12-05 14:22:47 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4049022)[0;0m ERROR 12-05 14:22:47 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049022)[0;0m ERROR 12-05 14:22:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4049022)[0;0m ERROR 12-05 14:22:47 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4049022)[0;0m ERROR 12-05 14:22:47 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049022)[0;0m ERROR 12-05 14:22:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4049022)[0;0m ERROR 12-05 14:22:47 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4049022)[0;0m ERROR 12-05 14:22:47 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049022)[0;0m ERROR 12-05 14:22:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4049022)[0;0m ERROR 12-05 14:22:47 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4049022)[0;0m ERROR 12-05 14:22:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4049022)[0;0m ERROR 12-05 14:22:47 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4049022)[0;0m ERROR 12-05 14:22:47 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049022)[0;0m ERROR 12-05 14:22:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4049022)[0;0m ERROR 12-05 14:22:47 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4049022)[0;0m ERROR 12-05 14:22:47 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049022)[0;0m ERROR 12-05 14:22:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4049022)[0;0m ERROR 12-05 14:22:47 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4049022)[0;0m ERROR 12-05 14:22:47 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049022)[0;0m ERROR 12-05 14:22:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4049022)[0;0m ERROR 12-05 14:22:47 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4049022)[0;0m ERROR 12-05 14:22:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4049022)[0;0m ERROR 12-05 14:22:47 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4049022)[0;0m ERROR 12-05 14:22:47 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049022)[0;0m ERROR 12-05 14:22:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4049022)[0;0m ERROR 12-05 14:22:47 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4049022)[0;0m ERROR 12-05 14:22:47 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049022)[0;0m ERROR 12-05 14:22:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4049022)[0;0m ERROR 12-05 14:22:47 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4049022)[0;0m ERROR 12-05 14:22:47 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049022)[0;0m ERROR 12-05 14:22:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4049022)[0;0m ERROR 12-05 14:22:47 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4049022)[0;0m ERROR 12-05 14:22:47 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049022)[0;0m ERROR 12-05 14:22:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4049022)[0;0m ERROR 12-05 14:22:47 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4049022)[0;0m ERROR 12-05 14:22:47 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7fa6f0271880>' raised:
[1;36m(EngineCore_DP0 pid=4049022)[0;0m ERROR 12-05 14:22:47 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4049022)[0;0m ERROR 12-05 14:22:47 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4049022)[0;0m ERROR 12-05 14:22:47 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4049022)[0;0m ERROR 12-05 14:22:47 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 14:23:09 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 14:23:09 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 14:23:09 [model.py:1745] Using max model len 131072
INFO 12-05 14:23:09 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4049106)[0;0m INFO 12-05 14:23:16 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4049106)[0;0m INFO 12-05 14:23:17 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:41187 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4049106)[0;0m INFO 12-05 14:23:17 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4049106)[0;0m INFO 12-05 14:23:18 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4049106)[0;0m INFO 12-05 14:23:18 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4049106)[0;0m INFO 12-05 14:23:18 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4049106)[0;0m INFO 12-05 14:23:19 [default_loader.py:314] Loading weights took 0.87 seconds
[1;36m(EngineCore_DP0 pid=4049106)[0;0m INFO 12-05 14:23:20 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.402974 seconds
[1;36m(EngineCore_DP0 pid=4049106)[0;0m INFO 12-05 14:23:23 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4049106)[0;0m INFO 12-05 14:23:23 [backends.py:647] Dynamo bytecode transform time: 3.66 s
[1;36m(EngineCore_DP0 pid=4049106)[0;0m ERROR 12-05 14:23:24 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4049106)[0;0m ERROR 12-05 14:23:24 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4049106)[0;0m ERROR 12-05 14:23:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4049106)[0;0m ERROR 12-05 14:23:24 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4049106)[0;0m ERROR 12-05 14:23:24 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049106)[0;0m ERROR 12-05 14:23:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4049106)[0;0m ERROR 12-05 14:23:24 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4049106)[0;0m ERROR 12-05 14:23:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4049106)[0;0m ERROR 12-05 14:23:24 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4049106)[0;0m ERROR 12-05 14:23:24 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049106)[0;0m ERROR 12-05 14:23:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4049106)[0;0m ERROR 12-05 14:23:24 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4049106)[0;0m ERROR 12-05 14:23:24 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049106)[0;0m ERROR 12-05 14:23:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4049106)[0;0m ERROR 12-05 14:23:24 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4049106)[0;0m ERROR 12-05 14:23:24 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049106)[0;0m ERROR 12-05 14:23:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4049106)[0;0m ERROR 12-05 14:23:24 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4049106)[0;0m ERROR 12-05 14:23:24 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049106)[0;0m ERROR 12-05 14:23:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4049106)[0;0m ERROR 12-05 14:23:24 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4049106)[0;0m ERROR 12-05 14:23:24 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049106)[0;0m ERROR 12-05 14:23:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4049106)[0;0m ERROR 12-05 14:23:24 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4049106)[0;0m ERROR 12-05 14:23:24 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049106)[0;0m ERROR 12-05 14:23:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4049106)[0;0m ERROR 12-05 14:23:24 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4049106)[0;0m ERROR 12-05 14:23:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4049106)[0;0m ERROR 12-05 14:23:24 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4049106)[0;0m ERROR 12-05 14:23:24 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049106)[0;0m ERROR 12-05 14:23:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4049106)[0;0m ERROR 12-05 14:23:24 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4049106)[0;0m ERROR 12-05 14:23:24 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049106)[0;0m ERROR 12-05 14:23:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4049106)[0;0m ERROR 12-05 14:23:24 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4049106)[0;0m ERROR 12-05 14:23:24 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049106)[0;0m ERROR 12-05 14:23:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4049106)[0;0m ERROR 12-05 14:23:24 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4049106)[0;0m ERROR 12-05 14:23:24 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049106)[0;0m ERROR 12-05 14:23:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4049106)[0;0m ERROR 12-05 14:23:24 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4049106)[0;0m ERROR 12-05 14:23:24 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049106)[0;0m ERROR 12-05 14:23:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4049106)[0;0m ERROR 12-05 14:23:24 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4049106)[0;0m ERROR 12-05 14:23:24 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049106)[0;0m ERROR 12-05 14:23:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4049106)[0;0m ERROR 12-05 14:23:24 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4049106)[0;0m ERROR 12-05 14:23:24 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049106)[0;0m ERROR 12-05 14:23:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4049106)[0;0m ERROR 12-05 14:23:24 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4049106)[0;0m ERROR 12-05 14:23:24 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049106)[0;0m ERROR 12-05 14:23:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4049106)[0;0m ERROR 12-05 14:23:24 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4049106)[0;0m ERROR 12-05 14:23:24 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049106)[0;0m ERROR 12-05 14:23:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4049106)[0;0m ERROR 12-05 14:23:24 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4049106)[0;0m ERROR 12-05 14:23:24 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049106)[0;0m ERROR 12-05 14:23:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4049106)[0;0m ERROR 12-05 14:23:24 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4049106)[0;0m ERROR 12-05 14:23:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4049106)[0;0m ERROR 12-05 14:23:24 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4049106)[0;0m ERROR 12-05 14:23:24 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049106)[0;0m ERROR 12-05 14:23:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4049106)[0;0m ERROR 12-05 14:23:24 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4049106)[0;0m ERROR 12-05 14:23:24 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049106)[0;0m ERROR 12-05 14:23:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4049106)[0;0m ERROR 12-05 14:23:24 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4049106)[0;0m ERROR 12-05 14:23:24 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049106)[0;0m ERROR 12-05 14:23:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4049106)[0;0m ERROR 12-05 14:23:24 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4049106)[0;0m ERROR 12-05 14:23:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4049106)[0;0m ERROR 12-05 14:23:24 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4049106)[0;0m ERROR 12-05 14:23:24 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049106)[0;0m ERROR 12-05 14:23:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4049106)[0;0m ERROR 12-05 14:23:24 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4049106)[0;0m ERROR 12-05 14:23:24 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049106)[0;0m ERROR 12-05 14:23:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4049106)[0;0m ERROR 12-05 14:23:24 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4049106)[0;0m ERROR 12-05 14:23:24 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049106)[0;0m ERROR 12-05 14:23:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4049106)[0;0m ERROR 12-05 14:23:24 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4049106)[0;0m ERROR 12-05 14:23:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4049106)[0;0m ERROR 12-05 14:23:24 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4049106)[0;0m ERROR 12-05 14:23:24 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049106)[0;0m ERROR 12-05 14:23:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4049106)[0;0m ERROR 12-05 14:23:24 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4049106)[0;0m ERROR 12-05 14:23:24 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049106)[0;0m ERROR 12-05 14:23:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4049106)[0;0m ERROR 12-05 14:23:24 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4049106)[0;0m ERROR 12-05 14:23:24 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049106)[0;0m ERROR 12-05 14:23:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4049106)[0;0m ERROR 12-05 14:23:24 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4049106)[0;0m ERROR 12-05 14:23:24 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049106)[0;0m ERROR 12-05 14:23:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4049106)[0;0m ERROR 12-05 14:23:24 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4049106)[0;0m ERROR 12-05 14:23:24 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7f1c901b84a0>' raised:
[1;36m(EngineCore_DP0 pid=4049106)[0;0m ERROR 12-05 14:23:24 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4049106)[0;0m ERROR 12-05 14:23:24 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4049106)[0;0m ERROR 12-05 14:23:24 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4049106)[0;0m ERROR 12-05 14:23:24 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 14:23:45 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 14:23:45 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 14:23:45 [model.py:1745] Using max model len 131072
INFO 12-05 14:23:45 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4049196)[0;0m INFO 12-05 14:23:53 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4049196)[0;0m INFO 12-05 14:23:54 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:33663 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4049196)[0;0m INFO 12-05 14:23:54 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4049196)[0;0m INFO 12-05 14:23:54 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4049196)[0;0m INFO 12-05 14:23:54 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4049196)[0;0m INFO 12-05 14:23:54 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4049196)[0;0m INFO 12-05 14:23:56 [default_loader.py:314] Loading weights took 0.87 seconds
[1;36m(EngineCore_DP0 pid=4049196)[0;0m INFO 12-05 14:23:56 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.411746 seconds
[1;36m(EngineCore_DP0 pid=4049196)[0;0m INFO 12-05 14:24:00 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4049196)[0;0m INFO 12-05 14:24:00 [backends.py:647] Dynamo bytecode transform time: 3.70 s
[1;36m(EngineCore_DP0 pid=4049196)[0;0m ERROR 12-05 14:24:00 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4049196)[0;0m ERROR 12-05 14:24:00 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4049196)[0;0m ERROR 12-05 14:24:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4049196)[0;0m ERROR 12-05 14:24:00 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4049196)[0;0m ERROR 12-05 14:24:00 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049196)[0;0m ERROR 12-05 14:24:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4049196)[0;0m ERROR 12-05 14:24:00 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4049196)[0;0m ERROR 12-05 14:24:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4049196)[0;0m ERROR 12-05 14:24:00 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4049196)[0;0m ERROR 12-05 14:24:00 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049196)[0;0m ERROR 12-05 14:24:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4049196)[0;0m ERROR 12-05 14:24:00 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4049196)[0;0m ERROR 12-05 14:24:00 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049196)[0;0m ERROR 12-05 14:24:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4049196)[0;0m ERROR 12-05 14:24:00 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4049196)[0;0m ERROR 12-05 14:24:00 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049196)[0;0m ERROR 12-05 14:24:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4049196)[0;0m ERROR 12-05 14:24:00 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4049196)[0;0m ERROR 12-05 14:24:00 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049196)[0;0m ERROR 12-05 14:24:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4049196)[0;0m ERROR 12-05 14:24:00 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4049196)[0;0m ERROR 12-05 14:24:00 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049196)[0;0m ERROR 12-05 14:24:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4049196)[0;0m ERROR 12-05 14:24:00 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4049196)[0;0m ERROR 12-05 14:24:00 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049196)[0;0m ERROR 12-05 14:24:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4049196)[0;0m ERROR 12-05 14:24:00 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4049196)[0;0m ERROR 12-05 14:24:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4049196)[0;0m ERROR 12-05 14:24:00 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4049196)[0;0m ERROR 12-05 14:24:00 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049196)[0;0m ERROR 12-05 14:24:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4049196)[0;0m ERROR 12-05 14:24:00 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4049196)[0;0m ERROR 12-05 14:24:00 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049196)[0;0m ERROR 12-05 14:24:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4049196)[0;0m ERROR 12-05 14:24:00 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4049196)[0;0m ERROR 12-05 14:24:00 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049196)[0;0m ERROR 12-05 14:24:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4049196)[0;0m ERROR 12-05 14:24:00 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4049196)[0;0m ERROR 12-05 14:24:00 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049196)[0;0m ERROR 12-05 14:24:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4049196)[0;0m ERROR 12-05 14:24:00 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4049196)[0;0m ERROR 12-05 14:24:00 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049196)[0;0m ERROR 12-05 14:24:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4049196)[0;0m ERROR 12-05 14:24:00 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4049196)[0;0m ERROR 12-05 14:24:00 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049196)[0;0m ERROR 12-05 14:24:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4049196)[0;0m ERROR 12-05 14:24:00 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4049196)[0;0m ERROR 12-05 14:24:00 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049196)[0;0m ERROR 12-05 14:24:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4049196)[0;0m ERROR 12-05 14:24:00 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4049196)[0;0m ERROR 12-05 14:24:00 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049196)[0;0m ERROR 12-05 14:24:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4049196)[0;0m ERROR 12-05 14:24:00 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4049196)[0;0m ERROR 12-05 14:24:00 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049196)[0;0m ERROR 12-05 14:24:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4049196)[0;0m ERROR 12-05 14:24:00 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4049196)[0;0m ERROR 12-05 14:24:00 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049196)[0;0m ERROR 12-05 14:24:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4049196)[0;0m ERROR 12-05 14:24:00 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4049196)[0;0m ERROR 12-05 14:24:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4049196)[0;0m ERROR 12-05 14:24:00 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4049196)[0;0m ERROR 12-05 14:24:00 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049196)[0;0m ERROR 12-05 14:24:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4049196)[0;0m ERROR 12-05 14:24:00 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4049196)[0;0m ERROR 12-05 14:24:00 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049196)[0;0m ERROR 12-05 14:24:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4049196)[0;0m ERROR 12-05 14:24:00 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4049196)[0;0m ERROR 12-05 14:24:00 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049196)[0;0m ERROR 12-05 14:24:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4049196)[0;0m ERROR 12-05 14:24:00 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4049196)[0;0m ERROR 12-05 14:24:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4049196)[0;0m ERROR 12-05 14:24:00 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4049196)[0;0m ERROR 12-05 14:24:00 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049196)[0;0m ERROR 12-05 14:24:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4049196)[0;0m ERROR 12-05 14:24:00 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4049196)[0;0m ERROR 12-05 14:24:00 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049196)[0;0m ERROR 12-05 14:24:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4049196)[0;0m ERROR 12-05 14:24:00 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4049196)[0;0m ERROR 12-05 14:24:00 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049196)[0;0m ERROR 12-05 14:24:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4049196)[0;0m ERROR 12-05 14:24:00 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4049196)[0;0m ERROR 12-05 14:24:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4049196)[0;0m ERROR 12-05 14:24:00 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4049196)[0;0m ERROR 12-05 14:24:00 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049196)[0;0m ERROR 12-05 14:24:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4049196)[0;0m ERROR 12-05 14:24:00 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4049196)[0;0m ERROR 12-05 14:24:00 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049196)[0;0m ERROR 12-05 14:24:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4049196)[0;0m ERROR 12-05 14:24:00 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4049196)[0;0m ERROR 12-05 14:24:00 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049196)[0;0m ERROR 12-05 14:24:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4049196)[0;0m ERROR 12-05 14:24:00 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4049196)[0;0m ERROR 12-05 14:24:00 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049196)[0;0m ERROR 12-05 14:24:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4049196)[0;0m ERROR 12-05 14:24:00 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4049196)[0;0m ERROR 12-05 14:24:00 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7f2820206090>' raised:
[1;36m(EngineCore_DP0 pid=4049196)[0;0m ERROR 12-05 14:24:00 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4049196)[0;0m ERROR 12-05 14:24:00 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4049196)[0;0m ERROR 12-05 14:24:00 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4049196)[0;0m ERROR 12-05 14:24:00 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 14:24:22 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 14:24:22 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 14:24:22 [model.py:1745] Using max model len 131072
INFO 12-05 14:24:22 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4049274)[0;0m INFO 12-05 14:24:30 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4049274)[0;0m INFO 12-05 14:24:30 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:34613 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4049274)[0;0m INFO 12-05 14:24:30 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4049274)[0;0m INFO 12-05 14:24:31 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4049274)[0;0m INFO 12-05 14:24:31 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4049274)[0;0m INFO 12-05 14:24:31 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4049274)[0;0m INFO 12-05 14:24:32 [default_loader.py:314] Loading weights took 0.88 seconds
[1;36m(EngineCore_DP0 pid=4049274)[0;0m INFO 12-05 14:24:33 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.392106 seconds
[1;36m(EngineCore_DP0 pid=4049274)[0;0m INFO 12-05 14:24:37 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4049274)[0;0m INFO 12-05 14:24:37 [backends.py:647] Dynamo bytecode transform time: 3.68 s
[1;36m(EngineCore_DP0 pid=4049274)[0;0m ERROR 12-05 14:24:37 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4049274)[0;0m ERROR 12-05 14:24:37 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4049274)[0;0m ERROR 12-05 14:24:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4049274)[0;0m ERROR 12-05 14:24:37 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4049274)[0;0m ERROR 12-05 14:24:37 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049274)[0;0m ERROR 12-05 14:24:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4049274)[0;0m ERROR 12-05 14:24:37 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4049274)[0;0m ERROR 12-05 14:24:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4049274)[0;0m ERROR 12-05 14:24:37 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4049274)[0;0m ERROR 12-05 14:24:37 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049274)[0;0m ERROR 12-05 14:24:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4049274)[0;0m ERROR 12-05 14:24:37 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4049274)[0;0m ERROR 12-05 14:24:37 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049274)[0;0m ERROR 12-05 14:24:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4049274)[0;0m ERROR 12-05 14:24:37 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4049274)[0;0m ERROR 12-05 14:24:37 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049274)[0;0m ERROR 12-05 14:24:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4049274)[0;0m ERROR 12-05 14:24:37 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4049274)[0;0m ERROR 12-05 14:24:37 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049274)[0;0m ERROR 12-05 14:24:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4049274)[0;0m ERROR 12-05 14:24:37 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4049274)[0;0m ERROR 12-05 14:24:37 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049274)[0;0m ERROR 12-05 14:24:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4049274)[0;0m ERROR 12-05 14:24:37 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4049274)[0;0m ERROR 12-05 14:24:37 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049274)[0;0m ERROR 12-05 14:24:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4049274)[0;0m ERROR 12-05 14:24:37 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4049274)[0;0m ERROR 12-05 14:24:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4049274)[0;0m ERROR 12-05 14:24:37 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4049274)[0;0m ERROR 12-05 14:24:37 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049274)[0;0m ERROR 12-05 14:24:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4049274)[0;0m ERROR 12-05 14:24:37 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4049274)[0;0m ERROR 12-05 14:24:37 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049274)[0;0m ERROR 12-05 14:24:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4049274)[0;0m ERROR 12-05 14:24:37 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4049274)[0;0m ERROR 12-05 14:24:37 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049274)[0;0m ERROR 12-05 14:24:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4049274)[0;0m ERROR 12-05 14:24:37 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4049274)[0;0m ERROR 12-05 14:24:37 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049274)[0;0m ERROR 12-05 14:24:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4049274)[0;0m ERROR 12-05 14:24:37 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4049274)[0;0m ERROR 12-05 14:24:37 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049274)[0;0m ERROR 12-05 14:24:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4049274)[0;0m ERROR 12-05 14:24:37 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4049274)[0;0m ERROR 12-05 14:24:37 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049274)[0;0m ERROR 12-05 14:24:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4049274)[0;0m ERROR 12-05 14:24:37 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4049274)[0;0m ERROR 12-05 14:24:37 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049274)[0;0m ERROR 12-05 14:24:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4049274)[0;0m ERROR 12-05 14:24:37 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4049274)[0;0m ERROR 12-05 14:24:37 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049274)[0;0m ERROR 12-05 14:24:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4049274)[0;0m ERROR 12-05 14:24:37 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4049274)[0;0m ERROR 12-05 14:24:37 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049274)[0;0m ERROR 12-05 14:24:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4049274)[0;0m ERROR 12-05 14:24:37 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4049274)[0;0m ERROR 12-05 14:24:37 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049274)[0;0m ERROR 12-05 14:24:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4049274)[0;0m ERROR 12-05 14:24:37 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4049274)[0;0m ERROR 12-05 14:24:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4049274)[0;0m ERROR 12-05 14:24:37 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4049274)[0;0m ERROR 12-05 14:24:37 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049274)[0;0m ERROR 12-05 14:24:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4049274)[0;0m ERROR 12-05 14:24:37 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4049274)[0;0m ERROR 12-05 14:24:37 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049274)[0;0m ERROR 12-05 14:24:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4049274)[0;0m ERROR 12-05 14:24:37 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4049274)[0;0m ERROR 12-05 14:24:37 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049274)[0;0m ERROR 12-05 14:24:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4049274)[0;0m ERROR 12-05 14:24:37 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4049274)[0;0m ERROR 12-05 14:24:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4049274)[0;0m ERROR 12-05 14:24:37 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4049274)[0;0m ERROR 12-05 14:24:37 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049274)[0;0m ERROR 12-05 14:24:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4049274)[0;0m ERROR 12-05 14:24:37 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4049274)[0;0m ERROR 12-05 14:24:37 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049274)[0;0m ERROR 12-05 14:24:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4049274)[0;0m ERROR 12-05 14:24:37 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4049274)[0;0m ERROR 12-05 14:24:37 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049274)[0;0m ERROR 12-05 14:24:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4049274)[0;0m ERROR 12-05 14:24:37 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4049274)[0;0m ERROR 12-05 14:24:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4049274)[0;0m ERROR 12-05 14:24:37 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4049274)[0;0m ERROR 12-05 14:24:37 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049274)[0;0m ERROR 12-05 14:24:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4049274)[0;0m ERROR 12-05 14:24:37 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4049274)[0;0m ERROR 12-05 14:24:37 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049274)[0;0m ERROR 12-05 14:24:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4049274)[0;0m ERROR 12-05 14:24:37 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4049274)[0;0m ERROR 12-05 14:24:37 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049274)[0;0m ERROR 12-05 14:24:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4049274)[0;0m ERROR 12-05 14:24:37 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4049274)[0;0m ERROR 12-05 14:24:37 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049274)[0;0m ERROR 12-05 14:24:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4049274)[0;0m ERROR 12-05 14:24:37 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4049274)[0;0m ERROR 12-05 14:24:37 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7f02142e9100>' raised:
[1;36m(EngineCore_DP0 pid=4049274)[0;0m ERROR 12-05 14:24:37 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4049274)[0;0m ERROR 12-05 14:24:37 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4049274)[0;0m ERROR 12-05 14:24:37 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4049274)[0;0m ERROR 12-05 14:24:37 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 14:24:58 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 14:24:58 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 14:24:58 [model.py:1745] Using max model len 131072
INFO 12-05 14:24:58 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4049362)[0;0m INFO 12-05 14:25:06 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4049362)[0;0m INFO 12-05 14:25:07 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:60097 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4049362)[0;0m INFO 12-05 14:25:07 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4049362)[0;0m INFO 12-05 14:25:07 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4049362)[0;0m INFO 12-05 14:25:08 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4049362)[0;0m INFO 12-05 14:25:08 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4049362)[0;0m INFO 12-05 14:25:09 [default_loader.py:314] Loading weights took 0.87 seconds
[1;36m(EngineCore_DP0 pid=4049362)[0;0m INFO 12-05 14:25:09 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.386860 seconds
[1;36m(EngineCore_DP0 pid=4049362)[0;0m INFO 12-05 14:25:13 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4049362)[0;0m INFO 12-05 14:25:13 [backends.py:647] Dynamo bytecode transform time: 3.65 s
[1;36m(EngineCore_DP0 pid=4049362)[0;0m ERROR 12-05 14:25:14 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4049362)[0;0m ERROR 12-05 14:25:14 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4049362)[0;0m ERROR 12-05 14:25:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4049362)[0;0m ERROR 12-05 14:25:14 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4049362)[0;0m ERROR 12-05 14:25:14 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049362)[0;0m ERROR 12-05 14:25:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4049362)[0;0m ERROR 12-05 14:25:14 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4049362)[0;0m ERROR 12-05 14:25:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4049362)[0;0m ERROR 12-05 14:25:14 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4049362)[0;0m ERROR 12-05 14:25:14 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049362)[0;0m ERROR 12-05 14:25:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4049362)[0;0m ERROR 12-05 14:25:14 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4049362)[0;0m ERROR 12-05 14:25:14 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049362)[0;0m ERROR 12-05 14:25:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4049362)[0;0m ERROR 12-05 14:25:14 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4049362)[0;0m ERROR 12-05 14:25:14 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049362)[0;0m ERROR 12-05 14:25:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4049362)[0;0m ERROR 12-05 14:25:14 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4049362)[0;0m ERROR 12-05 14:25:14 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049362)[0;0m ERROR 12-05 14:25:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4049362)[0;0m ERROR 12-05 14:25:14 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4049362)[0;0m ERROR 12-05 14:25:14 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049362)[0;0m ERROR 12-05 14:25:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4049362)[0;0m ERROR 12-05 14:25:14 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4049362)[0;0m ERROR 12-05 14:25:14 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049362)[0;0m ERROR 12-05 14:25:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4049362)[0;0m ERROR 12-05 14:25:14 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4049362)[0;0m ERROR 12-05 14:25:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4049362)[0;0m ERROR 12-05 14:25:14 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4049362)[0;0m ERROR 12-05 14:25:14 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049362)[0;0m ERROR 12-05 14:25:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4049362)[0;0m ERROR 12-05 14:25:14 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4049362)[0;0m ERROR 12-05 14:25:14 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049362)[0;0m ERROR 12-05 14:25:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4049362)[0;0m ERROR 12-05 14:25:14 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4049362)[0;0m ERROR 12-05 14:25:14 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049362)[0;0m ERROR 12-05 14:25:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4049362)[0;0m ERROR 12-05 14:25:14 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4049362)[0;0m ERROR 12-05 14:25:14 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049362)[0;0m ERROR 12-05 14:25:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4049362)[0;0m ERROR 12-05 14:25:14 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4049362)[0;0m ERROR 12-05 14:25:14 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049362)[0;0m ERROR 12-05 14:25:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4049362)[0;0m ERROR 12-05 14:25:14 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4049362)[0;0m ERROR 12-05 14:25:14 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049362)[0;0m ERROR 12-05 14:25:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4049362)[0;0m ERROR 12-05 14:25:14 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4049362)[0;0m ERROR 12-05 14:25:14 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049362)[0;0m ERROR 12-05 14:25:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4049362)[0;0m ERROR 12-05 14:25:14 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4049362)[0;0m ERROR 12-05 14:25:14 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049362)[0;0m ERROR 12-05 14:25:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4049362)[0;0m ERROR 12-05 14:25:14 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4049362)[0;0m ERROR 12-05 14:25:14 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049362)[0;0m ERROR 12-05 14:25:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4049362)[0;0m ERROR 12-05 14:25:14 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4049362)[0;0m ERROR 12-05 14:25:14 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049362)[0;0m ERROR 12-05 14:25:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4049362)[0;0m ERROR 12-05 14:25:14 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4049362)[0;0m ERROR 12-05 14:25:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4049362)[0;0m ERROR 12-05 14:25:14 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4049362)[0;0m ERROR 12-05 14:25:14 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049362)[0;0m ERROR 12-05 14:25:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4049362)[0;0m ERROR 12-05 14:25:14 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4049362)[0;0m ERROR 12-05 14:25:14 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049362)[0;0m ERROR 12-05 14:25:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4049362)[0;0m ERROR 12-05 14:25:14 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4049362)[0;0m ERROR 12-05 14:25:14 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049362)[0;0m ERROR 12-05 14:25:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4049362)[0;0m ERROR 12-05 14:25:14 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4049362)[0;0m ERROR 12-05 14:25:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4049362)[0;0m ERROR 12-05 14:25:14 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4049362)[0;0m ERROR 12-05 14:25:14 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049362)[0;0m ERROR 12-05 14:25:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4049362)[0;0m ERROR 12-05 14:25:14 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4049362)[0;0m ERROR 12-05 14:25:14 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049362)[0;0m ERROR 12-05 14:25:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4049362)[0;0m ERROR 12-05 14:25:14 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4049362)[0;0m ERROR 12-05 14:25:14 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049362)[0;0m ERROR 12-05 14:25:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4049362)[0;0m ERROR 12-05 14:25:14 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4049362)[0;0m ERROR 12-05 14:25:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4049362)[0;0m ERROR 12-05 14:25:14 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4049362)[0;0m ERROR 12-05 14:25:14 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049362)[0;0m ERROR 12-05 14:25:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4049362)[0;0m ERROR 12-05 14:25:14 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4049362)[0;0m ERROR 12-05 14:25:14 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049362)[0;0m ERROR 12-05 14:25:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4049362)[0;0m ERROR 12-05 14:25:14 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4049362)[0;0m ERROR 12-05 14:25:14 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049362)[0;0m ERROR 12-05 14:25:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4049362)[0;0m ERROR 12-05 14:25:14 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4049362)[0;0m ERROR 12-05 14:25:14 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049362)[0;0m ERROR 12-05 14:25:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4049362)[0;0m ERROR 12-05 14:25:14 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4049362)[0;0m ERROR 12-05 14:25:14 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7fbf9c054bf0>' raised:
[1;36m(EngineCore_DP0 pid=4049362)[0;0m ERROR 12-05 14:25:14 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4049362)[0;0m ERROR 12-05 14:25:14 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4049362)[0;0m ERROR 12-05 14:25:14 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4049362)[0;0m ERROR 12-05 14:25:14 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 14:25:35 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 14:25:35 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 14:25:35 [model.py:1745] Using max model len 131072
INFO 12-05 14:25:35 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4049444)[0;0m INFO 12-05 14:25:43 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4049444)[0;0m INFO 12-05 14:25:44 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:50241 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4049444)[0;0m INFO 12-05 14:25:44 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4049444)[0;0m INFO 12-05 14:25:44 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4049444)[0;0m INFO 12-05 14:25:44 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4049444)[0;0m INFO 12-05 14:25:44 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4049444)[0;0m INFO 12-05 14:25:46 [default_loader.py:314] Loading weights took 0.88 seconds
[1;36m(EngineCore_DP0 pid=4049444)[0;0m INFO 12-05 14:25:46 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.387789 seconds
[1;36m(EngineCore_DP0 pid=4049444)[0;0m INFO 12-05 14:25:50 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4049444)[0;0m INFO 12-05 14:25:50 [backends.py:647] Dynamo bytecode transform time: 4.07 s
[1;36m(EngineCore_DP0 pid=4049444)[0;0m ERROR 12-05 14:25:51 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4049444)[0;0m ERROR 12-05 14:25:51 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4049444)[0;0m ERROR 12-05 14:25:51 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4049444)[0;0m ERROR 12-05 14:25:51 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4049444)[0;0m ERROR 12-05 14:25:51 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049444)[0;0m ERROR 12-05 14:25:51 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4049444)[0;0m ERROR 12-05 14:25:51 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4049444)[0;0m ERROR 12-05 14:25:51 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4049444)[0;0m ERROR 12-05 14:25:51 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4049444)[0;0m ERROR 12-05 14:25:51 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049444)[0;0m ERROR 12-05 14:25:51 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4049444)[0;0m ERROR 12-05 14:25:51 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4049444)[0;0m ERROR 12-05 14:25:51 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049444)[0;0m ERROR 12-05 14:25:51 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4049444)[0;0m ERROR 12-05 14:25:51 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4049444)[0;0m ERROR 12-05 14:25:51 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049444)[0;0m ERROR 12-05 14:25:51 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4049444)[0;0m ERROR 12-05 14:25:51 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4049444)[0;0m ERROR 12-05 14:25:51 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049444)[0;0m ERROR 12-05 14:25:51 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4049444)[0;0m ERROR 12-05 14:25:51 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4049444)[0;0m ERROR 12-05 14:25:51 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049444)[0;0m ERROR 12-05 14:25:51 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4049444)[0;0m ERROR 12-05 14:25:51 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4049444)[0;0m ERROR 12-05 14:25:51 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049444)[0;0m ERROR 12-05 14:25:51 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4049444)[0;0m ERROR 12-05 14:25:51 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4049444)[0;0m ERROR 12-05 14:25:51 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4049444)[0;0m ERROR 12-05 14:25:51 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4049444)[0;0m ERROR 12-05 14:25:51 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049444)[0;0m ERROR 12-05 14:25:51 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4049444)[0;0m ERROR 12-05 14:25:51 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4049444)[0;0m ERROR 12-05 14:25:51 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049444)[0;0m ERROR 12-05 14:25:51 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4049444)[0;0m ERROR 12-05 14:25:51 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4049444)[0;0m ERROR 12-05 14:25:51 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049444)[0;0m ERROR 12-05 14:25:51 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4049444)[0;0m ERROR 12-05 14:25:51 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4049444)[0;0m ERROR 12-05 14:25:51 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049444)[0;0m ERROR 12-05 14:25:51 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4049444)[0;0m ERROR 12-05 14:25:51 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4049444)[0;0m ERROR 12-05 14:25:51 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049444)[0;0m ERROR 12-05 14:25:51 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4049444)[0;0m ERROR 12-05 14:25:51 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4049444)[0;0m ERROR 12-05 14:25:51 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049444)[0;0m ERROR 12-05 14:25:51 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4049444)[0;0m ERROR 12-05 14:25:51 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4049444)[0;0m ERROR 12-05 14:25:51 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049444)[0;0m ERROR 12-05 14:25:51 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4049444)[0;0m ERROR 12-05 14:25:51 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4049444)[0;0m ERROR 12-05 14:25:51 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049444)[0;0m ERROR 12-05 14:25:51 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4049444)[0;0m ERROR 12-05 14:25:51 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4049444)[0;0m ERROR 12-05 14:25:51 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049444)[0;0m ERROR 12-05 14:25:51 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4049444)[0;0m ERROR 12-05 14:25:51 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4049444)[0;0m ERROR 12-05 14:25:51 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049444)[0;0m ERROR 12-05 14:25:51 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4049444)[0;0m ERROR 12-05 14:25:51 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4049444)[0;0m ERROR 12-05 14:25:51 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4049444)[0;0m ERROR 12-05 14:25:51 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4049444)[0;0m ERROR 12-05 14:25:51 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049444)[0;0m ERROR 12-05 14:25:51 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4049444)[0;0m ERROR 12-05 14:25:51 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4049444)[0;0m ERROR 12-05 14:25:51 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049444)[0;0m ERROR 12-05 14:25:51 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4049444)[0;0m ERROR 12-05 14:25:51 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4049444)[0;0m ERROR 12-05 14:25:51 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049444)[0;0m ERROR 12-05 14:25:51 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4049444)[0;0m ERROR 12-05 14:25:51 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4049444)[0;0m ERROR 12-05 14:25:51 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4049444)[0;0m ERROR 12-05 14:25:51 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4049444)[0;0m ERROR 12-05 14:25:51 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049444)[0;0m ERROR 12-05 14:25:51 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4049444)[0;0m ERROR 12-05 14:25:51 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4049444)[0;0m ERROR 12-05 14:25:51 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049444)[0;0m ERROR 12-05 14:25:51 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4049444)[0;0m ERROR 12-05 14:25:51 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4049444)[0;0m ERROR 12-05 14:25:51 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049444)[0;0m ERROR 12-05 14:25:51 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4049444)[0;0m ERROR 12-05 14:25:51 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4049444)[0;0m ERROR 12-05 14:25:51 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4049444)[0;0m ERROR 12-05 14:25:51 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4049444)[0;0m ERROR 12-05 14:25:51 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049444)[0;0m ERROR 12-05 14:25:51 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4049444)[0;0m ERROR 12-05 14:25:51 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4049444)[0;0m ERROR 12-05 14:25:51 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049444)[0;0m ERROR 12-05 14:25:51 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4049444)[0;0m ERROR 12-05 14:25:51 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4049444)[0;0m ERROR 12-05 14:25:51 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049444)[0;0m ERROR 12-05 14:25:51 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4049444)[0;0m ERROR 12-05 14:25:51 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4049444)[0;0m ERROR 12-05 14:25:51 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049444)[0;0m ERROR 12-05 14:25:51 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4049444)[0;0m ERROR 12-05 14:25:51 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4049444)[0;0m ERROR 12-05 14:25:51 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7f001d6974d0>' raised:
[1;36m(EngineCore_DP0 pid=4049444)[0;0m ERROR 12-05 14:25:51 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4049444)[0;0m ERROR 12-05 14:25:51 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4049444)[0;0m ERROR 12-05 14:25:51 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4049444)[0;0m ERROR 12-05 14:25:51 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 14:26:12 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 14:26:12 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 14:26:12 [model.py:1745] Using max model len 131072
INFO 12-05 14:26:12 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4049644)[0;0m INFO 12-05 14:26:19 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4049644)[0;0m INFO 12-05 14:26:20 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:59221 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4049644)[0;0m INFO 12-05 14:26:20 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4049644)[0;0m INFO 12-05 14:26:21 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4049644)[0;0m INFO 12-05 14:26:21 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4049644)[0;0m INFO 12-05 14:26:21 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4049644)[0;0m INFO 12-05 14:26:22 [default_loader.py:314] Loading weights took 0.88 seconds
[1;36m(EngineCore_DP0 pid=4049644)[0;0m INFO 12-05 14:26:23 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.422137 seconds
[1;36m(EngineCore_DP0 pid=4049644)[0;0m INFO 12-05 14:26:27 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4049644)[0;0m INFO 12-05 14:26:27 [backends.py:647] Dynamo bytecode transform time: 3.68 s
[1;36m(EngineCore_DP0 pid=4049644)[0;0m ERROR 12-05 14:26:27 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4049644)[0;0m ERROR 12-05 14:26:27 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4049644)[0;0m ERROR 12-05 14:26:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4049644)[0;0m ERROR 12-05 14:26:27 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4049644)[0;0m ERROR 12-05 14:26:27 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049644)[0;0m ERROR 12-05 14:26:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4049644)[0;0m ERROR 12-05 14:26:27 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4049644)[0;0m ERROR 12-05 14:26:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4049644)[0;0m ERROR 12-05 14:26:27 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4049644)[0;0m ERROR 12-05 14:26:27 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049644)[0;0m ERROR 12-05 14:26:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4049644)[0;0m ERROR 12-05 14:26:27 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4049644)[0;0m ERROR 12-05 14:26:27 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049644)[0;0m ERROR 12-05 14:26:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4049644)[0;0m ERROR 12-05 14:26:27 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4049644)[0;0m ERROR 12-05 14:26:27 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049644)[0;0m ERROR 12-05 14:26:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4049644)[0;0m ERROR 12-05 14:26:27 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4049644)[0;0m ERROR 12-05 14:26:27 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049644)[0;0m ERROR 12-05 14:26:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4049644)[0;0m ERROR 12-05 14:26:27 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4049644)[0;0m ERROR 12-05 14:26:27 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049644)[0;0m ERROR 12-05 14:26:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4049644)[0;0m ERROR 12-05 14:26:27 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4049644)[0;0m ERROR 12-05 14:26:27 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049644)[0;0m ERROR 12-05 14:26:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4049644)[0;0m ERROR 12-05 14:26:27 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4049644)[0;0m ERROR 12-05 14:26:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4049644)[0;0m ERROR 12-05 14:26:27 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4049644)[0;0m ERROR 12-05 14:26:27 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049644)[0;0m ERROR 12-05 14:26:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4049644)[0;0m ERROR 12-05 14:26:27 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4049644)[0;0m ERROR 12-05 14:26:27 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049644)[0;0m ERROR 12-05 14:26:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4049644)[0;0m ERROR 12-05 14:26:27 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4049644)[0;0m ERROR 12-05 14:26:27 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049644)[0;0m ERROR 12-05 14:26:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4049644)[0;0m ERROR 12-05 14:26:27 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4049644)[0;0m ERROR 12-05 14:26:27 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049644)[0;0m ERROR 12-05 14:26:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4049644)[0;0m ERROR 12-05 14:26:27 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4049644)[0;0m ERROR 12-05 14:26:27 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049644)[0;0m ERROR 12-05 14:26:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4049644)[0;0m ERROR 12-05 14:26:27 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4049644)[0;0m ERROR 12-05 14:26:27 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049644)[0;0m ERROR 12-05 14:26:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4049644)[0;0m ERROR 12-05 14:26:27 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4049644)[0;0m ERROR 12-05 14:26:27 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049644)[0;0m ERROR 12-05 14:26:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4049644)[0;0m ERROR 12-05 14:26:27 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4049644)[0;0m ERROR 12-05 14:26:27 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049644)[0;0m ERROR 12-05 14:26:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4049644)[0;0m ERROR 12-05 14:26:27 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4049644)[0;0m ERROR 12-05 14:26:27 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049644)[0;0m ERROR 12-05 14:26:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4049644)[0;0m ERROR 12-05 14:26:27 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4049644)[0;0m ERROR 12-05 14:26:27 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049644)[0;0m ERROR 12-05 14:26:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4049644)[0;0m ERROR 12-05 14:26:27 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4049644)[0;0m ERROR 12-05 14:26:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4049644)[0;0m ERROR 12-05 14:26:27 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4049644)[0;0m ERROR 12-05 14:26:27 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049644)[0;0m ERROR 12-05 14:26:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4049644)[0;0m ERROR 12-05 14:26:27 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4049644)[0;0m ERROR 12-05 14:26:27 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049644)[0;0m ERROR 12-05 14:26:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4049644)[0;0m ERROR 12-05 14:26:27 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4049644)[0;0m ERROR 12-05 14:26:27 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049644)[0;0m ERROR 12-05 14:26:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4049644)[0;0m ERROR 12-05 14:26:27 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4049644)[0;0m ERROR 12-05 14:26:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4049644)[0;0m ERROR 12-05 14:26:27 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4049644)[0;0m ERROR 12-05 14:26:27 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049644)[0;0m ERROR 12-05 14:26:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4049644)[0;0m ERROR 12-05 14:26:27 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4049644)[0;0m ERROR 12-05 14:26:27 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049644)[0;0m ERROR 12-05 14:26:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4049644)[0;0m ERROR 12-05 14:26:27 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4049644)[0;0m ERROR 12-05 14:26:27 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049644)[0;0m ERROR 12-05 14:26:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4049644)[0;0m ERROR 12-05 14:26:27 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4049644)[0;0m ERROR 12-05 14:26:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4049644)[0;0m ERROR 12-05 14:26:27 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4049644)[0;0m ERROR 12-05 14:26:27 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049644)[0;0m ERROR 12-05 14:26:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4049644)[0;0m ERROR 12-05 14:26:27 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4049644)[0;0m ERROR 12-05 14:26:27 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049644)[0;0m ERROR 12-05 14:26:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4049644)[0;0m ERROR 12-05 14:26:27 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4049644)[0;0m ERROR 12-05 14:26:27 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049644)[0;0m ERROR 12-05 14:26:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4049644)[0;0m ERROR 12-05 14:26:27 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4049644)[0;0m ERROR 12-05 14:26:27 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049644)[0;0m ERROR 12-05 14:26:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4049644)[0;0m ERROR 12-05 14:26:27 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4049644)[0;0m ERROR 12-05 14:26:27 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7fdf301f1100>' raised:
[1;36m(EngineCore_DP0 pid=4049644)[0;0m ERROR 12-05 14:26:27 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4049644)[0;0m ERROR 12-05 14:26:27 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4049644)[0;0m ERROR 12-05 14:26:27 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4049644)[0;0m ERROR 12-05 14:26:27 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 14:26:48 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 14:26:49 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 14:26:49 [model.py:1745] Using max model len 131072
INFO 12-05 14:26:49 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4049733)[0;0m INFO 12-05 14:26:56 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4049733)[0;0m INFO 12-05 14:26:57 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:38305 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4049733)[0;0m INFO 12-05 14:26:57 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4049733)[0;0m INFO 12-05 14:26:57 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4049733)[0;0m INFO 12-05 14:26:58 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4049733)[0;0m INFO 12-05 14:26:58 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4049733)[0;0m INFO 12-05 14:26:59 [default_loader.py:314] Loading weights took 0.87 seconds
[1;36m(EngineCore_DP0 pid=4049733)[0;0m INFO 12-05 14:26:59 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.417374 seconds
[1;36m(EngineCore_DP0 pid=4049733)[0;0m INFO 12-05 14:27:03 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4049733)[0;0m INFO 12-05 14:27:03 [backends.py:647] Dynamo bytecode transform time: 3.68 s
[1;36m(EngineCore_DP0 pid=4049733)[0;0m ERROR 12-05 14:27:04 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4049733)[0;0m ERROR 12-05 14:27:04 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4049733)[0;0m ERROR 12-05 14:27:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4049733)[0;0m ERROR 12-05 14:27:04 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4049733)[0;0m ERROR 12-05 14:27:04 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049733)[0;0m ERROR 12-05 14:27:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4049733)[0;0m ERROR 12-05 14:27:04 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4049733)[0;0m ERROR 12-05 14:27:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4049733)[0;0m ERROR 12-05 14:27:04 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4049733)[0;0m ERROR 12-05 14:27:04 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049733)[0;0m ERROR 12-05 14:27:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4049733)[0;0m ERROR 12-05 14:27:04 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4049733)[0;0m ERROR 12-05 14:27:04 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049733)[0;0m ERROR 12-05 14:27:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4049733)[0;0m ERROR 12-05 14:27:04 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4049733)[0;0m ERROR 12-05 14:27:04 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049733)[0;0m ERROR 12-05 14:27:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4049733)[0;0m ERROR 12-05 14:27:04 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4049733)[0;0m ERROR 12-05 14:27:04 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049733)[0;0m ERROR 12-05 14:27:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4049733)[0;0m ERROR 12-05 14:27:04 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4049733)[0;0m ERROR 12-05 14:27:04 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049733)[0;0m ERROR 12-05 14:27:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4049733)[0;0m ERROR 12-05 14:27:04 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4049733)[0;0m ERROR 12-05 14:27:04 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049733)[0;0m ERROR 12-05 14:27:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4049733)[0;0m ERROR 12-05 14:27:04 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4049733)[0;0m ERROR 12-05 14:27:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4049733)[0;0m ERROR 12-05 14:27:04 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4049733)[0;0m ERROR 12-05 14:27:04 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049733)[0;0m ERROR 12-05 14:27:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4049733)[0;0m ERROR 12-05 14:27:04 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4049733)[0;0m ERROR 12-05 14:27:04 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049733)[0;0m ERROR 12-05 14:27:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4049733)[0;0m ERROR 12-05 14:27:04 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4049733)[0;0m ERROR 12-05 14:27:04 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049733)[0;0m ERROR 12-05 14:27:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4049733)[0;0m ERROR 12-05 14:27:04 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4049733)[0;0m ERROR 12-05 14:27:04 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049733)[0;0m ERROR 12-05 14:27:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4049733)[0;0m ERROR 12-05 14:27:04 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4049733)[0;0m ERROR 12-05 14:27:04 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049733)[0;0m ERROR 12-05 14:27:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4049733)[0;0m ERROR 12-05 14:27:04 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4049733)[0;0m ERROR 12-05 14:27:04 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049733)[0;0m ERROR 12-05 14:27:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4049733)[0;0m ERROR 12-05 14:27:04 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4049733)[0;0m ERROR 12-05 14:27:04 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049733)[0;0m ERROR 12-05 14:27:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4049733)[0;0m ERROR 12-05 14:27:04 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4049733)[0;0m ERROR 12-05 14:27:04 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049733)[0;0m ERROR 12-05 14:27:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4049733)[0;0m ERROR 12-05 14:27:04 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4049733)[0;0m ERROR 12-05 14:27:04 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049733)[0;0m ERROR 12-05 14:27:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4049733)[0;0m ERROR 12-05 14:27:04 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4049733)[0;0m ERROR 12-05 14:27:04 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049733)[0;0m ERROR 12-05 14:27:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4049733)[0;0m ERROR 12-05 14:27:04 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4049733)[0;0m ERROR 12-05 14:27:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4049733)[0;0m ERROR 12-05 14:27:04 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4049733)[0;0m ERROR 12-05 14:27:04 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049733)[0;0m ERROR 12-05 14:27:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4049733)[0;0m ERROR 12-05 14:27:04 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4049733)[0;0m ERROR 12-05 14:27:04 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049733)[0;0m ERROR 12-05 14:27:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4049733)[0;0m ERROR 12-05 14:27:04 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4049733)[0;0m ERROR 12-05 14:27:04 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049733)[0;0m ERROR 12-05 14:27:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4049733)[0;0m ERROR 12-05 14:27:04 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4049733)[0;0m ERROR 12-05 14:27:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4049733)[0;0m ERROR 12-05 14:27:04 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4049733)[0;0m ERROR 12-05 14:27:04 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049733)[0;0m ERROR 12-05 14:27:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4049733)[0;0m ERROR 12-05 14:27:04 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4049733)[0;0m ERROR 12-05 14:27:04 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049733)[0;0m ERROR 12-05 14:27:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4049733)[0;0m ERROR 12-05 14:27:04 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4049733)[0;0m ERROR 12-05 14:27:04 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049733)[0;0m ERROR 12-05 14:27:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4049733)[0;0m ERROR 12-05 14:27:04 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4049733)[0;0m ERROR 12-05 14:27:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4049733)[0;0m ERROR 12-05 14:27:04 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4049733)[0;0m ERROR 12-05 14:27:04 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049733)[0;0m ERROR 12-05 14:27:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4049733)[0;0m ERROR 12-05 14:27:04 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4049733)[0;0m ERROR 12-05 14:27:04 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049733)[0;0m ERROR 12-05 14:27:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4049733)[0;0m ERROR 12-05 14:27:04 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4049733)[0;0m ERROR 12-05 14:27:04 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049733)[0;0m ERROR 12-05 14:27:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4049733)[0;0m ERROR 12-05 14:27:04 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4049733)[0;0m ERROR 12-05 14:27:04 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049733)[0;0m ERROR 12-05 14:27:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4049733)[0;0m ERROR 12-05 14:27:04 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4049733)[0;0m ERROR 12-05 14:27:04 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7feec8252c00>' raised:
[1;36m(EngineCore_DP0 pid=4049733)[0;0m ERROR 12-05 14:27:04 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4049733)[0;0m ERROR 12-05 14:27:04 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4049733)[0;0m ERROR 12-05 14:27:04 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4049733)[0;0m ERROR 12-05 14:27:04 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 14:27:25 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 14:27:25 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 14:27:25 [model.py:1745] Using max model len 131072
INFO 12-05 14:27:25 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4049811)[0;0m INFO 12-05 14:27:32 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4049811)[0;0m INFO 12-05 14:27:33 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:46879 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4049811)[0;0m INFO 12-05 14:27:33 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4049811)[0;0m INFO 12-05 14:27:34 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4049811)[0;0m INFO 12-05 14:27:34 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4049811)[0;0m INFO 12-05 14:27:34 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4049811)[0;0m INFO 12-05 14:27:35 [default_loader.py:314] Loading weights took 0.88 seconds
[1;36m(EngineCore_DP0 pid=4049811)[0;0m INFO 12-05 14:27:36 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.415792 seconds
[1;36m(EngineCore_DP0 pid=4049811)[0;0m INFO 12-05 14:27:40 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4049811)[0;0m INFO 12-05 14:27:40 [backends.py:647] Dynamo bytecode transform time: 3.68 s
[1;36m(EngineCore_DP0 pid=4049811)[0;0m ERROR 12-05 14:27:40 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4049811)[0;0m ERROR 12-05 14:27:40 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4049811)[0;0m ERROR 12-05 14:27:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4049811)[0;0m ERROR 12-05 14:27:40 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4049811)[0;0m ERROR 12-05 14:27:40 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049811)[0;0m ERROR 12-05 14:27:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4049811)[0;0m ERROR 12-05 14:27:40 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4049811)[0;0m ERROR 12-05 14:27:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4049811)[0;0m ERROR 12-05 14:27:40 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4049811)[0;0m ERROR 12-05 14:27:40 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049811)[0;0m ERROR 12-05 14:27:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4049811)[0;0m ERROR 12-05 14:27:40 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4049811)[0;0m ERROR 12-05 14:27:40 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049811)[0;0m ERROR 12-05 14:27:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4049811)[0;0m ERROR 12-05 14:27:40 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4049811)[0;0m ERROR 12-05 14:27:40 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049811)[0;0m ERROR 12-05 14:27:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4049811)[0;0m ERROR 12-05 14:27:40 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4049811)[0;0m ERROR 12-05 14:27:40 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049811)[0;0m ERROR 12-05 14:27:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4049811)[0;0m ERROR 12-05 14:27:40 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4049811)[0;0m ERROR 12-05 14:27:40 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049811)[0;0m ERROR 12-05 14:27:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4049811)[0;0m ERROR 12-05 14:27:40 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4049811)[0;0m ERROR 12-05 14:27:40 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049811)[0;0m ERROR 12-05 14:27:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4049811)[0;0m ERROR 12-05 14:27:40 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4049811)[0;0m ERROR 12-05 14:27:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4049811)[0;0m ERROR 12-05 14:27:40 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4049811)[0;0m ERROR 12-05 14:27:40 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049811)[0;0m ERROR 12-05 14:27:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4049811)[0;0m ERROR 12-05 14:27:40 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4049811)[0;0m ERROR 12-05 14:27:40 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049811)[0;0m ERROR 12-05 14:27:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4049811)[0;0m ERROR 12-05 14:27:40 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4049811)[0;0m ERROR 12-05 14:27:40 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049811)[0;0m ERROR 12-05 14:27:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4049811)[0;0m ERROR 12-05 14:27:40 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4049811)[0;0m ERROR 12-05 14:27:40 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049811)[0;0m ERROR 12-05 14:27:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4049811)[0;0m ERROR 12-05 14:27:40 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4049811)[0;0m ERROR 12-05 14:27:40 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049811)[0;0m ERROR 12-05 14:27:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4049811)[0;0m ERROR 12-05 14:27:40 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4049811)[0;0m ERROR 12-05 14:27:40 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049811)[0;0m ERROR 12-05 14:27:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4049811)[0;0m ERROR 12-05 14:27:40 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4049811)[0;0m ERROR 12-05 14:27:40 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049811)[0;0m ERROR 12-05 14:27:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4049811)[0;0m ERROR 12-05 14:27:40 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4049811)[0;0m ERROR 12-05 14:27:40 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049811)[0;0m ERROR 12-05 14:27:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4049811)[0;0m ERROR 12-05 14:27:40 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4049811)[0;0m ERROR 12-05 14:27:40 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049811)[0;0m ERROR 12-05 14:27:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4049811)[0;0m ERROR 12-05 14:27:40 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4049811)[0;0m ERROR 12-05 14:27:40 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049811)[0;0m ERROR 12-05 14:27:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4049811)[0;0m ERROR 12-05 14:27:40 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4049811)[0;0m ERROR 12-05 14:27:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4049811)[0;0m ERROR 12-05 14:27:40 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4049811)[0;0m ERROR 12-05 14:27:40 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049811)[0;0m ERROR 12-05 14:27:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4049811)[0;0m ERROR 12-05 14:27:40 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4049811)[0;0m ERROR 12-05 14:27:40 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049811)[0;0m ERROR 12-05 14:27:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4049811)[0;0m ERROR 12-05 14:27:40 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4049811)[0;0m ERROR 12-05 14:27:40 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049811)[0;0m ERROR 12-05 14:27:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4049811)[0;0m ERROR 12-05 14:27:40 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4049811)[0;0m ERROR 12-05 14:27:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4049811)[0;0m ERROR 12-05 14:27:40 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4049811)[0;0m ERROR 12-05 14:27:40 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049811)[0;0m ERROR 12-05 14:27:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4049811)[0;0m ERROR 12-05 14:27:40 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4049811)[0;0m ERROR 12-05 14:27:40 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049811)[0;0m ERROR 12-05 14:27:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4049811)[0;0m ERROR 12-05 14:27:40 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4049811)[0;0m ERROR 12-05 14:27:40 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049811)[0;0m ERROR 12-05 14:27:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4049811)[0;0m ERROR 12-05 14:27:40 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4049811)[0;0m ERROR 12-05 14:27:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4049811)[0;0m ERROR 12-05 14:27:40 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4049811)[0;0m ERROR 12-05 14:27:40 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049811)[0;0m ERROR 12-05 14:27:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4049811)[0;0m ERROR 12-05 14:27:40 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4049811)[0;0m ERROR 12-05 14:27:40 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049811)[0;0m ERROR 12-05 14:27:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4049811)[0;0m ERROR 12-05 14:27:40 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4049811)[0;0m ERROR 12-05 14:27:40 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049811)[0;0m ERROR 12-05 14:27:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4049811)[0;0m ERROR 12-05 14:27:40 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4049811)[0;0m ERROR 12-05 14:27:40 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049811)[0;0m ERROR 12-05 14:27:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4049811)[0;0m ERROR 12-05 14:27:40 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4049811)[0;0m ERROR 12-05 14:27:40 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7fb5601f8980>' raised:
[1;36m(EngineCore_DP0 pid=4049811)[0;0m ERROR 12-05 14:27:40 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4049811)[0;0m ERROR 12-05 14:27:40 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4049811)[0;0m ERROR 12-05 14:27:40 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4049811)[0;0m ERROR 12-05 14:27:40 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 14:28:01 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 14:28:01 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 14:28:01 [model.py:1745] Using max model len 131072
INFO 12-05 14:28:01 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4049898)[0;0m INFO 12-05 14:28:09 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4049898)[0;0m INFO 12-05 14:28:10 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:58107 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4049898)[0;0m INFO 12-05 14:28:10 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4049898)[0;0m INFO 12-05 14:28:10 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4049898)[0;0m INFO 12-05 14:28:11 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4049898)[0;0m INFO 12-05 14:28:11 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4049898)[0;0m INFO 12-05 14:28:12 [default_loader.py:314] Loading weights took 0.87 seconds
[1;36m(EngineCore_DP0 pid=4049898)[0;0m INFO 12-05 14:28:12 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.377984 seconds
[1;36m(EngineCore_DP0 pid=4049898)[0;0m INFO 12-05 14:28:16 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4049898)[0;0m INFO 12-05 14:28:16 [backends.py:647] Dynamo bytecode transform time: 3.63 s
[1;36m(EngineCore_DP0 pid=4049898)[0;0m ERROR 12-05 14:28:16 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4049898)[0;0m ERROR 12-05 14:28:16 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4049898)[0;0m ERROR 12-05 14:28:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4049898)[0;0m ERROR 12-05 14:28:16 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4049898)[0;0m ERROR 12-05 14:28:16 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049898)[0;0m ERROR 12-05 14:28:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4049898)[0;0m ERROR 12-05 14:28:16 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4049898)[0;0m ERROR 12-05 14:28:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4049898)[0;0m ERROR 12-05 14:28:16 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4049898)[0;0m ERROR 12-05 14:28:16 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049898)[0;0m ERROR 12-05 14:28:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4049898)[0;0m ERROR 12-05 14:28:16 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4049898)[0;0m ERROR 12-05 14:28:16 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049898)[0;0m ERROR 12-05 14:28:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4049898)[0;0m ERROR 12-05 14:28:16 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4049898)[0;0m ERROR 12-05 14:28:16 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049898)[0;0m ERROR 12-05 14:28:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4049898)[0;0m ERROR 12-05 14:28:16 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4049898)[0;0m ERROR 12-05 14:28:16 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049898)[0;0m ERROR 12-05 14:28:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4049898)[0;0m ERROR 12-05 14:28:16 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4049898)[0;0m ERROR 12-05 14:28:16 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049898)[0;0m ERROR 12-05 14:28:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4049898)[0;0m ERROR 12-05 14:28:16 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4049898)[0;0m ERROR 12-05 14:28:16 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049898)[0;0m ERROR 12-05 14:28:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4049898)[0;0m ERROR 12-05 14:28:16 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4049898)[0;0m ERROR 12-05 14:28:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4049898)[0;0m ERROR 12-05 14:28:16 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4049898)[0;0m ERROR 12-05 14:28:16 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049898)[0;0m ERROR 12-05 14:28:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4049898)[0;0m ERROR 12-05 14:28:16 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4049898)[0;0m ERROR 12-05 14:28:16 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049898)[0;0m ERROR 12-05 14:28:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4049898)[0;0m ERROR 12-05 14:28:16 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4049898)[0;0m ERROR 12-05 14:28:16 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049898)[0;0m ERROR 12-05 14:28:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4049898)[0;0m ERROR 12-05 14:28:16 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4049898)[0;0m ERROR 12-05 14:28:16 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049898)[0;0m ERROR 12-05 14:28:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4049898)[0;0m ERROR 12-05 14:28:16 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4049898)[0;0m ERROR 12-05 14:28:16 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049898)[0;0m ERROR 12-05 14:28:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4049898)[0;0m ERROR 12-05 14:28:16 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4049898)[0;0m ERROR 12-05 14:28:16 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049898)[0;0m ERROR 12-05 14:28:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4049898)[0;0m ERROR 12-05 14:28:16 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4049898)[0;0m ERROR 12-05 14:28:16 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049898)[0;0m ERROR 12-05 14:28:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4049898)[0;0m ERROR 12-05 14:28:16 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4049898)[0;0m ERROR 12-05 14:28:16 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049898)[0;0m ERROR 12-05 14:28:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4049898)[0;0m ERROR 12-05 14:28:16 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4049898)[0;0m ERROR 12-05 14:28:16 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049898)[0;0m ERROR 12-05 14:28:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4049898)[0;0m ERROR 12-05 14:28:16 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4049898)[0;0m ERROR 12-05 14:28:16 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049898)[0;0m ERROR 12-05 14:28:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4049898)[0;0m ERROR 12-05 14:28:16 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4049898)[0;0m ERROR 12-05 14:28:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4049898)[0;0m ERROR 12-05 14:28:16 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4049898)[0;0m ERROR 12-05 14:28:16 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049898)[0;0m ERROR 12-05 14:28:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4049898)[0;0m ERROR 12-05 14:28:16 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4049898)[0;0m ERROR 12-05 14:28:16 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049898)[0;0m ERROR 12-05 14:28:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4049898)[0;0m ERROR 12-05 14:28:16 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4049898)[0;0m ERROR 12-05 14:28:16 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049898)[0;0m ERROR 12-05 14:28:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4049898)[0;0m ERROR 12-05 14:28:16 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4049898)[0;0m ERROR 12-05 14:28:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4049898)[0;0m ERROR 12-05 14:28:16 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4049898)[0;0m ERROR 12-05 14:28:16 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049898)[0;0m ERROR 12-05 14:28:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4049898)[0;0m ERROR 12-05 14:28:16 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4049898)[0;0m ERROR 12-05 14:28:16 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049898)[0;0m ERROR 12-05 14:28:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4049898)[0;0m ERROR 12-05 14:28:16 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4049898)[0;0m ERROR 12-05 14:28:16 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049898)[0;0m ERROR 12-05 14:28:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4049898)[0;0m ERROR 12-05 14:28:16 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4049898)[0;0m ERROR 12-05 14:28:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4049898)[0;0m ERROR 12-05 14:28:16 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4049898)[0;0m ERROR 12-05 14:28:16 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049898)[0;0m ERROR 12-05 14:28:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4049898)[0;0m ERROR 12-05 14:28:16 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4049898)[0;0m ERROR 12-05 14:28:16 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049898)[0;0m ERROR 12-05 14:28:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4049898)[0;0m ERROR 12-05 14:28:16 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4049898)[0;0m ERROR 12-05 14:28:16 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049898)[0;0m ERROR 12-05 14:28:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4049898)[0;0m ERROR 12-05 14:28:16 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4049898)[0;0m ERROR 12-05 14:28:16 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049898)[0;0m ERROR 12-05 14:28:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4049898)[0;0m ERROR 12-05 14:28:16 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4049898)[0;0m ERROR 12-05 14:28:16 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7fc94c131100>' raised:
[1;36m(EngineCore_DP0 pid=4049898)[0;0m ERROR 12-05 14:28:16 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4049898)[0;0m ERROR 12-05 14:28:16 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4049898)[0;0m ERROR 12-05 14:28:16 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4049898)[0;0m ERROR 12-05 14:28:16 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 14:28:38 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 14:28:38 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 14:28:38 [model.py:1745] Using max model len 131072
INFO 12-05 14:28:38 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4049979)[0;0m INFO 12-05 14:28:45 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4049979)[0;0m INFO 12-05 14:28:46 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:57403 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4049979)[0;0m INFO 12-05 14:28:46 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4049979)[0;0m INFO 12-05 14:28:46 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4049979)[0;0m INFO 12-05 14:28:47 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4049979)[0;0m INFO 12-05 14:28:47 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4049979)[0;0m INFO 12-05 14:28:48 [default_loader.py:314] Loading weights took 0.88 seconds
[1;36m(EngineCore_DP0 pid=4049979)[0;0m INFO 12-05 14:28:49 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.410432 seconds
[1;36m(EngineCore_DP0 pid=4049979)[0;0m INFO 12-05 14:28:52 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4049979)[0;0m INFO 12-05 14:28:52 [backends.py:647] Dynamo bytecode transform time: 3.63 s
[1;36m(EngineCore_DP0 pid=4049979)[0;0m ERROR 12-05 14:28:53 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4049979)[0;0m ERROR 12-05 14:28:53 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4049979)[0;0m ERROR 12-05 14:28:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4049979)[0;0m ERROR 12-05 14:28:53 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4049979)[0;0m ERROR 12-05 14:28:53 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049979)[0;0m ERROR 12-05 14:28:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4049979)[0;0m ERROR 12-05 14:28:53 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4049979)[0;0m ERROR 12-05 14:28:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4049979)[0;0m ERROR 12-05 14:28:53 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4049979)[0;0m ERROR 12-05 14:28:53 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049979)[0;0m ERROR 12-05 14:28:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4049979)[0;0m ERROR 12-05 14:28:53 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4049979)[0;0m ERROR 12-05 14:28:53 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049979)[0;0m ERROR 12-05 14:28:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4049979)[0;0m ERROR 12-05 14:28:53 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4049979)[0;0m ERROR 12-05 14:28:53 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049979)[0;0m ERROR 12-05 14:28:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4049979)[0;0m ERROR 12-05 14:28:53 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4049979)[0;0m ERROR 12-05 14:28:53 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049979)[0;0m ERROR 12-05 14:28:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4049979)[0;0m ERROR 12-05 14:28:53 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4049979)[0;0m ERROR 12-05 14:28:53 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049979)[0;0m ERROR 12-05 14:28:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4049979)[0;0m ERROR 12-05 14:28:53 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4049979)[0;0m ERROR 12-05 14:28:53 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049979)[0;0m ERROR 12-05 14:28:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4049979)[0;0m ERROR 12-05 14:28:53 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4049979)[0;0m ERROR 12-05 14:28:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4049979)[0;0m ERROR 12-05 14:28:53 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4049979)[0;0m ERROR 12-05 14:28:53 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049979)[0;0m ERROR 12-05 14:28:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4049979)[0;0m ERROR 12-05 14:28:53 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4049979)[0;0m ERROR 12-05 14:28:53 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049979)[0;0m ERROR 12-05 14:28:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4049979)[0;0m ERROR 12-05 14:28:53 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4049979)[0;0m ERROR 12-05 14:28:53 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049979)[0;0m ERROR 12-05 14:28:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4049979)[0;0m ERROR 12-05 14:28:53 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4049979)[0;0m ERROR 12-05 14:28:53 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049979)[0;0m ERROR 12-05 14:28:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4049979)[0;0m ERROR 12-05 14:28:53 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4049979)[0;0m ERROR 12-05 14:28:53 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049979)[0;0m ERROR 12-05 14:28:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4049979)[0;0m ERROR 12-05 14:28:53 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4049979)[0;0m ERROR 12-05 14:28:53 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049979)[0;0m ERROR 12-05 14:28:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4049979)[0;0m ERROR 12-05 14:28:53 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4049979)[0;0m ERROR 12-05 14:28:53 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049979)[0;0m ERROR 12-05 14:28:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4049979)[0;0m ERROR 12-05 14:28:53 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4049979)[0;0m ERROR 12-05 14:28:53 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049979)[0;0m ERROR 12-05 14:28:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4049979)[0;0m ERROR 12-05 14:28:53 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4049979)[0;0m ERROR 12-05 14:28:53 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049979)[0;0m ERROR 12-05 14:28:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4049979)[0;0m ERROR 12-05 14:28:53 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4049979)[0;0m ERROR 12-05 14:28:53 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049979)[0;0m ERROR 12-05 14:28:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4049979)[0;0m ERROR 12-05 14:28:53 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4049979)[0;0m ERROR 12-05 14:28:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4049979)[0;0m ERROR 12-05 14:28:53 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4049979)[0;0m ERROR 12-05 14:28:53 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049979)[0;0m ERROR 12-05 14:28:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4049979)[0;0m ERROR 12-05 14:28:53 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4049979)[0;0m ERROR 12-05 14:28:53 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049979)[0;0m ERROR 12-05 14:28:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4049979)[0;0m ERROR 12-05 14:28:53 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4049979)[0;0m ERROR 12-05 14:28:53 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049979)[0;0m ERROR 12-05 14:28:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4049979)[0;0m ERROR 12-05 14:28:53 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4049979)[0;0m ERROR 12-05 14:28:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4049979)[0;0m ERROR 12-05 14:28:53 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4049979)[0;0m ERROR 12-05 14:28:53 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049979)[0;0m ERROR 12-05 14:28:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4049979)[0;0m ERROR 12-05 14:28:53 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4049979)[0;0m ERROR 12-05 14:28:53 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049979)[0;0m ERROR 12-05 14:28:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4049979)[0;0m ERROR 12-05 14:28:53 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4049979)[0;0m ERROR 12-05 14:28:53 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049979)[0;0m ERROR 12-05 14:28:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4049979)[0;0m ERROR 12-05 14:28:53 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4049979)[0;0m ERROR 12-05 14:28:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4049979)[0;0m ERROR 12-05 14:28:53 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4049979)[0;0m ERROR 12-05 14:28:53 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049979)[0;0m ERROR 12-05 14:28:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4049979)[0;0m ERROR 12-05 14:28:53 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4049979)[0;0m ERROR 12-05 14:28:53 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049979)[0;0m ERROR 12-05 14:28:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4049979)[0;0m ERROR 12-05 14:28:53 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4049979)[0;0m ERROR 12-05 14:28:53 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049979)[0;0m ERROR 12-05 14:28:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4049979)[0;0m ERROR 12-05 14:28:53 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4049979)[0;0m ERROR 12-05 14:28:53 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4049979)[0;0m ERROR 12-05 14:28:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4049979)[0;0m ERROR 12-05 14:28:53 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4049979)[0;0m ERROR 12-05 14:28:53 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7f669c389550>' raised:
[1;36m(EngineCore_DP0 pid=4049979)[0;0m ERROR 12-05 14:28:53 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4049979)[0;0m ERROR 12-05 14:28:53 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4049979)[0;0m ERROR 12-05 14:28:53 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4049979)[0;0m ERROR 12-05 14:28:53 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 14:29:14 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 14:29:14 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 14:29:14 [model.py:1745] Using max model len 131072
INFO 12-05 14:29:14 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4050066)[0;0m INFO 12-05 14:29:22 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4050066)[0;0m INFO 12-05 14:29:23 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:44257 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4050066)[0;0m INFO 12-05 14:29:23 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4050066)[0;0m INFO 12-05 14:29:23 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4050066)[0;0m INFO 12-05 14:29:24 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4050066)[0;0m INFO 12-05 14:29:24 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4050066)[0;0m INFO 12-05 14:29:25 [default_loader.py:314] Loading weights took 0.87 seconds
[1;36m(EngineCore_DP0 pid=4050066)[0;0m INFO 12-05 14:29:25 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.415603 seconds
[1;36m(EngineCore_DP0 pid=4050066)[0;0m INFO 12-05 14:29:29 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4050066)[0;0m INFO 12-05 14:29:29 [backends.py:647] Dynamo bytecode transform time: 3.87 s
[1;36m(EngineCore_DP0 pid=4050066)[0;0m ERROR 12-05 14:29:30 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4050066)[0;0m ERROR 12-05 14:29:30 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4050066)[0;0m ERROR 12-05 14:29:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4050066)[0;0m ERROR 12-05 14:29:30 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4050066)[0;0m ERROR 12-05 14:29:30 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050066)[0;0m ERROR 12-05 14:29:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4050066)[0;0m ERROR 12-05 14:29:30 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4050066)[0;0m ERROR 12-05 14:29:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4050066)[0;0m ERROR 12-05 14:29:30 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4050066)[0;0m ERROR 12-05 14:29:30 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050066)[0;0m ERROR 12-05 14:29:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4050066)[0;0m ERROR 12-05 14:29:30 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4050066)[0;0m ERROR 12-05 14:29:30 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050066)[0;0m ERROR 12-05 14:29:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4050066)[0;0m ERROR 12-05 14:29:30 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4050066)[0;0m ERROR 12-05 14:29:30 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050066)[0;0m ERROR 12-05 14:29:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4050066)[0;0m ERROR 12-05 14:29:30 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4050066)[0;0m ERROR 12-05 14:29:30 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050066)[0;0m ERROR 12-05 14:29:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4050066)[0;0m ERROR 12-05 14:29:30 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4050066)[0;0m ERROR 12-05 14:29:30 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050066)[0;0m ERROR 12-05 14:29:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4050066)[0;0m ERROR 12-05 14:29:30 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4050066)[0;0m ERROR 12-05 14:29:30 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050066)[0;0m ERROR 12-05 14:29:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4050066)[0;0m ERROR 12-05 14:29:30 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4050066)[0;0m ERROR 12-05 14:29:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4050066)[0;0m ERROR 12-05 14:29:30 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4050066)[0;0m ERROR 12-05 14:29:30 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050066)[0;0m ERROR 12-05 14:29:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4050066)[0;0m ERROR 12-05 14:29:30 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4050066)[0;0m ERROR 12-05 14:29:30 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050066)[0;0m ERROR 12-05 14:29:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4050066)[0;0m ERROR 12-05 14:29:30 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4050066)[0;0m ERROR 12-05 14:29:30 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050066)[0;0m ERROR 12-05 14:29:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4050066)[0;0m ERROR 12-05 14:29:30 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4050066)[0;0m ERROR 12-05 14:29:30 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050066)[0;0m ERROR 12-05 14:29:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4050066)[0;0m ERROR 12-05 14:29:30 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4050066)[0;0m ERROR 12-05 14:29:30 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050066)[0;0m ERROR 12-05 14:29:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4050066)[0;0m ERROR 12-05 14:29:30 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4050066)[0;0m ERROR 12-05 14:29:30 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050066)[0;0m ERROR 12-05 14:29:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4050066)[0;0m ERROR 12-05 14:29:30 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4050066)[0;0m ERROR 12-05 14:29:30 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050066)[0;0m ERROR 12-05 14:29:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4050066)[0;0m ERROR 12-05 14:29:30 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4050066)[0;0m ERROR 12-05 14:29:30 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050066)[0;0m ERROR 12-05 14:29:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4050066)[0;0m ERROR 12-05 14:29:30 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4050066)[0;0m ERROR 12-05 14:29:30 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050066)[0;0m ERROR 12-05 14:29:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4050066)[0;0m ERROR 12-05 14:29:30 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4050066)[0;0m ERROR 12-05 14:29:30 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050066)[0;0m ERROR 12-05 14:29:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4050066)[0;0m ERROR 12-05 14:29:30 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4050066)[0;0m ERROR 12-05 14:29:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4050066)[0;0m ERROR 12-05 14:29:30 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4050066)[0;0m ERROR 12-05 14:29:30 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050066)[0;0m ERROR 12-05 14:29:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4050066)[0;0m ERROR 12-05 14:29:30 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4050066)[0;0m ERROR 12-05 14:29:30 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050066)[0;0m ERROR 12-05 14:29:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4050066)[0;0m ERROR 12-05 14:29:30 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4050066)[0;0m ERROR 12-05 14:29:30 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050066)[0;0m ERROR 12-05 14:29:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4050066)[0;0m ERROR 12-05 14:29:30 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4050066)[0;0m ERROR 12-05 14:29:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4050066)[0;0m ERROR 12-05 14:29:30 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4050066)[0;0m ERROR 12-05 14:29:30 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050066)[0;0m ERROR 12-05 14:29:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4050066)[0;0m ERROR 12-05 14:29:30 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4050066)[0;0m ERROR 12-05 14:29:30 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050066)[0;0m ERROR 12-05 14:29:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4050066)[0;0m ERROR 12-05 14:29:30 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4050066)[0;0m ERROR 12-05 14:29:30 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050066)[0;0m ERROR 12-05 14:29:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4050066)[0;0m ERROR 12-05 14:29:30 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4050066)[0;0m ERROR 12-05 14:29:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4050066)[0;0m ERROR 12-05 14:29:30 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4050066)[0;0m ERROR 12-05 14:29:30 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050066)[0;0m ERROR 12-05 14:29:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4050066)[0;0m ERROR 12-05 14:29:30 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4050066)[0;0m ERROR 12-05 14:29:30 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050066)[0;0m ERROR 12-05 14:29:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4050066)[0;0m ERROR 12-05 14:29:30 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4050066)[0;0m ERROR 12-05 14:29:30 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050066)[0;0m ERROR 12-05 14:29:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4050066)[0;0m ERROR 12-05 14:29:30 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4050066)[0;0m ERROR 12-05 14:29:30 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050066)[0;0m ERROR 12-05 14:29:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4050066)[0;0m ERROR 12-05 14:29:30 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4050066)[0;0m ERROR 12-05 14:29:30 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7f56382b6cf0>' raised:
[1;36m(EngineCore_DP0 pid=4050066)[0;0m ERROR 12-05 14:29:30 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4050066)[0;0m ERROR 12-05 14:29:30 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4050066)[0;0m ERROR 12-05 14:29:30 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4050066)[0;0m ERROR 12-05 14:29:30 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 14:29:51 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 14:29:51 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 14:29:51 [model.py:1745] Using max model len 131072
INFO 12-05 14:29:51 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4050155)[0;0m INFO 12-05 14:29:58 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4050155)[0;0m INFO 12-05 14:29:59 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:52071 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4050155)[0;0m INFO 12-05 14:29:59 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4050155)[0;0m INFO 12-05 14:30:00 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4050155)[0;0m INFO 12-05 14:30:00 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4050155)[0;0m INFO 12-05 14:30:00 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4050155)[0;0m INFO 12-05 14:30:01 [default_loader.py:314] Loading weights took 0.87 seconds
[1;36m(EngineCore_DP0 pid=4050155)[0;0m INFO 12-05 14:30:02 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.421717 seconds
[1;36m(EngineCore_DP0 pid=4050155)[0;0m INFO 12-05 14:30:06 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4050155)[0;0m INFO 12-05 14:30:06 [backends.py:647] Dynamo bytecode transform time: 3.78 s
[1;36m(EngineCore_DP0 pid=4050155)[0;0m ERROR 12-05 14:30:06 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4050155)[0;0m ERROR 12-05 14:30:06 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4050155)[0;0m ERROR 12-05 14:30:06 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4050155)[0;0m ERROR 12-05 14:30:06 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4050155)[0;0m ERROR 12-05 14:30:06 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050155)[0;0m ERROR 12-05 14:30:06 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4050155)[0;0m ERROR 12-05 14:30:06 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4050155)[0;0m ERROR 12-05 14:30:06 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4050155)[0;0m ERROR 12-05 14:30:06 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4050155)[0;0m ERROR 12-05 14:30:06 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050155)[0;0m ERROR 12-05 14:30:06 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4050155)[0;0m ERROR 12-05 14:30:06 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4050155)[0;0m ERROR 12-05 14:30:06 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050155)[0;0m ERROR 12-05 14:30:06 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4050155)[0;0m ERROR 12-05 14:30:06 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4050155)[0;0m ERROR 12-05 14:30:06 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050155)[0;0m ERROR 12-05 14:30:06 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4050155)[0;0m ERROR 12-05 14:30:06 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4050155)[0;0m ERROR 12-05 14:30:06 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050155)[0;0m ERROR 12-05 14:30:06 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4050155)[0;0m ERROR 12-05 14:30:06 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4050155)[0;0m ERROR 12-05 14:30:06 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050155)[0;0m ERROR 12-05 14:30:06 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4050155)[0;0m ERROR 12-05 14:30:06 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4050155)[0;0m ERROR 12-05 14:30:06 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050155)[0;0m ERROR 12-05 14:30:06 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4050155)[0;0m ERROR 12-05 14:30:06 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4050155)[0;0m ERROR 12-05 14:30:06 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4050155)[0;0m ERROR 12-05 14:30:06 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4050155)[0;0m ERROR 12-05 14:30:06 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050155)[0;0m ERROR 12-05 14:30:06 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4050155)[0;0m ERROR 12-05 14:30:06 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4050155)[0;0m ERROR 12-05 14:30:06 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050155)[0;0m ERROR 12-05 14:30:06 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4050155)[0;0m ERROR 12-05 14:30:06 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4050155)[0;0m ERROR 12-05 14:30:06 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050155)[0;0m ERROR 12-05 14:30:06 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4050155)[0;0m ERROR 12-05 14:30:06 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4050155)[0;0m ERROR 12-05 14:30:06 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050155)[0;0m ERROR 12-05 14:30:06 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4050155)[0;0m ERROR 12-05 14:30:06 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4050155)[0;0m ERROR 12-05 14:30:06 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050155)[0;0m ERROR 12-05 14:30:06 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4050155)[0;0m ERROR 12-05 14:30:06 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4050155)[0;0m ERROR 12-05 14:30:06 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050155)[0;0m ERROR 12-05 14:30:06 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4050155)[0;0m ERROR 12-05 14:30:06 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4050155)[0;0m ERROR 12-05 14:30:06 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050155)[0;0m ERROR 12-05 14:30:06 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4050155)[0;0m ERROR 12-05 14:30:06 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4050155)[0;0m ERROR 12-05 14:30:06 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050155)[0;0m ERROR 12-05 14:30:06 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4050155)[0;0m ERROR 12-05 14:30:06 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4050155)[0;0m ERROR 12-05 14:30:06 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050155)[0;0m ERROR 12-05 14:30:06 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4050155)[0;0m ERROR 12-05 14:30:06 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4050155)[0;0m ERROR 12-05 14:30:06 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050155)[0;0m ERROR 12-05 14:30:06 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4050155)[0;0m ERROR 12-05 14:30:06 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4050155)[0;0m ERROR 12-05 14:30:06 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4050155)[0;0m ERROR 12-05 14:30:06 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4050155)[0;0m ERROR 12-05 14:30:06 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050155)[0;0m ERROR 12-05 14:30:06 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4050155)[0;0m ERROR 12-05 14:30:06 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4050155)[0;0m ERROR 12-05 14:30:06 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050155)[0;0m ERROR 12-05 14:30:06 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4050155)[0;0m ERROR 12-05 14:30:06 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4050155)[0;0m ERROR 12-05 14:30:06 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050155)[0;0m ERROR 12-05 14:30:06 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4050155)[0;0m ERROR 12-05 14:30:06 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4050155)[0;0m ERROR 12-05 14:30:06 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4050155)[0;0m ERROR 12-05 14:30:06 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4050155)[0;0m ERROR 12-05 14:30:06 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050155)[0;0m ERROR 12-05 14:30:06 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4050155)[0;0m ERROR 12-05 14:30:06 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4050155)[0;0m ERROR 12-05 14:30:06 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050155)[0;0m ERROR 12-05 14:30:06 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4050155)[0;0m ERROR 12-05 14:30:06 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4050155)[0;0m ERROR 12-05 14:30:06 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050155)[0;0m ERROR 12-05 14:30:06 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4050155)[0;0m ERROR 12-05 14:30:06 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4050155)[0;0m ERROR 12-05 14:30:06 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4050155)[0;0m ERROR 12-05 14:30:06 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4050155)[0;0m ERROR 12-05 14:30:06 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050155)[0;0m ERROR 12-05 14:30:06 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4050155)[0;0m ERROR 12-05 14:30:06 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4050155)[0;0m ERROR 12-05 14:30:06 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050155)[0;0m ERROR 12-05 14:30:06 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4050155)[0;0m ERROR 12-05 14:30:06 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4050155)[0;0m ERROR 12-05 14:30:06 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050155)[0;0m ERROR 12-05 14:30:06 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4050155)[0;0m ERROR 12-05 14:30:06 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4050155)[0;0m ERROR 12-05 14:30:06 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050155)[0;0m ERROR 12-05 14:30:06 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4050155)[0;0m ERROR 12-05 14:30:06 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4050155)[0;0m ERROR 12-05 14:30:06 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7f9d9c283d40>' raised:
[1;36m(EngineCore_DP0 pid=4050155)[0;0m ERROR 12-05 14:30:06 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4050155)[0;0m ERROR 12-05 14:30:06 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4050155)[0;0m ERROR 12-05 14:30:06 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4050155)[0;0m ERROR 12-05 14:30:06 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 14:30:27 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 14:30:28 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 14:30:28 [model.py:1745] Using max model len 131072
INFO 12-05 14:30:28 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4050234)[0;0m INFO 12-05 14:30:35 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4050234)[0;0m INFO 12-05 14:30:36 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:58449 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4050234)[0;0m INFO 12-05 14:30:36 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4050234)[0;0m INFO 12-05 14:30:36 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4050234)[0;0m INFO 12-05 14:30:37 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4050234)[0;0m INFO 12-05 14:30:37 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4050234)[0;0m INFO 12-05 14:30:38 [default_loader.py:314] Loading weights took 0.87 seconds
[1;36m(EngineCore_DP0 pid=4050234)[0;0m INFO 12-05 14:30:39 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.437373 seconds
[1;36m(EngineCore_DP0 pid=4050234)[0;0m INFO 12-05 14:30:43 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4050234)[0;0m INFO 12-05 14:30:43 [backends.py:647] Dynamo bytecode transform time: 3.95 s
[1;36m(EngineCore_DP0 pid=4050234)[0;0m ERROR 12-05 14:30:43 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4050234)[0;0m ERROR 12-05 14:30:43 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4050234)[0;0m ERROR 12-05 14:30:43 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4050234)[0;0m ERROR 12-05 14:30:43 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4050234)[0;0m ERROR 12-05 14:30:43 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050234)[0;0m ERROR 12-05 14:30:43 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4050234)[0;0m ERROR 12-05 14:30:43 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4050234)[0;0m ERROR 12-05 14:30:43 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4050234)[0;0m ERROR 12-05 14:30:43 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4050234)[0;0m ERROR 12-05 14:30:43 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050234)[0;0m ERROR 12-05 14:30:43 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4050234)[0;0m ERROR 12-05 14:30:43 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4050234)[0;0m ERROR 12-05 14:30:43 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050234)[0;0m ERROR 12-05 14:30:43 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4050234)[0;0m ERROR 12-05 14:30:43 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4050234)[0;0m ERROR 12-05 14:30:43 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050234)[0;0m ERROR 12-05 14:30:43 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4050234)[0;0m ERROR 12-05 14:30:43 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4050234)[0;0m ERROR 12-05 14:30:43 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050234)[0;0m ERROR 12-05 14:30:43 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4050234)[0;0m ERROR 12-05 14:30:43 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4050234)[0;0m ERROR 12-05 14:30:43 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050234)[0;0m ERROR 12-05 14:30:43 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4050234)[0;0m ERROR 12-05 14:30:43 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4050234)[0;0m ERROR 12-05 14:30:43 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050234)[0;0m ERROR 12-05 14:30:43 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4050234)[0;0m ERROR 12-05 14:30:43 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4050234)[0;0m ERROR 12-05 14:30:43 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4050234)[0;0m ERROR 12-05 14:30:43 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4050234)[0;0m ERROR 12-05 14:30:43 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050234)[0;0m ERROR 12-05 14:30:43 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4050234)[0;0m ERROR 12-05 14:30:43 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4050234)[0;0m ERROR 12-05 14:30:43 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050234)[0;0m ERROR 12-05 14:30:43 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4050234)[0;0m ERROR 12-05 14:30:43 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4050234)[0;0m ERROR 12-05 14:30:43 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050234)[0;0m ERROR 12-05 14:30:43 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4050234)[0;0m ERROR 12-05 14:30:43 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4050234)[0;0m ERROR 12-05 14:30:43 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050234)[0;0m ERROR 12-05 14:30:43 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4050234)[0;0m ERROR 12-05 14:30:43 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4050234)[0;0m ERROR 12-05 14:30:43 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050234)[0;0m ERROR 12-05 14:30:43 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4050234)[0;0m ERROR 12-05 14:30:43 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4050234)[0;0m ERROR 12-05 14:30:43 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050234)[0;0m ERROR 12-05 14:30:43 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4050234)[0;0m ERROR 12-05 14:30:43 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4050234)[0;0m ERROR 12-05 14:30:43 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050234)[0;0m ERROR 12-05 14:30:43 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4050234)[0;0m ERROR 12-05 14:30:43 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4050234)[0;0m ERROR 12-05 14:30:43 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050234)[0;0m ERROR 12-05 14:30:43 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4050234)[0;0m ERROR 12-05 14:30:43 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4050234)[0;0m ERROR 12-05 14:30:43 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050234)[0;0m ERROR 12-05 14:30:43 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4050234)[0;0m ERROR 12-05 14:30:43 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4050234)[0;0m ERROR 12-05 14:30:43 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050234)[0;0m ERROR 12-05 14:30:43 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4050234)[0;0m ERROR 12-05 14:30:43 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4050234)[0;0m ERROR 12-05 14:30:43 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4050234)[0;0m ERROR 12-05 14:30:43 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4050234)[0;0m ERROR 12-05 14:30:43 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050234)[0;0m ERROR 12-05 14:30:43 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4050234)[0;0m ERROR 12-05 14:30:43 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4050234)[0;0m ERROR 12-05 14:30:43 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050234)[0;0m ERROR 12-05 14:30:43 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4050234)[0;0m ERROR 12-05 14:30:43 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4050234)[0;0m ERROR 12-05 14:30:43 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050234)[0;0m ERROR 12-05 14:30:43 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4050234)[0;0m ERROR 12-05 14:30:43 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4050234)[0;0m ERROR 12-05 14:30:43 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4050234)[0;0m ERROR 12-05 14:30:43 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4050234)[0;0m ERROR 12-05 14:30:43 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050234)[0;0m ERROR 12-05 14:30:43 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4050234)[0;0m ERROR 12-05 14:30:43 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4050234)[0;0m ERROR 12-05 14:30:43 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050234)[0;0m ERROR 12-05 14:30:43 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4050234)[0;0m ERROR 12-05 14:30:43 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4050234)[0;0m ERROR 12-05 14:30:43 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050234)[0;0m ERROR 12-05 14:30:43 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4050234)[0;0m ERROR 12-05 14:30:43 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4050234)[0;0m ERROR 12-05 14:30:43 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4050234)[0;0m ERROR 12-05 14:30:43 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4050234)[0;0m ERROR 12-05 14:30:43 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050234)[0;0m ERROR 12-05 14:30:43 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4050234)[0;0m ERROR 12-05 14:30:43 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4050234)[0;0m ERROR 12-05 14:30:43 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050234)[0;0m ERROR 12-05 14:30:43 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4050234)[0;0m ERROR 12-05 14:30:43 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4050234)[0;0m ERROR 12-05 14:30:43 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050234)[0;0m ERROR 12-05 14:30:43 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4050234)[0;0m ERROR 12-05 14:30:43 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4050234)[0;0m ERROR 12-05 14:30:43 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050234)[0;0m ERROR 12-05 14:30:43 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4050234)[0;0m ERROR 12-05 14:30:43 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4050234)[0;0m ERROR 12-05 14:30:43 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7efafd7210a0>' raised:
[1;36m(EngineCore_DP0 pid=4050234)[0;0m ERROR 12-05 14:30:43 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4050234)[0;0m ERROR 12-05 14:30:43 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4050234)[0;0m ERROR 12-05 14:30:43 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4050234)[0;0m ERROR 12-05 14:30:43 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 14:31:04 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 14:31:05 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 14:31:05 [model.py:1745] Using max model len 131072
INFO 12-05 14:31:05 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4050343)[0;0m INFO 12-05 14:31:13 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4050343)[0;0m INFO 12-05 14:31:14 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:44735 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4050343)[0;0m INFO 12-05 14:31:14 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4050343)[0;0m INFO 12-05 14:31:15 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4050343)[0;0m INFO 12-05 14:31:15 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4050343)[0;0m INFO 12-05 14:31:15 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4050343)[0;0m INFO 12-05 14:31:16 [default_loader.py:314] Loading weights took 0.87 seconds
[1;36m(EngineCore_DP0 pid=4050343)[0;0m INFO 12-05 14:31:17 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.467812 seconds
[1;36m(EngineCore_DP0 pid=4050343)[0;0m INFO 12-05 14:31:21 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4050343)[0;0m INFO 12-05 14:31:21 [backends.py:647] Dynamo bytecode transform time: 3.98 s
[1;36m(EngineCore_DP0 pid=4050343)[0;0m ERROR 12-05 14:31:21 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4050343)[0;0m ERROR 12-05 14:31:21 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4050343)[0;0m ERROR 12-05 14:31:21 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4050343)[0;0m ERROR 12-05 14:31:21 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4050343)[0;0m ERROR 12-05 14:31:21 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050343)[0;0m ERROR 12-05 14:31:21 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4050343)[0;0m ERROR 12-05 14:31:21 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4050343)[0;0m ERROR 12-05 14:31:21 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4050343)[0;0m ERROR 12-05 14:31:21 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4050343)[0;0m ERROR 12-05 14:31:21 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050343)[0;0m ERROR 12-05 14:31:21 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4050343)[0;0m ERROR 12-05 14:31:21 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4050343)[0;0m ERROR 12-05 14:31:21 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050343)[0;0m ERROR 12-05 14:31:21 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4050343)[0;0m ERROR 12-05 14:31:21 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4050343)[0;0m ERROR 12-05 14:31:21 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050343)[0;0m ERROR 12-05 14:31:21 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4050343)[0;0m ERROR 12-05 14:31:21 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4050343)[0;0m ERROR 12-05 14:31:21 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050343)[0;0m ERROR 12-05 14:31:21 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4050343)[0;0m ERROR 12-05 14:31:21 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4050343)[0;0m ERROR 12-05 14:31:21 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050343)[0;0m ERROR 12-05 14:31:21 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4050343)[0;0m ERROR 12-05 14:31:21 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4050343)[0;0m ERROR 12-05 14:31:21 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050343)[0;0m ERROR 12-05 14:31:21 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4050343)[0;0m ERROR 12-05 14:31:21 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4050343)[0;0m ERROR 12-05 14:31:21 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4050343)[0;0m ERROR 12-05 14:31:21 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4050343)[0;0m ERROR 12-05 14:31:21 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050343)[0;0m ERROR 12-05 14:31:21 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4050343)[0;0m ERROR 12-05 14:31:21 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4050343)[0;0m ERROR 12-05 14:31:21 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050343)[0;0m ERROR 12-05 14:31:21 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4050343)[0;0m ERROR 12-05 14:31:21 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4050343)[0;0m ERROR 12-05 14:31:21 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050343)[0;0m ERROR 12-05 14:31:21 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4050343)[0;0m ERROR 12-05 14:31:21 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4050343)[0;0m ERROR 12-05 14:31:21 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050343)[0;0m ERROR 12-05 14:31:21 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4050343)[0;0m ERROR 12-05 14:31:21 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4050343)[0;0m ERROR 12-05 14:31:21 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050343)[0;0m ERROR 12-05 14:31:21 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4050343)[0;0m ERROR 12-05 14:31:21 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4050343)[0;0m ERROR 12-05 14:31:21 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050343)[0;0m ERROR 12-05 14:31:21 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4050343)[0;0m ERROR 12-05 14:31:21 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4050343)[0;0m ERROR 12-05 14:31:21 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050343)[0;0m ERROR 12-05 14:31:21 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4050343)[0;0m ERROR 12-05 14:31:21 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4050343)[0;0m ERROR 12-05 14:31:21 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050343)[0;0m ERROR 12-05 14:31:21 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4050343)[0;0m ERROR 12-05 14:31:21 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4050343)[0;0m ERROR 12-05 14:31:21 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050343)[0;0m ERROR 12-05 14:31:21 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4050343)[0;0m ERROR 12-05 14:31:21 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4050343)[0;0m ERROR 12-05 14:31:21 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050343)[0;0m ERROR 12-05 14:31:21 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4050343)[0;0m ERROR 12-05 14:31:21 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4050343)[0;0m ERROR 12-05 14:31:21 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4050343)[0;0m ERROR 12-05 14:31:21 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4050343)[0;0m ERROR 12-05 14:31:21 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050343)[0;0m ERROR 12-05 14:31:21 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4050343)[0;0m ERROR 12-05 14:31:21 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4050343)[0;0m ERROR 12-05 14:31:21 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050343)[0;0m ERROR 12-05 14:31:21 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4050343)[0;0m ERROR 12-05 14:31:21 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4050343)[0;0m ERROR 12-05 14:31:21 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050343)[0;0m ERROR 12-05 14:31:21 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4050343)[0;0m ERROR 12-05 14:31:21 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4050343)[0;0m ERROR 12-05 14:31:21 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4050343)[0;0m ERROR 12-05 14:31:21 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4050343)[0;0m ERROR 12-05 14:31:21 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050343)[0;0m ERROR 12-05 14:31:21 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4050343)[0;0m ERROR 12-05 14:31:21 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4050343)[0;0m ERROR 12-05 14:31:21 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050343)[0;0m ERROR 12-05 14:31:21 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4050343)[0;0m ERROR 12-05 14:31:21 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4050343)[0;0m ERROR 12-05 14:31:21 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050343)[0;0m ERROR 12-05 14:31:21 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4050343)[0;0m ERROR 12-05 14:31:21 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4050343)[0;0m ERROR 12-05 14:31:21 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4050343)[0;0m ERROR 12-05 14:31:21 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4050343)[0;0m ERROR 12-05 14:31:21 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050343)[0;0m ERROR 12-05 14:31:21 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4050343)[0;0m ERROR 12-05 14:31:21 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4050343)[0;0m ERROR 12-05 14:31:21 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050343)[0;0m ERROR 12-05 14:31:21 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4050343)[0;0m ERROR 12-05 14:31:21 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4050343)[0;0m ERROR 12-05 14:31:21 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050343)[0;0m ERROR 12-05 14:31:21 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4050343)[0;0m ERROR 12-05 14:31:21 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4050343)[0;0m ERROR 12-05 14:31:21 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050343)[0;0m ERROR 12-05 14:31:21 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4050343)[0;0m ERROR 12-05 14:31:21 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4050343)[0;0m ERROR 12-05 14:31:21 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7fa83c5f70b0>' raised:
[1;36m(EngineCore_DP0 pid=4050343)[0;0m ERROR 12-05 14:31:21 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4050343)[0;0m ERROR 12-05 14:31:21 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4050343)[0;0m ERROR 12-05 14:31:21 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4050343)[0;0m ERROR 12-05 14:31:21 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 14:31:43 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 14:31:43 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 14:31:43 [model.py:1745] Using max model len 131072
INFO 12-05 14:31:43 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4050510)[0;0m INFO 12-05 14:31:51 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4050510)[0;0m INFO 12-05 14:31:52 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:56943 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4050510)[0;0m INFO 12-05 14:31:52 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4050510)[0;0m INFO 12-05 14:31:52 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4050510)[0;0m INFO 12-05 14:31:53 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4050510)[0;0m INFO 12-05 14:31:53 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4050510)[0;0m INFO 12-05 14:31:54 [default_loader.py:314] Loading weights took 0.87 seconds
[1;36m(EngineCore_DP0 pid=4050510)[0;0m INFO 12-05 14:31:54 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.433985 seconds
[1;36m(EngineCore_DP0 pid=4050510)[0;0m INFO 12-05 14:31:58 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4050510)[0;0m INFO 12-05 14:31:58 [backends.py:647] Dynamo bytecode transform time: 3.88 s
[1;36m(EngineCore_DP0 pid=4050510)[0;0m ERROR 12-05 14:31:59 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4050510)[0;0m ERROR 12-05 14:31:59 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4050510)[0;0m ERROR 12-05 14:31:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4050510)[0;0m ERROR 12-05 14:31:59 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4050510)[0;0m ERROR 12-05 14:31:59 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050510)[0;0m ERROR 12-05 14:31:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4050510)[0;0m ERROR 12-05 14:31:59 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4050510)[0;0m ERROR 12-05 14:31:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4050510)[0;0m ERROR 12-05 14:31:59 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4050510)[0;0m ERROR 12-05 14:31:59 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050510)[0;0m ERROR 12-05 14:31:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4050510)[0;0m ERROR 12-05 14:31:59 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4050510)[0;0m ERROR 12-05 14:31:59 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050510)[0;0m ERROR 12-05 14:31:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4050510)[0;0m ERROR 12-05 14:31:59 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4050510)[0;0m ERROR 12-05 14:31:59 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050510)[0;0m ERROR 12-05 14:31:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4050510)[0;0m ERROR 12-05 14:31:59 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4050510)[0;0m ERROR 12-05 14:31:59 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050510)[0;0m ERROR 12-05 14:31:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4050510)[0;0m ERROR 12-05 14:31:59 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4050510)[0;0m ERROR 12-05 14:31:59 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050510)[0;0m ERROR 12-05 14:31:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4050510)[0;0m ERROR 12-05 14:31:59 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4050510)[0;0m ERROR 12-05 14:31:59 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050510)[0;0m ERROR 12-05 14:31:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4050510)[0;0m ERROR 12-05 14:31:59 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4050510)[0;0m ERROR 12-05 14:31:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4050510)[0;0m ERROR 12-05 14:31:59 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4050510)[0;0m ERROR 12-05 14:31:59 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050510)[0;0m ERROR 12-05 14:31:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4050510)[0;0m ERROR 12-05 14:31:59 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4050510)[0;0m ERROR 12-05 14:31:59 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050510)[0;0m ERROR 12-05 14:31:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4050510)[0;0m ERROR 12-05 14:31:59 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4050510)[0;0m ERROR 12-05 14:31:59 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050510)[0;0m ERROR 12-05 14:31:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4050510)[0;0m ERROR 12-05 14:31:59 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4050510)[0;0m ERROR 12-05 14:31:59 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050510)[0;0m ERROR 12-05 14:31:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4050510)[0;0m ERROR 12-05 14:31:59 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4050510)[0;0m ERROR 12-05 14:31:59 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050510)[0;0m ERROR 12-05 14:31:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4050510)[0;0m ERROR 12-05 14:31:59 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4050510)[0;0m ERROR 12-05 14:31:59 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050510)[0;0m ERROR 12-05 14:31:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4050510)[0;0m ERROR 12-05 14:31:59 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4050510)[0;0m ERROR 12-05 14:31:59 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050510)[0;0m ERROR 12-05 14:31:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4050510)[0;0m ERROR 12-05 14:31:59 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4050510)[0;0m ERROR 12-05 14:31:59 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050510)[0;0m ERROR 12-05 14:31:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4050510)[0;0m ERROR 12-05 14:31:59 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4050510)[0;0m ERROR 12-05 14:31:59 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050510)[0;0m ERROR 12-05 14:31:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4050510)[0;0m ERROR 12-05 14:31:59 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4050510)[0;0m ERROR 12-05 14:31:59 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050510)[0;0m ERROR 12-05 14:31:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4050510)[0;0m ERROR 12-05 14:31:59 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4050510)[0;0m ERROR 12-05 14:31:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4050510)[0;0m ERROR 12-05 14:31:59 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4050510)[0;0m ERROR 12-05 14:31:59 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050510)[0;0m ERROR 12-05 14:31:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4050510)[0;0m ERROR 12-05 14:31:59 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4050510)[0;0m ERROR 12-05 14:31:59 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050510)[0;0m ERROR 12-05 14:31:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4050510)[0;0m ERROR 12-05 14:31:59 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4050510)[0;0m ERROR 12-05 14:31:59 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050510)[0;0m ERROR 12-05 14:31:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4050510)[0;0m ERROR 12-05 14:31:59 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4050510)[0;0m ERROR 12-05 14:31:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4050510)[0;0m ERROR 12-05 14:31:59 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4050510)[0;0m ERROR 12-05 14:31:59 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050510)[0;0m ERROR 12-05 14:31:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4050510)[0;0m ERROR 12-05 14:31:59 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4050510)[0;0m ERROR 12-05 14:31:59 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050510)[0;0m ERROR 12-05 14:31:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4050510)[0;0m ERROR 12-05 14:31:59 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4050510)[0;0m ERROR 12-05 14:31:59 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050510)[0;0m ERROR 12-05 14:31:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4050510)[0;0m ERROR 12-05 14:31:59 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4050510)[0;0m ERROR 12-05 14:31:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4050510)[0;0m ERROR 12-05 14:31:59 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4050510)[0;0m ERROR 12-05 14:31:59 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050510)[0;0m ERROR 12-05 14:31:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4050510)[0;0m ERROR 12-05 14:31:59 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4050510)[0;0m ERROR 12-05 14:31:59 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050510)[0;0m ERROR 12-05 14:31:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4050510)[0;0m ERROR 12-05 14:31:59 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4050510)[0;0m ERROR 12-05 14:31:59 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050510)[0;0m ERROR 12-05 14:31:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4050510)[0;0m ERROR 12-05 14:31:59 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4050510)[0;0m ERROR 12-05 14:31:59 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050510)[0;0m ERROR 12-05 14:31:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4050510)[0;0m ERROR 12-05 14:31:59 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4050510)[0;0m ERROR 12-05 14:31:59 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7fa5dc2d3e00>' raised:
[1;36m(EngineCore_DP0 pid=4050510)[0;0m ERROR 12-05 14:31:59 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4050510)[0;0m ERROR 12-05 14:31:59 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4050510)[0;0m ERROR 12-05 14:31:59 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4050510)[0;0m ERROR 12-05 14:31:59 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 14:32:20 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 14:32:20 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 14:32:20 [model.py:1745] Using max model len 131072
INFO 12-05 14:32:20 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4050587)[0;0m INFO 12-05 14:32:27 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4050587)[0;0m INFO 12-05 14:32:28 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:56779 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4050587)[0;0m INFO 12-05 14:32:28 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4050587)[0;0m INFO 12-05 14:32:29 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4050587)[0;0m INFO 12-05 14:32:29 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4050587)[0;0m INFO 12-05 14:32:29 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4050587)[0;0m INFO 12-05 14:32:30 [default_loader.py:314] Loading weights took 0.87 seconds
[1;36m(EngineCore_DP0 pid=4050587)[0;0m INFO 12-05 14:32:31 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.397534 seconds
[1;36m(EngineCore_DP0 pid=4050587)[0;0m INFO 12-05 14:32:35 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4050587)[0;0m INFO 12-05 14:32:35 [backends.py:647] Dynamo bytecode transform time: 3.65 s
[1;36m(EngineCore_DP0 pid=4050587)[0;0m ERROR 12-05 14:32:35 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4050587)[0;0m ERROR 12-05 14:32:35 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4050587)[0;0m ERROR 12-05 14:32:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4050587)[0;0m ERROR 12-05 14:32:35 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4050587)[0;0m ERROR 12-05 14:32:35 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050587)[0;0m ERROR 12-05 14:32:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4050587)[0;0m ERROR 12-05 14:32:35 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4050587)[0;0m ERROR 12-05 14:32:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4050587)[0;0m ERROR 12-05 14:32:35 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4050587)[0;0m ERROR 12-05 14:32:35 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050587)[0;0m ERROR 12-05 14:32:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4050587)[0;0m ERROR 12-05 14:32:35 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4050587)[0;0m ERROR 12-05 14:32:35 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050587)[0;0m ERROR 12-05 14:32:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4050587)[0;0m ERROR 12-05 14:32:35 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4050587)[0;0m ERROR 12-05 14:32:35 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050587)[0;0m ERROR 12-05 14:32:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4050587)[0;0m ERROR 12-05 14:32:35 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4050587)[0;0m ERROR 12-05 14:32:35 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050587)[0;0m ERROR 12-05 14:32:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4050587)[0;0m ERROR 12-05 14:32:35 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4050587)[0;0m ERROR 12-05 14:32:35 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050587)[0;0m ERROR 12-05 14:32:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4050587)[0;0m ERROR 12-05 14:32:35 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4050587)[0;0m ERROR 12-05 14:32:35 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050587)[0;0m ERROR 12-05 14:32:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4050587)[0;0m ERROR 12-05 14:32:35 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4050587)[0;0m ERROR 12-05 14:32:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4050587)[0;0m ERROR 12-05 14:32:35 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4050587)[0;0m ERROR 12-05 14:32:35 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050587)[0;0m ERROR 12-05 14:32:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4050587)[0;0m ERROR 12-05 14:32:35 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4050587)[0;0m ERROR 12-05 14:32:35 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050587)[0;0m ERROR 12-05 14:32:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4050587)[0;0m ERROR 12-05 14:32:35 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4050587)[0;0m ERROR 12-05 14:32:35 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050587)[0;0m ERROR 12-05 14:32:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4050587)[0;0m ERROR 12-05 14:32:35 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4050587)[0;0m ERROR 12-05 14:32:35 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050587)[0;0m ERROR 12-05 14:32:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4050587)[0;0m ERROR 12-05 14:32:35 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4050587)[0;0m ERROR 12-05 14:32:35 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050587)[0;0m ERROR 12-05 14:32:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4050587)[0;0m ERROR 12-05 14:32:35 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4050587)[0;0m ERROR 12-05 14:32:35 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050587)[0;0m ERROR 12-05 14:32:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4050587)[0;0m ERROR 12-05 14:32:35 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4050587)[0;0m ERROR 12-05 14:32:35 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050587)[0;0m ERROR 12-05 14:32:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4050587)[0;0m ERROR 12-05 14:32:35 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4050587)[0;0m ERROR 12-05 14:32:35 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050587)[0;0m ERROR 12-05 14:32:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4050587)[0;0m ERROR 12-05 14:32:35 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4050587)[0;0m ERROR 12-05 14:32:35 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050587)[0;0m ERROR 12-05 14:32:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4050587)[0;0m ERROR 12-05 14:32:35 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4050587)[0;0m ERROR 12-05 14:32:35 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050587)[0;0m ERROR 12-05 14:32:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4050587)[0;0m ERROR 12-05 14:32:35 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4050587)[0;0m ERROR 12-05 14:32:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4050587)[0;0m ERROR 12-05 14:32:35 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4050587)[0;0m ERROR 12-05 14:32:35 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050587)[0;0m ERROR 12-05 14:32:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4050587)[0;0m ERROR 12-05 14:32:35 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4050587)[0;0m ERROR 12-05 14:32:35 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050587)[0;0m ERROR 12-05 14:32:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4050587)[0;0m ERROR 12-05 14:32:35 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4050587)[0;0m ERROR 12-05 14:32:35 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050587)[0;0m ERROR 12-05 14:32:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4050587)[0;0m ERROR 12-05 14:32:35 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4050587)[0;0m ERROR 12-05 14:32:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4050587)[0;0m ERROR 12-05 14:32:35 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4050587)[0;0m ERROR 12-05 14:32:35 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050587)[0;0m ERROR 12-05 14:32:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4050587)[0;0m ERROR 12-05 14:32:35 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4050587)[0;0m ERROR 12-05 14:32:35 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050587)[0;0m ERROR 12-05 14:32:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4050587)[0;0m ERROR 12-05 14:32:35 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4050587)[0;0m ERROR 12-05 14:32:35 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050587)[0;0m ERROR 12-05 14:32:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4050587)[0;0m ERROR 12-05 14:32:35 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4050587)[0;0m ERROR 12-05 14:32:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4050587)[0;0m ERROR 12-05 14:32:35 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4050587)[0;0m ERROR 12-05 14:32:35 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050587)[0;0m ERROR 12-05 14:32:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4050587)[0;0m ERROR 12-05 14:32:35 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4050587)[0;0m ERROR 12-05 14:32:35 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050587)[0;0m ERROR 12-05 14:32:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4050587)[0;0m ERROR 12-05 14:32:35 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4050587)[0;0m ERROR 12-05 14:32:35 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050587)[0;0m ERROR 12-05 14:32:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4050587)[0;0m ERROR 12-05 14:32:35 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4050587)[0;0m ERROR 12-05 14:32:35 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050587)[0;0m ERROR 12-05 14:32:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4050587)[0;0m ERROR 12-05 14:32:35 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4050587)[0;0m ERROR 12-05 14:32:35 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7fa59828e180>' raised:
[1;36m(EngineCore_DP0 pid=4050587)[0;0m ERROR 12-05 14:32:35 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4050587)[0;0m ERROR 12-05 14:32:35 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4050587)[0;0m ERROR 12-05 14:32:35 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4050587)[0;0m ERROR 12-05 14:32:35 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 14:32:56 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 14:32:56 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 14:32:56 [model.py:1745] Using max model len 131072
INFO 12-05 14:32:56 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4050674)[0;0m INFO 12-05 14:33:04 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4050674)[0;0m INFO 12-05 14:33:05 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:52843 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4050674)[0;0m INFO 12-05 14:33:05 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4050674)[0;0m INFO 12-05 14:33:05 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4050674)[0;0m INFO 12-05 14:33:05 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4050674)[0;0m INFO 12-05 14:33:05 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4050674)[0;0m INFO 12-05 14:33:07 [default_loader.py:314] Loading weights took 0.88 seconds
[1;36m(EngineCore_DP0 pid=4050674)[0;0m INFO 12-05 14:33:07 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.428536 seconds
[1;36m(EngineCore_DP0 pid=4050674)[0;0m INFO 12-05 14:33:11 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4050674)[0;0m INFO 12-05 14:33:11 [backends.py:647] Dynamo bytecode transform time: 3.59 s
[1;36m(EngineCore_DP0 pid=4050674)[0;0m ERROR 12-05 14:33:11 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4050674)[0;0m ERROR 12-05 14:33:11 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4050674)[0;0m ERROR 12-05 14:33:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4050674)[0;0m ERROR 12-05 14:33:11 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4050674)[0;0m ERROR 12-05 14:33:11 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050674)[0;0m ERROR 12-05 14:33:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4050674)[0;0m ERROR 12-05 14:33:11 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4050674)[0;0m ERROR 12-05 14:33:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4050674)[0;0m ERROR 12-05 14:33:11 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4050674)[0;0m ERROR 12-05 14:33:11 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050674)[0;0m ERROR 12-05 14:33:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4050674)[0;0m ERROR 12-05 14:33:11 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4050674)[0;0m ERROR 12-05 14:33:11 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050674)[0;0m ERROR 12-05 14:33:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4050674)[0;0m ERROR 12-05 14:33:11 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4050674)[0;0m ERROR 12-05 14:33:11 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050674)[0;0m ERROR 12-05 14:33:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4050674)[0;0m ERROR 12-05 14:33:11 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4050674)[0;0m ERROR 12-05 14:33:11 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050674)[0;0m ERROR 12-05 14:33:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4050674)[0;0m ERROR 12-05 14:33:11 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4050674)[0;0m ERROR 12-05 14:33:11 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050674)[0;0m ERROR 12-05 14:33:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4050674)[0;0m ERROR 12-05 14:33:11 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4050674)[0;0m ERROR 12-05 14:33:11 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050674)[0;0m ERROR 12-05 14:33:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4050674)[0;0m ERROR 12-05 14:33:11 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4050674)[0;0m ERROR 12-05 14:33:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4050674)[0;0m ERROR 12-05 14:33:11 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4050674)[0;0m ERROR 12-05 14:33:11 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050674)[0;0m ERROR 12-05 14:33:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4050674)[0;0m ERROR 12-05 14:33:11 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4050674)[0;0m ERROR 12-05 14:33:11 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050674)[0;0m ERROR 12-05 14:33:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4050674)[0;0m ERROR 12-05 14:33:11 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4050674)[0;0m ERROR 12-05 14:33:11 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050674)[0;0m ERROR 12-05 14:33:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4050674)[0;0m ERROR 12-05 14:33:11 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4050674)[0;0m ERROR 12-05 14:33:11 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050674)[0;0m ERROR 12-05 14:33:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4050674)[0;0m ERROR 12-05 14:33:11 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4050674)[0;0m ERROR 12-05 14:33:11 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050674)[0;0m ERROR 12-05 14:33:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4050674)[0;0m ERROR 12-05 14:33:11 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4050674)[0;0m ERROR 12-05 14:33:11 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050674)[0;0m ERROR 12-05 14:33:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4050674)[0;0m ERROR 12-05 14:33:11 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4050674)[0;0m ERROR 12-05 14:33:11 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050674)[0;0m ERROR 12-05 14:33:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4050674)[0;0m ERROR 12-05 14:33:11 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4050674)[0;0m ERROR 12-05 14:33:11 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050674)[0;0m ERROR 12-05 14:33:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4050674)[0;0m ERROR 12-05 14:33:11 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4050674)[0;0m ERROR 12-05 14:33:11 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050674)[0;0m ERROR 12-05 14:33:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4050674)[0;0m ERROR 12-05 14:33:11 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4050674)[0;0m ERROR 12-05 14:33:11 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050674)[0;0m ERROR 12-05 14:33:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4050674)[0;0m ERROR 12-05 14:33:11 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4050674)[0;0m ERROR 12-05 14:33:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4050674)[0;0m ERROR 12-05 14:33:11 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4050674)[0;0m ERROR 12-05 14:33:11 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050674)[0;0m ERROR 12-05 14:33:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4050674)[0;0m ERROR 12-05 14:33:11 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4050674)[0;0m ERROR 12-05 14:33:11 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050674)[0;0m ERROR 12-05 14:33:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4050674)[0;0m ERROR 12-05 14:33:11 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4050674)[0;0m ERROR 12-05 14:33:11 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050674)[0;0m ERROR 12-05 14:33:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4050674)[0;0m ERROR 12-05 14:33:11 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4050674)[0;0m ERROR 12-05 14:33:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4050674)[0;0m ERROR 12-05 14:33:11 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4050674)[0;0m ERROR 12-05 14:33:11 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050674)[0;0m ERROR 12-05 14:33:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4050674)[0;0m ERROR 12-05 14:33:11 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4050674)[0;0m ERROR 12-05 14:33:11 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050674)[0;0m ERROR 12-05 14:33:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4050674)[0;0m ERROR 12-05 14:33:11 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4050674)[0;0m ERROR 12-05 14:33:11 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050674)[0;0m ERROR 12-05 14:33:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4050674)[0;0m ERROR 12-05 14:33:11 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4050674)[0;0m ERROR 12-05 14:33:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4050674)[0;0m ERROR 12-05 14:33:11 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4050674)[0;0m ERROR 12-05 14:33:11 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050674)[0;0m ERROR 12-05 14:33:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4050674)[0;0m ERROR 12-05 14:33:11 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4050674)[0;0m ERROR 12-05 14:33:11 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050674)[0;0m ERROR 12-05 14:33:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4050674)[0;0m ERROR 12-05 14:33:11 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4050674)[0;0m ERROR 12-05 14:33:11 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050674)[0;0m ERROR 12-05 14:33:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4050674)[0;0m ERROR 12-05 14:33:11 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4050674)[0;0m ERROR 12-05 14:33:11 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050674)[0;0m ERROR 12-05 14:33:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4050674)[0;0m ERROR 12-05 14:33:11 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4050674)[0;0m ERROR 12-05 14:33:11 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7f8e802270b0>' raised:
[1;36m(EngineCore_DP0 pid=4050674)[0;0m ERROR 12-05 14:33:11 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4050674)[0;0m ERROR 12-05 14:33:11 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4050674)[0;0m ERROR 12-05 14:33:11 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4050674)[0;0m ERROR 12-05 14:33:11 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 14:33:32 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 14:33:33 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 14:33:33 [model.py:1745] Using max model len 131072
INFO 12-05 14:33:33 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4050754)[0;0m INFO 12-05 14:33:40 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4050754)[0;0m INFO 12-05 14:33:41 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:45421 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4050754)[0;0m INFO 12-05 14:33:41 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4050754)[0;0m INFO 12-05 14:33:41 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4050754)[0;0m INFO 12-05 14:33:41 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4050754)[0;0m INFO 12-05 14:33:41 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4050754)[0;0m INFO 12-05 14:33:43 [default_loader.py:314] Loading weights took 0.88 seconds
[1;36m(EngineCore_DP0 pid=4050754)[0;0m INFO 12-05 14:33:43 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.421710 seconds
[1;36m(EngineCore_DP0 pid=4050754)[0;0m INFO 12-05 14:33:47 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4050754)[0;0m INFO 12-05 14:33:47 [backends.py:647] Dynamo bytecode transform time: 3.62 s
[1;36m(EngineCore_DP0 pid=4050754)[0;0m ERROR 12-05 14:33:47 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4050754)[0;0m ERROR 12-05 14:33:47 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4050754)[0;0m ERROR 12-05 14:33:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4050754)[0;0m ERROR 12-05 14:33:47 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4050754)[0;0m ERROR 12-05 14:33:47 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050754)[0;0m ERROR 12-05 14:33:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4050754)[0;0m ERROR 12-05 14:33:47 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4050754)[0;0m ERROR 12-05 14:33:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4050754)[0;0m ERROR 12-05 14:33:47 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4050754)[0;0m ERROR 12-05 14:33:47 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050754)[0;0m ERROR 12-05 14:33:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4050754)[0;0m ERROR 12-05 14:33:47 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4050754)[0;0m ERROR 12-05 14:33:47 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050754)[0;0m ERROR 12-05 14:33:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4050754)[0;0m ERROR 12-05 14:33:47 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4050754)[0;0m ERROR 12-05 14:33:47 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050754)[0;0m ERROR 12-05 14:33:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4050754)[0;0m ERROR 12-05 14:33:47 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4050754)[0;0m ERROR 12-05 14:33:47 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050754)[0;0m ERROR 12-05 14:33:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4050754)[0;0m ERROR 12-05 14:33:47 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4050754)[0;0m ERROR 12-05 14:33:47 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050754)[0;0m ERROR 12-05 14:33:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4050754)[0;0m ERROR 12-05 14:33:47 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4050754)[0;0m ERROR 12-05 14:33:47 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050754)[0;0m ERROR 12-05 14:33:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4050754)[0;0m ERROR 12-05 14:33:47 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4050754)[0;0m ERROR 12-05 14:33:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4050754)[0;0m ERROR 12-05 14:33:47 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4050754)[0;0m ERROR 12-05 14:33:47 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050754)[0;0m ERROR 12-05 14:33:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4050754)[0;0m ERROR 12-05 14:33:47 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4050754)[0;0m ERROR 12-05 14:33:47 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050754)[0;0m ERROR 12-05 14:33:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4050754)[0;0m ERROR 12-05 14:33:47 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4050754)[0;0m ERROR 12-05 14:33:47 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050754)[0;0m ERROR 12-05 14:33:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4050754)[0;0m ERROR 12-05 14:33:47 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4050754)[0;0m ERROR 12-05 14:33:47 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050754)[0;0m ERROR 12-05 14:33:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4050754)[0;0m ERROR 12-05 14:33:47 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4050754)[0;0m ERROR 12-05 14:33:47 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050754)[0;0m ERROR 12-05 14:33:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4050754)[0;0m ERROR 12-05 14:33:47 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4050754)[0;0m ERROR 12-05 14:33:47 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050754)[0;0m ERROR 12-05 14:33:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4050754)[0;0m ERROR 12-05 14:33:47 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4050754)[0;0m ERROR 12-05 14:33:47 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050754)[0;0m ERROR 12-05 14:33:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4050754)[0;0m ERROR 12-05 14:33:47 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4050754)[0;0m ERROR 12-05 14:33:47 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050754)[0;0m ERROR 12-05 14:33:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4050754)[0;0m ERROR 12-05 14:33:47 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4050754)[0;0m ERROR 12-05 14:33:47 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050754)[0;0m ERROR 12-05 14:33:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4050754)[0;0m ERROR 12-05 14:33:47 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4050754)[0;0m ERROR 12-05 14:33:47 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050754)[0;0m ERROR 12-05 14:33:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4050754)[0;0m ERROR 12-05 14:33:47 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4050754)[0;0m ERROR 12-05 14:33:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4050754)[0;0m ERROR 12-05 14:33:47 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4050754)[0;0m ERROR 12-05 14:33:47 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050754)[0;0m ERROR 12-05 14:33:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4050754)[0;0m ERROR 12-05 14:33:47 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4050754)[0;0m ERROR 12-05 14:33:47 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050754)[0;0m ERROR 12-05 14:33:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4050754)[0;0m ERROR 12-05 14:33:47 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4050754)[0;0m ERROR 12-05 14:33:47 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050754)[0;0m ERROR 12-05 14:33:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4050754)[0;0m ERROR 12-05 14:33:47 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4050754)[0;0m ERROR 12-05 14:33:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4050754)[0;0m ERROR 12-05 14:33:47 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4050754)[0;0m ERROR 12-05 14:33:47 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050754)[0;0m ERROR 12-05 14:33:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4050754)[0;0m ERROR 12-05 14:33:47 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4050754)[0;0m ERROR 12-05 14:33:47 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050754)[0;0m ERROR 12-05 14:33:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4050754)[0;0m ERROR 12-05 14:33:47 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4050754)[0;0m ERROR 12-05 14:33:47 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050754)[0;0m ERROR 12-05 14:33:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4050754)[0;0m ERROR 12-05 14:33:47 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4050754)[0;0m ERROR 12-05 14:33:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4050754)[0;0m ERROR 12-05 14:33:47 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4050754)[0;0m ERROR 12-05 14:33:47 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050754)[0;0m ERROR 12-05 14:33:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4050754)[0;0m ERROR 12-05 14:33:47 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4050754)[0;0m ERROR 12-05 14:33:47 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050754)[0;0m ERROR 12-05 14:33:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4050754)[0;0m ERROR 12-05 14:33:47 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4050754)[0;0m ERROR 12-05 14:33:47 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050754)[0;0m ERROR 12-05 14:33:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4050754)[0;0m ERROR 12-05 14:33:47 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4050754)[0;0m ERROR 12-05 14:33:47 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050754)[0;0m ERROR 12-05 14:33:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4050754)[0;0m ERROR 12-05 14:33:47 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4050754)[0;0m ERROR 12-05 14:33:47 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7f5d7409a270>' raised:
[1;36m(EngineCore_DP0 pid=4050754)[0;0m ERROR 12-05 14:33:47 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4050754)[0;0m ERROR 12-05 14:33:47 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4050754)[0;0m ERROR 12-05 14:33:47 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4050754)[0;0m ERROR 12-05 14:33:47 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 14:34:09 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 14:34:09 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 14:34:09 [model.py:1745] Using max model len 131072
INFO 12-05 14:34:09 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4050840)[0;0m INFO 12-05 14:34:16 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4050840)[0;0m INFO 12-05 14:34:17 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:53539 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4050840)[0;0m INFO 12-05 14:34:17 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4050840)[0;0m INFO 12-05 14:34:17 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4050840)[0;0m INFO 12-05 14:34:18 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4050840)[0;0m INFO 12-05 14:34:18 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4050840)[0;0m INFO 12-05 14:34:19 [default_loader.py:314] Loading weights took 0.88 seconds
[1;36m(EngineCore_DP0 pid=4050840)[0;0m INFO 12-05 14:34:19 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.399162 seconds
[1;36m(EngineCore_DP0 pid=4050840)[0;0m INFO 12-05 14:34:23 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4050840)[0;0m INFO 12-05 14:34:23 [backends.py:647] Dynamo bytecode transform time: 3.63 s
[1;36m(EngineCore_DP0 pid=4050840)[0;0m ERROR 12-05 14:34:23 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4050840)[0;0m ERROR 12-05 14:34:23 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4050840)[0;0m ERROR 12-05 14:34:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4050840)[0;0m ERROR 12-05 14:34:23 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4050840)[0;0m ERROR 12-05 14:34:23 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050840)[0;0m ERROR 12-05 14:34:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4050840)[0;0m ERROR 12-05 14:34:23 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4050840)[0;0m ERROR 12-05 14:34:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4050840)[0;0m ERROR 12-05 14:34:23 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4050840)[0;0m ERROR 12-05 14:34:23 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050840)[0;0m ERROR 12-05 14:34:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4050840)[0;0m ERROR 12-05 14:34:23 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4050840)[0;0m ERROR 12-05 14:34:23 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050840)[0;0m ERROR 12-05 14:34:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4050840)[0;0m ERROR 12-05 14:34:23 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4050840)[0;0m ERROR 12-05 14:34:23 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050840)[0;0m ERROR 12-05 14:34:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4050840)[0;0m ERROR 12-05 14:34:23 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4050840)[0;0m ERROR 12-05 14:34:23 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050840)[0;0m ERROR 12-05 14:34:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4050840)[0;0m ERROR 12-05 14:34:23 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4050840)[0;0m ERROR 12-05 14:34:23 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050840)[0;0m ERROR 12-05 14:34:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4050840)[0;0m ERROR 12-05 14:34:23 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4050840)[0;0m ERROR 12-05 14:34:23 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050840)[0;0m ERROR 12-05 14:34:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4050840)[0;0m ERROR 12-05 14:34:23 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4050840)[0;0m ERROR 12-05 14:34:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4050840)[0;0m ERROR 12-05 14:34:23 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4050840)[0;0m ERROR 12-05 14:34:23 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050840)[0;0m ERROR 12-05 14:34:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4050840)[0;0m ERROR 12-05 14:34:23 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4050840)[0;0m ERROR 12-05 14:34:23 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050840)[0;0m ERROR 12-05 14:34:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4050840)[0;0m ERROR 12-05 14:34:23 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4050840)[0;0m ERROR 12-05 14:34:23 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050840)[0;0m ERROR 12-05 14:34:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4050840)[0;0m ERROR 12-05 14:34:23 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4050840)[0;0m ERROR 12-05 14:34:23 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050840)[0;0m ERROR 12-05 14:34:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4050840)[0;0m ERROR 12-05 14:34:23 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4050840)[0;0m ERROR 12-05 14:34:23 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050840)[0;0m ERROR 12-05 14:34:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4050840)[0;0m ERROR 12-05 14:34:23 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4050840)[0;0m ERROR 12-05 14:34:23 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050840)[0;0m ERROR 12-05 14:34:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4050840)[0;0m ERROR 12-05 14:34:23 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4050840)[0;0m ERROR 12-05 14:34:23 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050840)[0;0m ERROR 12-05 14:34:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4050840)[0;0m ERROR 12-05 14:34:23 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4050840)[0;0m ERROR 12-05 14:34:23 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050840)[0;0m ERROR 12-05 14:34:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4050840)[0;0m ERROR 12-05 14:34:23 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4050840)[0;0m ERROR 12-05 14:34:23 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050840)[0;0m ERROR 12-05 14:34:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4050840)[0;0m ERROR 12-05 14:34:23 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4050840)[0;0m ERROR 12-05 14:34:23 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050840)[0;0m ERROR 12-05 14:34:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4050840)[0;0m ERROR 12-05 14:34:23 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4050840)[0;0m ERROR 12-05 14:34:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4050840)[0;0m ERROR 12-05 14:34:23 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4050840)[0;0m ERROR 12-05 14:34:23 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050840)[0;0m ERROR 12-05 14:34:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4050840)[0;0m ERROR 12-05 14:34:23 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4050840)[0;0m ERROR 12-05 14:34:23 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050840)[0;0m ERROR 12-05 14:34:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4050840)[0;0m ERROR 12-05 14:34:23 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4050840)[0;0m ERROR 12-05 14:34:23 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050840)[0;0m ERROR 12-05 14:34:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4050840)[0;0m ERROR 12-05 14:34:23 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4050840)[0;0m ERROR 12-05 14:34:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4050840)[0;0m ERROR 12-05 14:34:23 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4050840)[0;0m ERROR 12-05 14:34:23 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050840)[0;0m ERROR 12-05 14:34:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4050840)[0;0m ERROR 12-05 14:34:23 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4050840)[0;0m ERROR 12-05 14:34:23 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050840)[0;0m ERROR 12-05 14:34:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4050840)[0;0m ERROR 12-05 14:34:23 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4050840)[0;0m ERROR 12-05 14:34:23 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050840)[0;0m ERROR 12-05 14:34:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4050840)[0;0m ERROR 12-05 14:34:23 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4050840)[0;0m ERROR 12-05 14:34:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4050840)[0;0m ERROR 12-05 14:34:23 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4050840)[0;0m ERROR 12-05 14:34:23 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050840)[0;0m ERROR 12-05 14:34:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4050840)[0;0m ERROR 12-05 14:34:23 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4050840)[0;0m ERROR 12-05 14:34:23 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050840)[0;0m ERROR 12-05 14:34:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4050840)[0;0m ERROR 12-05 14:34:23 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4050840)[0;0m ERROR 12-05 14:34:23 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050840)[0;0m ERROR 12-05 14:34:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4050840)[0;0m ERROR 12-05 14:34:23 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4050840)[0;0m ERROR 12-05 14:34:23 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050840)[0;0m ERROR 12-05 14:34:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4050840)[0;0m ERROR 12-05 14:34:23 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4050840)[0;0m ERROR 12-05 14:34:23 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7fe520116f60>' raised:
[1;36m(EngineCore_DP0 pid=4050840)[0;0m ERROR 12-05 14:34:23 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4050840)[0;0m ERROR 12-05 14:34:23 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4050840)[0;0m ERROR 12-05 14:34:23 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4050840)[0;0m ERROR 12-05 14:34:23 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 14:34:45 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 14:34:45 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 14:34:45 [model.py:1745] Using max model len 131072
INFO 12-05 14:34:45 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4050932)[0;0m INFO 12-05 14:34:52 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4050932)[0;0m INFO 12-05 14:34:53 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:56255 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4050932)[0;0m INFO 12-05 14:34:53 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4050932)[0;0m INFO 12-05 14:34:53 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4050932)[0;0m INFO 12-05 14:34:54 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4050932)[0;0m INFO 12-05 14:34:54 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4050932)[0;0m INFO 12-05 14:34:55 [default_loader.py:314] Loading weights took 0.87 seconds
[1;36m(EngineCore_DP0 pid=4050932)[0;0m INFO 12-05 14:34:55 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.393118 seconds
[1;36m(EngineCore_DP0 pid=4050932)[0;0m INFO 12-05 14:34:59 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4050932)[0;0m INFO 12-05 14:34:59 [backends.py:647] Dynamo bytecode transform time: 3.66 s
[1;36m(EngineCore_DP0 pid=4050932)[0;0m ERROR 12-05 14:34:59 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4050932)[0;0m ERROR 12-05 14:34:59 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4050932)[0;0m ERROR 12-05 14:34:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4050932)[0;0m ERROR 12-05 14:34:59 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4050932)[0;0m ERROR 12-05 14:34:59 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050932)[0;0m ERROR 12-05 14:34:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4050932)[0;0m ERROR 12-05 14:34:59 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4050932)[0;0m ERROR 12-05 14:34:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4050932)[0;0m ERROR 12-05 14:34:59 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4050932)[0;0m ERROR 12-05 14:34:59 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050932)[0;0m ERROR 12-05 14:34:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4050932)[0;0m ERROR 12-05 14:34:59 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4050932)[0;0m ERROR 12-05 14:34:59 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050932)[0;0m ERROR 12-05 14:34:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4050932)[0;0m ERROR 12-05 14:34:59 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4050932)[0;0m ERROR 12-05 14:34:59 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050932)[0;0m ERROR 12-05 14:34:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4050932)[0;0m ERROR 12-05 14:34:59 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4050932)[0;0m ERROR 12-05 14:34:59 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050932)[0;0m ERROR 12-05 14:34:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4050932)[0;0m ERROR 12-05 14:34:59 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4050932)[0;0m ERROR 12-05 14:34:59 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050932)[0;0m ERROR 12-05 14:34:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4050932)[0;0m ERROR 12-05 14:34:59 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4050932)[0;0m ERROR 12-05 14:34:59 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050932)[0;0m ERROR 12-05 14:34:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4050932)[0;0m ERROR 12-05 14:34:59 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4050932)[0;0m ERROR 12-05 14:34:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4050932)[0;0m ERROR 12-05 14:34:59 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4050932)[0;0m ERROR 12-05 14:34:59 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050932)[0;0m ERROR 12-05 14:34:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4050932)[0;0m ERROR 12-05 14:34:59 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4050932)[0;0m ERROR 12-05 14:34:59 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050932)[0;0m ERROR 12-05 14:34:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4050932)[0;0m ERROR 12-05 14:34:59 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4050932)[0;0m ERROR 12-05 14:34:59 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050932)[0;0m ERROR 12-05 14:34:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4050932)[0;0m ERROR 12-05 14:34:59 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4050932)[0;0m ERROR 12-05 14:34:59 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050932)[0;0m ERROR 12-05 14:34:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4050932)[0;0m ERROR 12-05 14:34:59 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4050932)[0;0m ERROR 12-05 14:34:59 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050932)[0;0m ERROR 12-05 14:34:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4050932)[0;0m ERROR 12-05 14:34:59 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4050932)[0;0m ERROR 12-05 14:34:59 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050932)[0;0m ERROR 12-05 14:34:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4050932)[0;0m ERROR 12-05 14:34:59 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4050932)[0;0m ERROR 12-05 14:34:59 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050932)[0;0m ERROR 12-05 14:34:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4050932)[0;0m ERROR 12-05 14:34:59 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4050932)[0;0m ERROR 12-05 14:34:59 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050932)[0;0m ERROR 12-05 14:34:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4050932)[0;0m ERROR 12-05 14:34:59 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4050932)[0;0m ERROR 12-05 14:34:59 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050932)[0;0m ERROR 12-05 14:34:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4050932)[0;0m ERROR 12-05 14:34:59 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4050932)[0;0m ERROR 12-05 14:34:59 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050932)[0;0m ERROR 12-05 14:34:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4050932)[0;0m ERROR 12-05 14:34:59 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4050932)[0;0m ERROR 12-05 14:34:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4050932)[0;0m ERROR 12-05 14:34:59 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4050932)[0;0m ERROR 12-05 14:34:59 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050932)[0;0m ERROR 12-05 14:34:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4050932)[0;0m ERROR 12-05 14:34:59 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4050932)[0;0m ERROR 12-05 14:34:59 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050932)[0;0m ERROR 12-05 14:34:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4050932)[0;0m ERROR 12-05 14:34:59 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4050932)[0;0m ERROR 12-05 14:34:59 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050932)[0;0m ERROR 12-05 14:34:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4050932)[0;0m ERROR 12-05 14:34:59 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4050932)[0;0m ERROR 12-05 14:34:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4050932)[0;0m ERROR 12-05 14:34:59 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4050932)[0;0m ERROR 12-05 14:34:59 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050932)[0;0m ERROR 12-05 14:34:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4050932)[0;0m ERROR 12-05 14:34:59 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4050932)[0;0m ERROR 12-05 14:34:59 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050932)[0;0m ERROR 12-05 14:34:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4050932)[0;0m ERROR 12-05 14:34:59 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4050932)[0;0m ERROR 12-05 14:34:59 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050932)[0;0m ERROR 12-05 14:34:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4050932)[0;0m ERROR 12-05 14:34:59 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4050932)[0;0m ERROR 12-05 14:34:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4050932)[0;0m ERROR 12-05 14:34:59 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4050932)[0;0m ERROR 12-05 14:34:59 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050932)[0;0m ERROR 12-05 14:34:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4050932)[0;0m ERROR 12-05 14:34:59 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4050932)[0;0m ERROR 12-05 14:34:59 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050932)[0;0m ERROR 12-05 14:34:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4050932)[0;0m ERROR 12-05 14:34:59 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4050932)[0;0m ERROR 12-05 14:34:59 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050932)[0;0m ERROR 12-05 14:34:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4050932)[0;0m ERROR 12-05 14:34:59 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4050932)[0;0m ERROR 12-05 14:34:59 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4050932)[0;0m ERROR 12-05 14:34:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4050932)[0;0m ERROR 12-05 14:34:59 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4050932)[0;0m ERROR 12-05 14:34:59 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7fdf65336de0>' raised:
[1;36m(EngineCore_DP0 pid=4050932)[0;0m ERROR 12-05 14:34:59 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4050932)[0;0m ERROR 12-05 14:34:59 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4050932)[0;0m ERROR 12-05 14:34:59 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4050932)[0;0m ERROR 12-05 14:34:59 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 14:35:21 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 14:35:21 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 14:35:21 [model.py:1745] Using max model len 131072
INFO 12-05 14:35:21 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4051009)[0;0m INFO 12-05 14:35:29 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4051009)[0;0m INFO 12-05 14:35:29 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:44661 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4051009)[0;0m INFO 12-05 14:35:30 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4051009)[0;0m INFO 12-05 14:35:30 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4051009)[0;0m INFO 12-05 14:35:30 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4051009)[0;0m INFO 12-05 14:35:30 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4051009)[0;0m INFO 12-05 14:35:31 [default_loader.py:314] Loading weights took 0.87 seconds
[1;36m(EngineCore_DP0 pid=4051009)[0;0m INFO 12-05 14:35:32 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.429209 seconds
[1;36m(EngineCore_DP0 pid=4051009)[0;0m INFO 12-05 14:35:36 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4051009)[0;0m INFO 12-05 14:35:36 [backends.py:647] Dynamo bytecode transform time: 3.67 s
[1;36m(EngineCore_DP0 pid=4051009)[0;0m ERROR 12-05 14:35:36 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4051009)[0;0m ERROR 12-05 14:35:36 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4051009)[0;0m ERROR 12-05 14:35:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4051009)[0;0m ERROR 12-05 14:35:36 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4051009)[0;0m ERROR 12-05 14:35:36 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051009)[0;0m ERROR 12-05 14:35:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4051009)[0;0m ERROR 12-05 14:35:36 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4051009)[0;0m ERROR 12-05 14:35:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4051009)[0;0m ERROR 12-05 14:35:36 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4051009)[0;0m ERROR 12-05 14:35:36 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051009)[0;0m ERROR 12-05 14:35:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4051009)[0;0m ERROR 12-05 14:35:36 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4051009)[0;0m ERROR 12-05 14:35:36 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051009)[0;0m ERROR 12-05 14:35:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4051009)[0;0m ERROR 12-05 14:35:36 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4051009)[0;0m ERROR 12-05 14:35:36 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051009)[0;0m ERROR 12-05 14:35:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4051009)[0;0m ERROR 12-05 14:35:36 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4051009)[0;0m ERROR 12-05 14:35:36 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051009)[0;0m ERROR 12-05 14:35:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4051009)[0;0m ERROR 12-05 14:35:36 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4051009)[0;0m ERROR 12-05 14:35:36 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051009)[0;0m ERROR 12-05 14:35:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4051009)[0;0m ERROR 12-05 14:35:36 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4051009)[0;0m ERROR 12-05 14:35:36 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051009)[0;0m ERROR 12-05 14:35:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4051009)[0;0m ERROR 12-05 14:35:36 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4051009)[0;0m ERROR 12-05 14:35:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4051009)[0;0m ERROR 12-05 14:35:36 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4051009)[0;0m ERROR 12-05 14:35:36 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051009)[0;0m ERROR 12-05 14:35:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4051009)[0;0m ERROR 12-05 14:35:36 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4051009)[0;0m ERROR 12-05 14:35:36 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051009)[0;0m ERROR 12-05 14:35:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4051009)[0;0m ERROR 12-05 14:35:36 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4051009)[0;0m ERROR 12-05 14:35:36 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051009)[0;0m ERROR 12-05 14:35:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4051009)[0;0m ERROR 12-05 14:35:36 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4051009)[0;0m ERROR 12-05 14:35:36 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051009)[0;0m ERROR 12-05 14:35:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4051009)[0;0m ERROR 12-05 14:35:36 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4051009)[0;0m ERROR 12-05 14:35:36 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051009)[0;0m ERROR 12-05 14:35:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4051009)[0;0m ERROR 12-05 14:35:36 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4051009)[0;0m ERROR 12-05 14:35:36 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051009)[0;0m ERROR 12-05 14:35:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4051009)[0;0m ERROR 12-05 14:35:36 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4051009)[0;0m ERROR 12-05 14:35:36 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051009)[0;0m ERROR 12-05 14:35:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4051009)[0;0m ERROR 12-05 14:35:36 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4051009)[0;0m ERROR 12-05 14:35:36 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051009)[0;0m ERROR 12-05 14:35:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4051009)[0;0m ERROR 12-05 14:35:36 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4051009)[0;0m ERROR 12-05 14:35:36 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051009)[0;0m ERROR 12-05 14:35:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4051009)[0;0m ERROR 12-05 14:35:36 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4051009)[0;0m ERROR 12-05 14:35:36 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051009)[0;0m ERROR 12-05 14:35:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4051009)[0;0m ERROR 12-05 14:35:36 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4051009)[0;0m ERROR 12-05 14:35:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4051009)[0;0m ERROR 12-05 14:35:36 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4051009)[0;0m ERROR 12-05 14:35:36 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051009)[0;0m ERROR 12-05 14:35:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4051009)[0;0m ERROR 12-05 14:35:36 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4051009)[0;0m ERROR 12-05 14:35:36 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051009)[0;0m ERROR 12-05 14:35:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4051009)[0;0m ERROR 12-05 14:35:36 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4051009)[0;0m ERROR 12-05 14:35:36 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051009)[0;0m ERROR 12-05 14:35:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4051009)[0;0m ERROR 12-05 14:35:36 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4051009)[0;0m ERROR 12-05 14:35:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4051009)[0;0m ERROR 12-05 14:35:36 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4051009)[0;0m ERROR 12-05 14:35:36 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051009)[0;0m ERROR 12-05 14:35:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4051009)[0;0m ERROR 12-05 14:35:36 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4051009)[0;0m ERROR 12-05 14:35:36 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051009)[0;0m ERROR 12-05 14:35:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4051009)[0;0m ERROR 12-05 14:35:36 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4051009)[0;0m ERROR 12-05 14:35:36 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051009)[0;0m ERROR 12-05 14:35:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4051009)[0;0m ERROR 12-05 14:35:36 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4051009)[0;0m ERROR 12-05 14:35:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4051009)[0;0m ERROR 12-05 14:35:36 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4051009)[0;0m ERROR 12-05 14:35:36 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051009)[0;0m ERROR 12-05 14:35:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4051009)[0;0m ERROR 12-05 14:35:36 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4051009)[0;0m ERROR 12-05 14:35:36 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051009)[0;0m ERROR 12-05 14:35:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4051009)[0;0m ERROR 12-05 14:35:36 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4051009)[0;0m ERROR 12-05 14:35:36 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051009)[0;0m ERROR 12-05 14:35:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4051009)[0;0m ERROR 12-05 14:35:36 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4051009)[0;0m ERROR 12-05 14:35:36 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051009)[0;0m ERROR 12-05 14:35:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4051009)[0;0m ERROR 12-05 14:35:36 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4051009)[0;0m ERROR 12-05 14:35:36 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7f32780a6c30>' raised:
[1;36m(EngineCore_DP0 pid=4051009)[0;0m ERROR 12-05 14:35:36 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4051009)[0;0m ERROR 12-05 14:35:36 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4051009)[0;0m ERROR 12-05 14:35:36 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4051009)[0;0m ERROR 12-05 14:35:36 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 14:35:57 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 14:35:58 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 14:35:58 [model.py:1745] Using max model len 131072
INFO 12-05 14:35:58 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4051098)[0;0m INFO 12-05 14:36:05 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4051098)[0;0m INFO 12-05 14:36:06 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:53163 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4051098)[0;0m INFO 12-05 14:36:06 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4051098)[0;0m INFO 12-05 14:36:06 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4051098)[0;0m INFO 12-05 14:36:07 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4051098)[0;0m INFO 12-05 14:36:07 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4051098)[0;0m INFO 12-05 14:36:08 [default_loader.py:314] Loading weights took 0.88 seconds
[1;36m(EngineCore_DP0 pid=4051098)[0;0m INFO 12-05 14:36:08 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.424686 seconds
[1;36m(EngineCore_DP0 pid=4051098)[0;0m INFO 12-05 14:36:12 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4051098)[0;0m INFO 12-05 14:36:12 [backends.py:647] Dynamo bytecode transform time: 3.66 s
[1;36m(EngineCore_DP0 pid=4051098)[0;0m ERROR 12-05 14:36:12 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4051098)[0;0m ERROR 12-05 14:36:12 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4051098)[0;0m ERROR 12-05 14:36:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4051098)[0;0m ERROR 12-05 14:36:12 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4051098)[0;0m ERROR 12-05 14:36:12 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051098)[0;0m ERROR 12-05 14:36:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4051098)[0;0m ERROR 12-05 14:36:12 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4051098)[0;0m ERROR 12-05 14:36:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4051098)[0;0m ERROR 12-05 14:36:12 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4051098)[0;0m ERROR 12-05 14:36:12 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051098)[0;0m ERROR 12-05 14:36:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4051098)[0;0m ERROR 12-05 14:36:12 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4051098)[0;0m ERROR 12-05 14:36:12 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051098)[0;0m ERROR 12-05 14:36:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4051098)[0;0m ERROR 12-05 14:36:12 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4051098)[0;0m ERROR 12-05 14:36:12 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051098)[0;0m ERROR 12-05 14:36:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4051098)[0;0m ERROR 12-05 14:36:12 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4051098)[0;0m ERROR 12-05 14:36:12 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051098)[0;0m ERROR 12-05 14:36:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4051098)[0;0m ERROR 12-05 14:36:12 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4051098)[0;0m ERROR 12-05 14:36:12 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051098)[0;0m ERROR 12-05 14:36:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4051098)[0;0m ERROR 12-05 14:36:12 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4051098)[0;0m ERROR 12-05 14:36:12 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051098)[0;0m ERROR 12-05 14:36:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4051098)[0;0m ERROR 12-05 14:36:12 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4051098)[0;0m ERROR 12-05 14:36:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4051098)[0;0m ERROR 12-05 14:36:12 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4051098)[0;0m ERROR 12-05 14:36:12 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051098)[0;0m ERROR 12-05 14:36:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4051098)[0;0m ERROR 12-05 14:36:12 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4051098)[0;0m ERROR 12-05 14:36:12 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051098)[0;0m ERROR 12-05 14:36:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4051098)[0;0m ERROR 12-05 14:36:12 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4051098)[0;0m ERROR 12-05 14:36:12 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051098)[0;0m ERROR 12-05 14:36:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4051098)[0;0m ERROR 12-05 14:36:12 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4051098)[0;0m ERROR 12-05 14:36:12 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051098)[0;0m ERROR 12-05 14:36:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4051098)[0;0m ERROR 12-05 14:36:12 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4051098)[0;0m ERROR 12-05 14:36:12 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051098)[0;0m ERROR 12-05 14:36:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4051098)[0;0m ERROR 12-05 14:36:12 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4051098)[0;0m ERROR 12-05 14:36:12 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051098)[0;0m ERROR 12-05 14:36:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4051098)[0;0m ERROR 12-05 14:36:12 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4051098)[0;0m ERROR 12-05 14:36:12 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051098)[0;0m ERROR 12-05 14:36:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4051098)[0;0m ERROR 12-05 14:36:12 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4051098)[0;0m ERROR 12-05 14:36:12 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051098)[0;0m ERROR 12-05 14:36:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4051098)[0;0m ERROR 12-05 14:36:12 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4051098)[0;0m ERROR 12-05 14:36:12 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051098)[0;0m ERROR 12-05 14:36:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4051098)[0;0m ERROR 12-05 14:36:12 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4051098)[0;0m ERROR 12-05 14:36:12 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051098)[0;0m ERROR 12-05 14:36:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4051098)[0;0m ERROR 12-05 14:36:12 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4051098)[0;0m ERROR 12-05 14:36:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4051098)[0;0m ERROR 12-05 14:36:12 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4051098)[0;0m ERROR 12-05 14:36:12 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051098)[0;0m ERROR 12-05 14:36:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4051098)[0;0m ERROR 12-05 14:36:12 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4051098)[0;0m ERROR 12-05 14:36:12 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051098)[0;0m ERROR 12-05 14:36:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4051098)[0;0m ERROR 12-05 14:36:12 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4051098)[0;0m ERROR 12-05 14:36:12 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051098)[0;0m ERROR 12-05 14:36:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4051098)[0;0m ERROR 12-05 14:36:12 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4051098)[0;0m ERROR 12-05 14:36:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4051098)[0;0m ERROR 12-05 14:36:12 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4051098)[0;0m ERROR 12-05 14:36:12 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051098)[0;0m ERROR 12-05 14:36:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4051098)[0;0m ERROR 12-05 14:36:12 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4051098)[0;0m ERROR 12-05 14:36:12 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051098)[0;0m ERROR 12-05 14:36:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4051098)[0;0m ERROR 12-05 14:36:12 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4051098)[0;0m ERROR 12-05 14:36:12 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051098)[0;0m ERROR 12-05 14:36:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4051098)[0;0m ERROR 12-05 14:36:12 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4051098)[0;0m ERROR 12-05 14:36:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4051098)[0;0m ERROR 12-05 14:36:12 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4051098)[0;0m ERROR 12-05 14:36:12 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051098)[0;0m ERROR 12-05 14:36:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4051098)[0;0m ERROR 12-05 14:36:12 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4051098)[0;0m ERROR 12-05 14:36:12 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051098)[0;0m ERROR 12-05 14:36:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4051098)[0;0m ERROR 12-05 14:36:12 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4051098)[0;0m ERROR 12-05 14:36:12 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051098)[0;0m ERROR 12-05 14:36:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4051098)[0;0m ERROR 12-05 14:36:12 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4051098)[0;0m ERROR 12-05 14:36:12 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051098)[0;0m ERROR 12-05 14:36:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4051098)[0;0m ERROR 12-05 14:36:12 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4051098)[0;0m ERROR 12-05 14:36:12 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7f28ac1e1970>' raised:
[1;36m(EngineCore_DP0 pid=4051098)[0;0m ERROR 12-05 14:36:12 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4051098)[0;0m ERROR 12-05 14:36:12 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4051098)[0;0m ERROR 12-05 14:36:12 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4051098)[0;0m ERROR 12-05 14:36:12 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 14:36:34 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 14:36:34 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 14:36:34 [model.py:1745] Using max model len 131072
INFO 12-05 14:36:34 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4051275)[0;0m INFO 12-05 14:36:42 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4051275)[0;0m INFO 12-05 14:36:42 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:37699 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4051275)[0;0m INFO 12-05 14:36:42 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4051275)[0;0m INFO 12-05 14:36:43 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4051275)[0;0m INFO 12-05 14:36:43 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4051275)[0;0m INFO 12-05 14:36:43 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4051275)[0;0m INFO 12-05 14:36:44 [default_loader.py:314] Loading weights took 0.87 seconds
[1;36m(EngineCore_DP0 pid=4051275)[0;0m INFO 12-05 14:36:45 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.477816 seconds
[1;36m(EngineCore_DP0 pid=4051275)[0;0m INFO 12-05 14:36:49 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4051275)[0;0m INFO 12-05 14:36:49 [backends.py:647] Dynamo bytecode transform time: 3.59 s
[1;36m(EngineCore_DP0 pid=4051275)[0;0m ERROR 12-05 14:36:49 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4051275)[0;0m ERROR 12-05 14:36:49 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4051275)[0;0m ERROR 12-05 14:36:49 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4051275)[0;0m ERROR 12-05 14:36:49 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4051275)[0;0m ERROR 12-05 14:36:49 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051275)[0;0m ERROR 12-05 14:36:49 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4051275)[0;0m ERROR 12-05 14:36:49 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4051275)[0;0m ERROR 12-05 14:36:49 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4051275)[0;0m ERROR 12-05 14:36:49 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4051275)[0;0m ERROR 12-05 14:36:49 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051275)[0;0m ERROR 12-05 14:36:49 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4051275)[0;0m ERROR 12-05 14:36:49 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4051275)[0;0m ERROR 12-05 14:36:49 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051275)[0;0m ERROR 12-05 14:36:49 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4051275)[0;0m ERROR 12-05 14:36:49 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4051275)[0;0m ERROR 12-05 14:36:49 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051275)[0;0m ERROR 12-05 14:36:49 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4051275)[0;0m ERROR 12-05 14:36:49 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4051275)[0;0m ERROR 12-05 14:36:49 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051275)[0;0m ERROR 12-05 14:36:49 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4051275)[0;0m ERROR 12-05 14:36:49 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4051275)[0;0m ERROR 12-05 14:36:49 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051275)[0;0m ERROR 12-05 14:36:49 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4051275)[0;0m ERROR 12-05 14:36:49 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4051275)[0;0m ERROR 12-05 14:36:49 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051275)[0;0m ERROR 12-05 14:36:49 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4051275)[0;0m ERROR 12-05 14:36:49 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4051275)[0;0m ERROR 12-05 14:36:49 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4051275)[0;0m ERROR 12-05 14:36:49 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4051275)[0;0m ERROR 12-05 14:36:49 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051275)[0;0m ERROR 12-05 14:36:49 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4051275)[0;0m ERROR 12-05 14:36:49 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4051275)[0;0m ERROR 12-05 14:36:49 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051275)[0;0m ERROR 12-05 14:36:49 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4051275)[0;0m ERROR 12-05 14:36:49 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4051275)[0;0m ERROR 12-05 14:36:49 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051275)[0;0m ERROR 12-05 14:36:49 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4051275)[0;0m ERROR 12-05 14:36:49 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4051275)[0;0m ERROR 12-05 14:36:49 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051275)[0;0m ERROR 12-05 14:36:49 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4051275)[0;0m ERROR 12-05 14:36:49 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4051275)[0;0m ERROR 12-05 14:36:49 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051275)[0;0m ERROR 12-05 14:36:49 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4051275)[0;0m ERROR 12-05 14:36:49 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4051275)[0;0m ERROR 12-05 14:36:49 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051275)[0;0m ERROR 12-05 14:36:49 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4051275)[0;0m ERROR 12-05 14:36:49 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4051275)[0;0m ERROR 12-05 14:36:49 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051275)[0;0m ERROR 12-05 14:36:49 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4051275)[0;0m ERROR 12-05 14:36:49 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4051275)[0;0m ERROR 12-05 14:36:49 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051275)[0;0m ERROR 12-05 14:36:49 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4051275)[0;0m ERROR 12-05 14:36:49 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4051275)[0;0m ERROR 12-05 14:36:49 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051275)[0;0m ERROR 12-05 14:36:49 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4051275)[0;0m ERROR 12-05 14:36:49 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4051275)[0;0m ERROR 12-05 14:36:49 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051275)[0;0m ERROR 12-05 14:36:49 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4051275)[0;0m ERROR 12-05 14:36:49 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4051275)[0;0m ERROR 12-05 14:36:49 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4051275)[0;0m ERROR 12-05 14:36:49 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4051275)[0;0m ERROR 12-05 14:36:49 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051275)[0;0m ERROR 12-05 14:36:49 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4051275)[0;0m ERROR 12-05 14:36:49 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4051275)[0;0m ERROR 12-05 14:36:49 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051275)[0;0m ERROR 12-05 14:36:49 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4051275)[0;0m ERROR 12-05 14:36:49 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4051275)[0;0m ERROR 12-05 14:36:49 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051275)[0;0m ERROR 12-05 14:36:49 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4051275)[0;0m ERROR 12-05 14:36:49 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4051275)[0;0m ERROR 12-05 14:36:49 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4051275)[0;0m ERROR 12-05 14:36:49 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4051275)[0;0m ERROR 12-05 14:36:49 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051275)[0;0m ERROR 12-05 14:36:49 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4051275)[0;0m ERROR 12-05 14:36:49 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4051275)[0;0m ERROR 12-05 14:36:49 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051275)[0;0m ERROR 12-05 14:36:49 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4051275)[0;0m ERROR 12-05 14:36:49 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4051275)[0;0m ERROR 12-05 14:36:49 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051275)[0;0m ERROR 12-05 14:36:49 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4051275)[0;0m ERROR 12-05 14:36:49 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4051275)[0;0m ERROR 12-05 14:36:49 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4051275)[0;0m ERROR 12-05 14:36:49 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4051275)[0;0m ERROR 12-05 14:36:49 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051275)[0;0m ERROR 12-05 14:36:49 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4051275)[0;0m ERROR 12-05 14:36:49 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4051275)[0;0m ERROR 12-05 14:36:49 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051275)[0;0m ERROR 12-05 14:36:49 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4051275)[0;0m ERROR 12-05 14:36:49 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4051275)[0;0m ERROR 12-05 14:36:49 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051275)[0;0m ERROR 12-05 14:36:49 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4051275)[0;0m ERROR 12-05 14:36:49 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4051275)[0;0m ERROR 12-05 14:36:49 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051275)[0;0m ERROR 12-05 14:36:49 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4051275)[0;0m ERROR 12-05 14:36:49 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4051275)[0;0m ERROR 12-05 14:36:49 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7f446007bd40>' raised:
[1;36m(EngineCore_DP0 pid=4051275)[0;0m ERROR 12-05 14:36:49 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4051275)[0;0m ERROR 12-05 14:36:49 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4051275)[0;0m ERROR 12-05 14:36:49 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4051275)[0;0m ERROR 12-05 14:36:49 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 14:37:10 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 14:37:10 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 14:37:10 [model.py:1745] Using max model len 131072
INFO 12-05 14:37:10 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4051361)[0;0m INFO 12-05 14:37:18 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4051361)[0;0m INFO 12-05 14:37:19 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:42913 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4051361)[0;0m INFO 12-05 14:37:19 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4051361)[0;0m INFO 12-05 14:37:19 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4051361)[0;0m INFO 12-05 14:37:19 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4051361)[0;0m INFO 12-05 14:37:19 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4051361)[0;0m INFO 12-05 14:37:21 [default_loader.py:314] Loading weights took 0.87 seconds
[1;36m(EngineCore_DP0 pid=4051361)[0;0m INFO 12-05 14:37:21 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.415174 seconds
[1;36m(EngineCore_DP0 pid=4051361)[0;0m INFO 12-05 14:37:25 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4051361)[0;0m INFO 12-05 14:37:25 [backends.py:647] Dynamo bytecode transform time: 3.65 s
[1;36m(EngineCore_DP0 pid=4051361)[0;0m ERROR 12-05 14:37:25 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4051361)[0;0m ERROR 12-05 14:37:25 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4051361)[0;0m ERROR 12-05 14:37:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4051361)[0;0m ERROR 12-05 14:37:25 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4051361)[0;0m ERROR 12-05 14:37:25 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051361)[0;0m ERROR 12-05 14:37:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4051361)[0;0m ERROR 12-05 14:37:25 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4051361)[0;0m ERROR 12-05 14:37:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4051361)[0;0m ERROR 12-05 14:37:25 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4051361)[0;0m ERROR 12-05 14:37:25 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051361)[0;0m ERROR 12-05 14:37:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4051361)[0;0m ERROR 12-05 14:37:25 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4051361)[0;0m ERROR 12-05 14:37:25 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051361)[0;0m ERROR 12-05 14:37:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4051361)[0;0m ERROR 12-05 14:37:25 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4051361)[0;0m ERROR 12-05 14:37:25 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051361)[0;0m ERROR 12-05 14:37:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4051361)[0;0m ERROR 12-05 14:37:25 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4051361)[0;0m ERROR 12-05 14:37:25 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051361)[0;0m ERROR 12-05 14:37:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4051361)[0;0m ERROR 12-05 14:37:25 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4051361)[0;0m ERROR 12-05 14:37:25 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051361)[0;0m ERROR 12-05 14:37:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4051361)[0;0m ERROR 12-05 14:37:25 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4051361)[0;0m ERROR 12-05 14:37:25 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051361)[0;0m ERROR 12-05 14:37:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4051361)[0;0m ERROR 12-05 14:37:25 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4051361)[0;0m ERROR 12-05 14:37:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4051361)[0;0m ERROR 12-05 14:37:25 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4051361)[0;0m ERROR 12-05 14:37:25 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051361)[0;0m ERROR 12-05 14:37:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4051361)[0;0m ERROR 12-05 14:37:25 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4051361)[0;0m ERROR 12-05 14:37:25 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051361)[0;0m ERROR 12-05 14:37:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4051361)[0;0m ERROR 12-05 14:37:25 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4051361)[0;0m ERROR 12-05 14:37:25 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051361)[0;0m ERROR 12-05 14:37:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4051361)[0;0m ERROR 12-05 14:37:25 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4051361)[0;0m ERROR 12-05 14:37:25 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051361)[0;0m ERROR 12-05 14:37:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4051361)[0;0m ERROR 12-05 14:37:25 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4051361)[0;0m ERROR 12-05 14:37:25 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051361)[0;0m ERROR 12-05 14:37:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4051361)[0;0m ERROR 12-05 14:37:25 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4051361)[0;0m ERROR 12-05 14:37:25 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051361)[0;0m ERROR 12-05 14:37:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4051361)[0;0m ERROR 12-05 14:37:25 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4051361)[0;0m ERROR 12-05 14:37:25 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051361)[0;0m ERROR 12-05 14:37:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4051361)[0;0m ERROR 12-05 14:37:25 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4051361)[0;0m ERROR 12-05 14:37:25 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051361)[0;0m ERROR 12-05 14:37:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4051361)[0;0m ERROR 12-05 14:37:25 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4051361)[0;0m ERROR 12-05 14:37:25 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051361)[0;0m ERROR 12-05 14:37:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4051361)[0;0m ERROR 12-05 14:37:25 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4051361)[0;0m ERROR 12-05 14:37:25 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051361)[0;0m ERROR 12-05 14:37:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4051361)[0;0m ERROR 12-05 14:37:25 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4051361)[0;0m ERROR 12-05 14:37:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4051361)[0;0m ERROR 12-05 14:37:25 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4051361)[0;0m ERROR 12-05 14:37:25 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051361)[0;0m ERROR 12-05 14:37:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4051361)[0;0m ERROR 12-05 14:37:25 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4051361)[0;0m ERROR 12-05 14:37:25 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051361)[0;0m ERROR 12-05 14:37:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4051361)[0;0m ERROR 12-05 14:37:25 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4051361)[0;0m ERROR 12-05 14:37:25 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051361)[0;0m ERROR 12-05 14:37:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4051361)[0;0m ERROR 12-05 14:37:25 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4051361)[0;0m ERROR 12-05 14:37:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4051361)[0;0m ERROR 12-05 14:37:25 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4051361)[0;0m ERROR 12-05 14:37:25 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051361)[0;0m ERROR 12-05 14:37:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4051361)[0;0m ERROR 12-05 14:37:25 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4051361)[0;0m ERROR 12-05 14:37:25 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051361)[0;0m ERROR 12-05 14:37:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4051361)[0;0m ERROR 12-05 14:37:25 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4051361)[0;0m ERROR 12-05 14:37:25 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051361)[0;0m ERROR 12-05 14:37:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4051361)[0;0m ERROR 12-05 14:37:25 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4051361)[0;0m ERROR 12-05 14:37:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4051361)[0;0m ERROR 12-05 14:37:25 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4051361)[0;0m ERROR 12-05 14:37:25 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051361)[0;0m ERROR 12-05 14:37:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4051361)[0;0m ERROR 12-05 14:37:25 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4051361)[0;0m ERROR 12-05 14:37:25 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051361)[0;0m ERROR 12-05 14:37:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4051361)[0;0m ERROR 12-05 14:37:25 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4051361)[0;0m ERROR 12-05 14:37:25 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051361)[0;0m ERROR 12-05 14:37:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4051361)[0;0m ERROR 12-05 14:37:25 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4051361)[0;0m ERROR 12-05 14:37:25 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051361)[0;0m ERROR 12-05 14:37:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4051361)[0;0m ERROR 12-05 14:37:25 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4051361)[0;0m ERROR 12-05 14:37:25 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7f5835722ea0>' raised:
[1;36m(EngineCore_DP0 pid=4051361)[0;0m ERROR 12-05 14:37:25 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4051361)[0;0m ERROR 12-05 14:37:25 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4051361)[0;0m ERROR 12-05 14:37:25 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4051361)[0;0m ERROR 12-05 14:37:25 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 14:37:47 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 14:37:47 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 14:37:47 [model.py:1745] Using max model len 131072
INFO 12-05 14:37:47 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4051448)[0;0m INFO 12-05 14:37:54 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4051448)[0;0m INFO 12-05 14:37:55 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:51973 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4051448)[0;0m INFO 12-05 14:37:55 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4051448)[0;0m INFO 12-05 14:37:55 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4051448)[0;0m INFO 12-05 14:37:56 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4051448)[0;0m INFO 12-05 14:37:56 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4051448)[0;0m INFO 12-05 14:37:57 [default_loader.py:314] Loading weights took 0.87 seconds
[1;36m(EngineCore_DP0 pid=4051448)[0;0m INFO 12-05 14:37:57 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.390789 seconds
[1;36m(EngineCore_DP0 pid=4051448)[0;0m INFO 12-05 14:38:01 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4051448)[0;0m INFO 12-05 14:38:01 [backends.py:647] Dynamo bytecode transform time: 3.60 s
[1;36m(EngineCore_DP0 pid=4051448)[0;0m ERROR 12-05 14:38:01 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4051448)[0;0m ERROR 12-05 14:38:01 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4051448)[0;0m ERROR 12-05 14:38:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4051448)[0;0m ERROR 12-05 14:38:01 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4051448)[0;0m ERROR 12-05 14:38:01 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051448)[0;0m ERROR 12-05 14:38:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4051448)[0;0m ERROR 12-05 14:38:01 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4051448)[0;0m ERROR 12-05 14:38:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4051448)[0;0m ERROR 12-05 14:38:01 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4051448)[0;0m ERROR 12-05 14:38:01 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051448)[0;0m ERROR 12-05 14:38:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4051448)[0;0m ERROR 12-05 14:38:01 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4051448)[0;0m ERROR 12-05 14:38:01 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051448)[0;0m ERROR 12-05 14:38:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4051448)[0;0m ERROR 12-05 14:38:01 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4051448)[0;0m ERROR 12-05 14:38:01 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051448)[0;0m ERROR 12-05 14:38:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4051448)[0;0m ERROR 12-05 14:38:01 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4051448)[0;0m ERROR 12-05 14:38:01 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051448)[0;0m ERROR 12-05 14:38:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4051448)[0;0m ERROR 12-05 14:38:01 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4051448)[0;0m ERROR 12-05 14:38:01 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051448)[0;0m ERROR 12-05 14:38:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4051448)[0;0m ERROR 12-05 14:38:01 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4051448)[0;0m ERROR 12-05 14:38:01 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051448)[0;0m ERROR 12-05 14:38:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4051448)[0;0m ERROR 12-05 14:38:01 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4051448)[0;0m ERROR 12-05 14:38:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4051448)[0;0m ERROR 12-05 14:38:01 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4051448)[0;0m ERROR 12-05 14:38:01 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051448)[0;0m ERROR 12-05 14:38:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4051448)[0;0m ERROR 12-05 14:38:01 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4051448)[0;0m ERROR 12-05 14:38:01 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051448)[0;0m ERROR 12-05 14:38:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4051448)[0;0m ERROR 12-05 14:38:01 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4051448)[0;0m ERROR 12-05 14:38:01 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051448)[0;0m ERROR 12-05 14:38:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4051448)[0;0m ERROR 12-05 14:38:01 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4051448)[0;0m ERROR 12-05 14:38:01 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051448)[0;0m ERROR 12-05 14:38:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4051448)[0;0m ERROR 12-05 14:38:01 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4051448)[0;0m ERROR 12-05 14:38:01 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051448)[0;0m ERROR 12-05 14:38:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4051448)[0;0m ERROR 12-05 14:38:01 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4051448)[0;0m ERROR 12-05 14:38:01 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051448)[0;0m ERROR 12-05 14:38:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4051448)[0;0m ERROR 12-05 14:38:01 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4051448)[0;0m ERROR 12-05 14:38:01 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051448)[0;0m ERROR 12-05 14:38:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4051448)[0;0m ERROR 12-05 14:38:01 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4051448)[0;0m ERROR 12-05 14:38:01 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051448)[0;0m ERROR 12-05 14:38:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4051448)[0;0m ERROR 12-05 14:38:01 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4051448)[0;0m ERROR 12-05 14:38:01 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051448)[0;0m ERROR 12-05 14:38:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4051448)[0;0m ERROR 12-05 14:38:01 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4051448)[0;0m ERROR 12-05 14:38:01 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051448)[0;0m ERROR 12-05 14:38:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4051448)[0;0m ERROR 12-05 14:38:01 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4051448)[0;0m ERROR 12-05 14:38:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4051448)[0;0m ERROR 12-05 14:38:01 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4051448)[0;0m ERROR 12-05 14:38:01 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051448)[0;0m ERROR 12-05 14:38:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4051448)[0;0m ERROR 12-05 14:38:01 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4051448)[0;0m ERROR 12-05 14:38:01 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051448)[0;0m ERROR 12-05 14:38:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4051448)[0;0m ERROR 12-05 14:38:01 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4051448)[0;0m ERROR 12-05 14:38:01 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051448)[0;0m ERROR 12-05 14:38:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4051448)[0;0m ERROR 12-05 14:38:01 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4051448)[0;0m ERROR 12-05 14:38:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4051448)[0;0m ERROR 12-05 14:38:01 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4051448)[0;0m ERROR 12-05 14:38:01 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051448)[0;0m ERROR 12-05 14:38:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4051448)[0;0m ERROR 12-05 14:38:01 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4051448)[0;0m ERROR 12-05 14:38:01 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051448)[0;0m ERROR 12-05 14:38:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4051448)[0;0m ERROR 12-05 14:38:01 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4051448)[0;0m ERROR 12-05 14:38:01 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051448)[0;0m ERROR 12-05 14:38:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4051448)[0;0m ERROR 12-05 14:38:01 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4051448)[0;0m ERROR 12-05 14:38:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4051448)[0;0m ERROR 12-05 14:38:01 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4051448)[0;0m ERROR 12-05 14:38:01 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051448)[0;0m ERROR 12-05 14:38:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4051448)[0;0m ERROR 12-05 14:38:01 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4051448)[0;0m ERROR 12-05 14:38:01 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051448)[0;0m ERROR 12-05 14:38:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4051448)[0;0m ERROR 12-05 14:38:01 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4051448)[0;0m ERROR 12-05 14:38:01 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051448)[0;0m ERROR 12-05 14:38:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4051448)[0;0m ERROR 12-05 14:38:01 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4051448)[0;0m ERROR 12-05 14:38:01 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051448)[0;0m ERROR 12-05 14:38:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4051448)[0;0m ERROR 12-05 14:38:01 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4051448)[0;0m ERROR 12-05 14:38:01 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7fb6a00a8d70>' raised:
[1;36m(EngineCore_DP0 pid=4051448)[0;0m ERROR 12-05 14:38:01 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4051448)[0;0m ERROR 12-05 14:38:01 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4051448)[0;0m ERROR 12-05 14:38:01 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4051448)[0;0m ERROR 12-05 14:38:01 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 14:38:23 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 14:38:23 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 14:38:23 [model.py:1745] Using max model len 131072
INFO 12-05 14:38:23 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4051525)[0;0m INFO 12-05 14:38:30 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4051525)[0;0m INFO 12-05 14:38:31 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:55413 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4051525)[0;0m INFO 12-05 14:38:31 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4051525)[0;0m INFO 12-05 14:38:31 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4051525)[0;0m INFO 12-05 14:38:32 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4051525)[0;0m INFO 12-05 14:38:32 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4051525)[0;0m INFO 12-05 14:38:33 [default_loader.py:314] Loading weights took 0.88 seconds
[1;36m(EngineCore_DP0 pid=4051525)[0;0m INFO 12-05 14:38:33 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.414106 seconds
[1;36m(EngineCore_DP0 pid=4051525)[0;0m INFO 12-05 14:38:37 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4051525)[0;0m INFO 12-05 14:38:37 [backends.py:647] Dynamo bytecode transform time: 3.61 s
[1;36m(EngineCore_DP0 pid=4051525)[0;0m ERROR 12-05 14:38:37 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4051525)[0;0m ERROR 12-05 14:38:37 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4051525)[0;0m ERROR 12-05 14:38:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4051525)[0;0m ERROR 12-05 14:38:37 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4051525)[0;0m ERROR 12-05 14:38:37 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051525)[0;0m ERROR 12-05 14:38:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4051525)[0;0m ERROR 12-05 14:38:37 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4051525)[0;0m ERROR 12-05 14:38:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4051525)[0;0m ERROR 12-05 14:38:37 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4051525)[0;0m ERROR 12-05 14:38:37 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051525)[0;0m ERROR 12-05 14:38:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4051525)[0;0m ERROR 12-05 14:38:37 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4051525)[0;0m ERROR 12-05 14:38:37 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051525)[0;0m ERROR 12-05 14:38:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4051525)[0;0m ERROR 12-05 14:38:37 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4051525)[0;0m ERROR 12-05 14:38:37 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051525)[0;0m ERROR 12-05 14:38:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4051525)[0;0m ERROR 12-05 14:38:37 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4051525)[0;0m ERROR 12-05 14:38:37 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051525)[0;0m ERROR 12-05 14:38:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4051525)[0;0m ERROR 12-05 14:38:37 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4051525)[0;0m ERROR 12-05 14:38:37 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051525)[0;0m ERROR 12-05 14:38:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4051525)[0;0m ERROR 12-05 14:38:37 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4051525)[0;0m ERROR 12-05 14:38:37 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051525)[0;0m ERROR 12-05 14:38:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4051525)[0;0m ERROR 12-05 14:38:37 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4051525)[0;0m ERROR 12-05 14:38:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4051525)[0;0m ERROR 12-05 14:38:37 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4051525)[0;0m ERROR 12-05 14:38:37 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051525)[0;0m ERROR 12-05 14:38:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4051525)[0;0m ERROR 12-05 14:38:37 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4051525)[0;0m ERROR 12-05 14:38:37 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051525)[0;0m ERROR 12-05 14:38:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4051525)[0;0m ERROR 12-05 14:38:37 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4051525)[0;0m ERROR 12-05 14:38:37 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051525)[0;0m ERROR 12-05 14:38:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4051525)[0;0m ERROR 12-05 14:38:37 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4051525)[0;0m ERROR 12-05 14:38:37 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051525)[0;0m ERROR 12-05 14:38:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4051525)[0;0m ERROR 12-05 14:38:37 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4051525)[0;0m ERROR 12-05 14:38:37 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051525)[0;0m ERROR 12-05 14:38:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4051525)[0;0m ERROR 12-05 14:38:37 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4051525)[0;0m ERROR 12-05 14:38:37 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051525)[0;0m ERROR 12-05 14:38:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4051525)[0;0m ERROR 12-05 14:38:37 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4051525)[0;0m ERROR 12-05 14:38:37 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051525)[0;0m ERROR 12-05 14:38:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4051525)[0;0m ERROR 12-05 14:38:37 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4051525)[0;0m ERROR 12-05 14:38:37 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051525)[0;0m ERROR 12-05 14:38:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4051525)[0;0m ERROR 12-05 14:38:37 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4051525)[0;0m ERROR 12-05 14:38:37 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051525)[0;0m ERROR 12-05 14:38:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4051525)[0;0m ERROR 12-05 14:38:37 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4051525)[0;0m ERROR 12-05 14:38:37 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051525)[0;0m ERROR 12-05 14:38:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4051525)[0;0m ERROR 12-05 14:38:37 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4051525)[0;0m ERROR 12-05 14:38:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4051525)[0;0m ERROR 12-05 14:38:37 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4051525)[0;0m ERROR 12-05 14:38:37 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051525)[0;0m ERROR 12-05 14:38:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4051525)[0;0m ERROR 12-05 14:38:37 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4051525)[0;0m ERROR 12-05 14:38:37 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051525)[0;0m ERROR 12-05 14:38:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4051525)[0;0m ERROR 12-05 14:38:37 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4051525)[0;0m ERROR 12-05 14:38:37 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051525)[0;0m ERROR 12-05 14:38:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4051525)[0;0m ERROR 12-05 14:38:37 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4051525)[0;0m ERROR 12-05 14:38:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4051525)[0;0m ERROR 12-05 14:38:37 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4051525)[0;0m ERROR 12-05 14:38:37 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051525)[0;0m ERROR 12-05 14:38:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4051525)[0;0m ERROR 12-05 14:38:37 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4051525)[0;0m ERROR 12-05 14:38:37 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051525)[0;0m ERROR 12-05 14:38:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4051525)[0;0m ERROR 12-05 14:38:37 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4051525)[0;0m ERROR 12-05 14:38:37 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051525)[0;0m ERROR 12-05 14:38:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4051525)[0;0m ERROR 12-05 14:38:37 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4051525)[0;0m ERROR 12-05 14:38:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4051525)[0;0m ERROR 12-05 14:38:37 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4051525)[0;0m ERROR 12-05 14:38:37 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051525)[0;0m ERROR 12-05 14:38:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4051525)[0;0m ERROR 12-05 14:38:37 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4051525)[0;0m ERROR 12-05 14:38:37 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051525)[0;0m ERROR 12-05 14:38:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4051525)[0;0m ERROR 12-05 14:38:37 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4051525)[0;0m ERROR 12-05 14:38:37 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051525)[0;0m ERROR 12-05 14:38:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4051525)[0;0m ERROR 12-05 14:38:37 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4051525)[0;0m ERROR 12-05 14:38:37 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051525)[0;0m ERROR 12-05 14:38:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4051525)[0;0m ERROR 12-05 14:38:37 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4051525)[0;0m ERROR 12-05 14:38:37 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7f6850275a30>' raised:
[1;36m(EngineCore_DP0 pid=4051525)[0;0m ERROR 12-05 14:38:37 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4051525)[0;0m ERROR 12-05 14:38:37 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4051525)[0;0m ERROR 12-05 14:38:37 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4051525)[0;0m ERROR 12-05 14:38:37 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 14:38:59 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 14:38:59 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 14:38:59 [model.py:1745] Using max model len 131072
INFO 12-05 14:38:59 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4051614)[0;0m INFO 12-05 14:39:06 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4051614)[0;0m INFO 12-05 14:39:07 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:43033 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4051614)[0;0m INFO 12-05 14:39:07 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4051614)[0;0m INFO 12-05 14:39:08 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4051614)[0;0m INFO 12-05 14:39:08 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4051614)[0;0m INFO 12-05 14:39:08 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4051614)[0;0m INFO 12-05 14:39:09 [default_loader.py:314] Loading weights took 0.87 seconds
[1;36m(EngineCore_DP0 pid=4051614)[0;0m INFO 12-05 14:39:10 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.392493 seconds
[1;36m(EngineCore_DP0 pid=4051614)[0;0m INFO 12-05 14:39:13 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4051614)[0;0m INFO 12-05 14:39:13 [backends.py:647] Dynamo bytecode transform time: 3.62 s
[1;36m(EngineCore_DP0 pid=4051614)[0;0m ERROR 12-05 14:39:14 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4051614)[0;0m ERROR 12-05 14:39:14 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4051614)[0;0m ERROR 12-05 14:39:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4051614)[0;0m ERROR 12-05 14:39:14 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4051614)[0;0m ERROR 12-05 14:39:14 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051614)[0;0m ERROR 12-05 14:39:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4051614)[0;0m ERROR 12-05 14:39:14 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4051614)[0;0m ERROR 12-05 14:39:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4051614)[0;0m ERROR 12-05 14:39:14 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4051614)[0;0m ERROR 12-05 14:39:14 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051614)[0;0m ERROR 12-05 14:39:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4051614)[0;0m ERROR 12-05 14:39:14 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4051614)[0;0m ERROR 12-05 14:39:14 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051614)[0;0m ERROR 12-05 14:39:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4051614)[0;0m ERROR 12-05 14:39:14 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4051614)[0;0m ERROR 12-05 14:39:14 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051614)[0;0m ERROR 12-05 14:39:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4051614)[0;0m ERROR 12-05 14:39:14 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4051614)[0;0m ERROR 12-05 14:39:14 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051614)[0;0m ERROR 12-05 14:39:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4051614)[0;0m ERROR 12-05 14:39:14 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4051614)[0;0m ERROR 12-05 14:39:14 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051614)[0;0m ERROR 12-05 14:39:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4051614)[0;0m ERROR 12-05 14:39:14 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4051614)[0;0m ERROR 12-05 14:39:14 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051614)[0;0m ERROR 12-05 14:39:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4051614)[0;0m ERROR 12-05 14:39:14 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4051614)[0;0m ERROR 12-05 14:39:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4051614)[0;0m ERROR 12-05 14:39:14 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4051614)[0;0m ERROR 12-05 14:39:14 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051614)[0;0m ERROR 12-05 14:39:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4051614)[0;0m ERROR 12-05 14:39:14 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4051614)[0;0m ERROR 12-05 14:39:14 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051614)[0;0m ERROR 12-05 14:39:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4051614)[0;0m ERROR 12-05 14:39:14 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4051614)[0;0m ERROR 12-05 14:39:14 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051614)[0;0m ERROR 12-05 14:39:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4051614)[0;0m ERROR 12-05 14:39:14 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4051614)[0;0m ERROR 12-05 14:39:14 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051614)[0;0m ERROR 12-05 14:39:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4051614)[0;0m ERROR 12-05 14:39:14 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4051614)[0;0m ERROR 12-05 14:39:14 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051614)[0;0m ERROR 12-05 14:39:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4051614)[0;0m ERROR 12-05 14:39:14 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4051614)[0;0m ERROR 12-05 14:39:14 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051614)[0;0m ERROR 12-05 14:39:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4051614)[0;0m ERROR 12-05 14:39:14 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4051614)[0;0m ERROR 12-05 14:39:14 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051614)[0;0m ERROR 12-05 14:39:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4051614)[0;0m ERROR 12-05 14:39:14 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4051614)[0;0m ERROR 12-05 14:39:14 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051614)[0;0m ERROR 12-05 14:39:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4051614)[0;0m ERROR 12-05 14:39:14 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4051614)[0;0m ERROR 12-05 14:39:14 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051614)[0;0m ERROR 12-05 14:39:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4051614)[0;0m ERROR 12-05 14:39:14 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4051614)[0;0m ERROR 12-05 14:39:14 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051614)[0;0m ERROR 12-05 14:39:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4051614)[0;0m ERROR 12-05 14:39:14 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4051614)[0;0m ERROR 12-05 14:39:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4051614)[0;0m ERROR 12-05 14:39:14 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4051614)[0;0m ERROR 12-05 14:39:14 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051614)[0;0m ERROR 12-05 14:39:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4051614)[0;0m ERROR 12-05 14:39:14 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4051614)[0;0m ERROR 12-05 14:39:14 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051614)[0;0m ERROR 12-05 14:39:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4051614)[0;0m ERROR 12-05 14:39:14 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4051614)[0;0m ERROR 12-05 14:39:14 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051614)[0;0m ERROR 12-05 14:39:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4051614)[0;0m ERROR 12-05 14:39:14 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4051614)[0;0m ERROR 12-05 14:39:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4051614)[0;0m ERROR 12-05 14:39:14 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4051614)[0;0m ERROR 12-05 14:39:14 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051614)[0;0m ERROR 12-05 14:39:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4051614)[0;0m ERROR 12-05 14:39:14 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4051614)[0;0m ERROR 12-05 14:39:14 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051614)[0;0m ERROR 12-05 14:39:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4051614)[0;0m ERROR 12-05 14:39:14 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4051614)[0;0m ERROR 12-05 14:39:14 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051614)[0;0m ERROR 12-05 14:39:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4051614)[0;0m ERROR 12-05 14:39:14 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4051614)[0;0m ERROR 12-05 14:39:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4051614)[0;0m ERROR 12-05 14:39:14 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4051614)[0;0m ERROR 12-05 14:39:14 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051614)[0;0m ERROR 12-05 14:39:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4051614)[0;0m ERROR 12-05 14:39:14 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4051614)[0;0m ERROR 12-05 14:39:14 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051614)[0;0m ERROR 12-05 14:39:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4051614)[0;0m ERROR 12-05 14:39:14 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4051614)[0;0m ERROR 12-05 14:39:14 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051614)[0;0m ERROR 12-05 14:39:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4051614)[0;0m ERROR 12-05 14:39:14 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4051614)[0;0m ERROR 12-05 14:39:14 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051614)[0;0m ERROR 12-05 14:39:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4051614)[0;0m ERROR 12-05 14:39:14 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4051614)[0;0m ERROR 12-05 14:39:14 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7fe3ece375c0>' raised:
[1;36m(EngineCore_DP0 pid=4051614)[0;0m ERROR 12-05 14:39:14 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4051614)[0;0m ERROR 12-05 14:39:14 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4051614)[0;0m ERROR 12-05 14:39:14 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4051614)[0;0m ERROR 12-05 14:39:14 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 14:39:35 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 14:39:35 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 14:39:35 [model.py:1745] Using max model len 131072
INFO 12-05 14:39:35 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4051696)[0;0m INFO 12-05 14:39:42 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4051696)[0;0m INFO 12-05 14:39:43 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:45873 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4051696)[0;0m INFO 12-05 14:39:43 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4051696)[0;0m INFO 12-05 14:39:44 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4051696)[0;0m INFO 12-05 14:39:44 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4051696)[0;0m INFO 12-05 14:39:44 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4051696)[0;0m INFO 12-05 14:39:45 [default_loader.py:314] Loading weights took 0.88 seconds
[1;36m(EngineCore_DP0 pid=4051696)[0;0m INFO 12-05 14:39:46 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.416862 seconds
[1;36m(EngineCore_DP0 pid=4051696)[0;0m INFO 12-05 14:39:50 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4051696)[0;0m INFO 12-05 14:39:50 [backends.py:647] Dynamo bytecode transform time: 3.62 s
[1;36m(EngineCore_DP0 pid=4051696)[0;0m ERROR 12-05 14:39:50 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4051696)[0;0m ERROR 12-05 14:39:50 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4051696)[0;0m ERROR 12-05 14:39:50 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4051696)[0;0m ERROR 12-05 14:39:50 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4051696)[0;0m ERROR 12-05 14:39:50 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051696)[0;0m ERROR 12-05 14:39:50 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4051696)[0;0m ERROR 12-05 14:39:50 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4051696)[0;0m ERROR 12-05 14:39:50 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4051696)[0;0m ERROR 12-05 14:39:50 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4051696)[0;0m ERROR 12-05 14:39:50 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051696)[0;0m ERROR 12-05 14:39:50 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4051696)[0;0m ERROR 12-05 14:39:50 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4051696)[0;0m ERROR 12-05 14:39:50 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051696)[0;0m ERROR 12-05 14:39:50 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4051696)[0;0m ERROR 12-05 14:39:50 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4051696)[0;0m ERROR 12-05 14:39:50 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051696)[0;0m ERROR 12-05 14:39:50 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4051696)[0;0m ERROR 12-05 14:39:50 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4051696)[0;0m ERROR 12-05 14:39:50 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051696)[0;0m ERROR 12-05 14:39:50 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4051696)[0;0m ERROR 12-05 14:39:50 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4051696)[0;0m ERROR 12-05 14:39:50 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051696)[0;0m ERROR 12-05 14:39:50 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4051696)[0;0m ERROR 12-05 14:39:50 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4051696)[0;0m ERROR 12-05 14:39:50 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051696)[0;0m ERROR 12-05 14:39:50 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4051696)[0;0m ERROR 12-05 14:39:50 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4051696)[0;0m ERROR 12-05 14:39:50 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4051696)[0;0m ERROR 12-05 14:39:50 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4051696)[0;0m ERROR 12-05 14:39:50 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051696)[0;0m ERROR 12-05 14:39:50 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4051696)[0;0m ERROR 12-05 14:39:50 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4051696)[0;0m ERROR 12-05 14:39:50 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051696)[0;0m ERROR 12-05 14:39:50 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4051696)[0;0m ERROR 12-05 14:39:50 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4051696)[0;0m ERROR 12-05 14:39:50 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051696)[0;0m ERROR 12-05 14:39:50 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4051696)[0;0m ERROR 12-05 14:39:50 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4051696)[0;0m ERROR 12-05 14:39:50 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051696)[0;0m ERROR 12-05 14:39:50 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4051696)[0;0m ERROR 12-05 14:39:50 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4051696)[0;0m ERROR 12-05 14:39:50 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051696)[0;0m ERROR 12-05 14:39:50 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4051696)[0;0m ERROR 12-05 14:39:50 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4051696)[0;0m ERROR 12-05 14:39:50 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051696)[0;0m ERROR 12-05 14:39:50 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4051696)[0;0m ERROR 12-05 14:39:50 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4051696)[0;0m ERROR 12-05 14:39:50 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051696)[0;0m ERROR 12-05 14:39:50 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4051696)[0;0m ERROR 12-05 14:39:50 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4051696)[0;0m ERROR 12-05 14:39:50 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051696)[0;0m ERROR 12-05 14:39:50 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4051696)[0;0m ERROR 12-05 14:39:50 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4051696)[0;0m ERROR 12-05 14:39:50 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051696)[0;0m ERROR 12-05 14:39:50 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4051696)[0;0m ERROR 12-05 14:39:50 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4051696)[0;0m ERROR 12-05 14:39:50 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051696)[0;0m ERROR 12-05 14:39:50 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4051696)[0;0m ERROR 12-05 14:39:50 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4051696)[0;0m ERROR 12-05 14:39:50 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4051696)[0;0m ERROR 12-05 14:39:50 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4051696)[0;0m ERROR 12-05 14:39:50 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051696)[0;0m ERROR 12-05 14:39:50 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4051696)[0;0m ERROR 12-05 14:39:50 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4051696)[0;0m ERROR 12-05 14:39:50 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051696)[0;0m ERROR 12-05 14:39:50 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4051696)[0;0m ERROR 12-05 14:39:50 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4051696)[0;0m ERROR 12-05 14:39:50 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051696)[0;0m ERROR 12-05 14:39:50 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4051696)[0;0m ERROR 12-05 14:39:50 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4051696)[0;0m ERROR 12-05 14:39:50 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4051696)[0;0m ERROR 12-05 14:39:50 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4051696)[0;0m ERROR 12-05 14:39:50 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051696)[0;0m ERROR 12-05 14:39:50 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4051696)[0;0m ERROR 12-05 14:39:50 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4051696)[0;0m ERROR 12-05 14:39:50 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051696)[0;0m ERROR 12-05 14:39:50 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4051696)[0;0m ERROR 12-05 14:39:50 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4051696)[0;0m ERROR 12-05 14:39:50 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051696)[0;0m ERROR 12-05 14:39:50 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4051696)[0;0m ERROR 12-05 14:39:50 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4051696)[0;0m ERROR 12-05 14:39:50 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4051696)[0;0m ERROR 12-05 14:39:50 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4051696)[0;0m ERROR 12-05 14:39:50 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051696)[0;0m ERROR 12-05 14:39:50 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4051696)[0;0m ERROR 12-05 14:39:50 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4051696)[0;0m ERROR 12-05 14:39:50 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051696)[0;0m ERROR 12-05 14:39:50 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4051696)[0;0m ERROR 12-05 14:39:50 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4051696)[0;0m ERROR 12-05 14:39:50 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051696)[0;0m ERROR 12-05 14:39:50 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4051696)[0;0m ERROR 12-05 14:39:50 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4051696)[0;0m ERROR 12-05 14:39:50 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051696)[0;0m ERROR 12-05 14:39:50 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4051696)[0;0m ERROR 12-05 14:39:50 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4051696)[0;0m ERROR 12-05 14:39:50 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7f32d02b5550>' raised:
[1;36m(EngineCore_DP0 pid=4051696)[0;0m ERROR 12-05 14:39:50 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4051696)[0;0m ERROR 12-05 14:39:50 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4051696)[0;0m ERROR 12-05 14:39:50 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4051696)[0;0m ERROR 12-05 14:39:50 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 14:40:11 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 14:40:11 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 14:40:11 [model.py:1745] Using max model len 131072
INFO 12-05 14:40:11 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4051782)[0;0m INFO 12-05 14:40:19 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4051782)[0;0m INFO 12-05 14:40:20 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:37339 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4051782)[0;0m INFO 12-05 14:40:20 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4051782)[0;0m INFO 12-05 14:40:20 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4051782)[0;0m INFO 12-05 14:40:20 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4051782)[0;0m INFO 12-05 14:40:20 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4051782)[0;0m INFO 12-05 14:40:22 [default_loader.py:314] Loading weights took 0.87 seconds
[1;36m(EngineCore_DP0 pid=4051782)[0;0m INFO 12-05 14:40:22 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.388381 seconds
[1;36m(EngineCore_DP0 pid=4051782)[0;0m INFO 12-05 14:40:26 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4051782)[0;0m INFO 12-05 14:40:26 [backends.py:647] Dynamo bytecode transform time: 3.59 s
[1;36m(EngineCore_DP0 pid=4051782)[0;0m ERROR 12-05 14:40:26 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4051782)[0;0m ERROR 12-05 14:40:26 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4051782)[0;0m ERROR 12-05 14:40:26 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4051782)[0;0m ERROR 12-05 14:40:26 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4051782)[0;0m ERROR 12-05 14:40:26 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051782)[0;0m ERROR 12-05 14:40:26 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4051782)[0;0m ERROR 12-05 14:40:26 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4051782)[0;0m ERROR 12-05 14:40:26 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4051782)[0;0m ERROR 12-05 14:40:26 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4051782)[0;0m ERROR 12-05 14:40:26 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051782)[0;0m ERROR 12-05 14:40:26 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4051782)[0;0m ERROR 12-05 14:40:26 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4051782)[0;0m ERROR 12-05 14:40:26 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051782)[0;0m ERROR 12-05 14:40:26 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4051782)[0;0m ERROR 12-05 14:40:26 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4051782)[0;0m ERROR 12-05 14:40:26 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051782)[0;0m ERROR 12-05 14:40:26 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4051782)[0;0m ERROR 12-05 14:40:26 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4051782)[0;0m ERROR 12-05 14:40:26 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051782)[0;0m ERROR 12-05 14:40:26 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4051782)[0;0m ERROR 12-05 14:40:26 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4051782)[0;0m ERROR 12-05 14:40:26 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051782)[0;0m ERROR 12-05 14:40:26 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4051782)[0;0m ERROR 12-05 14:40:26 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4051782)[0;0m ERROR 12-05 14:40:26 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051782)[0;0m ERROR 12-05 14:40:26 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4051782)[0;0m ERROR 12-05 14:40:26 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4051782)[0;0m ERROR 12-05 14:40:26 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4051782)[0;0m ERROR 12-05 14:40:26 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4051782)[0;0m ERROR 12-05 14:40:26 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051782)[0;0m ERROR 12-05 14:40:26 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4051782)[0;0m ERROR 12-05 14:40:26 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4051782)[0;0m ERROR 12-05 14:40:26 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051782)[0;0m ERROR 12-05 14:40:26 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4051782)[0;0m ERROR 12-05 14:40:26 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4051782)[0;0m ERROR 12-05 14:40:26 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051782)[0;0m ERROR 12-05 14:40:26 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4051782)[0;0m ERROR 12-05 14:40:26 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4051782)[0;0m ERROR 12-05 14:40:26 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051782)[0;0m ERROR 12-05 14:40:26 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4051782)[0;0m ERROR 12-05 14:40:26 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4051782)[0;0m ERROR 12-05 14:40:26 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051782)[0;0m ERROR 12-05 14:40:26 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4051782)[0;0m ERROR 12-05 14:40:26 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4051782)[0;0m ERROR 12-05 14:40:26 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051782)[0;0m ERROR 12-05 14:40:26 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4051782)[0;0m ERROR 12-05 14:40:26 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4051782)[0;0m ERROR 12-05 14:40:26 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051782)[0;0m ERROR 12-05 14:40:26 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4051782)[0;0m ERROR 12-05 14:40:26 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4051782)[0;0m ERROR 12-05 14:40:26 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051782)[0;0m ERROR 12-05 14:40:26 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4051782)[0;0m ERROR 12-05 14:40:26 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4051782)[0;0m ERROR 12-05 14:40:26 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051782)[0;0m ERROR 12-05 14:40:26 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4051782)[0;0m ERROR 12-05 14:40:26 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4051782)[0;0m ERROR 12-05 14:40:26 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051782)[0;0m ERROR 12-05 14:40:26 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4051782)[0;0m ERROR 12-05 14:40:26 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4051782)[0;0m ERROR 12-05 14:40:26 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4051782)[0;0m ERROR 12-05 14:40:26 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4051782)[0;0m ERROR 12-05 14:40:26 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051782)[0;0m ERROR 12-05 14:40:26 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4051782)[0;0m ERROR 12-05 14:40:26 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4051782)[0;0m ERROR 12-05 14:40:26 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051782)[0;0m ERROR 12-05 14:40:26 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4051782)[0;0m ERROR 12-05 14:40:26 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4051782)[0;0m ERROR 12-05 14:40:26 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051782)[0;0m ERROR 12-05 14:40:26 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4051782)[0;0m ERROR 12-05 14:40:26 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4051782)[0;0m ERROR 12-05 14:40:26 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4051782)[0;0m ERROR 12-05 14:40:26 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4051782)[0;0m ERROR 12-05 14:40:26 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051782)[0;0m ERROR 12-05 14:40:26 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4051782)[0;0m ERROR 12-05 14:40:26 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4051782)[0;0m ERROR 12-05 14:40:26 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051782)[0;0m ERROR 12-05 14:40:26 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4051782)[0;0m ERROR 12-05 14:40:26 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4051782)[0;0m ERROR 12-05 14:40:26 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051782)[0;0m ERROR 12-05 14:40:26 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4051782)[0;0m ERROR 12-05 14:40:26 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4051782)[0;0m ERROR 12-05 14:40:26 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4051782)[0;0m ERROR 12-05 14:40:26 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4051782)[0;0m ERROR 12-05 14:40:26 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051782)[0;0m ERROR 12-05 14:40:26 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4051782)[0;0m ERROR 12-05 14:40:26 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4051782)[0;0m ERROR 12-05 14:40:26 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051782)[0;0m ERROR 12-05 14:40:26 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4051782)[0;0m ERROR 12-05 14:40:26 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4051782)[0;0m ERROR 12-05 14:40:26 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051782)[0;0m ERROR 12-05 14:40:26 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4051782)[0;0m ERROR 12-05 14:40:26 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4051782)[0;0m ERROR 12-05 14:40:26 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051782)[0;0m ERROR 12-05 14:40:26 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4051782)[0;0m ERROR 12-05 14:40:26 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4051782)[0;0m ERROR 12-05 14:40:26 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7f7c78daaff0>' raised:
[1;36m(EngineCore_DP0 pid=4051782)[0;0m ERROR 12-05 14:40:26 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4051782)[0;0m ERROR 12-05 14:40:26 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4051782)[0;0m ERROR 12-05 14:40:26 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4051782)[0;0m ERROR 12-05 14:40:26 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 14:40:47 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 14:40:48 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 14:40:48 [model.py:1745] Using max model len 131072
INFO 12-05 14:40:48 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4051868)[0;0m INFO 12-05 14:40:55 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4051868)[0;0m INFO 12-05 14:40:56 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:52409 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4051868)[0;0m INFO 12-05 14:40:56 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4051868)[0;0m INFO 12-05 14:40:56 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4051868)[0;0m INFO 12-05 14:40:56 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4051868)[0;0m INFO 12-05 14:40:56 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4051868)[0;0m INFO 12-05 14:40:58 [default_loader.py:314] Loading weights took 0.87 seconds
[1;36m(EngineCore_DP0 pid=4051868)[0;0m INFO 12-05 14:40:58 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.371740 seconds
[1;36m(EngineCore_DP0 pid=4051868)[0;0m INFO 12-05 14:41:02 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4051868)[0;0m INFO 12-05 14:41:02 [backends.py:647] Dynamo bytecode transform time: 3.86 s
[1;36m(EngineCore_DP0 pid=4051868)[0;0m ERROR 12-05 14:41:02 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4051868)[0;0m ERROR 12-05 14:41:02 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4051868)[0;0m ERROR 12-05 14:41:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4051868)[0;0m ERROR 12-05 14:41:02 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4051868)[0;0m ERROR 12-05 14:41:02 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051868)[0;0m ERROR 12-05 14:41:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4051868)[0;0m ERROR 12-05 14:41:02 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4051868)[0;0m ERROR 12-05 14:41:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4051868)[0;0m ERROR 12-05 14:41:02 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4051868)[0;0m ERROR 12-05 14:41:02 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051868)[0;0m ERROR 12-05 14:41:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4051868)[0;0m ERROR 12-05 14:41:02 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4051868)[0;0m ERROR 12-05 14:41:02 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051868)[0;0m ERROR 12-05 14:41:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4051868)[0;0m ERROR 12-05 14:41:02 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4051868)[0;0m ERROR 12-05 14:41:02 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051868)[0;0m ERROR 12-05 14:41:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4051868)[0;0m ERROR 12-05 14:41:02 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4051868)[0;0m ERROR 12-05 14:41:02 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051868)[0;0m ERROR 12-05 14:41:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4051868)[0;0m ERROR 12-05 14:41:02 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4051868)[0;0m ERROR 12-05 14:41:02 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051868)[0;0m ERROR 12-05 14:41:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4051868)[0;0m ERROR 12-05 14:41:02 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4051868)[0;0m ERROR 12-05 14:41:02 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051868)[0;0m ERROR 12-05 14:41:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4051868)[0;0m ERROR 12-05 14:41:02 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4051868)[0;0m ERROR 12-05 14:41:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4051868)[0;0m ERROR 12-05 14:41:02 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4051868)[0;0m ERROR 12-05 14:41:02 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051868)[0;0m ERROR 12-05 14:41:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4051868)[0;0m ERROR 12-05 14:41:02 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4051868)[0;0m ERROR 12-05 14:41:02 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051868)[0;0m ERROR 12-05 14:41:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4051868)[0;0m ERROR 12-05 14:41:02 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4051868)[0;0m ERROR 12-05 14:41:02 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051868)[0;0m ERROR 12-05 14:41:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4051868)[0;0m ERROR 12-05 14:41:02 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4051868)[0;0m ERROR 12-05 14:41:02 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051868)[0;0m ERROR 12-05 14:41:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4051868)[0;0m ERROR 12-05 14:41:02 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4051868)[0;0m ERROR 12-05 14:41:02 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051868)[0;0m ERROR 12-05 14:41:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4051868)[0;0m ERROR 12-05 14:41:02 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4051868)[0;0m ERROR 12-05 14:41:02 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051868)[0;0m ERROR 12-05 14:41:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4051868)[0;0m ERROR 12-05 14:41:02 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4051868)[0;0m ERROR 12-05 14:41:02 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051868)[0;0m ERROR 12-05 14:41:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4051868)[0;0m ERROR 12-05 14:41:02 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4051868)[0;0m ERROR 12-05 14:41:02 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051868)[0;0m ERROR 12-05 14:41:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4051868)[0;0m ERROR 12-05 14:41:02 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4051868)[0;0m ERROR 12-05 14:41:02 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051868)[0;0m ERROR 12-05 14:41:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4051868)[0;0m ERROR 12-05 14:41:02 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4051868)[0;0m ERROR 12-05 14:41:02 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051868)[0;0m ERROR 12-05 14:41:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4051868)[0;0m ERROR 12-05 14:41:02 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4051868)[0;0m ERROR 12-05 14:41:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4051868)[0;0m ERROR 12-05 14:41:02 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4051868)[0;0m ERROR 12-05 14:41:02 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051868)[0;0m ERROR 12-05 14:41:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4051868)[0;0m ERROR 12-05 14:41:02 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4051868)[0;0m ERROR 12-05 14:41:02 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051868)[0;0m ERROR 12-05 14:41:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4051868)[0;0m ERROR 12-05 14:41:02 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4051868)[0;0m ERROR 12-05 14:41:02 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051868)[0;0m ERROR 12-05 14:41:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4051868)[0;0m ERROR 12-05 14:41:02 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4051868)[0;0m ERROR 12-05 14:41:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4051868)[0;0m ERROR 12-05 14:41:02 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4051868)[0;0m ERROR 12-05 14:41:02 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051868)[0;0m ERROR 12-05 14:41:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4051868)[0;0m ERROR 12-05 14:41:02 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4051868)[0;0m ERROR 12-05 14:41:02 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051868)[0;0m ERROR 12-05 14:41:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4051868)[0;0m ERROR 12-05 14:41:02 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4051868)[0;0m ERROR 12-05 14:41:02 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051868)[0;0m ERROR 12-05 14:41:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4051868)[0;0m ERROR 12-05 14:41:02 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4051868)[0;0m ERROR 12-05 14:41:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4051868)[0;0m ERROR 12-05 14:41:02 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4051868)[0;0m ERROR 12-05 14:41:02 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051868)[0;0m ERROR 12-05 14:41:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4051868)[0;0m ERROR 12-05 14:41:02 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4051868)[0;0m ERROR 12-05 14:41:02 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051868)[0;0m ERROR 12-05 14:41:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4051868)[0;0m ERROR 12-05 14:41:02 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4051868)[0;0m ERROR 12-05 14:41:02 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051868)[0;0m ERROR 12-05 14:41:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4051868)[0;0m ERROR 12-05 14:41:02 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4051868)[0;0m ERROR 12-05 14:41:02 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4051868)[0;0m ERROR 12-05 14:41:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4051868)[0;0m ERROR 12-05 14:41:02 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4051868)[0;0m ERROR 12-05 14:41:02 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7f21fc253230>' raised:
[1;36m(EngineCore_DP0 pid=4051868)[0;0m ERROR 12-05 14:41:02 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4051868)[0;0m ERROR 12-05 14:41:02 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4051868)[0;0m ERROR 12-05 14:41:02 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4051868)[0;0m ERROR 12-05 14:41:02 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 14:41:24 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 14:41:24 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 14:41:24 [model.py:1745] Using max model len 131072
INFO 12-05 14:41:24 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4052040)[0;0m INFO 12-05 14:41:31 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4052040)[0;0m INFO 12-05 14:41:32 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:51369 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4052040)[0;0m INFO 12-05 14:41:32 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4052040)[0;0m INFO 12-05 14:41:32 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4052040)[0;0m INFO 12-05 14:41:33 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4052040)[0;0m INFO 12-05 14:41:33 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4052040)[0;0m INFO 12-05 14:41:34 [default_loader.py:314] Loading weights took 0.87 seconds
[1;36m(EngineCore_DP0 pid=4052040)[0;0m INFO 12-05 14:41:35 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.445367 seconds
[1;36m(EngineCore_DP0 pid=4052040)[0;0m INFO 12-05 14:41:38 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4052040)[0;0m INFO 12-05 14:41:38 [backends.py:647] Dynamo bytecode transform time: 3.62 s
[1;36m(EngineCore_DP0 pid=4052040)[0;0m ERROR 12-05 14:41:39 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4052040)[0;0m ERROR 12-05 14:41:39 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4052040)[0;0m ERROR 12-05 14:41:39 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4052040)[0;0m ERROR 12-05 14:41:39 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4052040)[0;0m ERROR 12-05 14:41:39 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052040)[0;0m ERROR 12-05 14:41:39 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4052040)[0;0m ERROR 12-05 14:41:39 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4052040)[0;0m ERROR 12-05 14:41:39 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4052040)[0;0m ERROR 12-05 14:41:39 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4052040)[0;0m ERROR 12-05 14:41:39 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052040)[0;0m ERROR 12-05 14:41:39 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4052040)[0;0m ERROR 12-05 14:41:39 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4052040)[0;0m ERROR 12-05 14:41:39 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052040)[0;0m ERROR 12-05 14:41:39 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4052040)[0;0m ERROR 12-05 14:41:39 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4052040)[0;0m ERROR 12-05 14:41:39 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052040)[0;0m ERROR 12-05 14:41:39 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4052040)[0;0m ERROR 12-05 14:41:39 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4052040)[0;0m ERROR 12-05 14:41:39 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052040)[0;0m ERROR 12-05 14:41:39 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4052040)[0;0m ERROR 12-05 14:41:39 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4052040)[0;0m ERROR 12-05 14:41:39 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052040)[0;0m ERROR 12-05 14:41:39 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4052040)[0;0m ERROR 12-05 14:41:39 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4052040)[0;0m ERROR 12-05 14:41:39 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052040)[0;0m ERROR 12-05 14:41:39 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4052040)[0;0m ERROR 12-05 14:41:39 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4052040)[0;0m ERROR 12-05 14:41:39 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4052040)[0;0m ERROR 12-05 14:41:39 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4052040)[0;0m ERROR 12-05 14:41:39 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052040)[0;0m ERROR 12-05 14:41:39 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4052040)[0;0m ERROR 12-05 14:41:39 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4052040)[0;0m ERROR 12-05 14:41:39 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052040)[0;0m ERROR 12-05 14:41:39 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4052040)[0;0m ERROR 12-05 14:41:39 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4052040)[0;0m ERROR 12-05 14:41:39 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052040)[0;0m ERROR 12-05 14:41:39 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4052040)[0;0m ERROR 12-05 14:41:39 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4052040)[0;0m ERROR 12-05 14:41:39 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052040)[0;0m ERROR 12-05 14:41:39 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4052040)[0;0m ERROR 12-05 14:41:39 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4052040)[0;0m ERROR 12-05 14:41:39 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052040)[0;0m ERROR 12-05 14:41:39 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4052040)[0;0m ERROR 12-05 14:41:39 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4052040)[0;0m ERROR 12-05 14:41:39 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052040)[0;0m ERROR 12-05 14:41:39 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4052040)[0;0m ERROR 12-05 14:41:39 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4052040)[0;0m ERROR 12-05 14:41:39 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052040)[0;0m ERROR 12-05 14:41:39 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4052040)[0;0m ERROR 12-05 14:41:39 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4052040)[0;0m ERROR 12-05 14:41:39 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052040)[0;0m ERROR 12-05 14:41:39 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4052040)[0;0m ERROR 12-05 14:41:39 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4052040)[0;0m ERROR 12-05 14:41:39 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052040)[0;0m ERROR 12-05 14:41:39 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4052040)[0;0m ERROR 12-05 14:41:39 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4052040)[0;0m ERROR 12-05 14:41:39 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052040)[0;0m ERROR 12-05 14:41:39 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4052040)[0;0m ERROR 12-05 14:41:39 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4052040)[0;0m ERROR 12-05 14:41:39 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4052040)[0;0m ERROR 12-05 14:41:39 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4052040)[0;0m ERROR 12-05 14:41:39 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052040)[0;0m ERROR 12-05 14:41:39 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4052040)[0;0m ERROR 12-05 14:41:39 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4052040)[0;0m ERROR 12-05 14:41:39 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052040)[0;0m ERROR 12-05 14:41:39 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4052040)[0;0m ERROR 12-05 14:41:39 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4052040)[0;0m ERROR 12-05 14:41:39 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052040)[0;0m ERROR 12-05 14:41:39 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4052040)[0;0m ERROR 12-05 14:41:39 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4052040)[0;0m ERROR 12-05 14:41:39 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4052040)[0;0m ERROR 12-05 14:41:39 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4052040)[0;0m ERROR 12-05 14:41:39 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052040)[0;0m ERROR 12-05 14:41:39 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4052040)[0;0m ERROR 12-05 14:41:39 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4052040)[0;0m ERROR 12-05 14:41:39 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052040)[0;0m ERROR 12-05 14:41:39 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4052040)[0;0m ERROR 12-05 14:41:39 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4052040)[0;0m ERROR 12-05 14:41:39 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052040)[0;0m ERROR 12-05 14:41:39 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4052040)[0;0m ERROR 12-05 14:41:39 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4052040)[0;0m ERROR 12-05 14:41:39 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4052040)[0;0m ERROR 12-05 14:41:39 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4052040)[0;0m ERROR 12-05 14:41:39 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052040)[0;0m ERROR 12-05 14:41:39 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4052040)[0;0m ERROR 12-05 14:41:39 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4052040)[0;0m ERROR 12-05 14:41:39 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052040)[0;0m ERROR 12-05 14:41:39 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4052040)[0;0m ERROR 12-05 14:41:39 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4052040)[0;0m ERROR 12-05 14:41:39 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052040)[0;0m ERROR 12-05 14:41:39 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4052040)[0;0m ERROR 12-05 14:41:39 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4052040)[0;0m ERROR 12-05 14:41:39 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052040)[0;0m ERROR 12-05 14:41:39 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4052040)[0;0m ERROR 12-05 14:41:39 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4052040)[0;0m ERROR 12-05 14:41:39 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7ff8f430a930>' raised:
[1;36m(EngineCore_DP0 pid=4052040)[0;0m ERROR 12-05 14:41:39 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4052040)[0;0m ERROR 12-05 14:41:39 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4052040)[0;0m ERROR 12-05 14:41:39 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4052040)[0;0m ERROR 12-05 14:41:39 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 14:42:00 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 14:42:00 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 14:42:00 [model.py:1745] Using max model len 131072
INFO 12-05 14:42:00 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4052129)[0;0m INFO 12-05 14:42:07 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4052129)[0;0m INFO 12-05 14:42:08 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:34773 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4052129)[0;0m INFO 12-05 14:42:08 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4052129)[0;0m INFO 12-05 14:42:09 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4052129)[0;0m INFO 12-05 14:42:09 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4052129)[0;0m INFO 12-05 14:42:09 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4052129)[0;0m INFO 12-05 14:42:10 [default_loader.py:314] Loading weights took 0.87 seconds
[1;36m(EngineCore_DP0 pid=4052129)[0;0m INFO 12-05 14:42:11 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.389352 seconds
[1;36m(EngineCore_DP0 pid=4052129)[0;0m INFO 12-05 14:42:15 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4052129)[0;0m INFO 12-05 14:42:15 [backends.py:647] Dynamo bytecode transform time: 3.64 s
[1;36m(EngineCore_DP0 pid=4052129)[0;0m ERROR 12-05 14:42:15 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4052129)[0;0m ERROR 12-05 14:42:15 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4052129)[0;0m ERROR 12-05 14:42:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4052129)[0;0m ERROR 12-05 14:42:15 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4052129)[0;0m ERROR 12-05 14:42:15 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052129)[0;0m ERROR 12-05 14:42:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4052129)[0;0m ERROR 12-05 14:42:15 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4052129)[0;0m ERROR 12-05 14:42:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4052129)[0;0m ERROR 12-05 14:42:15 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4052129)[0;0m ERROR 12-05 14:42:15 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052129)[0;0m ERROR 12-05 14:42:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4052129)[0;0m ERROR 12-05 14:42:15 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4052129)[0;0m ERROR 12-05 14:42:15 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052129)[0;0m ERROR 12-05 14:42:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4052129)[0;0m ERROR 12-05 14:42:15 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4052129)[0;0m ERROR 12-05 14:42:15 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052129)[0;0m ERROR 12-05 14:42:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4052129)[0;0m ERROR 12-05 14:42:15 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4052129)[0;0m ERROR 12-05 14:42:15 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052129)[0;0m ERROR 12-05 14:42:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4052129)[0;0m ERROR 12-05 14:42:15 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4052129)[0;0m ERROR 12-05 14:42:15 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052129)[0;0m ERROR 12-05 14:42:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4052129)[0;0m ERROR 12-05 14:42:15 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4052129)[0;0m ERROR 12-05 14:42:15 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052129)[0;0m ERROR 12-05 14:42:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4052129)[0;0m ERROR 12-05 14:42:15 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4052129)[0;0m ERROR 12-05 14:42:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4052129)[0;0m ERROR 12-05 14:42:15 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4052129)[0;0m ERROR 12-05 14:42:15 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052129)[0;0m ERROR 12-05 14:42:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4052129)[0;0m ERROR 12-05 14:42:15 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4052129)[0;0m ERROR 12-05 14:42:15 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052129)[0;0m ERROR 12-05 14:42:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4052129)[0;0m ERROR 12-05 14:42:15 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4052129)[0;0m ERROR 12-05 14:42:15 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052129)[0;0m ERROR 12-05 14:42:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4052129)[0;0m ERROR 12-05 14:42:15 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4052129)[0;0m ERROR 12-05 14:42:15 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052129)[0;0m ERROR 12-05 14:42:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4052129)[0;0m ERROR 12-05 14:42:15 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4052129)[0;0m ERROR 12-05 14:42:15 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052129)[0;0m ERROR 12-05 14:42:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4052129)[0;0m ERROR 12-05 14:42:15 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4052129)[0;0m ERROR 12-05 14:42:15 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052129)[0;0m ERROR 12-05 14:42:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4052129)[0;0m ERROR 12-05 14:42:15 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4052129)[0;0m ERROR 12-05 14:42:15 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052129)[0;0m ERROR 12-05 14:42:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4052129)[0;0m ERROR 12-05 14:42:15 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4052129)[0;0m ERROR 12-05 14:42:15 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052129)[0;0m ERROR 12-05 14:42:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4052129)[0;0m ERROR 12-05 14:42:15 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4052129)[0;0m ERROR 12-05 14:42:15 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052129)[0;0m ERROR 12-05 14:42:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4052129)[0;0m ERROR 12-05 14:42:15 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4052129)[0;0m ERROR 12-05 14:42:15 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052129)[0;0m ERROR 12-05 14:42:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4052129)[0;0m ERROR 12-05 14:42:15 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4052129)[0;0m ERROR 12-05 14:42:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4052129)[0;0m ERROR 12-05 14:42:15 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4052129)[0;0m ERROR 12-05 14:42:15 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052129)[0;0m ERROR 12-05 14:42:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4052129)[0;0m ERROR 12-05 14:42:15 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4052129)[0;0m ERROR 12-05 14:42:15 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052129)[0;0m ERROR 12-05 14:42:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4052129)[0;0m ERROR 12-05 14:42:15 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4052129)[0;0m ERROR 12-05 14:42:15 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052129)[0;0m ERROR 12-05 14:42:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4052129)[0;0m ERROR 12-05 14:42:15 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4052129)[0;0m ERROR 12-05 14:42:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4052129)[0;0m ERROR 12-05 14:42:15 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4052129)[0;0m ERROR 12-05 14:42:15 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052129)[0;0m ERROR 12-05 14:42:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4052129)[0;0m ERROR 12-05 14:42:15 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4052129)[0;0m ERROR 12-05 14:42:15 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052129)[0;0m ERROR 12-05 14:42:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4052129)[0;0m ERROR 12-05 14:42:15 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4052129)[0;0m ERROR 12-05 14:42:15 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052129)[0;0m ERROR 12-05 14:42:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4052129)[0;0m ERROR 12-05 14:42:15 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4052129)[0;0m ERROR 12-05 14:42:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4052129)[0;0m ERROR 12-05 14:42:15 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4052129)[0;0m ERROR 12-05 14:42:15 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052129)[0;0m ERROR 12-05 14:42:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4052129)[0;0m ERROR 12-05 14:42:15 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4052129)[0;0m ERROR 12-05 14:42:15 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052129)[0;0m ERROR 12-05 14:42:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4052129)[0;0m ERROR 12-05 14:42:15 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4052129)[0;0m ERROR 12-05 14:42:15 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052129)[0;0m ERROR 12-05 14:42:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4052129)[0;0m ERROR 12-05 14:42:15 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4052129)[0;0m ERROR 12-05 14:42:15 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052129)[0;0m ERROR 12-05 14:42:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4052129)[0;0m ERROR 12-05 14:42:15 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4052129)[0;0m ERROR 12-05 14:42:15 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7f9330043770>' raised:
[1;36m(EngineCore_DP0 pid=4052129)[0;0m ERROR 12-05 14:42:15 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4052129)[0;0m ERROR 12-05 14:42:15 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4052129)[0;0m ERROR 12-05 14:42:15 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4052129)[0;0m ERROR 12-05 14:42:15 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 14:42:36 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 14:42:36 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 14:42:36 [model.py:1745] Using max model len 131072
INFO 12-05 14:42:36 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4052211)[0;0m INFO 12-05 14:42:44 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4052211)[0;0m INFO 12-05 14:42:45 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:37955 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4052211)[0;0m INFO 12-05 14:42:45 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4052211)[0;0m INFO 12-05 14:42:45 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4052211)[0;0m INFO 12-05 14:42:45 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4052211)[0;0m INFO 12-05 14:42:45 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4052211)[0;0m INFO 12-05 14:42:47 [default_loader.py:314] Loading weights took 0.87 seconds
[1;36m(EngineCore_DP0 pid=4052211)[0;0m INFO 12-05 14:42:47 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.434295 seconds
[1;36m(EngineCore_DP0 pid=4052211)[0;0m INFO 12-05 14:42:51 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4052211)[0;0m INFO 12-05 14:42:51 [backends.py:647] Dynamo bytecode transform time: 3.57 s
[1;36m(EngineCore_DP0 pid=4052211)[0;0m ERROR 12-05 14:42:51 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4052211)[0;0m ERROR 12-05 14:42:51 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4052211)[0;0m ERROR 12-05 14:42:51 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4052211)[0;0m ERROR 12-05 14:42:51 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4052211)[0;0m ERROR 12-05 14:42:51 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052211)[0;0m ERROR 12-05 14:42:51 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4052211)[0;0m ERROR 12-05 14:42:51 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4052211)[0;0m ERROR 12-05 14:42:51 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4052211)[0;0m ERROR 12-05 14:42:51 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4052211)[0;0m ERROR 12-05 14:42:51 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052211)[0;0m ERROR 12-05 14:42:51 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4052211)[0;0m ERROR 12-05 14:42:51 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4052211)[0;0m ERROR 12-05 14:42:51 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052211)[0;0m ERROR 12-05 14:42:51 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4052211)[0;0m ERROR 12-05 14:42:51 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4052211)[0;0m ERROR 12-05 14:42:51 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052211)[0;0m ERROR 12-05 14:42:51 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4052211)[0;0m ERROR 12-05 14:42:51 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4052211)[0;0m ERROR 12-05 14:42:51 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052211)[0;0m ERROR 12-05 14:42:51 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4052211)[0;0m ERROR 12-05 14:42:51 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4052211)[0;0m ERROR 12-05 14:42:51 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052211)[0;0m ERROR 12-05 14:42:51 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4052211)[0;0m ERROR 12-05 14:42:51 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4052211)[0;0m ERROR 12-05 14:42:51 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052211)[0;0m ERROR 12-05 14:42:51 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4052211)[0;0m ERROR 12-05 14:42:51 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4052211)[0;0m ERROR 12-05 14:42:51 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4052211)[0;0m ERROR 12-05 14:42:51 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4052211)[0;0m ERROR 12-05 14:42:51 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052211)[0;0m ERROR 12-05 14:42:51 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4052211)[0;0m ERROR 12-05 14:42:51 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4052211)[0;0m ERROR 12-05 14:42:51 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052211)[0;0m ERROR 12-05 14:42:51 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4052211)[0;0m ERROR 12-05 14:42:51 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4052211)[0;0m ERROR 12-05 14:42:51 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052211)[0;0m ERROR 12-05 14:42:51 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4052211)[0;0m ERROR 12-05 14:42:51 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4052211)[0;0m ERROR 12-05 14:42:51 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052211)[0;0m ERROR 12-05 14:42:51 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4052211)[0;0m ERROR 12-05 14:42:51 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4052211)[0;0m ERROR 12-05 14:42:51 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052211)[0;0m ERROR 12-05 14:42:51 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4052211)[0;0m ERROR 12-05 14:42:51 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4052211)[0;0m ERROR 12-05 14:42:51 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052211)[0;0m ERROR 12-05 14:42:51 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4052211)[0;0m ERROR 12-05 14:42:51 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4052211)[0;0m ERROR 12-05 14:42:51 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052211)[0;0m ERROR 12-05 14:42:51 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4052211)[0;0m ERROR 12-05 14:42:51 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4052211)[0;0m ERROR 12-05 14:42:51 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052211)[0;0m ERROR 12-05 14:42:51 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4052211)[0;0m ERROR 12-05 14:42:51 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4052211)[0;0m ERROR 12-05 14:42:51 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052211)[0;0m ERROR 12-05 14:42:51 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4052211)[0;0m ERROR 12-05 14:42:51 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4052211)[0;0m ERROR 12-05 14:42:51 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052211)[0;0m ERROR 12-05 14:42:51 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4052211)[0;0m ERROR 12-05 14:42:51 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4052211)[0;0m ERROR 12-05 14:42:51 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4052211)[0;0m ERROR 12-05 14:42:51 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4052211)[0;0m ERROR 12-05 14:42:51 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052211)[0;0m ERROR 12-05 14:42:51 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4052211)[0;0m ERROR 12-05 14:42:51 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4052211)[0;0m ERROR 12-05 14:42:51 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052211)[0;0m ERROR 12-05 14:42:51 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4052211)[0;0m ERROR 12-05 14:42:51 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4052211)[0;0m ERROR 12-05 14:42:51 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052211)[0;0m ERROR 12-05 14:42:51 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4052211)[0;0m ERROR 12-05 14:42:51 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4052211)[0;0m ERROR 12-05 14:42:51 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4052211)[0;0m ERROR 12-05 14:42:51 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4052211)[0;0m ERROR 12-05 14:42:51 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052211)[0;0m ERROR 12-05 14:42:51 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4052211)[0;0m ERROR 12-05 14:42:51 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4052211)[0;0m ERROR 12-05 14:42:51 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052211)[0;0m ERROR 12-05 14:42:51 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4052211)[0;0m ERROR 12-05 14:42:51 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4052211)[0;0m ERROR 12-05 14:42:51 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052211)[0;0m ERROR 12-05 14:42:51 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4052211)[0;0m ERROR 12-05 14:42:51 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4052211)[0;0m ERROR 12-05 14:42:51 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4052211)[0;0m ERROR 12-05 14:42:51 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4052211)[0;0m ERROR 12-05 14:42:51 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052211)[0;0m ERROR 12-05 14:42:51 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4052211)[0;0m ERROR 12-05 14:42:51 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4052211)[0;0m ERROR 12-05 14:42:51 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052211)[0;0m ERROR 12-05 14:42:51 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4052211)[0;0m ERROR 12-05 14:42:51 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4052211)[0;0m ERROR 12-05 14:42:51 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052211)[0;0m ERROR 12-05 14:42:51 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4052211)[0;0m ERROR 12-05 14:42:51 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4052211)[0;0m ERROR 12-05 14:42:51 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052211)[0;0m ERROR 12-05 14:42:51 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4052211)[0;0m ERROR 12-05 14:42:51 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4052211)[0;0m ERROR 12-05 14:42:51 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7fc06c2cde20>' raised:
[1;36m(EngineCore_DP0 pid=4052211)[0;0m ERROR 12-05 14:42:51 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4052211)[0;0m ERROR 12-05 14:42:51 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4052211)[0;0m ERROR 12-05 14:42:51 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4052211)[0;0m ERROR 12-05 14:42:51 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 14:43:13 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 14:43:13 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 14:43:13 [model.py:1745] Using max model len 131072
INFO 12-05 14:43:13 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4052296)[0;0m INFO 12-05 14:43:20 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4052296)[0;0m INFO 12-05 14:43:21 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:37385 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4052296)[0;0m INFO 12-05 14:43:21 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4052296)[0;0m INFO 12-05 14:43:21 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4052296)[0;0m INFO 12-05 14:43:22 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4052296)[0;0m INFO 12-05 14:43:22 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4052296)[0;0m INFO 12-05 14:43:23 [default_loader.py:314] Loading weights took 0.87 seconds
[1;36m(EngineCore_DP0 pid=4052296)[0;0m INFO 12-05 14:43:23 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.376923 seconds
[1;36m(EngineCore_DP0 pid=4052296)[0;0m INFO 12-05 14:43:27 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4052296)[0;0m INFO 12-05 14:43:27 [backends.py:647] Dynamo bytecode transform time: 3.60 s
[1;36m(EngineCore_DP0 pid=4052296)[0;0m ERROR 12-05 14:43:27 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4052296)[0;0m ERROR 12-05 14:43:27 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4052296)[0;0m ERROR 12-05 14:43:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4052296)[0;0m ERROR 12-05 14:43:27 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4052296)[0;0m ERROR 12-05 14:43:27 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052296)[0;0m ERROR 12-05 14:43:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4052296)[0;0m ERROR 12-05 14:43:27 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4052296)[0;0m ERROR 12-05 14:43:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4052296)[0;0m ERROR 12-05 14:43:27 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4052296)[0;0m ERROR 12-05 14:43:27 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052296)[0;0m ERROR 12-05 14:43:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4052296)[0;0m ERROR 12-05 14:43:27 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4052296)[0;0m ERROR 12-05 14:43:27 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052296)[0;0m ERROR 12-05 14:43:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4052296)[0;0m ERROR 12-05 14:43:27 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4052296)[0;0m ERROR 12-05 14:43:27 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052296)[0;0m ERROR 12-05 14:43:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4052296)[0;0m ERROR 12-05 14:43:27 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4052296)[0;0m ERROR 12-05 14:43:27 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052296)[0;0m ERROR 12-05 14:43:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4052296)[0;0m ERROR 12-05 14:43:27 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4052296)[0;0m ERROR 12-05 14:43:27 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052296)[0;0m ERROR 12-05 14:43:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4052296)[0;0m ERROR 12-05 14:43:27 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4052296)[0;0m ERROR 12-05 14:43:27 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052296)[0;0m ERROR 12-05 14:43:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4052296)[0;0m ERROR 12-05 14:43:27 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4052296)[0;0m ERROR 12-05 14:43:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4052296)[0;0m ERROR 12-05 14:43:27 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4052296)[0;0m ERROR 12-05 14:43:27 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052296)[0;0m ERROR 12-05 14:43:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4052296)[0;0m ERROR 12-05 14:43:27 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4052296)[0;0m ERROR 12-05 14:43:27 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052296)[0;0m ERROR 12-05 14:43:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4052296)[0;0m ERROR 12-05 14:43:27 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4052296)[0;0m ERROR 12-05 14:43:27 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052296)[0;0m ERROR 12-05 14:43:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4052296)[0;0m ERROR 12-05 14:43:27 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4052296)[0;0m ERROR 12-05 14:43:27 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052296)[0;0m ERROR 12-05 14:43:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4052296)[0;0m ERROR 12-05 14:43:27 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4052296)[0;0m ERROR 12-05 14:43:27 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052296)[0;0m ERROR 12-05 14:43:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4052296)[0;0m ERROR 12-05 14:43:27 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4052296)[0;0m ERROR 12-05 14:43:27 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052296)[0;0m ERROR 12-05 14:43:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4052296)[0;0m ERROR 12-05 14:43:27 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4052296)[0;0m ERROR 12-05 14:43:27 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052296)[0;0m ERROR 12-05 14:43:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4052296)[0;0m ERROR 12-05 14:43:27 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4052296)[0;0m ERROR 12-05 14:43:27 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052296)[0;0m ERROR 12-05 14:43:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4052296)[0;0m ERROR 12-05 14:43:27 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4052296)[0;0m ERROR 12-05 14:43:27 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052296)[0;0m ERROR 12-05 14:43:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4052296)[0;0m ERROR 12-05 14:43:27 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4052296)[0;0m ERROR 12-05 14:43:27 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052296)[0;0m ERROR 12-05 14:43:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4052296)[0;0m ERROR 12-05 14:43:27 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4052296)[0;0m ERROR 12-05 14:43:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4052296)[0;0m ERROR 12-05 14:43:27 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4052296)[0;0m ERROR 12-05 14:43:27 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052296)[0;0m ERROR 12-05 14:43:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4052296)[0;0m ERROR 12-05 14:43:27 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4052296)[0;0m ERROR 12-05 14:43:27 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052296)[0;0m ERROR 12-05 14:43:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4052296)[0;0m ERROR 12-05 14:43:27 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4052296)[0;0m ERROR 12-05 14:43:27 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052296)[0;0m ERROR 12-05 14:43:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4052296)[0;0m ERROR 12-05 14:43:27 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4052296)[0;0m ERROR 12-05 14:43:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4052296)[0;0m ERROR 12-05 14:43:27 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4052296)[0;0m ERROR 12-05 14:43:27 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052296)[0;0m ERROR 12-05 14:43:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4052296)[0;0m ERROR 12-05 14:43:27 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4052296)[0;0m ERROR 12-05 14:43:27 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052296)[0;0m ERROR 12-05 14:43:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4052296)[0;0m ERROR 12-05 14:43:27 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4052296)[0;0m ERROR 12-05 14:43:27 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052296)[0;0m ERROR 12-05 14:43:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4052296)[0;0m ERROR 12-05 14:43:27 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4052296)[0;0m ERROR 12-05 14:43:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4052296)[0;0m ERROR 12-05 14:43:27 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4052296)[0;0m ERROR 12-05 14:43:27 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052296)[0;0m ERROR 12-05 14:43:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4052296)[0;0m ERROR 12-05 14:43:27 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4052296)[0;0m ERROR 12-05 14:43:27 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052296)[0;0m ERROR 12-05 14:43:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4052296)[0;0m ERROR 12-05 14:43:27 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4052296)[0;0m ERROR 12-05 14:43:27 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052296)[0;0m ERROR 12-05 14:43:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4052296)[0;0m ERROR 12-05 14:43:27 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4052296)[0;0m ERROR 12-05 14:43:27 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052296)[0;0m ERROR 12-05 14:43:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4052296)[0;0m ERROR 12-05 14:43:27 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4052296)[0;0m ERROR 12-05 14:43:27 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7f0fc816f290>' raised:
[1;36m(EngineCore_DP0 pid=4052296)[0;0m ERROR 12-05 14:43:27 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4052296)[0;0m ERROR 12-05 14:43:27 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4052296)[0;0m ERROR 12-05 14:43:27 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4052296)[0;0m ERROR 12-05 14:43:27 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 14:43:49 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 14:43:49 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 14:43:49 [model.py:1745] Using max model len 131072
INFO 12-05 14:43:49 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4052385)[0;0m INFO 12-05 14:43:56 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4052385)[0;0m INFO 12-05 14:43:57 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:51541 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4052385)[0;0m INFO 12-05 14:43:57 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4052385)[0;0m INFO 12-05 14:43:57 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4052385)[0;0m INFO 12-05 14:43:58 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4052385)[0;0m INFO 12-05 14:43:58 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4052385)[0;0m INFO 12-05 14:43:59 [default_loader.py:314] Loading weights took 0.88 seconds
[1;36m(EngineCore_DP0 pid=4052385)[0;0m INFO 12-05 14:43:59 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.454320 seconds
[1;36m(EngineCore_DP0 pid=4052385)[0;0m INFO 12-05 14:44:03 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4052385)[0;0m INFO 12-05 14:44:03 [backends.py:647] Dynamo bytecode transform time: 3.54 s
[1;36m(EngineCore_DP0 pid=4052385)[0;0m ERROR 12-05 14:44:03 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4052385)[0;0m ERROR 12-05 14:44:03 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4052385)[0;0m ERROR 12-05 14:44:03 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4052385)[0;0m ERROR 12-05 14:44:03 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4052385)[0;0m ERROR 12-05 14:44:03 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052385)[0;0m ERROR 12-05 14:44:03 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4052385)[0;0m ERROR 12-05 14:44:03 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4052385)[0;0m ERROR 12-05 14:44:03 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4052385)[0;0m ERROR 12-05 14:44:03 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4052385)[0;0m ERROR 12-05 14:44:03 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052385)[0;0m ERROR 12-05 14:44:03 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4052385)[0;0m ERROR 12-05 14:44:03 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4052385)[0;0m ERROR 12-05 14:44:03 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052385)[0;0m ERROR 12-05 14:44:03 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4052385)[0;0m ERROR 12-05 14:44:03 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4052385)[0;0m ERROR 12-05 14:44:03 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052385)[0;0m ERROR 12-05 14:44:03 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4052385)[0;0m ERROR 12-05 14:44:03 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4052385)[0;0m ERROR 12-05 14:44:03 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052385)[0;0m ERROR 12-05 14:44:03 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4052385)[0;0m ERROR 12-05 14:44:03 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4052385)[0;0m ERROR 12-05 14:44:03 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052385)[0;0m ERROR 12-05 14:44:03 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4052385)[0;0m ERROR 12-05 14:44:03 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4052385)[0;0m ERROR 12-05 14:44:03 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052385)[0;0m ERROR 12-05 14:44:03 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4052385)[0;0m ERROR 12-05 14:44:03 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4052385)[0;0m ERROR 12-05 14:44:03 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4052385)[0;0m ERROR 12-05 14:44:03 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4052385)[0;0m ERROR 12-05 14:44:03 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052385)[0;0m ERROR 12-05 14:44:03 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4052385)[0;0m ERROR 12-05 14:44:03 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4052385)[0;0m ERROR 12-05 14:44:03 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052385)[0;0m ERROR 12-05 14:44:03 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4052385)[0;0m ERROR 12-05 14:44:03 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4052385)[0;0m ERROR 12-05 14:44:03 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052385)[0;0m ERROR 12-05 14:44:03 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4052385)[0;0m ERROR 12-05 14:44:03 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4052385)[0;0m ERROR 12-05 14:44:03 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052385)[0;0m ERROR 12-05 14:44:03 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4052385)[0;0m ERROR 12-05 14:44:03 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4052385)[0;0m ERROR 12-05 14:44:03 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052385)[0;0m ERROR 12-05 14:44:03 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4052385)[0;0m ERROR 12-05 14:44:03 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4052385)[0;0m ERROR 12-05 14:44:03 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052385)[0;0m ERROR 12-05 14:44:03 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4052385)[0;0m ERROR 12-05 14:44:03 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4052385)[0;0m ERROR 12-05 14:44:03 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052385)[0;0m ERROR 12-05 14:44:03 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4052385)[0;0m ERROR 12-05 14:44:03 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4052385)[0;0m ERROR 12-05 14:44:03 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052385)[0;0m ERROR 12-05 14:44:03 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4052385)[0;0m ERROR 12-05 14:44:03 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4052385)[0;0m ERROR 12-05 14:44:03 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052385)[0;0m ERROR 12-05 14:44:03 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4052385)[0;0m ERROR 12-05 14:44:03 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4052385)[0;0m ERROR 12-05 14:44:03 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052385)[0;0m ERROR 12-05 14:44:03 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4052385)[0;0m ERROR 12-05 14:44:03 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4052385)[0;0m ERROR 12-05 14:44:03 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4052385)[0;0m ERROR 12-05 14:44:03 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4052385)[0;0m ERROR 12-05 14:44:03 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052385)[0;0m ERROR 12-05 14:44:03 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4052385)[0;0m ERROR 12-05 14:44:03 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4052385)[0;0m ERROR 12-05 14:44:03 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052385)[0;0m ERROR 12-05 14:44:03 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4052385)[0;0m ERROR 12-05 14:44:03 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4052385)[0;0m ERROR 12-05 14:44:03 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052385)[0;0m ERROR 12-05 14:44:03 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4052385)[0;0m ERROR 12-05 14:44:03 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4052385)[0;0m ERROR 12-05 14:44:03 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4052385)[0;0m ERROR 12-05 14:44:03 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4052385)[0;0m ERROR 12-05 14:44:03 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052385)[0;0m ERROR 12-05 14:44:03 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4052385)[0;0m ERROR 12-05 14:44:03 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4052385)[0;0m ERROR 12-05 14:44:03 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052385)[0;0m ERROR 12-05 14:44:03 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4052385)[0;0m ERROR 12-05 14:44:03 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4052385)[0;0m ERROR 12-05 14:44:03 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052385)[0;0m ERROR 12-05 14:44:03 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4052385)[0;0m ERROR 12-05 14:44:03 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4052385)[0;0m ERROR 12-05 14:44:03 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4052385)[0;0m ERROR 12-05 14:44:03 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4052385)[0;0m ERROR 12-05 14:44:03 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052385)[0;0m ERROR 12-05 14:44:03 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4052385)[0;0m ERROR 12-05 14:44:03 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4052385)[0;0m ERROR 12-05 14:44:03 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052385)[0;0m ERROR 12-05 14:44:03 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4052385)[0;0m ERROR 12-05 14:44:03 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4052385)[0;0m ERROR 12-05 14:44:03 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052385)[0;0m ERROR 12-05 14:44:03 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4052385)[0;0m ERROR 12-05 14:44:03 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4052385)[0;0m ERROR 12-05 14:44:03 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052385)[0;0m ERROR 12-05 14:44:03 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4052385)[0;0m ERROR 12-05 14:44:03 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4052385)[0;0m ERROR 12-05 14:44:03 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7eff84322de0>' raised:
[1;36m(EngineCore_DP0 pid=4052385)[0;0m ERROR 12-05 14:44:03 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4052385)[0;0m ERROR 12-05 14:44:03 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4052385)[0;0m ERROR 12-05 14:44:03 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4052385)[0;0m ERROR 12-05 14:44:03 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 14:44:25 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 14:44:25 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 14:44:25 [model.py:1745] Using max model len 131072
INFO 12-05 14:44:25 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4052469)[0;0m INFO 12-05 14:44:32 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4052469)[0;0m INFO 12-05 14:44:33 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:50619 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4052469)[0;0m INFO 12-05 14:44:33 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4052469)[0;0m INFO 12-05 14:44:33 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4052469)[0;0m INFO 12-05 14:44:34 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4052469)[0;0m INFO 12-05 14:44:34 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4052469)[0;0m INFO 12-05 14:44:35 [default_loader.py:314] Loading weights took 0.87 seconds
[1;36m(EngineCore_DP0 pid=4052469)[0;0m INFO 12-05 14:44:35 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.379111 seconds
[1;36m(EngineCore_DP0 pid=4052469)[0;0m INFO 12-05 14:44:39 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4052469)[0;0m INFO 12-05 14:44:39 [backends.py:647] Dynamo bytecode transform time: 3.59 s
[1;36m(EngineCore_DP0 pid=4052469)[0;0m ERROR 12-05 14:44:39 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4052469)[0;0m ERROR 12-05 14:44:39 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4052469)[0;0m ERROR 12-05 14:44:39 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4052469)[0;0m ERROR 12-05 14:44:39 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4052469)[0;0m ERROR 12-05 14:44:39 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052469)[0;0m ERROR 12-05 14:44:39 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4052469)[0;0m ERROR 12-05 14:44:39 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4052469)[0;0m ERROR 12-05 14:44:39 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4052469)[0;0m ERROR 12-05 14:44:39 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4052469)[0;0m ERROR 12-05 14:44:39 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052469)[0;0m ERROR 12-05 14:44:39 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4052469)[0;0m ERROR 12-05 14:44:39 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4052469)[0;0m ERROR 12-05 14:44:39 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052469)[0;0m ERROR 12-05 14:44:39 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4052469)[0;0m ERROR 12-05 14:44:39 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4052469)[0;0m ERROR 12-05 14:44:39 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052469)[0;0m ERROR 12-05 14:44:39 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4052469)[0;0m ERROR 12-05 14:44:39 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4052469)[0;0m ERROR 12-05 14:44:39 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052469)[0;0m ERROR 12-05 14:44:39 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4052469)[0;0m ERROR 12-05 14:44:39 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4052469)[0;0m ERROR 12-05 14:44:39 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052469)[0;0m ERROR 12-05 14:44:39 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4052469)[0;0m ERROR 12-05 14:44:39 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4052469)[0;0m ERROR 12-05 14:44:39 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052469)[0;0m ERROR 12-05 14:44:39 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4052469)[0;0m ERROR 12-05 14:44:39 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4052469)[0;0m ERROR 12-05 14:44:39 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4052469)[0;0m ERROR 12-05 14:44:39 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4052469)[0;0m ERROR 12-05 14:44:39 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052469)[0;0m ERROR 12-05 14:44:39 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4052469)[0;0m ERROR 12-05 14:44:39 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4052469)[0;0m ERROR 12-05 14:44:39 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052469)[0;0m ERROR 12-05 14:44:39 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4052469)[0;0m ERROR 12-05 14:44:39 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4052469)[0;0m ERROR 12-05 14:44:39 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052469)[0;0m ERROR 12-05 14:44:39 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4052469)[0;0m ERROR 12-05 14:44:39 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4052469)[0;0m ERROR 12-05 14:44:39 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052469)[0;0m ERROR 12-05 14:44:39 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4052469)[0;0m ERROR 12-05 14:44:39 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4052469)[0;0m ERROR 12-05 14:44:39 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052469)[0;0m ERROR 12-05 14:44:39 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4052469)[0;0m ERROR 12-05 14:44:39 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4052469)[0;0m ERROR 12-05 14:44:39 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052469)[0;0m ERROR 12-05 14:44:39 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4052469)[0;0m ERROR 12-05 14:44:39 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4052469)[0;0m ERROR 12-05 14:44:39 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052469)[0;0m ERROR 12-05 14:44:39 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4052469)[0;0m ERROR 12-05 14:44:39 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4052469)[0;0m ERROR 12-05 14:44:39 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052469)[0;0m ERROR 12-05 14:44:39 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4052469)[0;0m ERROR 12-05 14:44:39 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4052469)[0;0m ERROR 12-05 14:44:39 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052469)[0;0m ERROR 12-05 14:44:39 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4052469)[0;0m ERROR 12-05 14:44:39 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4052469)[0;0m ERROR 12-05 14:44:39 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052469)[0;0m ERROR 12-05 14:44:39 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4052469)[0;0m ERROR 12-05 14:44:39 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4052469)[0;0m ERROR 12-05 14:44:39 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4052469)[0;0m ERROR 12-05 14:44:39 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4052469)[0;0m ERROR 12-05 14:44:39 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052469)[0;0m ERROR 12-05 14:44:39 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4052469)[0;0m ERROR 12-05 14:44:39 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4052469)[0;0m ERROR 12-05 14:44:39 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052469)[0;0m ERROR 12-05 14:44:39 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4052469)[0;0m ERROR 12-05 14:44:39 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4052469)[0;0m ERROR 12-05 14:44:39 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052469)[0;0m ERROR 12-05 14:44:39 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4052469)[0;0m ERROR 12-05 14:44:39 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4052469)[0;0m ERROR 12-05 14:44:39 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4052469)[0;0m ERROR 12-05 14:44:39 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4052469)[0;0m ERROR 12-05 14:44:39 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052469)[0;0m ERROR 12-05 14:44:39 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4052469)[0;0m ERROR 12-05 14:44:39 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4052469)[0;0m ERROR 12-05 14:44:39 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052469)[0;0m ERROR 12-05 14:44:39 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4052469)[0;0m ERROR 12-05 14:44:39 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4052469)[0;0m ERROR 12-05 14:44:39 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052469)[0;0m ERROR 12-05 14:44:39 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4052469)[0;0m ERROR 12-05 14:44:39 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4052469)[0;0m ERROR 12-05 14:44:39 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4052469)[0;0m ERROR 12-05 14:44:39 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4052469)[0;0m ERROR 12-05 14:44:39 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052469)[0;0m ERROR 12-05 14:44:39 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4052469)[0;0m ERROR 12-05 14:44:39 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4052469)[0;0m ERROR 12-05 14:44:39 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052469)[0;0m ERROR 12-05 14:44:39 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4052469)[0;0m ERROR 12-05 14:44:39 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4052469)[0;0m ERROR 12-05 14:44:39 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052469)[0;0m ERROR 12-05 14:44:39 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4052469)[0;0m ERROR 12-05 14:44:39 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4052469)[0;0m ERROR 12-05 14:44:39 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052469)[0;0m ERROR 12-05 14:44:39 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4052469)[0;0m ERROR 12-05 14:44:39 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4052469)[0;0m ERROR 12-05 14:44:39 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7fbc0568ef00>' raised:
[1;36m(EngineCore_DP0 pid=4052469)[0;0m ERROR 12-05 14:44:39 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4052469)[0;0m ERROR 12-05 14:44:39 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4052469)[0;0m ERROR 12-05 14:44:39 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4052469)[0;0m ERROR 12-05 14:44:39 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 14:45:01 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 14:45:01 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 14:45:01 [model.py:1745] Using max model len 131072
INFO 12-05 14:45:01 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4052558)[0;0m INFO 12-05 14:45:10 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4052558)[0;0m INFO 12-05 14:45:10 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:47465 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4052558)[0;0m INFO 12-05 14:45:11 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4052558)[0;0m INFO 12-05 14:45:11 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4052558)[0;0m INFO 12-05 14:45:11 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4052558)[0;0m INFO 12-05 14:45:11 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4052558)[0;0m INFO 12-05 14:45:12 [default_loader.py:314] Loading weights took 0.88 seconds
[1;36m(EngineCore_DP0 pid=4052558)[0;0m INFO 12-05 14:45:13 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.415858 seconds
[1;36m(EngineCore_DP0 pid=4052558)[0;0m INFO 12-05 14:45:17 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4052558)[0;0m INFO 12-05 14:45:17 [backends.py:647] Dynamo bytecode transform time: 3.80 s
[1;36m(EngineCore_DP0 pid=4052558)[0;0m ERROR 12-05 14:45:17 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4052558)[0;0m ERROR 12-05 14:45:17 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4052558)[0;0m ERROR 12-05 14:45:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4052558)[0;0m ERROR 12-05 14:45:17 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4052558)[0;0m ERROR 12-05 14:45:17 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052558)[0;0m ERROR 12-05 14:45:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4052558)[0;0m ERROR 12-05 14:45:17 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4052558)[0;0m ERROR 12-05 14:45:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4052558)[0;0m ERROR 12-05 14:45:17 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4052558)[0;0m ERROR 12-05 14:45:17 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052558)[0;0m ERROR 12-05 14:45:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4052558)[0;0m ERROR 12-05 14:45:17 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4052558)[0;0m ERROR 12-05 14:45:17 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052558)[0;0m ERROR 12-05 14:45:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4052558)[0;0m ERROR 12-05 14:45:17 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4052558)[0;0m ERROR 12-05 14:45:17 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052558)[0;0m ERROR 12-05 14:45:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4052558)[0;0m ERROR 12-05 14:45:17 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4052558)[0;0m ERROR 12-05 14:45:17 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052558)[0;0m ERROR 12-05 14:45:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4052558)[0;0m ERROR 12-05 14:45:17 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4052558)[0;0m ERROR 12-05 14:45:17 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052558)[0;0m ERROR 12-05 14:45:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4052558)[0;0m ERROR 12-05 14:45:17 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4052558)[0;0m ERROR 12-05 14:45:17 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052558)[0;0m ERROR 12-05 14:45:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4052558)[0;0m ERROR 12-05 14:45:17 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4052558)[0;0m ERROR 12-05 14:45:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4052558)[0;0m ERROR 12-05 14:45:17 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4052558)[0;0m ERROR 12-05 14:45:17 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052558)[0;0m ERROR 12-05 14:45:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4052558)[0;0m ERROR 12-05 14:45:17 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4052558)[0;0m ERROR 12-05 14:45:17 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052558)[0;0m ERROR 12-05 14:45:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4052558)[0;0m ERROR 12-05 14:45:17 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4052558)[0;0m ERROR 12-05 14:45:17 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052558)[0;0m ERROR 12-05 14:45:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4052558)[0;0m ERROR 12-05 14:45:17 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4052558)[0;0m ERROR 12-05 14:45:17 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052558)[0;0m ERROR 12-05 14:45:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4052558)[0;0m ERROR 12-05 14:45:17 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4052558)[0;0m ERROR 12-05 14:45:17 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052558)[0;0m ERROR 12-05 14:45:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4052558)[0;0m ERROR 12-05 14:45:17 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4052558)[0;0m ERROR 12-05 14:45:17 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052558)[0;0m ERROR 12-05 14:45:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4052558)[0;0m ERROR 12-05 14:45:17 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4052558)[0;0m ERROR 12-05 14:45:17 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052558)[0;0m ERROR 12-05 14:45:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4052558)[0;0m ERROR 12-05 14:45:17 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4052558)[0;0m ERROR 12-05 14:45:17 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052558)[0;0m ERROR 12-05 14:45:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4052558)[0;0m ERROR 12-05 14:45:17 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4052558)[0;0m ERROR 12-05 14:45:17 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052558)[0;0m ERROR 12-05 14:45:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4052558)[0;0m ERROR 12-05 14:45:17 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4052558)[0;0m ERROR 12-05 14:45:17 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052558)[0;0m ERROR 12-05 14:45:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4052558)[0;0m ERROR 12-05 14:45:17 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4052558)[0;0m ERROR 12-05 14:45:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4052558)[0;0m ERROR 12-05 14:45:17 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4052558)[0;0m ERROR 12-05 14:45:17 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052558)[0;0m ERROR 12-05 14:45:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4052558)[0;0m ERROR 12-05 14:45:17 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4052558)[0;0m ERROR 12-05 14:45:17 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052558)[0;0m ERROR 12-05 14:45:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4052558)[0;0m ERROR 12-05 14:45:17 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4052558)[0;0m ERROR 12-05 14:45:17 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052558)[0;0m ERROR 12-05 14:45:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4052558)[0;0m ERROR 12-05 14:45:17 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4052558)[0;0m ERROR 12-05 14:45:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4052558)[0;0m ERROR 12-05 14:45:17 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4052558)[0;0m ERROR 12-05 14:45:17 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052558)[0;0m ERROR 12-05 14:45:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4052558)[0;0m ERROR 12-05 14:45:17 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4052558)[0;0m ERROR 12-05 14:45:17 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052558)[0;0m ERROR 12-05 14:45:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4052558)[0;0m ERROR 12-05 14:45:17 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4052558)[0;0m ERROR 12-05 14:45:17 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052558)[0;0m ERROR 12-05 14:45:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4052558)[0;0m ERROR 12-05 14:45:17 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4052558)[0;0m ERROR 12-05 14:45:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4052558)[0;0m ERROR 12-05 14:45:17 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4052558)[0;0m ERROR 12-05 14:45:17 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052558)[0;0m ERROR 12-05 14:45:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4052558)[0;0m ERROR 12-05 14:45:17 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4052558)[0;0m ERROR 12-05 14:45:17 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052558)[0;0m ERROR 12-05 14:45:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4052558)[0;0m ERROR 12-05 14:45:17 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4052558)[0;0m ERROR 12-05 14:45:17 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052558)[0;0m ERROR 12-05 14:45:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4052558)[0;0m ERROR 12-05 14:45:17 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4052558)[0;0m ERROR 12-05 14:45:17 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052558)[0;0m ERROR 12-05 14:45:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4052558)[0;0m ERROR 12-05 14:45:17 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4052558)[0;0m ERROR 12-05 14:45:17 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7fdb041f9bb0>' raised:
[1;36m(EngineCore_DP0 pid=4052558)[0;0m ERROR 12-05 14:45:17 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4052558)[0;0m ERROR 12-05 14:45:17 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4052558)[0;0m ERROR 12-05 14:45:17 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4052558)[0;0m ERROR 12-05 14:45:17 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 14:45:39 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 14:45:39 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 14:45:39 [model.py:1745] Using max model len 131072
INFO 12-05 14:45:39 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4052639)[0;0m INFO 12-05 14:45:46 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4052639)[0;0m INFO 12-05 14:45:47 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:38757 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4052639)[0;0m INFO 12-05 14:45:47 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4052639)[0;0m INFO 12-05 14:45:47 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4052639)[0;0m INFO 12-05 14:45:48 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4052639)[0;0m INFO 12-05 14:45:48 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4052639)[0;0m INFO 12-05 14:45:49 [default_loader.py:314] Loading weights took 0.88 seconds
[1;36m(EngineCore_DP0 pid=4052639)[0;0m INFO 12-05 14:45:49 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.416747 seconds
[1;36m(EngineCore_DP0 pid=4052639)[0;0m INFO 12-05 14:45:53 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4052639)[0;0m INFO 12-05 14:45:53 [backends.py:647] Dynamo bytecode transform time: 3.69 s
[1;36m(EngineCore_DP0 pid=4052639)[0;0m ERROR 12-05 14:45:54 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4052639)[0;0m ERROR 12-05 14:45:54 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4052639)[0;0m ERROR 12-05 14:45:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4052639)[0;0m ERROR 12-05 14:45:54 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4052639)[0;0m ERROR 12-05 14:45:54 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052639)[0;0m ERROR 12-05 14:45:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4052639)[0;0m ERROR 12-05 14:45:54 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4052639)[0;0m ERROR 12-05 14:45:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4052639)[0;0m ERROR 12-05 14:45:54 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4052639)[0;0m ERROR 12-05 14:45:54 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052639)[0;0m ERROR 12-05 14:45:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4052639)[0;0m ERROR 12-05 14:45:54 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4052639)[0;0m ERROR 12-05 14:45:54 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052639)[0;0m ERROR 12-05 14:45:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4052639)[0;0m ERROR 12-05 14:45:54 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4052639)[0;0m ERROR 12-05 14:45:54 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052639)[0;0m ERROR 12-05 14:45:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4052639)[0;0m ERROR 12-05 14:45:54 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4052639)[0;0m ERROR 12-05 14:45:54 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052639)[0;0m ERROR 12-05 14:45:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4052639)[0;0m ERROR 12-05 14:45:54 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4052639)[0;0m ERROR 12-05 14:45:54 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052639)[0;0m ERROR 12-05 14:45:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4052639)[0;0m ERROR 12-05 14:45:54 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4052639)[0;0m ERROR 12-05 14:45:54 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052639)[0;0m ERROR 12-05 14:45:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4052639)[0;0m ERROR 12-05 14:45:54 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4052639)[0;0m ERROR 12-05 14:45:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4052639)[0;0m ERROR 12-05 14:45:54 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4052639)[0;0m ERROR 12-05 14:45:54 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052639)[0;0m ERROR 12-05 14:45:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4052639)[0;0m ERROR 12-05 14:45:54 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4052639)[0;0m ERROR 12-05 14:45:54 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052639)[0;0m ERROR 12-05 14:45:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4052639)[0;0m ERROR 12-05 14:45:54 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4052639)[0;0m ERROR 12-05 14:45:54 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052639)[0;0m ERROR 12-05 14:45:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4052639)[0;0m ERROR 12-05 14:45:54 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4052639)[0;0m ERROR 12-05 14:45:54 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052639)[0;0m ERROR 12-05 14:45:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4052639)[0;0m ERROR 12-05 14:45:54 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4052639)[0;0m ERROR 12-05 14:45:54 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052639)[0;0m ERROR 12-05 14:45:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4052639)[0;0m ERROR 12-05 14:45:54 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4052639)[0;0m ERROR 12-05 14:45:54 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052639)[0;0m ERROR 12-05 14:45:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4052639)[0;0m ERROR 12-05 14:45:54 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4052639)[0;0m ERROR 12-05 14:45:54 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052639)[0;0m ERROR 12-05 14:45:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4052639)[0;0m ERROR 12-05 14:45:54 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4052639)[0;0m ERROR 12-05 14:45:54 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052639)[0;0m ERROR 12-05 14:45:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4052639)[0;0m ERROR 12-05 14:45:54 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4052639)[0;0m ERROR 12-05 14:45:54 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052639)[0;0m ERROR 12-05 14:45:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4052639)[0;0m ERROR 12-05 14:45:54 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4052639)[0;0m ERROR 12-05 14:45:54 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052639)[0;0m ERROR 12-05 14:45:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4052639)[0;0m ERROR 12-05 14:45:54 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4052639)[0;0m ERROR 12-05 14:45:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4052639)[0;0m ERROR 12-05 14:45:54 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4052639)[0;0m ERROR 12-05 14:45:54 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052639)[0;0m ERROR 12-05 14:45:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4052639)[0;0m ERROR 12-05 14:45:54 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4052639)[0;0m ERROR 12-05 14:45:54 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052639)[0;0m ERROR 12-05 14:45:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4052639)[0;0m ERROR 12-05 14:45:54 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4052639)[0;0m ERROR 12-05 14:45:54 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052639)[0;0m ERROR 12-05 14:45:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4052639)[0;0m ERROR 12-05 14:45:54 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4052639)[0;0m ERROR 12-05 14:45:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4052639)[0;0m ERROR 12-05 14:45:54 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4052639)[0;0m ERROR 12-05 14:45:54 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052639)[0;0m ERROR 12-05 14:45:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4052639)[0;0m ERROR 12-05 14:45:54 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4052639)[0;0m ERROR 12-05 14:45:54 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052639)[0;0m ERROR 12-05 14:45:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4052639)[0;0m ERROR 12-05 14:45:54 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4052639)[0;0m ERROR 12-05 14:45:54 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052639)[0;0m ERROR 12-05 14:45:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4052639)[0;0m ERROR 12-05 14:45:54 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4052639)[0;0m ERROR 12-05 14:45:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4052639)[0;0m ERROR 12-05 14:45:54 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4052639)[0;0m ERROR 12-05 14:45:54 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052639)[0;0m ERROR 12-05 14:45:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4052639)[0;0m ERROR 12-05 14:45:54 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4052639)[0;0m ERROR 12-05 14:45:54 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052639)[0;0m ERROR 12-05 14:45:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4052639)[0;0m ERROR 12-05 14:45:54 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4052639)[0;0m ERROR 12-05 14:45:54 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052639)[0;0m ERROR 12-05 14:45:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4052639)[0;0m ERROR 12-05 14:45:54 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4052639)[0;0m ERROR 12-05 14:45:54 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052639)[0;0m ERROR 12-05 14:45:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4052639)[0;0m ERROR 12-05 14:45:54 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4052639)[0;0m ERROR 12-05 14:45:54 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7feedc716ff0>' raised:
[1;36m(EngineCore_DP0 pid=4052639)[0;0m ERROR 12-05 14:45:54 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4052639)[0;0m ERROR 12-05 14:45:54 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4052639)[0;0m ERROR 12-05 14:45:54 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4052639)[0;0m ERROR 12-05 14:45:54 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 14:46:15 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 14:46:15 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 14:46:15 [model.py:1745] Using max model len 131072
INFO 12-05 14:46:15 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4052820)[0;0m INFO 12-05 14:46:23 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4052820)[0;0m INFO 12-05 14:46:23 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:51215 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4052820)[0;0m INFO 12-05 14:46:23 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4052820)[0;0m INFO 12-05 14:46:24 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4052820)[0;0m INFO 12-05 14:46:24 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4052820)[0;0m INFO 12-05 14:46:24 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4052820)[0;0m INFO 12-05 14:46:25 [default_loader.py:314] Loading weights took 0.88 seconds
[1;36m(EngineCore_DP0 pid=4052820)[0;0m INFO 12-05 14:46:26 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.444134 seconds
[1;36m(EngineCore_DP0 pid=4052820)[0;0m INFO 12-05 14:46:30 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4052820)[0;0m INFO 12-05 14:46:30 [backends.py:647] Dynamo bytecode transform time: 3.59 s
[1;36m(EngineCore_DP0 pid=4052820)[0;0m ERROR 12-05 14:46:30 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4052820)[0;0m ERROR 12-05 14:46:30 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4052820)[0;0m ERROR 12-05 14:46:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4052820)[0;0m ERROR 12-05 14:46:30 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4052820)[0;0m ERROR 12-05 14:46:30 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052820)[0;0m ERROR 12-05 14:46:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4052820)[0;0m ERROR 12-05 14:46:30 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4052820)[0;0m ERROR 12-05 14:46:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4052820)[0;0m ERROR 12-05 14:46:30 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4052820)[0;0m ERROR 12-05 14:46:30 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052820)[0;0m ERROR 12-05 14:46:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4052820)[0;0m ERROR 12-05 14:46:30 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4052820)[0;0m ERROR 12-05 14:46:30 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052820)[0;0m ERROR 12-05 14:46:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4052820)[0;0m ERROR 12-05 14:46:30 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4052820)[0;0m ERROR 12-05 14:46:30 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052820)[0;0m ERROR 12-05 14:46:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4052820)[0;0m ERROR 12-05 14:46:30 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4052820)[0;0m ERROR 12-05 14:46:30 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052820)[0;0m ERROR 12-05 14:46:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4052820)[0;0m ERROR 12-05 14:46:30 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4052820)[0;0m ERROR 12-05 14:46:30 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052820)[0;0m ERROR 12-05 14:46:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4052820)[0;0m ERROR 12-05 14:46:30 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4052820)[0;0m ERROR 12-05 14:46:30 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052820)[0;0m ERROR 12-05 14:46:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4052820)[0;0m ERROR 12-05 14:46:30 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4052820)[0;0m ERROR 12-05 14:46:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4052820)[0;0m ERROR 12-05 14:46:30 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4052820)[0;0m ERROR 12-05 14:46:30 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052820)[0;0m ERROR 12-05 14:46:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4052820)[0;0m ERROR 12-05 14:46:30 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4052820)[0;0m ERROR 12-05 14:46:30 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052820)[0;0m ERROR 12-05 14:46:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4052820)[0;0m ERROR 12-05 14:46:30 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4052820)[0;0m ERROR 12-05 14:46:30 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052820)[0;0m ERROR 12-05 14:46:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4052820)[0;0m ERROR 12-05 14:46:30 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4052820)[0;0m ERROR 12-05 14:46:30 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052820)[0;0m ERROR 12-05 14:46:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4052820)[0;0m ERROR 12-05 14:46:30 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4052820)[0;0m ERROR 12-05 14:46:30 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052820)[0;0m ERROR 12-05 14:46:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4052820)[0;0m ERROR 12-05 14:46:30 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4052820)[0;0m ERROR 12-05 14:46:30 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052820)[0;0m ERROR 12-05 14:46:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4052820)[0;0m ERROR 12-05 14:46:30 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4052820)[0;0m ERROR 12-05 14:46:30 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052820)[0;0m ERROR 12-05 14:46:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4052820)[0;0m ERROR 12-05 14:46:30 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4052820)[0;0m ERROR 12-05 14:46:30 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052820)[0;0m ERROR 12-05 14:46:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4052820)[0;0m ERROR 12-05 14:46:30 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4052820)[0;0m ERROR 12-05 14:46:30 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052820)[0;0m ERROR 12-05 14:46:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4052820)[0;0m ERROR 12-05 14:46:30 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4052820)[0;0m ERROR 12-05 14:46:30 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052820)[0;0m ERROR 12-05 14:46:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4052820)[0;0m ERROR 12-05 14:46:30 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4052820)[0;0m ERROR 12-05 14:46:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4052820)[0;0m ERROR 12-05 14:46:30 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4052820)[0;0m ERROR 12-05 14:46:30 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052820)[0;0m ERROR 12-05 14:46:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4052820)[0;0m ERROR 12-05 14:46:30 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4052820)[0;0m ERROR 12-05 14:46:30 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052820)[0;0m ERROR 12-05 14:46:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4052820)[0;0m ERROR 12-05 14:46:30 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4052820)[0;0m ERROR 12-05 14:46:30 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052820)[0;0m ERROR 12-05 14:46:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4052820)[0;0m ERROR 12-05 14:46:30 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4052820)[0;0m ERROR 12-05 14:46:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4052820)[0;0m ERROR 12-05 14:46:30 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4052820)[0;0m ERROR 12-05 14:46:30 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052820)[0;0m ERROR 12-05 14:46:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4052820)[0;0m ERROR 12-05 14:46:30 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4052820)[0;0m ERROR 12-05 14:46:30 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052820)[0;0m ERROR 12-05 14:46:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4052820)[0;0m ERROR 12-05 14:46:30 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4052820)[0;0m ERROR 12-05 14:46:30 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052820)[0;0m ERROR 12-05 14:46:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4052820)[0;0m ERROR 12-05 14:46:30 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4052820)[0;0m ERROR 12-05 14:46:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4052820)[0;0m ERROR 12-05 14:46:30 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4052820)[0;0m ERROR 12-05 14:46:30 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052820)[0;0m ERROR 12-05 14:46:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4052820)[0;0m ERROR 12-05 14:46:30 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4052820)[0;0m ERROR 12-05 14:46:30 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052820)[0;0m ERROR 12-05 14:46:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4052820)[0;0m ERROR 12-05 14:46:30 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4052820)[0;0m ERROR 12-05 14:46:30 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052820)[0;0m ERROR 12-05 14:46:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4052820)[0;0m ERROR 12-05 14:46:30 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4052820)[0;0m ERROR 12-05 14:46:30 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052820)[0;0m ERROR 12-05 14:46:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4052820)[0;0m ERROR 12-05 14:46:30 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4052820)[0;0m ERROR 12-05 14:46:30 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7f6eec3b7da0>' raised:
[1;36m(EngineCore_DP0 pid=4052820)[0;0m ERROR 12-05 14:46:30 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4052820)[0;0m ERROR 12-05 14:46:30 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4052820)[0;0m ERROR 12-05 14:46:30 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4052820)[0;0m ERROR 12-05 14:46:30 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 14:46:51 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 14:46:51 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 14:46:51 [model.py:1745] Using max model len 131072
INFO 12-05 14:46:51 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4052925)[0;0m INFO 12-05 14:46:58 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4052925)[0;0m INFO 12-05 14:46:59 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:35511 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4052925)[0;0m INFO 12-05 14:46:59 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4052925)[0;0m INFO 12-05 14:47:00 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4052925)[0;0m INFO 12-05 14:47:00 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4052925)[0;0m INFO 12-05 14:47:00 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4052925)[0;0m INFO 12-05 14:47:01 [default_loader.py:314] Loading weights took 0.88 seconds
[1;36m(EngineCore_DP0 pid=4052925)[0;0m INFO 12-05 14:47:02 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.416446 seconds
[1;36m(EngineCore_DP0 pid=4052925)[0;0m INFO 12-05 14:47:06 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4052925)[0;0m INFO 12-05 14:47:06 [backends.py:647] Dynamo bytecode transform time: 3.80 s
[1;36m(EngineCore_DP0 pid=4052925)[0;0m ERROR 12-05 14:47:06 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4052925)[0;0m ERROR 12-05 14:47:06 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4052925)[0;0m ERROR 12-05 14:47:06 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4052925)[0;0m ERROR 12-05 14:47:06 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4052925)[0;0m ERROR 12-05 14:47:06 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052925)[0;0m ERROR 12-05 14:47:06 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4052925)[0;0m ERROR 12-05 14:47:06 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4052925)[0;0m ERROR 12-05 14:47:06 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4052925)[0;0m ERROR 12-05 14:47:06 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4052925)[0;0m ERROR 12-05 14:47:06 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052925)[0;0m ERROR 12-05 14:47:06 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4052925)[0;0m ERROR 12-05 14:47:06 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4052925)[0;0m ERROR 12-05 14:47:06 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052925)[0;0m ERROR 12-05 14:47:06 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4052925)[0;0m ERROR 12-05 14:47:06 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4052925)[0;0m ERROR 12-05 14:47:06 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052925)[0;0m ERROR 12-05 14:47:06 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4052925)[0;0m ERROR 12-05 14:47:06 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4052925)[0;0m ERROR 12-05 14:47:06 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052925)[0;0m ERROR 12-05 14:47:06 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4052925)[0;0m ERROR 12-05 14:47:06 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4052925)[0;0m ERROR 12-05 14:47:06 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052925)[0;0m ERROR 12-05 14:47:06 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4052925)[0;0m ERROR 12-05 14:47:06 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4052925)[0;0m ERROR 12-05 14:47:06 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052925)[0;0m ERROR 12-05 14:47:06 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4052925)[0;0m ERROR 12-05 14:47:06 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4052925)[0;0m ERROR 12-05 14:47:06 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4052925)[0;0m ERROR 12-05 14:47:06 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4052925)[0;0m ERROR 12-05 14:47:06 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052925)[0;0m ERROR 12-05 14:47:06 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4052925)[0;0m ERROR 12-05 14:47:06 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4052925)[0;0m ERROR 12-05 14:47:06 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052925)[0;0m ERROR 12-05 14:47:06 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4052925)[0;0m ERROR 12-05 14:47:06 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4052925)[0;0m ERROR 12-05 14:47:06 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052925)[0;0m ERROR 12-05 14:47:06 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4052925)[0;0m ERROR 12-05 14:47:06 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4052925)[0;0m ERROR 12-05 14:47:06 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052925)[0;0m ERROR 12-05 14:47:06 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4052925)[0;0m ERROR 12-05 14:47:06 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4052925)[0;0m ERROR 12-05 14:47:06 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052925)[0;0m ERROR 12-05 14:47:06 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4052925)[0;0m ERROR 12-05 14:47:06 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4052925)[0;0m ERROR 12-05 14:47:06 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052925)[0;0m ERROR 12-05 14:47:06 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4052925)[0;0m ERROR 12-05 14:47:06 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4052925)[0;0m ERROR 12-05 14:47:06 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052925)[0;0m ERROR 12-05 14:47:06 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4052925)[0;0m ERROR 12-05 14:47:06 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4052925)[0;0m ERROR 12-05 14:47:06 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052925)[0;0m ERROR 12-05 14:47:06 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4052925)[0;0m ERROR 12-05 14:47:06 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4052925)[0;0m ERROR 12-05 14:47:06 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052925)[0;0m ERROR 12-05 14:47:06 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4052925)[0;0m ERROR 12-05 14:47:06 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4052925)[0;0m ERROR 12-05 14:47:06 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052925)[0;0m ERROR 12-05 14:47:06 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4052925)[0;0m ERROR 12-05 14:47:06 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4052925)[0;0m ERROR 12-05 14:47:06 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4052925)[0;0m ERROR 12-05 14:47:06 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4052925)[0;0m ERROR 12-05 14:47:06 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052925)[0;0m ERROR 12-05 14:47:06 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4052925)[0;0m ERROR 12-05 14:47:06 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4052925)[0;0m ERROR 12-05 14:47:06 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052925)[0;0m ERROR 12-05 14:47:06 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4052925)[0;0m ERROR 12-05 14:47:06 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4052925)[0;0m ERROR 12-05 14:47:06 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052925)[0;0m ERROR 12-05 14:47:06 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4052925)[0;0m ERROR 12-05 14:47:06 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4052925)[0;0m ERROR 12-05 14:47:06 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4052925)[0;0m ERROR 12-05 14:47:06 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4052925)[0;0m ERROR 12-05 14:47:06 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052925)[0;0m ERROR 12-05 14:47:06 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4052925)[0;0m ERROR 12-05 14:47:06 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4052925)[0;0m ERROR 12-05 14:47:06 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052925)[0;0m ERROR 12-05 14:47:06 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4052925)[0;0m ERROR 12-05 14:47:06 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4052925)[0;0m ERROR 12-05 14:47:06 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052925)[0;0m ERROR 12-05 14:47:06 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4052925)[0;0m ERROR 12-05 14:47:06 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4052925)[0;0m ERROR 12-05 14:47:06 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4052925)[0;0m ERROR 12-05 14:47:06 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4052925)[0;0m ERROR 12-05 14:47:06 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052925)[0;0m ERROR 12-05 14:47:06 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4052925)[0;0m ERROR 12-05 14:47:06 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4052925)[0;0m ERROR 12-05 14:47:06 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052925)[0;0m ERROR 12-05 14:47:06 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4052925)[0;0m ERROR 12-05 14:47:06 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4052925)[0;0m ERROR 12-05 14:47:06 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052925)[0;0m ERROR 12-05 14:47:06 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4052925)[0;0m ERROR 12-05 14:47:06 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4052925)[0;0m ERROR 12-05 14:47:06 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4052925)[0;0m ERROR 12-05 14:47:06 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4052925)[0;0m ERROR 12-05 14:47:06 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4052925)[0;0m ERROR 12-05 14:47:06 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7f94003abfe0>' raised:
[1;36m(EngineCore_DP0 pid=4052925)[0;0m ERROR 12-05 14:47:06 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4052925)[0;0m ERROR 12-05 14:47:06 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4052925)[0;0m ERROR 12-05 14:47:06 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4052925)[0;0m ERROR 12-05 14:47:06 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 14:47:27 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 14:47:28 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 14:47:28 [model.py:1745] Using max model len 131072
INFO 12-05 14:47:28 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4053005)[0;0m INFO 12-05 14:47:36 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4053005)[0;0m INFO 12-05 14:47:37 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:60119 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4053005)[0;0m INFO 12-05 14:47:37 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4053005)[0;0m INFO 12-05 14:47:37 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4053005)[0;0m INFO 12-05 14:47:38 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4053005)[0;0m INFO 12-05 14:47:38 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4053005)[0;0m INFO 12-05 14:47:39 [default_loader.py:314] Loading weights took 0.88 seconds
[1;36m(EngineCore_DP0 pid=4053005)[0;0m INFO 12-05 14:47:40 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.425169 seconds
[1;36m(EngineCore_DP0 pid=4053005)[0;0m INFO 12-05 14:47:43 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4053005)[0;0m INFO 12-05 14:47:43 [backends.py:647] Dynamo bytecode transform time: 3.59 s
[1;36m(EngineCore_DP0 pid=4053005)[0;0m ERROR 12-05 14:47:44 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4053005)[0;0m ERROR 12-05 14:47:44 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4053005)[0;0m ERROR 12-05 14:47:44 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4053005)[0;0m ERROR 12-05 14:47:44 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4053005)[0;0m ERROR 12-05 14:47:44 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053005)[0;0m ERROR 12-05 14:47:44 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4053005)[0;0m ERROR 12-05 14:47:44 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4053005)[0;0m ERROR 12-05 14:47:44 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4053005)[0;0m ERROR 12-05 14:47:44 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4053005)[0;0m ERROR 12-05 14:47:44 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053005)[0;0m ERROR 12-05 14:47:44 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4053005)[0;0m ERROR 12-05 14:47:44 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4053005)[0;0m ERROR 12-05 14:47:44 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053005)[0;0m ERROR 12-05 14:47:44 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4053005)[0;0m ERROR 12-05 14:47:44 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4053005)[0;0m ERROR 12-05 14:47:44 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053005)[0;0m ERROR 12-05 14:47:44 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4053005)[0;0m ERROR 12-05 14:47:44 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4053005)[0;0m ERROR 12-05 14:47:44 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053005)[0;0m ERROR 12-05 14:47:44 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4053005)[0;0m ERROR 12-05 14:47:44 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4053005)[0;0m ERROR 12-05 14:47:44 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053005)[0;0m ERROR 12-05 14:47:44 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4053005)[0;0m ERROR 12-05 14:47:44 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4053005)[0;0m ERROR 12-05 14:47:44 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053005)[0;0m ERROR 12-05 14:47:44 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4053005)[0;0m ERROR 12-05 14:47:44 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4053005)[0;0m ERROR 12-05 14:47:44 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4053005)[0;0m ERROR 12-05 14:47:44 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4053005)[0;0m ERROR 12-05 14:47:44 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053005)[0;0m ERROR 12-05 14:47:44 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4053005)[0;0m ERROR 12-05 14:47:44 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4053005)[0;0m ERROR 12-05 14:47:44 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053005)[0;0m ERROR 12-05 14:47:44 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4053005)[0;0m ERROR 12-05 14:47:44 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4053005)[0;0m ERROR 12-05 14:47:44 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053005)[0;0m ERROR 12-05 14:47:44 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4053005)[0;0m ERROR 12-05 14:47:44 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4053005)[0;0m ERROR 12-05 14:47:44 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053005)[0;0m ERROR 12-05 14:47:44 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4053005)[0;0m ERROR 12-05 14:47:44 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4053005)[0;0m ERROR 12-05 14:47:44 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053005)[0;0m ERROR 12-05 14:47:44 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4053005)[0;0m ERROR 12-05 14:47:44 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4053005)[0;0m ERROR 12-05 14:47:44 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053005)[0;0m ERROR 12-05 14:47:44 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4053005)[0;0m ERROR 12-05 14:47:44 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4053005)[0;0m ERROR 12-05 14:47:44 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053005)[0;0m ERROR 12-05 14:47:44 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4053005)[0;0m ERROR 12-05 14:47:44 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4053005)[0;0m ERROR 12-05 14:47:44 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053005)[0;0m ERROR 12-05 14:47:44 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4053005)[0;0m ERROR 12-05 14:47:44 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4053005)[0;0m ERROR 12-05 14:47:44 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053005)[0;0m ERROR 12-05 14:47:44 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4053005)[0;0m ERROR 12-05 14:47:44 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4053005)[0;0m ERROR 12-05 14:47:44 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053005)[0;0m ERROR 12-05 14:47:44 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4053005)[0;0m ERROR 12-05 14:47:44 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4053005)[0;0m ERROR 12-05 14:47:44 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4053005)[0;0m ERROR 12-05 14:47:44 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4053005)[0;0m ERROR 12-05 14:47:44 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053005)[0;0m ERROR 12-05 14:47:44 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4053005)[0;0m ERROR 12-05 14:47:44 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4053005)[0;0m ERROR 12-05 14:47:44 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053005)[0;0m ERROR 12-05 14:47:44 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4053005)[0;0m ERROR 12-05 14:47:44 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4053005)[0;0m ERROR 12-05 14:47:44 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053005)[0;0m ERROR 12-05 14:47:44 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4053005)[0;0m ERROR 12-05 14:47:44 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4053005)[0;0m ERROR 12-05 14:47:44 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4053005)[0;0m ERROR 12-05 14:47:44 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4053005)[0;0m ERROR 12-05 14:47:44 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053005)[0;0m ERROR 12-05 14:47:44 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4053005)[0;0m ERROR 12-05 14:47:44 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4053005)[0;0m ERROR 12-05 14:47:44 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053005)[0;0m ERROR 12-05 14:47:44 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4053005)[0;0m ERROR 12-05 14:47:44 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4053005)[0;0m ERROR 12-05 14:47:44 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053005)[0;0m ERROR 12-05 14:47:44 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4053005)[0;0m ERROR 12-05 14:47:44 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4053005)[0;0m ERROR 12-05 14:47:44 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4053005)[0;0m ERROR 12-05 14:47:44 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4053005)[0;0m ERROR 12-05 14:47:44 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053005)[0;0m ERROR 12-05 14:47:44 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4053005)[0;0m ERROR 12-05 14:47:44 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4053005)[0;0m ERROR 12-05 14:47:44 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053005)[0;0m ERROR 12-05 14:47:44 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4053005)[0;0m ERROR 12-05 14:47:44 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4053005)[0;0m ERROR 12-05 14:47:44 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053005)[0;0m ERROR 12-05 14:47:44 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4053005)[0;0m ERROR 12-05 14:47:44 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4053005)[0;0m ERROR 12-05 14:47:44 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053005)[0;0m ERROR 12-05 14:47:44 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4053005)[0;0m ERROR 12-05 14:47:44 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4053005)[0;0m ERROR 12-05 14:47:44 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7f92e037c770>' raised:
[1;36m(EngineCore_DP0 pid=4053005)[0;0m ERROR 12-05 14:47:44 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4053005)[0;0m ERROR 12-05 14:47:44 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4053005)[0;0m ERROR 12-05 14:47:44 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4053005)[0;0m ERROR 12-05 14:47:44 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 14:48:05 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 14:48:05 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 14:48:05 [model.py:1745] Using max model len 131072
INFO 12-05 14:48:05 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4053090)[0;0m INFO 12-05 14:48:12 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4053090)[0;0m INFO 12-05 14:48:13 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:57235 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4053090)[0;0m INFO 12-05 14:48:13 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4053090)[0;0m INFO 12-05 14:48:13 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4053090)[0;0m INFO 12-05 14:48:14 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4053090)[0;0m INFO 12-05 14:48:14 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4053090)[0;0m INFO 12-05 14:48:15 [default_loader.py:314] Loading weights took 0.88 seconds
[1;36m(EngineCore_DP0 pid=4053090)[0;0m INFO 12-05 14:48:16 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.448828 seconds
[1;36m(EngineCore_DP0 pid=4053090)[0;0m INFO 12-05 14:48:19 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4053090)[0;0m INFO 12-05 14:48:19 [backends.py:647] Dynamo bytecode transform time: 3.63 s
[1;36m(EngineCore_DP0 pid=4053090)[0;0m ERROR 12-05 14:48:20 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4053090)[0;0m ERROR 12-05 14:48:20 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4053090)[0;0m ERROR 12-05 14:48:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4053090)[0;0m ERROR 12-05 14:48:20 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4053090)[0;0m ERROR 12-05 14:48:20 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053090)[0;0m ERROR 12-05 14:48:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4053090)[0;0m ERROR 12-05 14:48:20 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4053090)[0;0m ERROR 12-05 14:48:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4053090)[0;0m ERROR 12-05 14:48:20 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4053090)[0;0m ERROR 12-05 14:48:20 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053090)[0;0m ERROR 12-05 14:48:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4053090)[0;0m ERROR 12-05 14:48:20 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4053090)[0;0m ERROR 12-05 14:48:20 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053090)[0;0m ERROR 12-05 14:48:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4053090)[0;0m ERROR 12-05 14:48:20 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4053090)[0;0m ERROR 12-05 14:48:20 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053090)[0;0m ERROR 12-05 14:48:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4053090)[0;0m ERROR 12-05 14:48:20 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4053090)[0;0m ERROR 12-05 14:48:20 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053090)[0;0m ERROR 12-05 14:48:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4053090)[0;0m ERROR 12-05 14:48:20 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4053090)[0;0m ERROR 12-05 14:48:20 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053090)[0;0m ERROR 12-05 14:48:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4053090)[0;0m ERROR 12-05 14:48:20 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4053090)[0;0m ERROR 12-05 14:48:20 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053090)[0;0m ERROR 12-05 14:48:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4053090)[0;0m ERROR 12-05 14:48:20 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4053090)[0;0m ERROR 12-05 14:48:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4053090)[0;0m ERROR 12-05 14:48:20 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4053090)[0;0m ERROR 12-05 14:48:20 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053090)[0;0m ERROR 12-05 14:48:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4053090)[0;0m ERROR 12-05 14:48:20 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4053090)[0;0m ERROR 12-05 14:48:20 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053090)[0;0m ERROR 12-05 14:48:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4053090)[0;0m ERROR 12-05 14:48:20 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4053090)[0;0m ERROR 12-05 14:48:20 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053090)[0;0m ERROR 12-05 14:48:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4053090)[0;0m ERROR 12-05 14:48:20 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4053090)[0;0m ERROR 12-05 14:48:20 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053090)[0;0m ERROR 12-05 14:48:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4053090)[0;0m ERROR 12-05 14:48:20 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4053090)[0;0m ERROR 12-05 14:48:20 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053090)[0;0m ERROR 12-05 14:48:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4053090)[0;0m ERROR 12-05 14:48:20 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4053090)[0;0m ERROR 12-05 14:48:20 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053090)[0;0m ERROR 12-05 14:48:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4053090)[0;0m ERROR 12-05 14:48:20 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4053090)[0;0m ERROR 12-05 14:48:20 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053090)[0;0m ERROR 12-05 14:48:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4053090)[0;0m ERROR 12-05 14:48:20 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4053090)[0;0m ERROR 12-05 14:48:20 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053090)[0;0m ERROR 12-05 14:48:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4053090)[0;0m ERROR 12-05 14:48:20 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4053090)[0;0m ERROR 12-05 14:48:20 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053090)[0;0m ERROR 12-05 14:48:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4053090)[0;0m ERROR 12-05 14:48:20 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4053090)[0;0m ERROR 12-05 14:48:20 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053090)[0;0m ERROR 12-05 14:48:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4053090)[0;0m ERROR 12-05 14:48:20 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4053090)[0;0m ERROR 12-05 14:48:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4053090)[0;0m ERROR 12-05 14:48:20 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4053090)[0;0m ERROR 12-05 14:48:20 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053090)[0;0m ERROR 12-05 14:48:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4053090)[0;0m ERROR 12-05 14:48:20 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4053090)[0;0m ERROR 12-05 14:48:20 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053090)[0;0m ERROR 12-05 14:48:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4053090)[0;0m ERROR 12-05 14:48:20 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4053090)[0;0m ERROR 12-05 14:48:20 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053090)[0;0m ERROR 12-05 14:48:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4053090)[0;0m ERROR 12-05 14:48:20 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4053090)[0;0m ERROR 12-05 14:48:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4053090)[0;0m ERROR 12-05 14:48:20 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4053090)[0;0m ERROR 12-05 14:48:20 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053090)[0;0m ERROR 12-05 14:48:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4053090)[0;0m ERROR 12-05 14:48:20 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4053090)[0;0m ERROR 12-05 14:48:20 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053090)[0;0m ERROR 12-05 14:48:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4053090)[0;0m ERROR 12-05 14:48:20 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4053090)[0;0m ERROR 12-05 14:48:20 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053090)[0;0m ERROR 12-05 14:48:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4053090)[0;0m ERROR 12-05 14:48:20 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4053090)[0;0m ERROR 12-05 14:48:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4053090)[0;0m ERROR 12-05 14:48:20 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4053090)[0;0m ERROR 12-05 14:48:20 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053090)[0;0m ERROR 12-05 14:48:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4053090)[0;0m ERROR 12-05 14:48:20 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4053090)[0;0m ERROR 12-05 14:48:20 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053090)[0;0m ERROR 12-05 14:48:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4053090)[0;0m ERROR 12-05 14:48:20 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4053090)[0;0m ERROR 12-05 14:48:20 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053090)[0;0m ERROR 12-05 14:48:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4053090)[0;0m ERROR 12-05 14:48:20 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4053090)[0;0m ERROR 12-05 14:48:20 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053090)[0;0m ERROR 12-05 14:48:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4053090)[0;0m ERROR 12-05 14:48:20 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4053090)[0;0m ERROR 12-05 14:48:20 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7f1365663200>' raised:
[1;36m(EngineCore_DP0 pid=4053090)[0;0m ERROR 12-05 14:48:20 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4053090)[0;0m ERROR 12-05 14:48:20 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4053090)[0;0m ERROR 12-05 14:48:20 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4053090)[0;0m ERROR 12-05 14:48:20 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 14:48:41 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 14:48:41 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 14:48:41 [model.py:1745] Using max model len 131072
INFO 12-05 14:48:41 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4053170)[0;0m INFO 12-05 14:48:50 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4053170)[0;0m INFO 12-05 14:48:51 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:37179 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4053170)[0;0m INFO 12-05 14:48:51 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4053170)[0;0m INFO 12-05 14:48:51 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4053170)[0;0m INFO 12-05 14:48:52 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4053170)[0;0m INFO 12-05 14:48:52 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4053170)[0;0m INFO 12-05 14:48:53 [default_loader.py:314] Loading weights took 0.87 seconds
[1;36m(EngineCore_DP0 pid=4053170)[0;0m INFO 12-05 14:48:53 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.402496 seconds
[1;36m(EngineCore_DP0 pid=4053170)[0;0m INFO 12-05 14:48:57 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4053170)[0;0m INFO 12-05 14:48:57 [backends.py:647] Dynamo bytecode transform time: 3.66 s
[1;36m(EngineCore_DP0 pid=4053170)[0;0m ERROR 12-05 14:48:57 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4053170)[0;0m ERROR 12-05 14:48:57 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4053170)[0;0m ERROR 12-05 14:48:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4053170)[0;0m ERROR 12-05 14:48:57 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4053170)[0;0m ERROR 12-05 14:48:57 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053170)[0;0m ERROR 12-05 14:48:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4053170)[0;0m ERROR 12-05 14:48:57 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4053170)[0;0m ERROR 12-05 14:48:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4053170)[0;0m ERROR 12-05 14:48:57 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4053170)[0;0m ERROR 12-05 14:48:57 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053170)[0;0m ERROR 12-05 14:48:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4053170)[0;0m ERROR 12-05 14:48:57 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4053170)[0;0m ERROR 12-05 14:48:57 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053170)[0;0m ERROR 12-05 14:48:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4053170)[0;0m ERROR 12-05 14:48:57 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4053170)[0;0m ERROR 12-05 14:48:57 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053170)[0;0m ERROR 12-05 14:48:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4053170)[0;0m ERROR 12-05 14:48:57 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4053170)[0;0m ERROR 12-05 14:48:57 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053170)[0;0m ERROR 12-05 14:48:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4053170)[0;0m ERROR 12-05 14:48:57 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4053170)[0;0m ERROR 12-05 14:48:57 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053170)[0;0m ERROR 12-05 14:48:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4053170)[0;0m ERROR 12-05 14:48:57 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4053170)[0;0m ERROR 12-05 14:48:57 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053170)[0;0m ERROR 12-05 14:48:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4053170)[0;0m ERROR 12-05 14:48:57 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4053170)[0;0m ERROR 12-05 14:48:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4053170)[0;0m ERROR 12-05 14:48:57 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4053170)[0;0m ERROR 12-05 14:48:57 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053170)[0;0m ERROR 12-05 14:48:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4053170)[0;0m ERROR 12-05 14:48:57 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4053170)[0;0m ERROR 12-05 14:48:57 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053170)[0;0m ERROR 12-05 14:48:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4053170)[0;0m ERROR 12-05 14:48:57 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4053170)[0;0m ERROR 12-05 14:48:57 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053170)[0;0m ERROR 12-05 14:48:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4053170)[0;0m ERROR 12-05 14:48:57 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4053170)[0;0m ERROR 12-05 14:48:57 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053170)[0;0m ERROR 12-05 14:48:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4053170)[0;0m ERROR 12-05 14:48:57 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4053170)[0;0m ERROR 12-05 14:48:57 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053170)[0;0m ERROR 12-05 14:48:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4053170)[0;0m ERROR 12-05 14:48:57 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4053170)[0;0m ERROR 12-05 14:48:57 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053170)[0;0m ERROR 12-05 14:48:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4053170)[0;0m ERROR 12-05 14:48:57 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4053170)[0;0m ERROR 12-05 14:48:57 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053170)[0;0m ERROR 12-05 14:48:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4053170)[0;0m ERROR 12-05 14:48:57 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4053170)[0;0m ERROR 12-05 14:48:57 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053170)[0;0m ERROR 12-05 14:48:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4053170)[0;0m ERROR 12-05 14:48:57 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4053170)[0;0m ERROR 12-05 14:48:57 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053170)[0;0m ERROR 12-05 14:48:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4053170)[0;0m ERROR 12-05 14:48:57 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4053170)[0;0m ERROR 12-05 14:48:57 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053170)[0;0m ERROR 12-05 14:48:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4053170)[0;0m ERROR 12-05 14:48:57 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4053170)[0;0m ERROR 12-05 14:48:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4053170)[0;0m ERROR 12-05 14:48:57 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4053170)[0;0m ERROR 12-05 14:48:57 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053170)[0;0m ERROR 12-05 14:48:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4053170)[0;0m ERROR 12-05 14:48:57 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4053170)[0;0m ERROR 12-05 14:48:57 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053170)[0;0m ERROR 12-05 14:48:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4053170)[0;0m ERROR 12-05 14:48:57 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4053170)[0;0m ERROR 12-05 14:48:57 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053170)[0;0m ERROR 12-05 14:48:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4053170)[0;0m ERROR 12-05 14:48:57 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4053170)[0;0m ERROR 12-05 14:48:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4053170)[0;0m ERROR 12-05 14:48:57 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4053170)[0;0m ERROR 12-05 14:48:57 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053170)[0;0m ERROR 12-05 14:48:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4053170)[0;0m ERROR 12-05 14:48:57 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4053170)[0;0m ERROR 12-05 14:48:57 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053170)[0;0m ERROR 12-05 14:48:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4053170)[0;0m ERROR 12-05 14:48:57 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4053170)[0;0m ERROR 12-05 14:48:57 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053170)[0;0m ERROR 12-05 14:48:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4053170)[0;0m ERROR 12-05 14:48:57 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4053170)[0;0m ERROR 12-05 14:48:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4053170)[0;0m ERROR 12-05 14:48:57 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4053170)[0;0m ERROR 12-05 14:48:57 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053170)[0;0m ERROR 12-05 14:48:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4053170)[0;0m ERROR 12-05 14:48:57 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4053170)[0;0m ERROR 12-05 14:48:57 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053170)[0;0m ERROR 12-05 14:48:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4053170)[0;0m ERROR 12-05 14:48:57 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4053170)[0;0m ERROR 12-05 14:48:57 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053170)[0;0m ERROR 12-05 14:48:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4053170)[0;0m ERROR 12-05 14:48:57 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4053170)[0;0m ERROR 12-05 14:48:57 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053170)[0;0m ERROR 12-05 14:48:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4053170)[0;0m ERROR 12-05 14:48:57 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4053170)[0;0m ERROR 12-05 14:48:57 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7fe8030571a0>' raised:
[1;36m(EngineCore_DP0 pid=4053170)[0;0m ERROR 12-05 14:48:57 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4053170)[0;0m ERROR 12-05 14:48:57 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4053170)[0;0m ERROR 12-05 14:48:57 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4053170)[0;0m ERROR 12-05 14:48:57 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 14:49:19 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 14:49:19 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 14:49:19 [model.py:1745] Using max model len 131072
INFO 12-05 14:49:19 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4053259)[0;0m INFO 12-05 14:49:26 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4053259)[0;0m INFO 12-05 14:49:27 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:51431 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4053259)[0;0m INFO 12-05 14:49:27 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4053259)[0;0m INFO 12-05 14:49:27 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4053259)[0;0m INFO 12-05 14:49:28 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4053259)[0;0m INFO 12-05 14:49:28 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4053259)[0;0m INFO 12-05 14:49:29 [default_loader.py:314] Loading weights took 0.88 seconds
[1;36m(EngineCore_DP0 pid=4053259)[0;0m INFO 12-05 14:49:29 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.449412 seconds
[1;36m(EngineCore_DP0 pid=4053259)[0;0m INFO 12-05 14:49:33 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4053259)[0;0m INFO 12-05 14:49:33 [backends.py:647] Dynamo bytecode transform time: 3.66 s
[1;36m(EngineCore_DP0 pid=4053259)[0;0m ERROR 12-05 14:49:33 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4053259)[0;0m ERROR 12-05 14:49:33 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4053259)[0;0m ERROR 12-05 14:49:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4053259)[0;0m ERROR 12-05 14:49:33 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4053259)[0;0m ERROR 12-05 14:49:33 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053259)[0;0m ERROR 12-05 14:49:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4053259)[0;0m ERROR 12-05 14:49:33 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4053259)[0;0m ERROR 12-05 14:49:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4053259)[0;0m ERROR 12-05 14:49:33 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4053259)[0;0m ERROR 12-05 14:49:33 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053259)[0;0m ERROR 12-05 14:49:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4053259)[0;0m ERROR 12-05 14:49:33 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4053259)[0;0m ERROR 12-05 14:49:33 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053259)[0;0m ERROR 12-05 14:49:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4053259)[0;0m ERROR 12-05 14:49:33 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4053259)[0;0m ERROR 12-05 14:49:33 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053259)[0;0m ERROR 12-05 14:49:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4053259)[0;0m ERROR 12-05 14:49:33 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4053259)[0;0m ERROR 12-05 14:49:33 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053259)[0;0m ERROR 12-05 14:49:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4053259)[0;0m ERROR 12-05 14:49:33 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4053259)[0;0m ERROR 12-05 14:49:33 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053259)[0;0m ERROR 12-05 14:49:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4053259)[0;0m ERROR 12-05 14:49:33 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4053259)[0;0m ERROR 12-05 14:49:33 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053259)[0;0m ERROR 12-05 14:49:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4053259)[0;0m ERROR 12-05 14:49:33 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4053259)[0;0m ERROR 12-05 14:49:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4053259)[0;0m ERROR 12-05 14:49:33 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4053259)[0;0m ERROR 12-05 14:49:33 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053259)[0;0m ERROR 12-05 14:49:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4053259)[0;0m ERROR 12-05 14:49:33 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4053259)[0;0m ERROR 12-05 14:49:33 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053259)[0;0m ERROR 12-05 14:49:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4053259)[0;0m ERROR 12-05 14:49:33 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4053259)[0;0m ERROR 12-05 14:49:33 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053259)[0;0m ERROR 12-05 14:49:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4053259)[0;0m ERROR 12-05 14:49:33 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4053259)[0;0m ERROR 12-05 14:49:33 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053259)[0;0m ERROR 12-05 14:49:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4053259)[0;0m ERROR 12-05 14:49:33 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4053259)[0;0m ERROR 12-05 14:49:33 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053259)[0;0m ERROR 12-05 14:49:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4053259)[0;0m ERROR 12-05 14:49:33 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4053259)[0;0m ERROR 12-05 14:49:33 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053259)[0;0m ERROR 12-05 14:49:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4053259)[0;0m ERROR 12-05 14:49:33 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4053259)[0;0m ERROR 12-05 14:49:33 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053259)[0;0m ERROR 12-05 14:49:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4053259)[0;0m ERROR 12-05 14:49:33 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4053259)[0;0m ERROR 12-05 14:49:33 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053259)[0;0m ERROR 12-05 14:49:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4053259)[0;0m ERROR 12-05 14:49:33 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4053259)[0;0m ERROR 12-05 14:49:33 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053259)[0;0m ERROR 12-05 14:49:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4053259)[0;0m ERROR 12-05 14:49:33 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4053259)[0;0m ERROR 12-05 14:49:33 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053259)[0;0m ERROR 12-05 14:49:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4053259)[0;0m ERROR 12-05 14:49:33 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4053259)[0;0m ERROR 12-05 14:49:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4053259)[0;0m ERROR 12-05 14:49:33 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4053259)[0;0m ERROR 12-05 14:49:33 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053259)[0;0m ERROR 12-05 14:49:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4053259)[0;0m ERROR 12-05 14:49:33 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4053259)[0;0m ERROR 12-05 14:49:33 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053259)[0;0m ERROR 12-05 14:49:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4053259)[0;0m ERROR 12-05 14:49:33 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4053259)[0;0m ERROR 12-05 14:49:33 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053259)[0;0m ERROR 12-05 14:49:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4053259)[0;0m ERROR 12-05 14:49:33 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4053259)[0;0m ERROR 12-05 14:49:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4053259)[0;0m ERROR 12-05 14:49:33 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4053259)[0;0m ERROR 12-05 14:49:33 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053259)[0;0m ERROR 12-05 14:49:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4053259)[0;0m ERROR 12-05 14:49:33 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4053259)[0;0m ERROR 12-05 14:49:33 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053259)[0;0m ERROR 12-05 14:49:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4053259)[0;0m ERROR 12-05 14:49:33 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4053259)[0;0m ERROR 12-05 14:49:33 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053259)[0;0m ERROR 12-05 14:49:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4053259)[0;0m ERROR 12-05 14:49:33 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4053259)[0;0m ERROR 12-05 14:49:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4053259)[0;0m ERROR 12-05 14:49:33 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4053259)[0;0m ERROR 12-05 14:49:33 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053259)[0;0m ERROR 12-05 14:49:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4053259)[0;0m ERROR 12-05 14:49:33 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4053259)[0;0m ERROR 12-05 14:49:33 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053259)[0;0m ERROR 12-05 14:49:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4053259)[0;0m ERROR 12-05 14:49:33 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4053259)[0;0m ERROR 12-05 14:49:33 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053259)[0;0m ERROR 12-05 14:49:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4053259)[0;0m ERROR 12-05 14:49:33 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4053259)[0;0m ERROR 12-05 14:49:33 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053259)[0;0m ERROR 12-05 14:49:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4053259)[0;0m ERROR 12-05 14:49:33 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4053259)[0;0m ERROR 12-05 14:49:33 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7f1711d3f290>' raised:
[1;36m(EngineCore_DP0 pid=4053259)[0;0m ERROR 12-05 14:49:33 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4053259)[0;0m ERROR 12-05 14:49:33 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4053259)[0;0m ERROR 12-05 14:49:33 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4053259)[0;0m ERROR 12-05 14:49:33 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 14:49:55 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 14:49:55 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 14:49:55 [model.py:1745] Using max model len 131072
INFO 12-05 14:49:55 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4053351)[0;0m INFO 12-05 14:50:03 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4053351)[0;0m INFO 12-05 14:50:03 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:60249 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4053351)[0;0m INFO 12-05 14:50:03 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4053351)[0;0m INFO 12-05 14:50:04 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4053351)[0;0m INFO 12-05 14:50:04 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4053351)[0;0m INFO 12-05 14:50:04 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4053351)[0;0m INFO 12-05 14:50:05 [default_loader.py:314] Loading weights took 0.87 seconds
[1;36m(EngineCore_DP0 pid=4053351)[0;0m INFO 12-05 14:50:06 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.395835 seconds
[1;36m(EngineCore_DP0 pid=4053351)[0;0m INFO 12-05 14:50:10 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4053351)[0;0m INFO 12-05 14:50:10 [backends.py:647] Dynamo bytecode transform time: 3.64 s
[1;36m(EngineCore_DP0 pid=4053351)[0;0m ERROR 12-05 14:50:10 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4053351)[0;0m ERROR 12-05 14:50:10 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4053351)[0;0m ERROR 12-05 14:50:10 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4053351)[0;0m ERROR 12-05 14:50:10 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4053351)[0;0m ERROR 12-05 14:50:10 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053351)[0;0m ERROR 12-05 14:50:10 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4053351)[0;0m ERROR 12-05 14:50:10 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4053351)[0;0m ERROR 12-05 14:50:10 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4053351)[0;0m ERROR 12-05 14:50:10 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4053351)[0;0m ERROR 12-05 14:50:10 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053351)[0;0m ERROR 12-05 14:50:10 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4053351)[0;0m ERROR 12-05 14:50:10 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4053351)[0;0m ERROR 12-05 14:50:10 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053351)[0;0m ERROR 12-05 14:50:10 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4053351)[0;0m ERROR 12-05 14:50:10 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4053351)[0;0m ERROR 12-05 14:50:10 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053351)[0;0m ERROR 12-05 14:50:10 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4053351)[0;0m ERROR 12-05 14:50:10 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4053351)[0;0m ERROR 12-05 14:50:10 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053351)[0;0m ERROR 12-05 14:50:10 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4053351)[0;0m ERROR 12-05 14:50:10 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4053351)[0;0m ERROR 12-05 14:50:10 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053351)[0;0m ERROR 12-05 14:50:10 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4053351)[0;0m ERROR 12-05 14:50:10 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4053351)[0;0m ERROR 12-05 14:50:10 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053351)[0;0m ERROR 12-05 14:50:10 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4053351)[0;0m ERROR 12-05 14:50:10 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4053351)[0;0m ERROR 12-05 14:50:10 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4053351)[0;0m ERROR 12-05 14:50:10 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4053351)[0;0m ERROR 12-05 14:50:10 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053351)[0;0m ERROR 12-05 14:50:10 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4053351)[0;0m ERROR 12-05 14:50:10 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4053351)[0;0m ERROR 12-05 14:50:10 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053351)[0;0m ERROR 12-05 14:50:10 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4053351)[0;0m ERROR 12-05 14:50:10 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4053351)[0;0m ERROR 12-05 14:50:10 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053351)[0;0m ERROR 12-05 14:50:10 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4053351)[0;0m ERROR 12-05 14:50:10 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4053351)[0;0m ERROR 12-05 14:50:10 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053351)[0;0m ERROR 12-05 14:50:10 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4053351)[0;0m ERROR 12-05 14:50:10 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4053351)[0;0m ERROR 12-05 14:50:10 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053351)[0;0m ERROR 12-05 14:50:10 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4053351)[0;0m ERROR 12-05 14:50:10 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4053351)[0;0m ERROR 12-05 14:50:10 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053351)[0;0m ERROR 12-05 14:50:10 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4053351)[0;0m ERROR 12-05 14:50:10 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4053351)[0;0m ERROR 12-05 14:50:10 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053351)[0;0m ERROR 12-05 14:50:10 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4053351)[0;0m ERROR 12-05 14:50:10 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4053351)[0;0m ERROR 12-05 14:50:10 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053351)[0;0m ERROR 12-05 14:50:10 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4053351)[0;0m ERROR 12-05 14:50:10 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4053351)[0;0m ERROR 12-05 14:50:10 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053351)[0;0m ERROR 12-05 14:50:10 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4053351)[0;0m ERROR 12-05 14:50:10 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4053351)[0;0m ERROR 12-05 14:50:10 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053351)[0;0m ERROR 12-05 14:50:10 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4053351)[0;0m ERROR 12-05 14:50:10 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4053351)[0;0m ERROR 12-05 14:50:10 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4053351)[0;0m ERROR 12-05 14:50:10 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4053351)[0;0m ERROR 12-05 14:50:10 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053351)[0;0m ERROR 12-05 14:50:10 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4053351)[0;0m ERROR 12-05 14:50:10 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4053351)[0;0m ERROR 12-05 14:50:10 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053351)[0;0m ERROR 12-05 14:50:10 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4053351)[0;0m ERROR 12-05 14:50:10 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4053351)[0;0m ERROR 12-05 14:50:10 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053351)[0;0m ERROR 12-05 14:50:10 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4053351)[0;0m ERROR 12-05 14:50:10 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4053351)[0;0m ERROR 12-05 14:50:10 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4053351)[0;0m ERROR 12-05 14:50:10 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4053351)[0;0m ERROR 12-05 14:50:10 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053351)[0;0m ERROR 12-05 14:50:10 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4053351)[0;0m ERROR 12-05 14:50:10 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4053351)[0;0m ERROR 12-05 14:50:10 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053351)[0;0m ERROR 12-05 14:50:10 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4053351)[0;0m ERROR 12-05 14:50:10 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4053351)[0;0m ERROR 12-05 14:50:10 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053351)[0;0m ERROR 12-05 14:50:10 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4053351)[0;0m ERROR 12-05 14:50:10 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4053351)[0;0m ERROR 12-05 14:50:10 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4053351)[0;0m ERROR 12-05 14:50:10 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4053351)[0;0m ERROR 12-05 14:50:10 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053351)[0;0m ERROR 12-05 14:50:10 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4053351)[0;0m ERROR 12-05 14:50:10 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4053351)[0;0m ERROR 12-05 14:50:10 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053351)[0;0m ERROR 12-05 14:50:10 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4053351)[0;0m ERROR 12-05 14:50:10 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4053351)[0;0m ERROR 12-05 14:50:10 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053351)[0;0m ERROR 12-05 14:50:10 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4053351)[0;0m ERROR 12-05 14:50:10 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4053351)[0;0m ERROR 12-05 14:50:10 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053351)[0;0m ERROR 12-05 14:50:10 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4053351)[0;0m ERROR 12-05 14:50:10 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4053351)[0;0m ERROR 12-05 14:50:10 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7f66a01a3d40>' raised:
[1;36m(EngineCore_DP0 pid=4053351)[0;0m ERROR 12-05 14:50:10 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4053351)[0;0m ERROR 12-05 14:50:10 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4053351)[0;0m ERROR 12-05 14:50:10 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4053351)[0;0m ERROR 12-05 14:50:10 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 14:50:31 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 14:50:31 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 14:50:31 [model.py:1745] Using max model len 131072
INFO 12-05 14:50:31 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4053432)[0;0m INFO 12-05 14:50:38 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4053432)[0;0m INFO 12-05 14:50:39 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:58803 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4053432)[0;0m INFO 12-05 14:50:39 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4053432)[0;0m INFO 12-05 14:50:40 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4053432)[0;0m INFO 12-05 14:50:40 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4053432)[0;0m INFO 12-05 14:50:40 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4053432)[0;0m INFO 12-05 14:50:41 [default_loader.py:314] Loading weights took 0.88 seconds
[1;36m(EngineCore_DP0 pid=4053432)[0;0m INFO 12-05 14:50:42 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.416004 seconds
[1;36m(EngineCore_DP0 pid=4053432)[0;0m INFO 12-05 14:50:46 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4053432)[0;0m INFO 12-05 14:50:46 [backends.py:647] Dynamo bytecode transform time: 3.69 s
[1;36m(EngineCore_DP0 pid=4053432)[0;0m ERROR 12-05 14:50:46 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4053432)[0;0m ERROR 12-05 14:50:46 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4053432)[0;0m ERROR 12-05 14:50:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4053432)[0;0m ERROR 12-05 14:50:46 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4053432)[0;0m ERROR 12-05 14:50:46 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053432)[0;0m ERROR 12-05 14:50:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4053432)[0;0m ERROR 12-05 14:50:46 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4053432)[0;0m ERROR 12-05 14:50:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4053432)[0;0m ERROR 12-05 14:50:46 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4053432)[0;0m ERROR 12-05 14:50:46 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053432)[0;0m ERROR 12-05 14:50:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4053432)[0;0m ERROR 12-05 14:50:46 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4053432)[0;0m ERROR 12-05 14:50:46 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053432)[0;0m ERROR 12-05 14:50:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4053432)[0;0m ERROR 12-05 14:50:46 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4053432)[0;0m ERROR 12-05 14:50:46 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053432)[0;0m ERROR 12-05 14:50:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4053432)[0;0m ERROR 12-05 14:50:46 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4053432)[0;0m ERROR 12-05 14:50:46 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053432)[0;0m ERROR 12-05 14:50:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4053432)[0;0m ERROR 12-05 14:50:46 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4053432)[0;0m ERROR 12-05 14:50:46 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053432)[0;0m ERROR 12-05 14:50:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4053432)[0;0m ERROR 12-05 14:50:46 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4053432)[0;0m ERROR 12-05 14:50:46 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053432)[0;0m ERROR 12-05 14:50:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4053432)[0;0m ERROR 12-05 14:50:46 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4053432)[0;0m ERROR 12-05 14:50:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4053432)[0;0m ERROR 12-05 14:50:46 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4053432)[0;0m ERROR 12-05 14:50:46 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053432)[0;0m ERROR 12-05 14:50:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4053432)[0;0m ERROR 12-05 14:50:46 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4053432)[0;0m ERROR 12-05 14:50:46 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053432)[0;0m ERROR 12-05 14:50:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4053432)[0;0m ERROR 12-05 14:50:46 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4053432)[0;0m ERROR 12-05 14:50:46 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053432)[0;0m ERROR 12-05 14:50:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4053432)[0;0m ERROR 12-05 14:50:46 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4053432)[0;0m ERROR 12-05 14:50:46 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053432)[0;0m ERROR 12-05 14:50:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4053432)[0;0m ERROR 12-05 14:50:46 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4053432)[0;0m ERROR 12-05 14:50:46 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053432)[0;0m ERROR 12-05 14:50:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4053432)[0;0m ERROR 12-05 14:50:46 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4053432)[0;0m ERROR 12-05 14:50:46 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053432)[0;0m ERROR 12-05 14:50:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4053432)[0;0m ERROR 12-05 14:50:46 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4053432)[0;0m ERROR 12-05 14:50:46 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053432)[0;0m ERROR 12-05 14:50:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4053432)[0;0m ERROR 12-05 14:50:46 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4053432)[0;0m ERROR 12-05 14:50:46 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053432)[0;0m ERROR 12-05 14:50:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4053432)[0;0m ERROR 12-05 14:50:46 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4053432)[0;0m ERROR 12-05 14:50:46 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053432)[0;0m ERROR 12-05 14:50:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4053432)[0;0m ERROR 12-05 14:50:46 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4053432)[0;0m ERROR 12-05 14:50:46 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053432)[0;0m ERROR 12-05 14:50:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4053432)[0;0m ERROR 12-05 14:50:46 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4053432)[0;0m ERROR 12-05 14:50:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4053432)[0;0m ERROR 12-05 14:50:46 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4053432)[0;0m ERROR 12-05 14:50:46 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053432)[0;0m ERROR 12-05 14:50:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4053432)[0;0m ERROR 12-05 14:50:46 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4053432)[0;0m ERROR 12-05 14:50:46 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053432)[0;0m ERROR 12-05 14:50:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4053432)[0;0m ERROR 12-05 14:50:46 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4053432)[0;0m ERROR 12-05 14:50:46 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053432)[0;0m ERROR 12-05 14:50:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4053432)[0;0m ERROR 12-05 14:50:46 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4053432)[0;0m ERROR 12-05 14:50:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4053432)[0;0m ERROR 12-05 14:50:46 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4053432)[0;0m ERROR 12-05 14:50:46 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053432)[0;0m ERROR 12-05 14:50:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4053432)[0;0m ERROR 12-05 14:50:46 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4053432)[0;0m ERROR 12-05 14:50:46 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053432)[0;0m ERROR 12-05 14:50:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4053432)[0;0m ERROR 12-05 14:50:46 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4053432)[0;0m ERROR 12-05 14:50:46 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053432)[0;0m ERROR 12-05 14:50:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4053432)[0;0m ERROR 12-05 14:50:46 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4053432)[0;0m ERROR 12-05 14:50:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4053432)[0;0m ERROR 12-05 14:50:46 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4053432)[0;0m ERROR 12-05 14:50:46 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053432)[0;0m ERROR 12-05 14:50:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4053432)[0;0m ERROR 12-05 14:50:46 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4053432)[0;0m ERROR 12-05 14:50:46 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053432)[0;0m ERROR 12-05 14:50:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4053432)[0;0m ERROR 12-05 14:50:46 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4053432)[0;0m ERROR 12-05 14:50:46 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053432)[0;0m ERROR 12-05 14:50:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4053432)[0;0m ERROR 12-05 14:50:46 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4053432)[0;0m ERROR 12-05 14:50:46 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053432)[0;0m ERROR 12-05 14:50:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4053432)[0;0m ERROR 12-05 14:50:46 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4053432)[0;0m ERROR 12-05 14:50:46 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7fd1dc325b50>' raised:
[1;36m(EngineCore_DP0 pid=4053432)[0;0m ERROR 12-05 14:50:46 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4053432)[0;0m ERROR 12-05 14:50:46 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4053432)[0;0m ERROR 12-05 14:50:46 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4053432)[0;0m ERROR 12-05 14:50:46 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 14:51:07 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 14:51:07 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 14:51:07 [model.py:1745] Using max model len 131072
INFO 12-05 14:51:07 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4053533)[0;0m INFO 12-05 14:51:15 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4053533)[0;0m INFO 12-05 14:51:16 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:55485 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4053533)[0;0m INFO 12-05 14:51:16 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4053533)[0;0m INFO 12-05 14:51:16 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4053533)[0;0m INFO 12-05 14:51:16 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4053533)[0;0m INFO 12-05 14:51:16 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4053533)[0;0m INFO 12-05 14:51:18 [default_loader.py:314] Loading weights took 0.88 seconds
[1;36m(EngineCore_DP0 pid=4053533)[0;0m INFO 12-05 14:51:18 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.448077 seconds
[1;36m(EngineCore_DP0 pid=4053533)[0;0m INFO 12-05 14:51:22 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4053533)[0;0m INFO 12-05 14:51:22 [backends.py:647] Dynamo bytecode transform time: 3.63 s
[1;36m(EngineCore_DP0 pid=4053533)[0;0m ERROR 12-05 14:51:22 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4053533)[0;0m ERROR 12-05 14:51:22 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4053533)[0;0m ERROR 12-05 14:51:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4053533)[0;0m ERROR 12-05 14:51:22 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4053533)[0;0m ERROR 12-05 14:51:22 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053533)[0;0m ERROR 12-05 14:51:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4053533)[0;0m ERROR 12-05 14:51:22 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4053533)[0;0m ERROR 12-05 14:51:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4053533)[0;0m ERROR 12-05 14:51:22 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4053533)[0;0m ERROR 12-05 14:51:22 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053533)[0;0m ERROR 12-05 14:51:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4053533)[0;0m ERROR 12-05 14:51:22 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4053533)[0;0m ERROR 12-05 14:51:22 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053533)[0;0m ERROR 12-05 14:51:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4053533)[0;0m ERROR 12-05 14:51:22 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4053533)[0;0m ERROR 12-05 14:51:22 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053533)[0;0m ERROR 12-05 14:51:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4053533)[0;0m ERROR 12-05 14:51:22 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4053533)[0;0m ERROR 12-05 14:51:22 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053533)[0;0m ERROR 12-05 14:51:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4053533)[0;0m ERROR 12-05 14:51:22 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4053533)[0;0m ERROR 12-05 14:51:22 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053533)[0;0m ERROR 12-05 14:51:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4053533)[0;0m ERROR 12-05 14:51:22 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4053533)[0;0m ERROR 12-05 14:51:22 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053533)[0;0m ERROR 12-05 14:51:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4053533)[0;0m ERROR 12-05 14:51:22 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4053533)[0;0m ERROR 12-05 14:51:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4053533)[0;0m ERROR 12-05 14:51:22 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4053533)[0;0m ERROR 12-05 14:51:22 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053533)[0;0m ERROR 12-05 14:51:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4053533)[0;0m ERROR 12-05 14:51:22 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4053533)[0;0m ERROR 12-05 14:51:22 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053533)[0;0m ERROR 12-05 14:51:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4053533)[0;0m ERROR 12-05 14:51:22 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4053533)[0;0m ERROR 12-05 14:51:22 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053533)[0;0m ERROR 12-05 14:51:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4053533)[0;0m ERROR 12-05 14:51:22 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4053533)[0;0m ERROR 12-05 14:51:22 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053533)[0;0m ERROR 12-05 14:51:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4053533)[0;0m ERROR 12-05 14:51:22 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4053533)[0;0m ERROR 12-05 14:51:22 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053533)[0;0m ERROR 12-05 14:51:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4053533)[0;0m ERROR 12-05 14:51:22 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4053533)[0;0m ERROR 12-05 14:51:22 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053533)[0;0m ERROR 12-05 14:51:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4053533)[0;0m ERROR 12-05 14:51:22 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4053533)[0;0m ERROR 12-05 14:51:22 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053533)[0;0m ERROR 12-05 14:51:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4053533)[0;0m ERROR 12-05 14:51:22 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4053533)[0;0m ERROR 12-05 14:51:22 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053533)[0;0m ERROR 12-05 14:51:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4053533)[0;0m ERROR 12-05 14:51:22 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4053533)[0;0m ERROR 12-05 14:51:22 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053533)[0;0m ERROR 12-05 14:51:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4053533)[0;0m ERROR 12-05 14:51:22 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4053533)[0;0m ERROR 12-05 14:51:22 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053533)[0;0m ERROR 12-05 14:51:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4053533)[0;0m ERROR 12-05 14:51:22 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4053533)[0;0m ERROR 12-05 14:51:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4053533)[0;0m ERROR 12-05 14:51:22 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4053533)[0;0m ERROR 12-05 14:51:22 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053533)[0;0m ERROR 12-05 14:51:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4053533)[0;0m ERROR 12-05 14:51:22 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4053533)[0;0m ERROR 12-05 14:51:22 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053533)[0;0m ERROR 12-05 14:51:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4053533)[0;0m ERROR 12-05 14:51:22 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4053533)[0;0m ERROR 12-05 14:51:22 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053533)[0;0m ERROR 12-05 14:51:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4053533)[0;0m ERROR 12-05 14:51:22 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4053533)[0;0m ERROR 12-05 14:51:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4053533)[0;0m ERROR 12-05 14:51:22 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4053533)[0;0m ERROR 12-05 14:51:22 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053533)[0;0m ERROR 12-05 14:51:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4053533)[0;0m ERROR 12-05 14:51:22 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4053533)[0;0m ERROR 12-05 14:51:22 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053533)[0;0m ERROR 12-05 14:51:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4053533)[0;0m ERROR 12-05 14:51:22 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4053533)[0;0m ERROR 12-05 14:51:22 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053533)[0;0m ERROR 12-05 14:51:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4053533)[0;0m ERROR 12-05 14:51:22 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4053533)[0;0m ERROR 12-05 14:51:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4053533)[0;0m ERROR 12-05 14:51:22 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4053533)[0;0m ERROR 12-05 14:51:22 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053533)[0;0m ERROR 12-05 14:51:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4053533)[0;0m ERROR 12-05 14:51:22 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4053533)[0;0m ERROR 12-05 14:51:22 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053533)[0;0m ERROR 12-05 14:51:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4053533)[0;0m ERROR 12-05 14:51:22 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4053533)[0;0m ERROR 12-05 14:51:22 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053533)[0;0m ERROR 12-05 14:51:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4053533)[0;0m ERROR 12-05 14:51:22 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4053533)[0;0m ERROR 12-05 14:51:22 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053533)[0;0m ERROR 12-05 14:51:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4053533)[0;0m ERROR 12-05 14:51:22 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4053533)[0;0m ERROR 12-05 14:51:22 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7fb02c27bb90>' raised:
[1;36m(EngineCore_DP0 pid=4053533)[0;0m ERROR 12-05 14:51:22 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4053533)[0;0m ERROR 12-05 14:51:22 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4053533)[0;0m ERROR 12-05 14:51:22 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4053533)[0;0m ERROR 12-05 14:51:22 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 14:51:44 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 14:51:44 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 14:51:44 [model.py:1745] Using max model len 131072
INFO 12-05 14:51:44 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4053700)[0;0m INFO 12-05 14:51:51 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4053700)[0;0m INFO 12-05 14:51:52 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:44067 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4053700)[0;0m INFO 12-05 14:51:52 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4053700)[0;0m INFO 12-05 14:51:52 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4053700)[0;0m INFO 12-05 14:51:52 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4053700)[0;0m INFO 12-05 14:51:52 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4053700)[0;0m INFO 12-05 14:51:54 [default_loader.py:314] Loading weights took 0.87 seconds
[1;36m(EngineCore_DP0 pid=4053700)[0;0m INFO 12-05 14:51:54 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.436127 seconds
[1;36m(EngineCore_DP0 pid=4053700)[0;0m INFO 12-05 14:51:58 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4053700)[0;0m INFO 12-05 14:51:58 [backends.py:647] Dynamo bytecode transform time: 3.60 s
[1;36m(EngineCore_DP0 pid=4053700)[0;0m ERROR 12-05 14:51:58 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4053700)[0;0m ERROR 12-05 14:51:58 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4053700)[0;0m ERROR 12-05 14:51:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4053700)[0;0m ERROR 12-05 14:51:58 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4053700)[0;0m ERROR 12-05 14:51:58 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053700)[0;0m ERROR 12-05 14:51:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4053700)[0;0m ERROR 12-05 14:51:58 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4053700)[0;0m ERROR 12-05 14:51:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4053700)[0;0m ERROR 12-05 14:51:58 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4053700)[0;0m ERROR 12-05 14:51:58 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053700)[0;0m ERROR 12-05 14:51:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4053700)[0;0m ERROR 12-05 14:51:58 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4053700)[0;0m ERROR 12-05 14:51:58 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053700)[0;0m ERROR 12-05 14:51:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4053700)[0;0m ERROR 12-05 14:51:58 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4053700)[0;0m ERROR 12-05 14:51:58 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053700)[0;0m ERROR 12-05 14:51:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4053700)[0;0m ERROR 12-05 14:51:58 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4053700)[0;0m ERROR 12-05 14:51:58 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053700)[0;0m ERROR 12-05 14:51:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4053700)[0;0m ERROR 12-05 14:51:58 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4053700)[0;0m ERROR 12-05 14:51:58 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053700)[0;0m ERROR 12-05 14:51:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4053700)[0;0m ERROR 12-05 14:51:58 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4053700)[0;0m ERROR 12-05 14:51:58 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053700)[0;0m ERROR 12-05 14:51:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4053700)[0;0m ERROR 12-05 14:51:58 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4053700)[0;0m ERROR 12-05 14:51:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4053700)[0;0m ERROR 12-05 14:51:58 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4053700)[0;0m ERROR 12-05 14:51:58 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053700)[0;0m ERROR 12-05 14:51:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4053700)[0;0m ERROR 12-05 14:51:58 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4053700)[0;0m ERROR 12-05 14:51:58 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053700)[0;0m ERROR 12-05 14:51:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4053700)[0;0m ERROR 12-05 14:51:58 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4053700)[0;0m ERROR 12-05 14:51:58 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053700)[0;0m ERROR 12-05 14:51:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4053700)[0;0m ERROR 12-05 14:51:58 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4053700)[0;0m ERROR 12-05 14:51:58 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053700)[0;0m ERROR 12-05 14:51:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4053700)[0;0m ERROR 12-05 14:51:58 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4053700)[0;0m ERROR 12-05 14:51:58 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053700)[0;0m ERROR 12-05 14:51:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4053700)[0;0m ERROR 12-05 14:51:58 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4053700)[0;0m ERROR 12-05 14:51:58 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053700)[0;0m ERROR 12-05 14:51:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4053700)[0;0m ERROR 12-05 14:51:58 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4053700)[0;0m ERROR 12-05 14:51:58 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053700)[0;0m ERROR 12-05 14:51:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4053700)[0;0m ERROR 12-05 14:51:58 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4053700)[0;0m ERROR 12-05 14:51:58 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053700)[0;0m ERROR 12-05 14:51:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4053700)[0;0m ERROR 12-05 14:51:58 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4053700)[0;0m ERROR 12-05 14:51:58 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053700)[0;0m ERROR 12-05 14:51:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4053700)[0;0m ERROR 12-05 14:51:58 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4053700)[0;0m ERROR 12-05 14:51:58 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053700)[0;0m ERROR 12-05 14:51:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4053700)[0;0m ERROR 12-05 14:51:58 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4053700)[0;0m ERROR 12-05 14:51:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4053700)[0;0m ERROR 12-05 14:51:58 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4053700)[0;0m ERROR 12-05 14:51:58 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053700)[0;0m ERROR 12-05 14:51:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4053700)[0;0m ERROR 12-05 14:51:58 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4053700)[0;0m ERROR 12-05 14:51:58 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053700)[0;0m ERROR 12-05 14:51:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4053700)[0;0m ERROR 12-05 14:51:58 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4053700)[0;0m ERROR 12-05 14:51:58 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053700)[0;0m ERROR 12-05 14:51:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4053700)[0;0m ERROR 12-05 14:51:58 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4053700)[0;0m ERROR 12-05 14:51:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4053700)[0;0m ERROR 12-05 14:51:58 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4053700)[0;0m ERROR 12-05 14:51:58 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053700)[0;0m ERROR 12-05 14:51:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4053700)[0;0m ERROR 12-05 14:51:58 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4053700)[0;0m ERROR 12-05 14:51:58 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053700)[0;0m ERROR 12-05 14:51:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4053700)[0;0m ERROR 12-05 14:51:58 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4053700)[0;0m ERROR 12-05 14:51:58 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053700)[0;0m ERROR 12-05 14:51:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4053700)[0;0m ERROR 12-05 14:51:58 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4053700)[0;0m ERROR 12-05 14:51:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4053700)[0;0m ERROR 12-05 14:51:58 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4053700)[0;0m ERROR 12-05 14:51:58 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053700)[0;0m ERROR 12-05 14:51:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4053700)[0;0m ERROR 12-05 14:51:58 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4053700)[0;0m ERROR 12-05 14:51:58 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053700)[0;0m ERROR 12-05 14:51:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4053700)[0;0m ERROR 12-05 14:51:58 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4053700)[0;0m ERROR 12-05 14:51:58 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053700)[0;0m ERROR 12-05 14:51:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4053700)[0;0m ERROR 12-05 14:51:58 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4053700)[0;0m ERROR 12-05 14:51:58 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053700)[0;0m ERROR 12-05 14:51:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4053700)[0;0m ERROR 12-05 14:51:58 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4053700)[0;0m ERROR 12-05 14:51:58 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7f5f8a2b2d20>' raised:
[1;36m(EngineCore_DP0 pid=4053700)[0;0m ERROR 12-05 14:51:58 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4053700)[0;0m ERROR 12-05 14:51:58 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4053700)[0;0m ERROR 12-05 14:51:58 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4053700)[0;0m ERROR 12-05 14:51:58 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 14:52:19 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 14:52:20 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 14:52:20 [model.py:1745] Using max model len 131072
INFO 12-05 14:52:20 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4053776)[0;0m INFO 12-05 14:52:27 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4053776)[0;0m INFO 12-05 14:52:28 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:32873 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4053776)[0;0m INFO 12-05 14:52:28 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4053776)[0;0m INFO 12-05 14:52:28 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4053776)[0;0m INFO 12-05 14:52:29 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4053776)[0;0m INFO 12-05 14:52:29 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4053776)[0;0m INFO 12-05 14:52:30 [default_loader.py:314] Loading weights took 0.87 seconds
[1;36m(EngineCore_DP0 pid=4053776)[0;0m INFO 12-05 14:52:31 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.426999 seconds
[1;36m(EngineCore_DP0 pid=4053776)[0;0m INFO 12-05 14:52:34 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4053776)[0;0m INFO 12-05 14:52:34 [backends.py:647] Dynamo bytecode transform time: 3.66 s
[1;36m(EngineCore_DP0 pid=4053776)[0;0m ERROR 12-05 14:52:35 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4053776)[0;0m ERROR 12-05 14:52:35 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4053776)[0;0m ERROR 12-05 14:52:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4053776)[0;0m ERROR 12-05 14:52:35 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4053776)[0;0m ERROR 12-05 14:52:35 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053776)[0;0m ERROR 12-05 14:52:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4053776)[0;0m ERROR 12-05 14:52:35 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4053776)[0;0m ERROR 12-05 14:52:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4053776)[0;0m ERROR 12-05 14:52:35 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4053776)[0;0m ERROR 12-05 14:52:35 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053776)[0;0m ERROR 12-05 14:52:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4053776)[0;0m ERROR 12-05 14:52:35 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4053776)[0;0m ERROR 12-05 14:52:35 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053776)[0;0m ERROR 12-05 14:52:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4053776)[0;0m ERROR 12-05 14:52:35 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4053776)[0;0m ERROR 12-05 14:52:35 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053776)[0;0m ERROR 12-05 14:52:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4053776)[0;0m ERROR 12-05 14:52:35 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4053776)[0;0m ERROR 12-05 14:52:35 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053776)[0;0m ERROR 12-05 14:52:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4053776)[0;0m ERROR 12-05 14:52:35 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4053776)[0;0m ERROR 12-05 14:52:35 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053776)[0;0m ERROR 12-05 14:52:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4053776)[0;0m ERROR 12-05 14:52:35 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4053776)[0;0m ERROR 12-05 14:52:35 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053776)[0;0m ERROR 12-05 14:52:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4053776)[0;0m ERROR 12-05 14:52:35 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4053776)[0;0m ERROR 12-05 14:52:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4053776)[0;0m ERROR 12-05 14:52:35 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4053776)[0;0m ERROR 12-05 14:52:35 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053776)[0;0m ERROR 12-05 14:52:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4053776)[0;0m ERROR 12-05 14:52:35 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4053776)[0;0m ERROR 12-05 14:52:35 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053776)[0;0m ERROR 12-05 14:52:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4053776)[0;0m ERROR 12-05 14:52:35 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4053776)[0;0m ERROR 12-05 14:52:35 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053776)[0;0m ERROR 12-05 14:52:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4053776)[0;0m ERROR 12-05 14:52:35 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4053776)[0;0m ERROR 12-05 14:52:35 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053776)[0;0m ERROR 12-05 14:52:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4053776)[0;0m ERROR 12-05 14:52:35 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4053776)[0;0m ERROR 12-05 14:52:35 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053776)[0;0m ERROR 12-05 14:52:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4053776)[0;0m ERROR 12-05 14:52:35 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4053776)[0;0m ERROR 12-05 14:52:35 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053776)[0;0m ERROR 12-05 14:52:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4053776)[0;0m ERROR 12-05 14:52:35 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4053776)[0;0m ERROR 12-05 14:52:35 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053776)[0;0m ERROR 12-05 14:52:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4053776)[0;0m ERROR 12-05 14:52:35 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4053776)[0;0m ERROR 12-05 14:52:35 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053776)[0;0m ERROR 12-05 14:52:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4053776)[0;0m ERROR 12-05 14:52:35 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4053776)[0;0m ERROR 12-05 14:52:35 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053776)[0;0m ERROR 12-05 14:52:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4053776)[0;0m ERROR 12-05 14:52:35 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4053776)[0;0m ERROR 12-05 14:52:35 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053776)[0;0m ERROR 12-05 14:52:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4053776)[0;0m ERROR 12-05 14:52:35 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4053776)[0;0m ERROR 12-05 14:52:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4053776)[0;0m ERROR 12-05 14:52:35 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4053776)[0;0m ERROR 12-05 14:52:35 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053776)[0;0m ERROR 12-05 14:52:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4053776)[0;0m ERROR 12-05 14:52:35 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4053776)[0;0m ERROR 12-05 14:52:35 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053776)[0;0m ERROR 12-05 14:52:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4053776)[0;0m ERROR 12-05 14:52:35 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4053776)[0;0m ERROR 12-05 14:52:35 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053776)[0;0m ERROR 12-05 14:52:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4053776)[0;0m ERROR 12-05 14:52:35 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4053776)[0;0m ERROR 12-05 14:52:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4053776)[0;0m ERROR 12-05 14:52:35 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4053776)[0;0m ERROR 12-05 14:52:35 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053776)[0;0m ERROR 12-05 14:52:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4053776)[0;0m ERROR 12-05 14:52:35 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4053776)[0;0m ERROR 12-05 14:52:35 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053776)[0;0m ERROR 12-05 14:52:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4053776)[0;0m ERROR 12-05 14:52:35 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4053776)[0;0m ERROR 12-05 14:52:35 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053776)[0;0m ERROR 12-05 14:52:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4053776)[0;0m ERROR 12-05 14:52:35 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4053776)[0;0m ERROR 12-05 14:52:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4053776)[0;0m ERROR 12-05 14:52:35 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4053776)[0;0m ERROR 12-05 14:52:35 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053776)[0;0m ERROR 12-05 14:52:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4053776)[0;0m ERROR 12-05 14:52:35 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4053776)[0;0m ERROR 12-05 14:52:35 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053776)[0;0m ERROR 12-05 14:52:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4053776)[0;0m ERROR 12-05 14:52:35 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4053776)[0;0m ERROR 12-05 14:52:35 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053776)[0;0m ERROR 12-05 14:52:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4053776)[0;0m ERROR 12-05 14:52:35 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4053776)[0;0m ERROR 12-05 14:52:35 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4053776)[0;0m ERROR 12-05 14:52:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4053776)[0;0m ERROR 12-05 14:52:35 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4053776)[0;0m ERROR 12-05 14:52:35 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7ff5d00f7410>' raised:
[1;36m(EngineCore_DP0 pid=4053776)[0;0m ERROR 12-05 14:52:35 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4053776)[0;0m ERROR 12-05 14:52:35 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4053776)[0;0m ERROR 12-05 14:52:35 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4053776)[0;0m ERROR 12-05 14:52:35 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 14:52:56 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 14:52:56 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 14:52:56 [model.py:1745] Using max model len 131072
INFO 12-05 14:52:56 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4057283)[0;0m INFO 12-05 14:53:03 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4057283)[0;0m INFO 12-05 14:53:04 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:39965 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4057283)[0;0m INFO 12-05 14:53:04 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4057283)[0;0m INFO 12-05 14:53:04 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4057283)[0;0m INFO 12-05 14:53:05 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4057283)[0;0m INFO 12-05 14:53:05 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4057283)[0;0m INFO 12-05 14:53:06 [default_loader.py:314] Loading weights took 0.88 seconds
[1;36m(EngineCore_DP0 pid=4057283)[0;0m INFO 12-05 14:53:06 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.415552 seconds
[1;36m(EngineCore_DP0 pid=4057283)[0;0m INFO 12-05 14:53:10 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4057283)[0;0m INFO 12-05 14:53:10 [backends.py:647] Dynamo bytecode transform time: 3.62 s
[1;36m(EngineCore_DP0 pid=4057283)[0;0m ERROR 12-05 14:53:11 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4057283)[0;0m ERROR 12-05 14:53:11 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4057283)[0;0m ERROR 12-05 14:53:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4057283)[0;0m ERROR 12-05 14:53:11 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4057283)[0;0m ERROR 12-05 14:53:11 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057283)[0;0m ERROR 12-05 14:53:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4057283)[0;0m ERROR 12-05 14:53:11 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4057283)[0;0m ERROR 12-05 14:53:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4057283)[0;0m ERROR 12-05 14:53:11 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4057283)[0;0m ERROR 12-05 14:53:11 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057283)[0;0m ERROR 12-05 14:53:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4057283)[0;0m ERROR 12-05 14:53:11 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4057283)[0;0m ERROR 12-05 14:53:11 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057283)[0;0m ERROR 12-05 14:53:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4057283)[0;0m ERROR 12-05 14:53:11 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4057283)[0;0m ERROR 12-05 14:53:11 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057283)[0;0m ERROR 12-05 14:53:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4057283)[0;0m ERROR 12-05 14:53:11 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4057283)[0;0m ERROR 12-05 14:53:11 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057283)[0;0m ERROR 12-05 14:53:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4057283)[0;0m ERROR 12-05 14:53:11 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4057283)[0;0m ERROR 12-05 14:53:11 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057283)[0;0m ERROR 12-05 14:53:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4057283)[0;0m ERROR 12-05 14:53:11 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4057283)[0;0m ERROR 12-05 14:53:11 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057283)[0;0m ERROR 12-05 14:53:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4057283)[0;0m ERROR 12-05 14:53:11 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4057283)[0;0m ERROR 12-05 14:53:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4057283)[0;0m ERROR 12-05 14:53:11 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4057283)[0;0m ERROR 12-05 14:53:11 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057283)[0;0m ERROR 12-05 14:53:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4057283)[0;0m ERROR 12-05 14:53:11 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4057283)[0;0m ERROR 12-05 14:53:11 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057283)[0;0m ERROR 12-05 14:53:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4057283)[0;0m ERROR 12-05 14:53:11 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4057283)[0;0m ERROR 12-05 14:53:11 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057283)[0;0m ERROR 12-05 14:53:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4057283)[0;0m ERROR 12-05 14:53:11 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4057283)[0;0m ERROR 12-05 14:53:11 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057283)[0;0m ERROR 12-05 14:53:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4057283)[0;0m ERROR 12-05 14:53:11 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4057283)[0;0m ERROR 12-05 14:53:11 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057283)[0;0m ERROR 12-05 14:53:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4057283)[0;0m ERROR 12-05 14:53:11 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4057283)[0;0m ERROR 12-05 14:53:11 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057283)[0;0m ERROR 12-05 14:53:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4057283)[0;0m ERROR 12-05 14:53:11 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4057283)[0;0m ERROR 12-05 14:53:11 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057283)[0;0m ERROR 12-05 14:53:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4057283)[0;0m ERROR 12-05 14:53:11 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4057283)[0;0m ERROR 12-05 14:53:11 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057283)[0;0m ERROR 12-05 14:53:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4057283)[0;0m ERROR 12-05 14:53:11 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4057283)[0;0m ERROR 12-05 14:53:11 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057283)[0;0m ERROR 12-05 14:53:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4057283)[0;0m ERROR 12-05 14:53:11 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4057283)[0;0m ERROR 12-05 14:53:11 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057283)[0;0m ERROR 12-05 14:53:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4057283)[0;0m ERROR 12-05 14:53:11 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4057283)[0;0m ERROR 12-05 14:53:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4057283)[0;0m ERROR 12-05 14:53:11 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4057283)[0;0m ERROR 12-05 14:53:11 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057283)[0;0m ERROR 12-05 14:53:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4057283)[0;0m ERROR 12-05 14:53:11 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4057283)[0;0m ERROR 12-05 14:53:11 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057283)[0;0m ERROR 12-05 14:53:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4057283)[0;0m ERROR 12-05 14:53:11 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4057283)[0;0m ERROR 12-05 14:53:11 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057283)[0;0m ERROR 12-05 14:53:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4057283)[0;0m ERROR 12-05 14:53:11 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4057283)[0;0m ERROR 12-05 14:53:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4057283)[0;0m ERROR 12-05 14:53:11 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4057283)[0;0m ERROR 12-05 14:53:11 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057283)[0;0m ERROR 12-05 14:53:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4057283)[0;0m ERROR 12-05 14:53:11 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4057283)[0;0m ERROR 12-05 14:53:11 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057283)[0;0m ERROR 12-05 14:53:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4057283)[0;0m ERROR 12-05 14:53:11 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4057283)[0;0m ERROR 12-05 14:53:11 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057283)[0;0m ERROR 12-05 14:53:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4057283)[0;0m ERROR 12-05 14:53:11 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4057283)[0;0m ERROR 12-05 14:53:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4057283)[0;0m ERROR 12-05 14:53:11 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4057283)[0;0m ERROR 12-05 14:53:11 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057283)[0;0m ERROR 12-05 14:53:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4057283)[0;0m ERROR 12-05 14:53:11 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4057283)[0;0m ERROR 12-05 14:53:11 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057283)[0;0m ERROR 12-05 14:53:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4057283)[0;0m ERROR 12-05 14:53:11 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4057283)[0;0m ERROR 12-05 14:53:11 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057283)[0;0m ERROR 12-05 14:53:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4057283)[0;0m ERROR 12-05 14:53:11 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4057283)[0;0m ERROR 12-05 14:53:11 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057283)[0;0m ERROR 12-05 14:53:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4057283)[0;0m ERROR 12-05 14:53:11 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4057283)[0;0m ERROR 12-05 14:53:11 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7f0925502f60>' raised:
[1;36m(EngineCore_DP0 pid=4057283)[0;0m ERROR 12-05 14:53:11 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4057283)[0;0m ERROR 12-05 14:53:11 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4057283)[0;0m ERROR 12-05 14:53:11 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4057283)[0;0m ERROR 12-05 14:53:11 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 14:53:32 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 14:53:32 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 14:53:32 [model.py:1745] Using max model len 131072
INFO 12-05 14:53:32 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4057364)[0;0m INFO 12-05 14:53:39 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4057364)[0;0m INFO 12-05 14:53:40 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:39555 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4057364)[0;0m INFO 12-05 14:53:40 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4057364)[0;0m INFO 12-05 14:53:41 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4057364)[0;0m INFO 12-05 14:53:41 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4057364)[0;0m INFO 12-05 14:53:41 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4057364)[0;0m INFO 12-05 14:53:42 [default_loader.py:314] Loading weights took 0.87 seconds
[1;36m(EngineCore_DP0 pid=4057364)[0;0m INFO 12-05 14:53:43 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.364731 seconds
[1;36m(EngineCore_DP0 pid=4057364)[0;0m INFO 12-05 14:53:46 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4057364)[0;0m INFO 12-05 14:53:46 [backends.py:647] Dynamo bytecode transform time: 3.56 s
[1;36m(EngineCore_DP0 pid=4057364)[0;0m ERROR 12-05 14:53:47 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4057364)[0;0m ERROR 12-05 14:53:47 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4057364)[0;0m ERROR 12-05 14:53:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4057364)[0;0m ERROR 12-05 14:53:47 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4057364)[0;0m ERROR 12-05 14:53:47 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057364)[0;0m ERROR 12-05 14:53:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4057364)[0;0m ERROR 12-05 14:53:47 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4057364)[0;0m ERROR 12-05 14:53:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4057364)[0;0m ERROR 12-05 14:53:47 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4057364)[0;0m ERROR 12-05 14:53:47 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057364)[0;0m ERROR 12-05 14:53:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4057364)[0;0m ERROR 12-05 14:53:47 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4057364)[0;0m ERROR 12-05 14:53:47 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057364)[0;0m ERROR 12-05 14:53:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4057364)[0;0m ERROR 12-05 14:53:47 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4057364)[0;0m ERROR 12-05 14:53:47 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057364)[0;0m ERROR 12-05 14:53:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4057364)[0;0m ERROR 12-05 14:53:47 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4057364)[0;0m ERROR 12-05 14:53:47 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057364)[0;0m ERROR 12-05 14:53:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4057364)[0;0m ERROR 12-05 14:53:47 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4057364)[0;0m ERROR 12-05 14:53:47 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057364)[0;0m ERROR 12-05 14:53:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4057364)[0;0m ERROR 12-05 14:53:47 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4057364)[0;0m ERROR 12-05 14:53:47 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057364)[0;0m ERROR 12-05 14:53:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4057364)[0;0m ERROR 12-05 14:53:47 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4057364)[0;0m ERROR 12-05 14:53:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4057364)[0;0m ERROR 12-05 14:53:47 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4057364)[0;0m ERROR 12-05 14:53:47 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057364)[0;0m ERROR 12-05 14:53:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4057364)[0;0m ERROR 12-05 14:53:47 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4057364)[0;0m ERROR 12-05 14:53:47 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057364)[0;0m ERROR 12-05 14:53:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4057364)[0;0m ERROR 12-05 14:53:47 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4057364)[0;0m ERROR 12-05 14:53:47 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057364)[0;0m ERROR 12-05 14:53:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4057364)[0;0m ERROR 12-05 14:53:47 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4057364)[0;0m ERROR 12-05 14:53:47 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057364)[0;0m ERROR 12-05 14:53:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4057364)[0;0m ERROR 12-05 14:53:47 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4057364)[0;0m ERROR 12-05 14:53:47 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057364)[0;0m ERROR 12-05 14:53:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4057364)[0;0m ERROR 12-05 14:53:47 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4057364)[0;0m ERROR 12-05 14:53:47 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057364)[0;0m ERROR 12-05 14:53:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4057364)[0;0m ERROR 12-05 14:53:47 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4057364)[0;0m ERROR 12-05 14:53:47 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057364)[0;0m ERROR 12-05 14:53:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4057364)[0;0m ERROR 12-05 14:53:47 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4057364)[0;0m ERROR 12-05 14:53:47 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057364)[0;0m ERROR 12-05 14:53:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4057364)[0;0m ERROR 12-05 14:53:47 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4057364)[0;0m ERROR 12-05 14:53:47 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057364)[0;0m ERROR 12-05 14:53:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4057364)[0;0m ERROR 12-05 14:53:47 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4057364)[0;0m ERROR 12-05 14:53:47 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057364)[0;0m ERROR 12-05 14:53:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4057364)[0;0m ERROR 12-05 14:53:47 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4057364)[0;0m ERROR 12-05 14:53:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4057364)[0;0m ERROR 12-05 14:53:47 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4057364)[0;0m ERROR 12-05 14:53:47 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057364)[0;0m ERROR 12-05 14:53:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4057364)[0;0m ERROR 12-05 14:53:47 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4057364)[0;0m ERROR 12-05 14:53:47 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057364)[0;0m ERROR 12-05 14:53:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4057364)[0;0m ERROR 12-05 14:53:47 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4057364)[0;0m ERROR 12-05 14:53:47 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057364)[0;0m ERROR 12-05 14:53:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4057364)[0;0m ERROR 12-05 14:53:47 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4057364)[0;0m ERROR 12-05 14:53:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4057364)[0;0m ERROR 12-05 14:53:47 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4057364)[0;0m ERROR 12-05 14:53:47 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057364)[0;0m ERROR 12-05 14:53:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4057364)[0;0m ERROR 12-05 14:53:47 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4057364)[0;0m ERROR 12-05 14:53:47 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057364)[0;0m ERROR 12-05 14:53:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4057364)[0;0m ERROR 12-05 14:53:47 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4057364)[0;0m ERROR 12-05 14:53:47 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057364)[0;0m ERROR 12-05 14:53:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4057364)[0;0m ERROR 12-05 14:53:47 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4057364)[0;0m ERROR 12-05 14:53:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4057364)[0;0m ERROR 12-05 14:53:47 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4057364)[0;0m ERROR 12-05 14:53:47 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057364)[0;0m ERROR 12-05 14:53:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4057364)[0;0m ERROR 12-05 14:53:47 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4057364)[0;0m ERROR 12-05 14:53:47 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057364)[0;0m ERROR 12-05 14:53:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4057364)[0;0m ERROR 12-05 14:53:47 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4057364)[0;0m ERROR 12-05 14:53:47 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057364)[0;0m ERROR 12-05 14:53:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4057364)[0;0m ERROR 12-05 14:53:47 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4057364)[0;0m ERROR 12-05 14:53:47 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057364)[0;0m ERROR 12-05 14:53:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4057364)[0;0m ERROR 12-05 14:53:47 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4057364)[0;0m ERROR 12-05 14:53:47 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7fdebc0fc230>' raised:
[1;36m(EngineCore_DP0 pid=4057364)[0;0m ERROR 12-05 14:53:47 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4057364)[0;0m ERROR 12-05 14:53:47 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4057364)[0;0m ERROR 12-05 14:53:47 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4057364)[0;0m ERROR 12-05 14:53:47 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 14:54:08 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 14:54:08 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 14:54:08 [model.py:1745] Using max model len 131072
INFO 12-05 14:54:08 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4057449)[0;0m INFO 12-05 14:54:15 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4057449)[0;0m INFO 12-05 14:54:16 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:33583 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4057449)[0;0m INFO 12-05 14:54:16 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4057449)[0;0m INFO 12-05 14:54:16 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4057449)[0;0m INFO 12-05 14:54:17 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4057449)[0;0m INFO 12-05 14:54:17 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4057449)[0;0m INFO 12-05 14:54:18 [default_loader.py:314] Loading weights took 0.87 seconds
[1;36m(EngineCore_DP0 pid=4057449)[0;0m INFO 12-05 14:54:18 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.396173 seconds
[1;36m(EngineCore_DP0 pid=4057449)[0;0m INFO 12-05 14:54:22 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4057449)[0;0m INFO 12-05 14:54:22 [backends.py:647] Dynamo bytecode transform time: 3.66 s
[1;36m(EngineCore_DP0 pid=4057449)[0;0m ERROR 12-05 14:54:23 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4057449)[0;0m ERROR 12-05 14:54:23 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4057449)[0;0m ERROR 12-05 14:54:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4057449)[0;0m ERROR 12-05 14:54:23 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4057449)[0;0m ERROR 12-05 14:54:23 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057449)[0;0m ERROR 12-05 14:54:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4057449)[0;0m ERROR 12-05 14:54:23 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4057449)[0;0m ERROR 12-05 14:54:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4057449)[0;0m ERROR 12-05 14:54:23 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4057449)[0;0m ERROR 12-05 14:54:23 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057449)[0;0m ERROR 12-05 14:54:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4057449)[0;0m ERROR 12-05 14:54:23 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4057449)[0;0m ERROR 12-05 14:54:23 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057449)[0;0m ERROR 12-05 14:54:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4057449)[0;0m ERROR 12-05 14:54:23 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4057449)[0;0m ERROR 12-05 14:54:23 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057449)[0;0m ERROR 12-05 14:54:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4057449)[0;0m ERROR 12-05 14:54:23 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4057449)[0;0m ERROR 12-05 14:54:23 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057449)[0;0m ERROR 12-05 14:54:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4057449)[0;0m ERROR 12-05 14:54:23 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4057449)[0;0m ERROR 12-05 14:54:23 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057449)[0;0m ERROR 12-05 14:54:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4057449)[0;0m ERROR 12-05 14:54:23 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4057449)[0;0m ERROR 12-05 14:54:23 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057449)[0;0m ERROR 12-05 14:54:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4057449)[0;0m ERROR 12-05 14:54:23 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4057449)[0;0m ERROR 12-05 14:54:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4057449)[0;0m ERROR 12-05 14:54:23 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4057449)[0;0m ERROR 12-05 14:54:23 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057449)[0;0m ERROR 12-05 14:54:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4057449)[0;0m ERROR 12-05 14:54:23 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4057449)[0;0m ERROR 12-05 14:54:23 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057449)[0;0m ERROR 12-05 14:54:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4057449)[0;0m ERROR 12-05 14:54:23 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4057449)[0;0m ERROR 12-05 14:54:23 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057449)[0;0m ERROR 12-05 14:54:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4057449)[0;0m ERROR 12-05 14:54:23 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4057449)[0;0m ERROR 12-05 14:54:23 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057449)[0;0m ERROR 12-05 14:54:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4057449)[0;0m ERROR 12-05 14:54:23 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4057449)[0;0m ERROR 12-05 14:54:23 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057449)[0;0m ERROR 12-05 14:54:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4057449)[0;0m ERROR 12-05 14:54:23 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4057449)[0;0m ERROR 12-05 14:54:23 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057449)[0;0m ERROR 12-05 14:54:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4057449)[0;0m ERROR 12-05 14:54:23 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4057449)[0;0m ERROR 12-05 14:54:23 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057449)[0;0m ERROR 12-05 14:54:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4057449)[0;0m ERROR 12-05 14:54:23 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4057449)[0;0m ERROR 12-05 14:54:23 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057449)[0;0m ERROR 12-05 14:54:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4057449)[0;0m ERROR 12-05 14:54:23 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4057449)[0;0m ERROR 12-05 14:54:23 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057449)[0;0m ERROR 12-05 14:54:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4057449)[0;0m ERROR 12-05 14:54:23 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4057449)[0;0m ERROR 12-05 14:54:23 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057449)[0;0m ERROR 12-05 14:54:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4057449)[0;0m ERROR 12-05 14:54:23 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4057449)[0;0m ERROR 12-05 14:54:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4057449)[0;0m ERROR 12-05 14:54:23 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4057449)[0;0m ERROR 12-05 14:54:23 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057449)[0;0m ERROR 12-05 14:54:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4057449)[0;0m ERROR 12-05 14:54:23 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4057449)[0;0m ERROR 12-05 14:54:23 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057449)[0;0m ERROR 12-05 14:54:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4057449)[0;0m ERROR 12-05 14:54:23 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4057449)[0;0m ERROR 12-05 14:54:23 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057449)[0;0m ERROR 12-05 14:54:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4057449)[0;0m ERROR 12-05 14:54:23 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4057449)[0;0m ERROR 12-05 14:54:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4057449)[0;0m ERROR 12-05 14:54:23 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4057449)[0;0m ERROR 12-05 14:54:23 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057449)[0;0m ERROR 12-05 14:54:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4057449)[0;0m ERROR 12-05 14:54:23 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4057449)[0;0m ERROR 12-05 14:54:23 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057449)[0;0m ERROR 12-05 14:54:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4057449)[0;0m ERROR 12-05 14:54:23 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4057449)[0;0m ERROR 12-05 14:54:23 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057449)[0;0m ERROR 12-05 14:54:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4057449)[0;0m ERROR 12-05 14:54:23 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4057449)[0;0m ERROR 12-05 14:54:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4057449)[0;0m ERROR 12-05 14:54:23 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4057449)[0;0m ERROR 12-05 14:54:23 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057449)[0;0m ERROR 12-05 14:54:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4057449)[0;0m ERROR 12-05 14:54:23 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4057449)[0;0m ERROR 12-05 14:54:23 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057449)[0;0m ERROR 12-05 14:54:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4057449)[0;0m ERROR 12-05 14:54:23 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4057449)[0;0m ERROR 12-05 14:54:23 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057449)[0;0m ERROR 12-05 14:54:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4057449)[0;0m ERROR 12-05 14:54:23 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4057449)[0;0m ERROR 12-05 14:54:23 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057449)[0;0m ERROR 12-05 14:54:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4057449)[0;0m ERROR 12-05 14:54:23 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4057449)[0;0m ERROR 12-05 14:54:23 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7fd09c1f32f0>' raised:
[1;36m(EngineCore_DP0 pid=4057449)[0;0m ERROR 12-05 14:54:23 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4057449)[0;0m ERROR 12-05 14:54:23 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4057449)[0;0m ERROR 12-05 14:54:23 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4057449)[0;0m ERROR 12-05 14:54:23 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 14:54:44 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 14:54:44 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 14:54:44 [model.py:1745] Using max model len 131072
INFO 12-05 14:54:44 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4057538)[0;0m INFO 12-05 14:54:52 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4057538)[0;0m INFO 12-05 14:54:52 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:41481 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4057538)[0;0m INFO 12-05 14:54:52 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4057538)[0;0m INFO 12-05 14:54:53 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4057538)[0;0m INFO 12-05 14:54:53 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4057538)[0;0m INFO 12-05 14:54:53 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4057538)[0;0m INFO 12-05 14:54:54 [default_loader.py:314] Loading weights took 0.87 seconds
[1;36m(EngineCore_DP0 pid=4057538)[0;0m INFO 12-05 14:54:55 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.412308 seconds
[1;36m(EngineCore_DP0 pid=4057538)[0;0m INFO 12-05 14:54:59 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4057538)[0;0m INFO 12-05 14:54:59 [backends.py:647] Dynamo bytecode transform time: 3.63 s
[1;36m(EngineCore_DP0 pid=4057538)[0;0m ERROR 12-05 14:54:59 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4057538)[0;0m ERROR 12-05 14:54:59 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4057538)[0;0m ERROR 12-05 14:54:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4057538)[0;0m ERROR 12-05 14:54:59 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4057538)[0;0m ERROR 12-05 14:54:59 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057538)[0;0m ERROR 12-05 14:54:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4057538)[0;0m ERROR 12-05 14:54:59 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4057538)[0;0m ERROR 12-05 14:54:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4057538)[0;0m ERROR 12-05 14:54:59 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4057538)[0;0m ERROR 12-05 14:54:59 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057538)[0;0m ERROR 12-05 14:54:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4057538)[0;0m ERROR 12-05 14:54:59 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4057538)[0;0m ERROR 12-05 14:54:59 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057538)[0;0m ERROR 12-05 14:54:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4057538)[0;0m ERROR 12-05 14:54:59 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4057538)[0;0m ERROR 12-05 14:54:59 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057538)[0;0m ERROR 12-05 14:54:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4057538)[0;0m ERROR 12-05 14:54:59 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4057538)[0;0m ERROR 12-05 14:54:59 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057538)[0;0m ERROR 12-05 14:54:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4057538)[0;0m ERROR 12-05 14:54:59 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4057538)[0;0m ERROR 12-05 14:54:59 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057538)[0;0m ERROR 12-05 14:54:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4057538)[0;0m ERROR 12-05 14:54:59 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4057538)[0;0m ERROR 12-05 14:54:59 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057538)[0;0m ERROR 12-05 14:54:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4057538)[0;0m ERROR 12-05 14:54:59 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4057538)[0;0m ERROR 12-05 14:54:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4057538)[0;0m ERROR 12-05 14:54:59 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4057538)[0;0m ERROR 12-05 14:54:59 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057538)[0;0m ERROR 12-05 14:54:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4057538)[0;0m ERROR 12-05 14:54:59 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4057538)[0;0m ERROR 12-05 14:54:59 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057538)[0;0m ERROR 12-05 14:54:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4057538)[0;0m ERROR 12-05 14:54:59 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4057538)[0;0m ERROR 12-05 14:54:59 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057538)[0;0m ERROR 12-05 14:54:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4057538)[0;0m ERROR 12-05 14:54:59 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4057538)[0;0m ERROR 12-05 14:54:59 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057538)[0;0m ERROR 12-05 14:54:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4057538)[0;0m ERROR 12-05 14:54:59 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4057538)[0;0m ERROR 12-05 14:54:59 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057538)[0;0m ERROR 12-05 14:54:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4057538)[0;0m ERROR 12-05 14:54:59 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4057538)[0;0m ERROR 12-05 14:54:59 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057538)[0;0m ERROR 12-05 14:54:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4057538)[0;0m ERROR 12-05 14:54:59 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4057538)[0;0m ERROR 12-05 14:54:59 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057538)[0;0m ERROR 12-05 14:54:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4057538)[0;0m ERROR 12-05 14:54:59 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4057538)[0;0m ERROR 12-05 14:54:59 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057538)[0;0m ERROR 12-05 14:54:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4057538)[0;0m ERROR 12-05 14:54:59 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4057538)[0;0m ERROR 12-05 14:54:59 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057538)[0;0m ERROR 12-05 14:54:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4057538)[0;0m ERROR 12-05 14:54:59 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4057538)[0;0m ERROR 12-05 14:54:59 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057538)[0;0m ERROR 12-05 14:54:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4057538)[0;0m ERROR 12-05 14:54:59 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4057538)[0;0m ERROR 12-05 14:54:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4057538)[0;0m ERROR 12-05 14:54:59 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4057538)[0;0m ERROR 12-05 14:54:59 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057538)[0;0m ERROR 12-05 14:54:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4057538)[0;0m ERROR 12-05 14:54:59 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4057538)[0;0m ERROR 12-05 14:54:59 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057538)[0;0m ERROR 12-05 14:54:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4057538)[0;0m ERROR 12-05 14:54:59 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4057538)[0;0m ERROR 12-05 14:54:59 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057538)[0;0m ERROR 12-05 14:54:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4057538)[0;0m ERROR 12-05 14:54:59 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4057538)[0;0m ERROR 12-05 14:54:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4057538)[0;0m ERROR 12-05 14:54:59 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4057538)[0;0m ERROR 12-05 14:54:59 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057538)[0;0m ERROR 12-05 14:54:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4057538)[0;0m ERROR 12-05 14:54:59 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4057538)[0;0m ERROR 12-05 14:54:59 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057538)[0;0m ERROR 12-05 14:54:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4057538)[0;0m ERROR 12-05 14:54:59 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4057538)[0;0m ERROR 12-05 14:54:59 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057538)[0;0m ERROR 12-05 14:54:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4057538)[0;0m ERROR 12-05 14:54:59 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4057538)[0;0m ERROR 12-05 14:54:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4057538)[0;0m ERROR 12-05 14:54:59 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4057538)[0;0m ERROR 12-05 14:54:59 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057538)[0;0m ERROR 12-05 14:54:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4057538)[0;0m ERROR 12-05 14:54:59 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4057538)[0;0m ERROR 12-05 14:54:59 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057538)[0;0m ERROR 12-05 14:54:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4057538)[0;0m ERROR 12-05 14:54:59 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4057538)[0;0m ERROR 12-05 14:54:59 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057538)[0;0m ERROR 12-05 14:54:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4057538)[0;0m ERROR 12-05 14:54:59 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4057538)[0;0m ERROR 12-05 14:54:59 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057538)[0;0m ERROR 12-05 14:54:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4057538)[0;0m ERROR 12-05 14:54:59 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4057538)[0;0m ERROR 12-05 14:54:59 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7fb228f02b10>' raised:
[1;36m(EngineCore_DP0 pid=4057538)[0;0m ERROR 12-05 14:54:59 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4057538)[0;0m ERROR 12-05 14:54:59 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4057538)[0;0m ERROR 12-05 14:54:59 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4057538)[0;0m ERROR 12-05 14:54:59 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 14:55:20 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 14:55:20 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 14:55:20 [model.py:1745] Using max model len 131072
INFO 12-05 14:55:20 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4057615)[0;0m INFO 12-05 14:55:28 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4057615)[0;0m INFO 12-05 14:55:28 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:57325 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4057615)[0;0m INFO 12-05 14:55:28 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4057615)[0;0m INFO 12-05 14:55:29 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4057615)[0;0m INFO 12-05 14:55:29 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4057615)[0;0m INFO 12-05 14:55:29 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4057615)[0;0m INFO 12-05 14:55:30 [default_loader.py:314] Loading weights took 0.88 seconds
[1;36m(EngineCore_DP0 pid=4057615)[0;0m INFO 12-05 14:55:31 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.529537 seconds
[1;36m(EngineCore_DP0 pid=4057615)[0;0m INFO 12-05 14:55:35 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4057615)[0;0m INFO 12-05 14:55:35 [backends.py:647] Dynamo bytecode transform time: 3.63 s
[1;36m(EngineCore_DP0 pid=4057615)[0;0m ERROR 12-05 14:55:35 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4057615)[0;0m ERROR 12-05 14:55:35 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4057615)[0;0m ERROR 12-05 14:55:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4057615)[0;0m ERROR 12-05 14:55:35 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4057615)[0;0m ERROR 12-05 14:55:35 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057615)[0;0m ERROR 12-05 14:55:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4057615)[0;0m ERROR 12-05 14:55:35 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4057615)[0;0m ERROR 12-05 14:55:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4057615)[0;0m ERROR 12-05 14:55:35 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4057615)[0;0m ERROR 12-05 14:55:35 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057615)[0;0m ERROR 12-05 14:55:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4057615)[0;0m ERROR 12-05 14:55:35 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4057615)[0;0m ERROR 12-05 14:55:35 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057615)[0;0m ERROR 12-05 14:55:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4057615)[0;0m ERROR 12-05 14:55:35 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4057615)[0;0m ERROR 12-05 14:55:35 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057615)[0;0m ERROR 12-05 14:55:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4057615)[0;0m ERROR 12-05 14:55:35 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4057615)[0;0m ERROR 12-05 14:55:35 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057615)[0;0m ERROR 12-05 14:55:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4057615)[0;0m ERROR 12-05 14:55:35 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4057615)[0;0m ERROR 12-05 14:55:35 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057615)[0;0m ERROR 12-05 14:55:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4057615)[0;0m ERROR 12-05 14:55:35 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4057615)[0;0m ERROR 12-05 14:55:35 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057615)[0;0m ERROR 12-05 14:55:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4057615)[0;0m ERROR 12-05 14:55:35 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4057615)[0;0m ERROR 12-05 14:55:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4057615)[0;0m ERROR 12-05 14:55:35 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4057615)[0;0m ERROR 12-05 14:55:35 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057615)[0;0m ERROR 12-05 14:55:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4057615)[0;0m ERROR 12-05 14:55:35 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4057615)[0;0m ERROR 12-05 14:55:35 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057615)[0;0m ERROR 12-05 14:55:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4057615)[0;0m ERROR 12-05 14:55:35 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4057615)[0;0m ERROR 12-05 14:55:35 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057615)[0;0m ERROR 12-05 14:55:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4057615)[0;0m ERROR 12-05 14:55:35 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4057615)[0;0m ERROR 12-05 14:55:35 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057615)[0;0m ERROR 12-05 14:55:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4057615)[0;0m ERROR 12-05 14:55:35 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4057615)[0;0m ERROR 12-05 14:55:35 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057615)[0;0m ERROR 12-05 14:55:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4057615)[0;0m ERROR 12-05 14:55:35 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4057615)[0;0m ERROR 12-05 14:55:35 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057615)[0;0m ERROR 12-05 14:55:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4057615)[0;0m ERROR 12-05 14:55:35 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4057615)[0;0m ERROR 12-05 14:55:35 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057615)[0;0m ERROR 12-05 14:55:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4057615)[0;0m ERROR 12-05 14:55:35 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4057615)[0;0m ERROR 12-05 14:55:35 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057615)[0;0m ERROR 12-05 14:55:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4057615)[0;0m ERROR 12-05 14:55:35 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4057615)[0;0m ERROR 12-05 14:55:35 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057615)[0;0m ERROR 12-05 14:55:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4057615)[0;0m ERROR 12-05 14:55:35 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4057615)[0;0m ERROR 12-05 14:55:35 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057615)[0;0m ERROR 12-05 14:55:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4057615)[0;0m ERROR 12-05 14:55:35 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4057615)[0;0m ERROR 12-05 14:55:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4057615)[0;0m ERROR 12-05 14:55:35 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4057615)[0;0m ERROR 12-05 14:55:35 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057615)[0;0m ERROR 12-05 14:55:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4057615)[0;0m ERROR 12-05 14:55:35 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4057615)[0;0m ERROR 12-05 14:55:35 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057615)[0;0m ERROR 12-05 14:55:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4057615)[0;0m ERROR 12-05 14:55:35 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4057615)[0;0m ERROR 12-05 14:55:35 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057615)[0;0m ERROR 12-05 14:55:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4057615)[0;0m ERROR 12-05 14:55:35 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4057615)[0;0m ERROR 12-05 14:55:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4057615)[0;0m ERROR 12-05 14:55:35 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4057615)[0;0m ERROR 12-05 14:55:35 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057615)[0;0m ERROR 12-05 14:55:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4057615)[0;0m ERROR 12-05 14:55:35 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4057615)[0;0m ERROR 12-05 14:55:35 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057615)[0;0m ERROR 12-05 14:55:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4057615)[0;0m ERROR 12-05 14:55:35 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4057615)[0;0m ERROR 12-05 14:55:35 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057615)[0;0m ERROR 12-05 14:55:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4057615)[0;0m ERROR 12-05 14:55:35 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4057615)[0;0m ERROR 12-05 14:55:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4057615)[0;0m ERROR 12-05 14:55:35 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4057615)[0;0m ERROR 12-05 14:55:35 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057615)[0;0m ERROR 12-05 14:55:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4057615)[0;0m ERROR 12-05 14:55:35 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4057615)[0;0m ERROR 12-05 14:55:35 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057615)[0;0m ERROR 12-05 14:55:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4057615)[0;0m ERROR 12-05 14:55:35 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4057615)[0;0m ERROR 12-05 14:55:35 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057615)[0;0m ERROR 12-05 14:55:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4057615)[0;0m ERROR 12-05 14:55:35 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4057615)[0;0m ERROR 12-05 14:55:35 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057615)[0;0m ERROR 12-05 14:55:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4057615)[0;0m ERROR 12-05 14:55:35 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4057615)[0;0m ERROR 12-05 14:55:35 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7ff42422d880>' raised:
[1;36m(EngineCore_DP0 pid=4057615)[0;0m ERROR 12-05 14:55:35 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4057615)[0;0m ERROR 12-05 14:55:35 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4057615)[0;0m ERROR 12-05 14:55:35 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4057615)[0;0m ERROR 12-05 14:55:35 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 14:55:56 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 14:55:57 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 14:55:57 [model.py:1745] Using max model len 131072
INFO 12-05 14:55:57 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4057704)[0;0m INFO 12-05 14:56:04 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4057704)[0;0m INFO 12-05 14:56:05 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:56493 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4057704)[0;0m INFO 12-05 14:56:05 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4057704)[0;0m INFO 12-05 14:56:05 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4057704)[0;0m INFO 12-05 14:56:05 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4057704)[0;0m INFO 12-05 14:56:05 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4057704)[0;0m INFO 12-05 14:56:07 [default_loader.py:314] Loading weights took 0.87 seconds
[1;36m(EngineCore_DP0 pid=4057704)[0;0m INFO 12-05 14:56:07 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.412317 seconds
[1;36m(EngineCore_DP0 pid=4057704)[0;0m INFO 12-05 14:56:11 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4057704)[0;0m INFO 12-05 14:56:11 [backends.py:647] Dynamo bytecode transform time: 4.24 s
[1;36m(EngineCore_DP0 pid=4057704)[0;0m ERROR 12-05 14:56:12 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4057704)[0;0m ERROR 12-05 14:56:12 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4057704)[0;0m ERROR 12-05 14:56:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4057704)[0;0m ERROR 12-05 14:56:12 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4057704)[0;0m ERROR 12-05 14:56:12 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057704)[0;0m ERROR 12-05 14:56:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4057704)[0;0m ERROR 12-05 14:56:12 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4057704)[0;0m ERROR 12-05 14:56:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4057704)[0;0m ERROR 12-05 14:56:12 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4057704)[0;0m ERROR 12-05 14:56:12 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057704)[0;0m ERROR 12-05 14:56:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4057704)[0;0m ERROR 12-05 14:56:12 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4057704)[0;0m ERROR 12-05 14:56:12 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057704)[0;0m ERROR 12-05 14:56:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4057704)[0;0m ERROR 12-05 14:56:12 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4057704)[0;0m ERROR 12-05 14:56:12 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057704)[0;0m ERROR 12-05 14:56:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4057704)[0;0m ERROR 12-05 14:56:12 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4057704)[0;0m ERROR 12-05 14:56:12 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057704)[0;0m ERROR 12-05 14:56:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4057704)[0;0m ERROR 12-05 14:56:12 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4057704)[0;0m ERROR 12-05 14:56:12 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057704)[0;0m ERROR 12-05 14:56:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4057704)[0;0m ERROR 12-05 14:56:12 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4057704)[0;0m ERROR 12-05 14:56:12 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057704)[0;0m ERROR 12-05 14:56:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4057704)[0;0m ERROR 12-05 14:56:12 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4057704)[0;0m ERROR 12-05 14:56:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4057704)[0;0m ERROR 12-05 14:56:12 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4057704)[0;0m ERROR 12-05 14:56:12 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057704)[0;0m ERROR 12-05 14:56:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4057704)[0;0m ERROR 12-05 14:56:12 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4057704)[0;0m ERROR 12-05 14:56:12 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057704)[0;0m ERROR 12-05 14:56:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4057704)[0;0m ERROR 12-05 14:56:12 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4057704)[0;0m ERROR 12-05 14:56:12 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057704)[0;0m ERROR 12-05 14:56:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4057704)[0;0m ERROR 12-05 14:56:12 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4057704)[0;0m ERROR 12-05 14:56:12 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057704)[0;0m ERROR 12-05 14:56:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4057704)[0;0m ERROR 12-05 14:56:12 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4057704)[0;0m ERROR 12-05 14:56:12 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057704)[0;0m ERROR 12-05 14:56:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4057704)[0;0m ERROR 12-05 14:56:12 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4057704)[0;0m ERROR 12-05 14:56:12 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057704)[0;0m ERROR 12-05 14:56:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4057704)[0;0m ERROR 12-05 14:56:12 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4057704)[0;0m ERROR 12-05 14:56:12 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057704)[0;0m ERROR 12-05 14:56:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4057704)[0;0m ERROR 12-05 14:56:12 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4057704)[0;0m ERROR 12-05 14:56:12 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057704)[0;0m ERROR 12-05 14:56:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4057704)[0;0m ERROR 12-05 14:56:12 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4057704)[0;0m ERROR 12-05 14:56:12 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057704)[0;0m ERROR 12-05 14:56:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4057704)[0;0m ERROR 12-05 14:56:12 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4057704)[0;0m ERROR 12-05 14:56:12 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057704)[0;0m ERROR 12-05 14:56:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4057704)[0;0m ERROR 12-05 14:56:12 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4057704)[0;0m ERROR 12-05 14:56:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4057704)[0;0m ERROR 12-05 14:56:12 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4057704)[0;0m ERROR 12-05 14:56:12 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057704)[0;0m ERROR 12-05 14:56:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4057704)[0;0m ERROR 12-05 14:56:12 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4057704)[0;0m ERROR 12-05 14:56:12 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057704)[0;0m ERROR 12-05 14:56:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4057704)[0;0m ERROR 12-05 14:56:12 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4057704)[0;0m ERROR 12-05 14:56:12 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057704)[0;0m ERROR 12-05 14:56:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4057704)[0;0m ERROR 12-05 14:56:12 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4057704)[0;0m ERROR 12-05 14:56:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4057704)[0;0m ERROR 12-05 14:56:12 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4057704)[0;0m ERROR 12-05 14:56:12 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057704)[0;0m ERROR 12-05 14:56:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4057704)[0;0m ERROR 12-05 14:56:12 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4057704)[0;0m ERROR 12-05 14:56:12 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057704)[0;0m ERROR 12-05 14:56:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4057704)[0;0m ERROR 12-05 14:56:12 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4057704)[0;0m ERROR 12-05 14:56:12 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057704)[0;0m ERROR 12-05 14:56:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4057704)[0;0m ERROR 12-05 14:56:12 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4057704)[0;0m ERROR 12-05 14:56:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4057704)[0;0m ERROR 12-05 14:56:12 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4057704)[0;0m ERROR 12-05 14:56:12 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057704)[0;0m ERROR 12-05 14:56:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4057704)[0;0m ERROR 12-05 14:56:12 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4057704)[0;0m ERROR 12-05 14:56:12 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057704)[0;0m ERROR 12-05 14:56:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4057704)[0;0m ERROR 12-05 14:56:12 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4057704)[0;0m ERROR 12-05 14:56:12 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057704)[0;0m ERROR 12-05 14:56:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4057704)[0;0m ERROR 12-05 14:56:12 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4057704)[0;0m ERROR 12-05 14:56:12 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057704)[0;0m ERROR 12-05 14:56:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4057704)[0;0m ERROR 12-05 14:56:12 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4057704)[0;0m ERROR 12-05 14:56:12 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7f3e7d1067e0>' raised:
[1;36m(EngineCore_DP0 pid=4057704)[0;0m ERROR 12-05 14:56:12 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4057704)[0;0m ERROR 12-05 14:56:12 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4057704)[0;0m ERROR 12-05 14:56:12 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4057704)[0;0m ERROR 12-05 14:56:12 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 14:56:33 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 14:56:33 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 14:56:33 [model.py:1745] Using max model len 131072
INFO 12-05 14:56:33 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4057881)[0;0m INFO 12-05 14:56:40 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4057881)[0;0m INFO 12-05 14:56:41 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:58989 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4057881)[0;0m INFO 12-05 14:56:41 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4057881)[0;0m INFO 12-05 14:56:41 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4057881)[0;0m INFO 12-05 14:56:42 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4057881)[0;0m INFO 12-05 14:56:42 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4057881)[0;0m INFO 12-05 14:56:43 [default_loader.py:314] Loading weights took 0.87 seconds
[1;36m(EngineCore_DP0 pid=4057881)[0;0m INFO 12-05 14:56:44 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.393216 seconds
[1;36m(EngineCore_DP0 pid=4057881)[0;0m INFO 12-05 14:56:47 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4057881)[0;0m INFO 12-05 14:56:47 [backends.py:647] Dynamo bytecode transform time: 3.62 s
[1;36m(EngineCore_DP0 pid=4057881)[0;0m ERROR 12-05 14:56:48 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4057881)[0;0m ERROR 12-05 14:56:48 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4057881)[0;0m ERROR 12-05 14:56:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4057881)[0;0m ERROR 12-05 14:56:48 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4057881)[0;0m ERROR 12-05 14:56:48 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057881)[0;0m ERROR 12-05 14:56:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4057881)[0;0m ERROR 12-05 14:56:48 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4057881)[0;0m ERROR 12-05 14:56:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4057881)[0;0m ERROR 12-05 14:56:48 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4057881)[0;0m ERROR 12-05 14:56:48 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057881)[0;0m ERROR 12-05 14:56:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4057881)[0;0m ERROR 12-05 14:56:48 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4057881)[0;0m ERROR 12-05 14:56:48 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057881)[0;0m ERROR 12-05 14:56:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4057881)[0;0m ERROR 12-05 14:56:48 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4057881)[0;0m ERROR 12-05 14:56:48 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057881)[0;0m ERROR 12-05 14:56:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4057881)[0;0m ERROR 12-05 14:56:48 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4057881)[0;0m ERROR 12-05 14:56:48 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057881)[0;0m ERROR 12-05 14:56:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4057881)[0;0m ERROR 12-05 14:56:48 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4057881)[0;0m ERROR 12-05 14:56:48 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057881)[0;0m ERROR 12-05 14:56:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4057881)[0;0m ERROR 12-05 14:56:48 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4057881)[0;0m ERROR 12-05 14:56:48 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057881)[0;0m ERROR 12-05 14:56:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4057881)[0;0m ERROR 12-05 14:56:48 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4057881)[0;0m ERROR 12-05 14:56:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4057881)[0;0m ERROR 12-05 14:56:48 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4057881)[0;0m ERROR 12-05 14:56:48 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057881)[0;0m ERROR 12-05 14:56:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4057881)[0;0m ERROR 12-05 14:56:48 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4057881)[0;0m ERROR 12-05 14:56:48 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057881)[0;0m ERROR 12-05 14:56:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4057881)[0;0m ERROR 12-05 14:56:48 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4057881)[0;0m ERROR 12-05 14:56:48 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057881)[0;0m ERROR 12-05 14:56:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4057881)[0;0m ERROR 12-05 14:56:48 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4057881)[0;0m ERROR 12-05 14:56:48 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057881)[0;0m ERROR 12-05 14:56:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4057881)[0;0m ERROR 12-05 14:56:48 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4057881)[0;0m ERROR 12-05 14:56:48 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057881)[0;0m ERROR 12-05 14:56:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4057881)[0;0m ERROR 12-05 14:56:48 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4057881)[0;0m ERROR 12-05 14:56:48 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057881)[0;0m ERROR 12-05 14:56:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4057881)[0;0m ERROR 12-05 14:56:48 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4057881)[0;0m ERROR 12-05 14:56:48 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057881)[0;0m ERROR 12-05 14:56:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4057881)[0;0m ERROR 12-05 14:56:48 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4057881)[0;0m ERROR 12-05 14:56:48 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057881)[0;0m ERROR 12-05 14:56:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4057881)[0;0m ERROR 12-05 14:56:48 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4057881)[0;0m ERROR 12-05 14:56:48 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057881)[0;0m ERROR 12-05 14:56:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4057881)[0;0m ERROR 12-05 14:56:48 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4057881)[0;0m ERROR 12-05 14:56:48 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057881)[0;0m ERROR 12-05 14:56:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4057881)[0;0m ERROR 12-05 14:56:48 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4057881)[0;0m ERROR 12-05 14:56:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4057881)[0;0m ERROR 12-05 14:56:48 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4057881)[0;0m ERROR 12-05 14:56:48 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057881)[0;0m ERROR 12-05 14:56:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4057881)[0;0m ERROR 12-05 14:56:48 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4057881)[0;0m ERROR 12-05 14:56:48 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057881)[0;0m ERROR 12-05 14:56:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4057881)[0;0m ERROR 12-05 14:56:48 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4057881)[0;0m ERROR 12-05 14:56:48 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057881)[0;0m ERROR 12-05 14:56:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4057881)[0;0m ERROR 12-05 14:56:48 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4057881)[0;0m ERROR 12-05 14:56:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4057881)[0;0m ERROR 12-05 14:56:48 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4057881)[0;0m ERROR 12-05 14:56:48 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057881)[0;0m ERROR 12-05 14:56:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4057881)[0;0m ERROR 12-05 14:56:48 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4057881)[0;0m ERROR 12-05 14:56:48 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057881)[0;0m ERROR 12-05 14:56:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4057881)[0;0m ERROR 12-05 14:56:48 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4057881)[0;0m ERROR 12-05 14:56:48 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057881)[0;0m ERROR 12-05 14:56:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4057881)[0;0m ERROR 12-05 14:56:48 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4057881)[0;0m ERROR 12-05 14:56:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4057881)[0;0m ERROR 12-05 14:56:48 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4057881)[0;0m ERROR 12-05 14:56:48 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057881)[0;0m ERROR 12-05 14:56:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4057881)[0;0m ERROR 12-05 14:56:48 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4057881)[0;0m ERROR 12-05 14:56:48 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057881)[0;0m ERROR 12-05 14:56:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4057881)[0;0m ERROR 12-05 14:56:48 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4057881)[0;0m ERROR 12-05 14:56:48 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057881)[0;0m ERROR 12-05 14:56:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4057881)[0;0m ERROR 12-05 14:56:48 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4057881)[0;0m ERROR 12-05 14:56:48 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057881)[0;0m ERROR 12-05 14:56:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4057881)[0;0m ERROR 12-05 14:56:48 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4057881)[0;0m ERROR 12-05 14:56:48 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7f42cc2656d0>' raised:
[1;36m(EngineCore_DP0 pid=4057881)[0;0m ERROR 12-05 14:56:48 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4057881)[0;0m ERROR 12-05 14:56:48 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4057881)[0;0m ERROR 12-05 14:56:48 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4057881)[0;0m ERROR 12-05 14:56:48 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 14:57:09 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 14:57:09 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 14:57:09 [model.py:1745] Using max model len 131072
INFO 12-05 14:57:09 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4057965)[0;0m INFO 12-05 14:57:17 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4057965)[0;0m INFO 12-05 14:57:17 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:45915 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4057965)[0;0m INFO 12-05 14:57:17 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4057965)[0;0m INFO 12-05 14:57:18 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4057965)[0;0m INFO 12-05 14:57:18 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4057965)[0;0m INFO 12-05 14:57:18 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4057965)[0;0m INFO 12-05 14:57:19 [default_loader.py:314] Loading weights took 0.87 seconds
[1;36m(EngineCore_DP0 pid=4057965)[0;0m INFO 12-05 14:57:20 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.406360 seconds
[1;36m(EngineCore_DP0 pid=4057965)[0;0m INFO 12-05 14:57:24 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4057965)[0;0m INFO 12-05 14:57:24 [backends.py:647] Dynamo bytecode transform time: 3.63 s
[1;36m(EngineCore_DP0 pid=4057965)[0;0m ERROR 12-05 14:57:24 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4057965)[0;0m ERROR 12-05 14:57:24 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4057965)[0;0m ERROR 12-05 14:57:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4057965)[0;0m ERROR 12-05 14:57:24 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4057965)[0;0m ERROR 12-05 14:57:24 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057965)[0;0m ERROR 12-05 14:57:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4057965)[0;0m ERROR 12-05 14:57:24 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4057965)[0;0m ERROR 12-05 14:57:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4057965)[0;0m ERROR 12-05 14:57:24 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4057965)[0;0m ERROR 12-05 14:57:24 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057965)[0;0m ERROR 12-05 14:57:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4057965)[0;0m ERROR 12-05 14:57:24 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4057965)[0;0m ERROR 12-05 14:57:24 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057965)[0;0m ERROR 12-05 14:57:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4057965)[0;0m ERROR 12-05 14:57:24 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4057965)[0;0m ERROR 12-05 14:57:24 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057965)[0;0m ERROR 12-05 14:57:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4057965)[0;0m ERROR 12-05 14:57:24 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4057965)[0;0m ERROR 12-05 14:57:24 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057965)[0;0m ERROR 12-05 14:57:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4057965)[0;0m ERROR 12-05 14:57:24 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4057965)[0;0m ERROR 12-05 14:57:24 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057965)[0;0m ERROR 12-05 14:57:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4057965)[0;0m ERROR 12-05 14:57:24 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4057965)[0;0m ERROR 12-05 14:57:24 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057965)[0;0m ERROR 12-05 14:57:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4057965)[0;0m ERROR 12-05 14:57:24 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4057965)[0;0m ERROR 12-05 14:57:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4057965)[0;0m ERROR 12-05 14:57:24 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4057965)[0;0m ERROR 12-05 14:57:24 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057965)[0;0m ERROR 12-05 14:57:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4057965)[0;0m ERROR 12-05 14:57:24 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4057965)[0;0m ERROR 12-05 14:57:24 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057965)[0;0m ERROR 12-05 14:57:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4057965)[0;0m ERROR 12-05 14:57:24 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4057965)[0;0m ERROR 12-05 14:57:24 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057965)[0;0m ERROR 12-05 14:57:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4057965)[0;0m ERROR 12-05 14:57:24 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4057965)[0;0m ERROR 12-05 14:57:24 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057965)[0;0m ERROR 12-05 14:57:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4057965)[0;0m ERROR 12-05 14:57:24 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4057965)[0;0m ERROR 12-05 14:57:24 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057965)[0;0m ERROR 12-05 14:57:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4057965)[0;0m ERROR 12-05 14:57:24 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4057965)[0;0m ERROR 12-05 14:57:24 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057965)[0;0m ERROR 12-05 14:57:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4057965)[0;0m ERROR 12-05 14:57:24 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4057965)[0;0m ERROR 12-05 14:57:24 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057965)[0;0m ERROR 12-05 14:57:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4057965)[0;0m ERROR 12-05 14:57:24 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4057965)[0;0m ERROR 12-05 14:57:24 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057965)[0;0m ERROR 12-05 14:57:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4057965)[0;0m ERROR 12-05 14:57:24 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4057965)[0;0m ERROR 12-05 14:57:24 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057965)[0;0m ERROR 12-05 14:57:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4057965)[0;0m ERROR 12-05 14:57:24 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4057965)[0;0m ERROR 12-05 14:57:24 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057965)[0;0m ERROR 12-05 14:57:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4057965)[0;0m ERROR 12-05 14:57:24 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4057965)[0;0m ERROR 12-05 14:57:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4057965)[0;0m ERROR 12-05 14:57:24 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4057965)[0;0m ERROR 12-05 14:57:24 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057965)[0;0m ERROR 12-05 14:57:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4057965)[0;0m ERROR 12-05 14:57:24 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4057965)[0;0m ERROR 12-05 14:57:24 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057965)[0;0m ERROR 12-05 14:57:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4057965)[0;0m ERROR 12-05 14:57:24 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4057965)[0;0m ERROR 12-05 14:57:24 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057965)[0;0m ERROR 12-05 14:57:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4057965)[0;0m ERROR 12-05 14:57:24 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4057965)[0;0m ERROR 12-05 14:57:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4057965)[0;0m ERROR 12-05 14:57:24 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4057965)[0;0m ERROR 12-05 14:57:24 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057965)[0;0m ERROR 12-05 14:57:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4057965)[0;0m ERROR 12-05 14:57:24 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4057965)[0;0m ERROR 12-05 14:57:24 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057965)[0;0m ERROR 12-05 14:57:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4057965)[0;0m ERROR 12-05 14:57:24 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4057965)[0;0m ERROR 12-05 14:57:24 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057965)[0;0m ERROR 12-05 14:57:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4057965)[0;0m ERROR 12-05 14:57:24 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4057965)[0;0m ERROR 12-05 14:57:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4057965)[0;0m ERROR 12-05 14:57:24 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4057965)[0;0m ERROR 12-05 14:57:24 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057965)[0;0m ERROR 12-05 14:57:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4057965)[0;0m ERROR 12-05 14:57:24 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4057965)[0;0m ERROR 12-05 14:57:24 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057965)[0;0m ERROR 12-05 14:57:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4057965)[0;0m ERROR 12-05 14:57:24 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4057965)[0;0m ERROR 12-05 14:57:24 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057965)[0;0m ERROR 12-05 14:57:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4057965)[0;0m ERROR 12-05 14:57:24 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4057965)[0;0m ERROR 12-05 14:57:24 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4057965)[0;0m ERROR 12-05 14:57:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4057965)[0;0m ERROR 12-05 14:57:24 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4057965)[0;0m ERROR 12-05 14:57:24 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7f3f38289f40>' raised:
[1;36m(EngineCore_DP0 pid=4057965)[0;0m ERROR 12-05 14:57:24 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4057965)[0;0m ERROR 12-05 14:57:24 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4057965)[0;0m ERROR 12-05 14:57:24 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4057965)[0;0m ERROR 12-05 14:57:24 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 14:57:45 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 14:57:45 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 14:57:45 [model.py:1745] Using max model len 131072
INFO 12-05 14:57:45 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4058055)[0;0m INFO 12-05 14:57:52 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4058055)[0;0m INFO 12-05 14:57:53 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:48187 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4058055)[0;0m INFO 12-05 14:57:53 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4058055)[0;0m INFO 12-05 14:57:53 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4058055)[0;0m INFO 12-05 14:57:54 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4058055)[0;0m INFO 12-05 14:57:54 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4058055)[0;0m INFO 12-05 14:57:55 [default_loader.py:314] Loading weights took 0.88 seconds
[1;36m(EngineCore_DP0 pid=4058055)[0;0m INFO 12-05 14:57:56 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.405412 seconds
[1;36m(EngineCore_DP0 pid=4058055)[0;0m INFO 12-05 14:57:59 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4058055)[0;0m INFO 12-05 14:57:59 [backends.py:647] Dynamo bytecode transform time: 3.61 s
[1;36m(EngineCore_DP0 pid=4058055)[0;0m ERROR 12-05 14:58:00 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4058055)[0;0m ERROR 12-05 14:58:00 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4058055)[0;0m ERROR 12-05 14:58:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4058055)[0;0m ERROR 12-05 14:58:00 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4058055)[0;0m ERROR 12-05 14:58:00 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058055)[0;0m ERROR 12-05 14:58:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4058055)[0;0m ERROR 12-05 14:58:00 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4058055)[0;0m ERROR 12-05 14:58:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4058055)[0;0m ERROR 12-05 14:58:00 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4058055)[0;0m ERROR 12-05 14:58:00 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058055)[0;0m ERROR 12-05 14:58:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4058055)[0;0m ERROR 12-05 14:58:00 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4058055)[0;0m ERROR 12-05 14:58:00 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058055)[0;0m ERROR 12-05 14:58:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4058055)[0;0m ERROR 12-05 14:58:00 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4058055)[0;0m ERROR 12-05 14:58:00 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058055)[0;0m ERROR 12-05 14:58:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4058055)[0;0m ERROR 12-05 14:58:00 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4058055)[0;0m ERROR 12-05 14:58:00 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058055)[0;0m ERROR 12-05 14:58:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4058055)[0;0m ERROR 12-05 14:58:00 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4058055)[0;0m ERROR 12-05 14:58:00 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058055)[0;0m ERROR 12-05 14:58:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4058055)[0;0m ERROR 12-05 14:58:00 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4058055)[0;0m ERROR 12-05 14:58:00 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058055)[0;0m ERROR 12-05 14:58:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4058055)[0;0m ERROR 12-05 14:58:00 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4058055)[0;0m ERROR 12-05 14:58:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4058055)[0;0m ERROR 12-05 14:58:00 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4058055)[0;0m ERROR 12-05 14:58:00 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058055)[0;0m ERROR 12-05 14:58:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4058055)[0;0m ERROR 12-05 14:58:00 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4058055)[0;0m ERROR 12-05 14:58:00 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058055)[0;0m ERROR 12-05 14:58:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4058055)[0;0m ERROR 12-05 14:58:00 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4058055)[0;0m ERROR 12-05 14:58:00 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058055)[0;0m ERROR 12-05 14:58:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4058055)[0;0m ERROR 12-05 14:58:00 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4058055)[0;0m ERROR 12-05 14:58:00 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058055)[0;0m ERROR 12-05 14:58:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4058055)[0;0m ERROR 12-05 14:58:00 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4058055)[0;0m ERROR 12-05 14:58:00 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058055)[0;0m ERROR 12-05 14:58:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4058055)[0;0m ERROR 12-05 14:58:00 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4058055)[0;0m ERROR 12-05 14:58:00 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058055)[0;0m ERROR 12-05 14:58:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4058055)[0;0m ERROR 12-05 14:58:00 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4058055)[0;0m ERROR 12-05 14:58:00 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058055)[0;0m ERROR 12-05 14:58:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4058055)[0;0m ERROR 12-05 14:58:00 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4058055)[0;0m ERROR 12-05 14:58:00 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058055)[0;0m ERROR 12-05 14:58:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4058055)[0;0m ERROR 12-05 14:58:00 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4058055)[0;0m ERROR 12-05 14:58:00 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058055)[0;0m ERROR 12-05 14:58:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4058055)[0;0m ERROR 12-05 14:58:00 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4058055)[0;0m ERROR 12-05 14:58:00 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058055)[0;0m ERROR 12-05 14:58:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4058055)[0;0m ERROR 12-05 14:58:00 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4058055)[0;0m ERROR 12-05 14:58:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4058055)[0;0m ERROR 12-05 14:58:00 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4058055)[0;0m ERROR 12-05 14:58:00 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058055)[0;0m ERROR 12-05 14:58:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4058055)[0;0m ERROR 12-05 14:58:00 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4058055)[0;0m ERROR 12-05 14:58:00 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058055)[0;0m ERROR 12-05 14:58:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4058055)[0;0m ERROR 12-05 14:58:00 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4058055)[0;0m ERROR 12-05 14:58:00 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058055)[0;0m ERROR 12-05 14:58:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4058055)[0;0m ERROR 12-05 14:58:00 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4058055)[0;0m ERROR 12-05 14:58:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4058055)[0;0m ERROR 12-05 14:58:00 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4058055)[0;0m ERROR 12-05 14:58:00 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058055)[0;0m ERROR 12-05 14:58:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4058055)[0;0m ERROR 12-05 14:58:00 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4058055)[0;0m ERROR 12-05 14:58:00 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058055)[0;0m ERROR 12-05 14:58:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4058055)[0;0m ERROR 12-05 14:58:00 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4058055)[0;0m ERROR 12-05 14:58:00 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058055)[0;0m ERROR 12-05 14:58:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4058055)[0;0m ERROR 12-05 14:58:00 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4058055)[0;0m ERROR 12-05 14:58:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4058055)[0;0m ERROR 12-05 14:58:00 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4058055)[0;0m ERROR 12-05 14:58:00 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058055)[0;0m ERROR 12-05 14:58:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4058055)[0;0m ERROR 12-05 14:58:00 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4058055)[0;0m ERROR 12-05 14:58:00 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058055)[0;0m ERROR 12-05 14:58:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4058055)[0;0m ERROR 12-05 14:58:00 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4058055)[0;0m ERROR 12-05 14:58:00 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058055)[0;0m ERROR 12-05 14:58:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4058055)[0;0m ERROR 12-05 14:58:00 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4058055)[0;0m ERROR 12-05 14:58:00 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058055)[0;0m ERROR 12-05 14:58:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4058055)[0;0m ERROR 12-05 14:58:00 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4058055)[0;0m ERROR 12-05 14:58:00 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7fd378128050>' raised:
[1;36m(EngineCore_DP0 pid=4058055)[0;0m ERROR 12-05 14:58:00 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4058055)[0;0m ERROR 12-05 14:58:00 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4058055)[0;0m ERROR 12-05 14:58:00 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4058055)[0;0m ERROR 12-05 14:58:00 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 14:58:21 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 14:58:21 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 14:58:21 [model.py:1745] Using max model len 131072
INFO 12-05 14:58:21 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4058131)[0;0m INFO 12-05 14:58:28 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4058131)[0;0m INFO 12-05 14:58:29 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:38957 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4058131)[0;0m INFO 12-05 14:58:29 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4058131)[0;0m INFO 12-05 14:58:30 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4058131)[0;0m INFO 12-05 14:58:30 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4058131)[0;0m INFO 12-05 14:58:30 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4058131)[0;0m INFO 12-05 14:58:31 [default_loader.py:314] Loading weights took 0.88 seconds
[1;36m(EngineCore_DP0 pid=4058131)[0;0m INFO 12-05 14:58:32 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.375970 seconds
[1;36m(EngineCore_DP0 pid=4058131)[0;0m INFO 12-05 14:58:35 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4058131)[0;0m INFO 12-05 14:58:35 [backends.py:647] Dynamo bytecode transform time: 3.65 s
[1;36m(EngineCore_DP0 pid=4058131)[0;0m ERROR 12-05 14:58:36 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4058131)[0;0m ERROR 12-05 14:58:36 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4058131)[0;0m ERROR 12-05 14:58:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4058131)[0;0m ERROR 12-05 14:58:36 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4058131)[0;0m ERROR 12-05 14:58:36 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058131)[0;0m ERROR 12-05 14:58:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4058131)[0;0m ERROR 12-05 14:58:36 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4058131)[0;0m ERROR 12-05 14:58:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4058131)[0;0m ERROR 12-05 14:58:36 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4058131)[0;0m ERROR 12-05 14:58:36 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058131)[0;0m ERROR 12-05 14:58:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4058131)[0;0m ERROR 12-05 14:58:36 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4058131)[0;0m ERROR 12-05 14:58:36 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058131)[0;0m ERROR 12-05 14:58:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4058131)[0;0m ERROR 12-05 14:58:36 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4058131)[0;0m ERROR 12-05 14:58:36 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058131)[0;0m ERROR 12-05 14:58:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4058131)[0;0m ERROR 12-05 14:58:36 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4058131)[0;0m ERROR 12-05 14:58:36 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058131)[0;0m ERROR 12-05 14:58:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4058131)[0;0m ERROR 12-05 14:58:36 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4058131)[0;0m ERROR 12-05 14:58:36 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058131)[0;0m ERROR 12-05 14:58:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4058131)[0;0m ERROR 12-05 14:58:36 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4058131)[0;0m ERROR 12-05 14:58:36 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058131)[0;0m ERROR 12-05 14:58:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4058131)[0;0m ERROR 12-05 14:58:36 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4058131)[0;0m ERROR 12-05 14:58:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4058131)[0;0m ERROR 12-05 14:58:36 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4058131)[0;0m ERROR 12-05 14:58:36 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058131)[0;0m ERROR 12-05 14:58:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4058131)[0;0m ERROR 12-05 14:58:36 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4058131)[0;0m ERROR 12-05 14:58:36 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058131)[0;0m ERROR 12-05 14:58:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4058131)[0;0m ERROR 12-05 14:58:36 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4058131)[0;0m ERROR 12-05 14:58:36 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058131)[0;0m ERROR 12-05 14:58:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4058131)[0;0m ERROR 12-05 14:58:36 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4058131)[0;0m ERROR 12-05 14:58:36 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058131)[0;0m ERROR 12-05 14:58:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4058131)[0;0m ERROR 12-05 14:58:36 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4058131)[0;0m ERROR 12-05 14:58:36 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058131)[0;0m ERROR 12-05 14:58:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4058131)[0;0m ERROR 12-05 14:58:36 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4058131)[0;0m ERROR 12-05 14:58:36 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058131)[0;0m ERROR 12-05 14:58:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4058131)[0;0m ERROR 12-05 14:58:36 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4058131)[0;0m ERROR 12-05 14:58:36 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058131)[0;0m ERROR 12-05 14:58:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4058131)[0;0m ERROR 12-05 14:58:36 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4058131)[0;0m ERROR 12-05 14:58:36 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058131)[0;0m ERROR 12-05 14:58:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4058131)[0;0m ERROR 12-05 14:58:36 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4058131)[0;0m ERROR 12-05 14:58:36 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058131)[0;0m ERROR 12-05 14:58:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4058131)[0;0m ERROR 12-05 14:58:36 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4058131)[0;0m ERROR 12-05 14:58:36 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058131)[0;0m ERROR 12-05 14:58:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4058131)[0;0m ERROR 12-05 14:58:36 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4058131)[0;0m ERROR 12-05 14:58:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4058131)[0;0m ERROR 12-05 14:58:36 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4058131)[0;0m ERROR 12-05 14:58:36 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058131)[0;0m ERROR 12-05 14:58:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4058131)[0;0m ERROR 12-05 14:58:36 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4058131)[0;0m ERROR 12-05 14:58:36 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058131)[0;0m ERROR 12-05 14:58:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4058131)[0;0m ERROR 12-05 14:58:36 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4058131)[0;0m ERROR 12-05 14:58:36 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058131)[0;0m ERROR 12-05 14:58:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4058131)[0;0m ERROR 12-05 14:58:36 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4058131)[0;0m ERROR 12-05 14:58:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4058131)[0;0m ERROR 12-05 14:58:36 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4058131)[0;0m ERROR 12-05 14:58:36 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058131)[0;0m ERROR 12-05 14:58:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4058131)[0;0m ERROR 12-05 14:58:36 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4058131)[0;0m ERROR 12-05 14:58:36 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058131)[0;0m ERROR 12-05 14:58:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4058131)[0;0m ERROR 12-05 14:58:36 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4058131)[0;0m ERROR 12-05 14:58:36 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058131)[0;0m ERROR 12-05 14:58:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4058131)[0;0m ERROR 12-05 14:58:36 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4058131)[0;0m ERROR 12-05 14:58:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4058131)[0;0m ERROR 12-05 14:58:36 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4058131)[0;0m ERROR 12-05 14:58:36 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058131)[0;0m ERROR 12-05 14:58:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4058131)[0;0m ERROR 12-05 14:58:36 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4058131)[0;0m ERROR 12-05 14:58:36 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058131)[0;0m ERROR 12-05 14:58:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4058131)[0;0m ERROR 12-05 14:58:36 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4058131)[0;0m ERROR 12-05 14:58:36 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058131)[0;0m ERROR 12-05 14:58:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4058131)[0;0m ERROR 12-05 14:58:36 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4058131)[0;0m ERROR 12-05 14:58:36 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058131)[0;0m ERROR 12-05 14:58:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4058131)[0;0m ERROR 12-05 14:58:36 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4058131)[0;0m ERROR 12-05 14:58:36 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7f1ae82eb3e0>' raised:
[1;36m(EngineCore_DP0 pid=4058131)[0;0m ERROR 12-05 14:58:36 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4058131)[0;0m ERROR 12-05 14:58:36 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4058131)[0;0m ERROR 12-05 14:58:36 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4058131)[0;0m ERROR 12-05 14:58:36 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 14:58:57 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 14:58:57 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 14:58:57 [model.py:1745] Using max model len 131072
INFO 12-05 14:58:57 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4058219)[0;0m INFO 12-05 14:59:04 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4058219)[0;0m INFO 12-05 14:59:05 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:35535 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4058219)[0;0m INFO 12-05 14:59:05 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4058219)[0;0m INFO 12-05 14:59:05 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4058219)[0;0m INFO 12-05 14:59:06 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4058219)[0;0m INFO 12-05 14:59:06 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4058219)[0;0m INFO 12-05 14:59:07 [default_loader.py:314] Loading weights took 0.87 seconds
[1;36m(EngineCore_DP0 pid=4058219)[0;0m INFO 12-05 14:59:08 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.424637 seconds
[1;36m(EngineCore_DP0 pid=4058219)[0;0m INFO 12-05 14:59:11 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4058219)[0;0m INFO 12-05 14:59:11 [backends.py:647] Dynamo bytecode transform time: 3.58 s
[1;36m(EngineCore_DP0 pid=4058219)[0;0m ERROR 12-05 14:59:12 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4058219)[0;0m ERROR 12-05 14:59:12 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4058219)[0;0m ERROR 12-05 14:59:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4058219)[0;0m ERROR 12-05 14:59:12 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4058219)[0;0m ERROR 12-05 14:59:12 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058219)[0;0m ERROR 12-05 14:59:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4058219)[0;0m ERROR 12-05 14:59:12 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4058219)[0;0m ERROR 12-05 14:59:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4058219)[0;0m ERROR 12-05 14:59:12 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4058219)[0;0m ERROR 12-05 14:59:12 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058219)[0;0m ERROR 12-05 14:59:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4058219)[0;0m ERROR 12-05 14:59:12 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4058219)[0;0m ERROR 12-05 14:59:12 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058219)[0;0m ERROR 12-05 14:59:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4058219)[0;0m ERROR 12-05 14:59:12 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4058219)[0;0m ERROR 12-05 14:59:12 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058219)[0;0m ERROR 12-05 14:59:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4058219)[0;0m ERROR 12-05 14:59:12 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4058219)[0;0m ERROR 12-05 14:59:12 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058219)[0;0m ERROR 12-05 14:59:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4058219)[0;0m ERROR 12-05 14:59:12 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4058219)[0;0m ERROR 12-05 14:59:12 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058219)[0;0m ERROR 12-05 14:59:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4058219)[0;0m ERROR 12-05 14:59:12 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4058219)[0;0m ERROR 12-05 14:59:12 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058219)[0;0m ERROR 12-05 14:59:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4058219)[0;0m ERROR 12-05 14:59:12 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4058219)[0;0m ERROR 12-05 14:59:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4058219)[0;0m ERROR 12-05 14:59:12 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4058219)[0;0m ERROR 12-05 14:59:12 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058219)[0;0m ERROR 12-05 14:59:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4058219)[0;0m ERROR 12-05 14:59:12 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4058219)[0;0m ERROR 12-05 14:59:12 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058219)[0;0m ERROR 12-05 14:59:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4058219)[0;0m ERROR 12-05 14:59:12 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4058219)[0;0m ERROR 12-05 14:59:12 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058219)[0;0m ERROR 12-05 14:59:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4058219)[0;0m ERROR 12-05 14:59:12 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4058219)[0;0m ERROR 12-05 14:59:12 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058219)[0;0m ERROR 12-05 14:59:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4058219)[0;0m ERROR 12-05 14:59:12 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4058219)[0;0m ERROR 12-05 14:59:12 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058219)[0;0m ERROR 12-05 14:59:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4058219)[0;0m ERROR 12-05 14:59:12 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4058219)[0;0m ERROR 12-05 14:59:12 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058219)[0;0m ERROR 12-05 14:59:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4058219)[0;0m ERROR 12-05 14:59:12 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4058219)[0;0m ERROR 12-05 14:59:12 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058219)[0;0m ERROR 12-05 14:59:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4058219)[0;0m ERROR 12-05 14:59:12 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4058219)[0;0m ERROR 12-05 14:59:12 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058219)[0;0m ERROR 12-05 14:59:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4058219)[0;0m ERROR 12-05 14:59:12 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4058219)[0;0m ERROR 12-05 14:59:12 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058219)[0;0m ERROR 12-05 14:59:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4058219)[0;0m ERROR 12-05 14:59:12 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4058219)[0;0m ERROR 12-05 14:59:12 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058219)[0;0m ERROR 12-05 14:59:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4058219)[0;0m ERROR 12-05 14:59:12 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4058219)[0;0m ERROR 12-05 14:59:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4058219)[0;0m ERROR 12-05 14:59:12 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4058219)[0;0m ERROR 12-05 14:59:12 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058219)[0;0m ERROR 12-05 14:59:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4058219)[0;0m ERROR 12-05 14:59:12 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4058219)[0;0m ERROR 12-05 14:59:12 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058219)[0;0m ERROR 12-05 14:59:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4058219)[0;0m ERROR 12-05 14:59:12 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4058219)[0;0m ERROR 12-05 14:59:12 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058219)[0;0m ERROR 12-05 14:59:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4058219)[0;0m ERROR 12-05 14:59:12 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4058219)[0;0m ERROR 12-05 14:59:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4058219)[0;0m ERROR 12-05 14:59:12 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4058219)[0;0m ERROR 12-05 14:59:12 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058219)[0;0m ERROR 12-05 14:59:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4058219)[0;0m ERROR 12-05 14:59:12 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4058219)[0;0m ERROR 12-05 14:59:12 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058219)[0;0m ERROR 12-05 14:59:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4058219)[0;0m ERROR 12-05 14:59:12 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4058219)[0;0m ERROR 12-05 14:59:12 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058219)[0;0m ERROR 12-05 14:59:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4058219)[0;0m ERROR 12-05 14:59:12 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4058219)[0;0m ERROR 12-05 14:59:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4058219)[0;0m ERROR 12-05 14:59:12 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4058219)[0;0m ERROR 12-05 14:59:12 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058219)[0;0m ERROR 12-05 14:59:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4058219)[0;0m ERROR 12-05 14:59:12 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4058219)[0;0m ERROR 12-05 14:59:12 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058219)[0;0m ERROR 12-05 14:59:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4058219)[0;0m ERROR 12-05 14:59:12 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4058219)[0;0m ERROR 12-05 14:59:12 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058219)[0;0m ERROR 12-05 14:59:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4058219)[0;0m ERROR 12-05 14:59:12 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4058219)[0;0m ERROR 12-05 14:59:12 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058219)[0;0m ERROR 12-05 14:59:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4058219)[0;0m ERROR 12-05 14:59:12 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4058219)[0;0m ERROR 12-05 14:59:12 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7fd9a0011a90>' raised:
[1;36m(EngineCore_DP0 pid=4058219)[0;0m ERROR 12-05 14:59:12 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4058219)[0;0m ERROR 12-05 14:59:12 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4058219)[0;0m ERROR 12-05 14:59:12 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4058219)[0;0m ERROR 12-05 14:59:12 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 14:59:33 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 14:59:33 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 14:59:33 [model.py:1745] Using max model len 131072
INFO 12-05 14:59:33 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4058301)[0;0m INFO 12-05 14:59:41 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4058301)[0;0m INFO 12-05 14:59:41 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:40627 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4058301)[0;0m INFO 12-05 14:59:41 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4058301)[0;0m INFO 12-05 14:59:42 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4058301)[0;0m INFO 12-05 14:59:42 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4058301)[0;0m INFO 12-05 14:59:42 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4058301)[0;0m INFO 12-05 14:59:43 [default_loader.py:314] Loading weights took 0.87 seconds
[1;36m(EngineCore_DP0 pid=4058301)[0;0m INFO 12-05 14:59:44 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.377442 seconds
[1;36m(EngineCore_DP0 pid=4058301)[0;0m INFO 12-05 14:59:48 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4058301)[0;0m INFO 12-05 14:59:48 [backends.py:647] Dynamo bytecode transform time: 3.59 s
[1;36m(EngineCore_DP0 pid=4058301)[0;0m ERROR 12-05 14:59:48 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4058301)[0;0m ERROR 12-05 14:59:48 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4058301)[0;0m ERROR 12-05 14:59:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4058301)[0;0m ERROR 12-05 14:59:48 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4058301)[0;0m ERROR 12-05 14:59:48 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058301)[0;0m ERROR 12-05 14:59:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4058301)[0;0m ERROR 12-05 14:59:48 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4058301)[0;0m ERROR 12-05 14:59:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4058301)[0;0m ERROR 12-05 14:59:48 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4058301)[0;0m ERROR 12-05 14:59:48 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058301)[0;0m ERROR 12-05 14:59:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4058301)[0;0m ERROR 12-05 14:59:48 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4058301)[0;0m ERROR 12-05 14:59:48 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058301)[0;0m ERROR 12-05 14:59:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4058301)[0;0m ERROR 12-05 14:59:48 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4058301)[0;0m ERROR 12-05 14:59:48 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058301)[0;0m ERROR 12-05 14:59:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4058301)[0;0m ERROR 12-05 14:59:48 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4058301)[0;0m ERROR 12-05 14:59:48 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058301)[0;0m ERROR 12-05 14:59:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4058301)[0;0m ERROR 12-05 14:59:48 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4058301)[0;0m ERROR 12-05 14:59:48 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058301)[0;0m ERROR 12-05 14:59:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4058301)[0;0m ERROR 12-05 14:59:48 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4058301)[0;0m ERROR 12-05 14:59:48 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058301)[0;0m ERROR 12-05 14:59:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4058301)[0;0m ERROR 12-05 14:59:48 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4058301)[0;0m ERROR 12-05 14:59:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4058301)[0;0m ERROR 12-05 14:59:48 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4058301)[0;0m ERROR 12-05 14:59:48 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058301)[0;0m ERROR 12-05 14:59:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4058301)[0;0m ERROR 12-05 14:59:48 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4058301)[0;0m ERROR 12-05 14:59:48 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058301)[0;0m ERROR 12-05 14:59:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4058301)[0;0m ERROR 12-05 14:59:48 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4058301)[0;0m ERROR 12-05 14:59:48 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058301)[0;0m ERROR 12-05 14:59:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4058301)[0;0m ERROR 12-05 14:59:48 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4058301)[0;0m ERROR 12-05 14:59:48 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058301)[0;0m ERROR 12-05 14:59:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4058301)[0;0m ERROR 12-05 14:59:48 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4058301)[0;0m ERROR 12-05 14:59:48 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058301)[0;0m ERROR 12-05 14:59:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4058301)[0;0m ERROR 12-05 14:59:48 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4058301)[0;0m ERROR 12-05 14:59:48 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058301)[0;0m ERROR 12-05 14:59:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4058301)[0;0m ERROR 12-05 14:59:48 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4058301)[0;0m ERROR 12-05 14:59:48 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058301)[0;0m ERROR 12-05 14:59:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4058301)[0;0m ERROR 12-05 14:59:48 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4058301)[0;0m ERROR 12-05 14:59:48 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058301)[0;0m ERROR 12-05 14:59:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4058301)[0;0m ERROR 12-05 14:59:48 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4058301)[0;0m ERROR 12-05 14:59:48 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058301)[0;0m ERROR 12-05 14:59:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4058301)[0;0m ERROR 12-05 14:59:48 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4058301)[0;0m ERROR 12-05 14:59:48 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058301)[0;0m ERROR 12-05 14:59:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4058301)[0;0m ERROR 12-05 14:59:48 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4058301)[0;0m ERROR 12-05 14:59:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4058301)[0;0m ERROR 12-05 14:59:48 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4058301)[0;0m ERROR 12-05 14:59:48 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058301)[0;0m ERROR 12-05 14:59:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4058301)[0;0m ERROR 12-05 14:59:48 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4058301)[0;0m ERROR 12-05 14:59:48 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058301)[0;0m ERROR 12-05 14:59:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4058301)[0;0m ERROR 12-05 14:59:48 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4058301)[0;0m ERROR 12-05 14:59:48 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058301)[0;0m ERROR 12-05 14:59:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4058301)[0;0m ERROR 12-05 14:59:48 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4058301)[0;0m ERROR 12-05 14:59:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4058301)[0;0m ERROR 12-05 14:59:48 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4058301)[0;0m ERROR 12-05 14:59:48 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058301)[0;0m ERROR 12-05 14:59:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4058301)[0;0m ERROR 12-05 14:59:48 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4058301)[0;0m ERROR 12-05 14:59:48 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058301)[0;0m ERROR 12-05 14:59:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4058301)[0;0m ERROR 12-05 14:59:48 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4058301)[0;0m ERROR 12-05 14:59:48 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058301)[0;0m ERROR 12-05 14:59:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4058301)[0;0m ERROR 12-05 14:59:48 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4058301)[0;0m ERROR 12-05 14:59:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4058301)[0;0m ERROR 12-05 14:59:48 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4058301)[0;0m ERROR 12-05 14:59:48 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058301)[0;0m ERROR 12-05 14:59:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4058301)[0;0m ERROR 12-05 14:59:48 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4058301)[0;0m ERROR 12-05 14:59:48 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058301)[0;0m ERROR 12-05 14:59:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4058301)[0;0m ERROR 12-05 14:59:48 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4058301)[0;0m ERROR 12-05 14:59:48 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058301)[0;0m ERROR 12-05 14:59:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4058301)[0;0m ERROR 12-05 14:59:48 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4058301)[0;0m ERROR 12-05 14:59:48 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058301)[0;0m ERROR 12-05 14:59:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4058301)[0;0m ERROR 12-05 14:59:48 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4058301)[0;0m ERROR 12-05 14:59:48 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7f312633f050>' raised:
[1;36m(EngineCore_DP0 pid=4058301)[0;0m ERROR 12-05 14:59:48 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4058301)[0;0m ERROR 12-05 14:59:48 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4058301)[0;0m ERROR 12-05 14:59:48 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4058301)[0;0m ERROR 12-05 14:59:48 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:00:09 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:00:09 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:00:09 [model.py:1745] Using max model len 131072
INFO 12-05 15:00:09 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4058386)[0;0m INFO 12-05 15:00:16 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4058386)[0;0m INFO 12-05 15:00:17 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:38551 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4058386)[0;0m INFO 12-05 15:00:17 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4058386)[0;0m INFO 12-05 15:00:18 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4058386)[0;0m INFO 12-05 15:00:18 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4058386)[0;0m INFO 12-05 15:00:18 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4058386)[0;0m INFO 12-05 15:00:19 [default_loader.py:314] Loading weights took 0.87 seconds
[1;36m(EngineCore_DP0 pid=4058386)[0;0m INFO 12-05 15:00:20 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.413735 seconds
[1;36m(EngineCore_DP0 pid=4058386)[0;0m INFO 12-05 15:00:24 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4058386)[0;0m INFO 12-05 15:00:24 [backends.py:647] Dynamo bytecode transform time: 3.86 s
[1;36m(EngineCore_DP0 pid=4058386)[0;0m ERROR 12-05 15:00:24 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4058386)[0;0m ERROR 12-05 15:00:24 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4058386)[0;0m ERROR 12-05 15:00:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4058386)[0;0m ERROR 12-05 15:00:24 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4058386)[0;0m ERROR 12-05 15:00:24 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058386)[0;0m ERROR 12-05 15:00:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4058386)[0;0m ERROR 12-05 15:00:24 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4058386)[0;0m ERROR 12-05 15:00:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4058386)[0;0m ERROR 12-05 15:00:24 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4058386)[0;0m ERROR 12-05 15:00:24 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058386)[0;0m ERROR 12-05 15:00:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4058386)[0;0m ERROR 12-05 15:00:24 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4058386)[0;0m ERROR 12-05 15:00:24 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058386)[0;0m ERROR 12-05 15:00:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4058386)[0;0m ERROR 12-05 15:00:24 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4058386)[0;0m ERROR 12-05 15:00:24 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058386)[0;0m ERROR 12-05 15:00:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4058386)[0;0m ERROR 12-05 15:00:24 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4058386)[0;0m ERROR 12-05 15:00:24 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058386)[0;0m ERROR 12-05 15:00:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4058386)[0;0m ERROR 12-05 15:00:24 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4058386)[0;0m ERROR 12-05 15:00:24 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058386)[0;0m ERROR 12-05 15:00:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4058386)[0;0m ERROR 12-05 15:00:24 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4058386)[0;0m ERROR 12-05 15:00:24 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058386)[0;0m ERROR 12-05 15:00:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4058386)[0;0m ERROR 12-05 15:00:24 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4058386)[0;0m ERROR 12-05 15:00:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4058386)[0;0m ERROR 12-05 15:00:24 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4058386)[0;0m ERROR 12-05 15:00:24 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058386)[0;0m ERROR 12-05 15:00:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4058386)[0;0m ERROR 12-05 15:00:24 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4058386)[0;0m ERROR 12-05 15:00:24 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058386)[0;0m ERROR 12-05 15:00:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4058386)[0;0m ERROR 12-05 15:00:24 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4058386)[0;0m ERROR 12-05 15:00:24 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058386)[0;0m ERROR 12-05 15:00:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4058386)[0;0m ERROR 12-05 15:00:24 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4058386)[0;0m ERROR 12-05 15:00:24 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058386)[0;0m ERROR 12-05 15:00:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4058386)[0;0m ERROR 12-05 15:00:24 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4058386)[0;0m ERROR 12-05 15:00:24 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058386)[0;0m ERROR 12-05 15:00:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4058386)[0;0m ERROR 12-05 15:00:24 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4058386)[0;0m ERROR 12-05 15:00:24 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058386)[0;0m ERROR 12-05 15:00:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4058386)[0;0m ERROR 12-05 15:00:24 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4058386)[0;0m ERROR 12-05 15:00:24 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058386)[0;0m ERROR 12-05 15:00:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4058386)[0;0m ERROR 12-05 15:00:24 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4058386)[0;0m ERROR 12-05 15:00:24 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058386)[0;0m ERROR 12-05 15:00:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4058386)[0;0m ERROR 12-05 15:00:24 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4058386)[0;0m ERROR 12-05 15:00:24 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058386)[0;0m ERROR 12-05 15:00:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4058386)[0;0m ERROR 12-05 15:00:24 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4058386)[0;0m ERROR 12-05 15:00:24 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058386)[0;0m ERROR 12-05 15:00:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4058386)[0;0m ERROR 12-05 15:00:24 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4058386)[0;0m ERROR 12-05 15:00:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4058386)[0;0m ERROR 12-05 15:00:24 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4058386)[0;0m ERROR 12-05 15:00:24 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058386)[0;0m ERROR 12-05 15:00:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4058386)[0;0m ERROR 12-05 15:00:24 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4058386)[0;0m ERROR 12-05 15:00:24 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058386)[0;0m ERROR 12-05 15:00:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4058386)[0;0m ERROR 12-05 15:00:24 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4058386)[0;0m ERROR 12-05 15:00:24 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058386)[0;0m ERROR 12-05 15:00:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4058386)[0;0m ERROR 12-05 15:00:24 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4058386)[0;0m ERROR 12-05 15:00:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4058386)[0;0m ERROR 12-05 15:00:24 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4058386)[0;0m ERROR 12-05 15:00:24 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058386)[0;0m ERROR 12-05 15:00:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4058386)[0;0m ERROR 12-05 15:00:24 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4058386)[0;0m ERROR 12-05 15:00:24 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058386)[0;0m ERROR 12-05 15:00:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4058386)[0;0m ERROR 12-05 15:00:24 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4058386)[0;0m ERROR 12-05 15:00:24 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058386)[0;0m ERROR 12-05 15:00:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4058386)[0;0m ERROR 12-05 15:00:24 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4058386)[0;0m ERROR 12-05 15:00:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4058386)[0;0m ERROR 12-05 15:00:24 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4058386)[0;0m ERROR 12-05 15:00:24 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058386)[0;0m ERROR 12-05 15:00:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4058386)[0;0m ERROR 12-05 15:00:24 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4058386)[0;0m ERROR 12-05 15:00:24 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058386)[0;0m ERROR 12-05 15:00:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4058386)[0;0m ERROR 12-05 15:00:24 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4058386)[0;0m ERROR 12-05 15:00:24 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058386)[0;0m ERROR 12-05 15:00:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4058386)[0;0m ERROR 12-05 15:00:24 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4058386)[0;0m ERROR 12-05 15:00:24 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058386)[0;0m ERROR 12-05 15:00:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4058386)[0;0m ERROR 12-05 15:00:24 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4058386)[0;0m ERROR 12-05 15:00:24 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7f9a442a3710>' raised:
[1;36m(EngineCore_DP0 pid=4058386)[0;0m ERROR 12-05 15:00:24 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4058386)[0;0m ERROR 12-05 15:00:24 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4058386)[0;0m ERROR 12-05 15:00:24 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4058386)[0;0m ERROR 12-05 15:00:24 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:00:45 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:00:46 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:00:46 [model.py:1745] Using max model len 131072
INFO 12-05 15:00:46 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4058474)[0;0m INFO 12-05 15:00:54 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4058474)[0;0m INFO 12-05 15:00:55 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:37991 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4058474)[0;0m INFO 12-05 15:00:55 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4058474)[0;0m INFO 12-05 15:00:55 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4058474)[0;0m INFO 12-05 15:00:55 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4058474)[0;0m INFO 12-05 15:00:55 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4058474)[0;0m INFO 12-05 15:00:57 [default_loader.py:314] Loading weights took 0.87 seconds
[1;36m(EngineCore_DP0 pid=4058474)[0;0m INFO 12-05 15:00:57 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.472260 seconds
[1;36m(EngineCore_DP0 pid=4058474)[0;0m INFO 12-05 15:01:01 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4058474)[0;0m INFO 12-05 15:01:01 [backends.py:647] Dynamo bytecode transform time: 3.73 s
[1;36m(EngineCore_DP0 pid=4058474)[0;0m ERROR 12-05 15:01:01 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4058474)[0;0m ERROR 12-05 15:01:01 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4058474)[0;0m ERROR 12-05 15:01:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4058474)[0;0m ERROR 12-05 15:01:01 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4058474)[0;0m ERROR 12-05 15:01:01 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058474)[0;0m ERROR 12-05 15:01:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4058474)[0;0m ERROR 12-05 15:01:01 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4058474)[0;0m ERROR 12-05 15:01:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4058474)[0;0m ERROR 12-05 15:01:01 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4058474)[0;0m ERROR 12-05 15:01:01 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058474)[0;0m ERROR 12-05 15:01:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4058474)[0;0m ERROR 12-05 15:01:01 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4058474)[0;0m ERROR 12-05 15:01:01 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058474)[0;0m ERROR 12-05 15:01:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4058474)[0;0m ERROR 12-05 15:01:01 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4058474)[0;0m ERROR 12-05 15:01:01 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058474)[0;0m ERROR 12-05 15:01:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4058474)[0;0m ERROR 12-05 15:01:01 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4058474)[0;0m ERROR 12-05 15:01:01 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058474)[0;0m ERROR 12-05 15:01:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4058474)[0;0m ERROR 12-05 15:01:01 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4058474)[0;0m ERROR 12-05 15:01:01 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058474)[0;0m ERROR 12-05 15:01:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4058474)[0;0m ERROR 12-05 15:01:01 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4058474)[0;0m ERROR 12-05 15:01:01 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058474)[0;0m ERROR 12-05 15:01:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4058474)[0;0m ERROR 12-05 15:01:01 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4058474)[0;0m ERROR 12-05 15:01:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4058474)[0;0m ERROR 12-05 15:01:01 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4058474)[0;0m ERROR 12-05 15:01:01 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058474)[0;0m ERROR 12-05 15:01:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4058474)[0;0m ERROR 12-05 15:01:01 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4058474)[0;0m ERROR 12-05 15:01:01 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058474)[0;0m ERROR 12-05 15:01:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4058474)[0;0m ERROR 12-05 15:01:01 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4058474)[0;0m ERROR 12-05 15:01:01 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058474)[0;0m ERROR 12-05 15:01:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4058474)[0;0m ERROR 12-05 15:01:01 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4058474)[0;0m ERROR 12-05 15:01:01 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058474)[0;0m ERROR 12-05 15:01:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4058474)[0;0m ERROR 12-05 15:01:01 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4058474)[0;0m ERROR 12-05 15:01:01 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058474)[0;0m ERROR 12-05 15:01:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4058474)[0;0m ERROR 12-05 15:01:01 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4058474)[0;0m ERROR 12-05 15:01:01 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058474)[0;0m ERROR 12-05 15:01:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4058474)[0;0m ERROR 12-05 15:01:01 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4058474)[0;0m ERROR 12-05 15:01:01 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058474)[0;0m ERROR 12-05 15:01:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4058474)[0;0m ERROR 12-05 15:01:01 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4058474)[0;0m ERROR 12-05 15:01:01 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058474)[0;0m ERROR 12-05 15:01:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4058474)[0;0m ERROR 12-05 15:01:01 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4058474)[0;0m ERROR 12-05 15:01:01 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058474)[0;0m ERROR 12-05 15:01:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4058474)[0;0m ERROR 12-05 15:01:01 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4058474)[0;0m ERROR 12-05 15:01:01 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058474)[0;0m ERROR 12-05 15:01:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4058474)[0;0m ERROR 12-05 15:01:01 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4058474)[0;0m ERROR 12-05 15:01:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4058474)[0;0m ERROR 12-05 15:01:01 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4058474)[0;0m ERROR 12-05 15:01:01 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058474)[0;0m ERROR 12-05 15:01:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4058474)[0;0m ERROR 12-05 15:01:01 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4058474)[0;0m ERROR 12-05 15:01:01 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058474)[0;0m ERROR 12-05 15:01:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4058474)[0;0m ERROR 12-05 15:01:01 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4058474)[0;0m ERROR 12-05 15:01:01 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058474)[0;0m ERROR 12-05 15:01:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4058474)[0;0m ERROR 12-05 15:01:01 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4058474)[0;0m ERROR 12-05 15:01:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4058474)[0;0m ERROR 12-05 15:01:01 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4058474)[0;0m ERROR 12-05 15:01:01 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058474)[0;0m ERROR 12-05 15:01:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4058474)[0;0m ERROR 12-05 15:01:01 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4058474)[0;0m ERROR 12-05 15:01:01 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058474)[0;0m ERROR 12-05 15:01:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4058474)[0;0m ERROR 12-05 15:01:01 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4058474)[0;0m ERROR 12-05 15:01:01 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058474)[0;0m ERROR 12-05 15:01:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4058474)[0;0m ERROR 12-05 15:01:01 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4058474)[0;0m ERROR 12-05 15:01:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4058474)[0;0m ERROR 12-05 15:01:01 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4058474)[0;0m ERROR 12-05 15:01:01 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058474)[0;0m ERROR 12-05 15:01:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4058474)[0;0m ERROR 12-05 15:01:01 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4058474)[0;0m ERROR 12-05 15:01:01 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058474)[0;0m ERROR 12-05 15:01:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4058474)[0;0m ERROR 12-05 15:01:01 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4058474)[0;0m ERROR 12-05 15:01:01 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058474)[0;0m ERROR 12-05 15:01:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4058474)[0;0m ERROR 12-05 15:01:01 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4058474)[0;0m ERROR 12-05 15:01:01 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058474)[0;0m ERROR 12-05 15:01:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4058474)[0;0m ERROR 12-05 15:01:01 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4058474)[0;0m ERROR 12-05 15:01:01 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7f85508fee70>' raised:
[1;36m(EngineCore_DP0 pid=4058474)[0;0m ERROR 12-05 15:01:01 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4058474)[0;0m ERROR 12-05 15:01:01 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4058474)[0;0m ERROR 12-05 15:01:01 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4058474)[0;0m ERROR 12-05 15:01:01 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:01:23 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:01:23 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:01:23 [model.py:1745] Using max model len 131072
INFO 12-05 15:01:23 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4058658)[0;0m INFO 12-05 15:01:31 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4058658)[0;0m INFO 12-05 15:01:32 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:34369 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4058658)[0;0m INFO 12-05 15:01:32 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4058658)[0;0m INFO 12-05 15:01:32 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4058658)[0;0m INFO 12-05 15:01:32 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4058658)[0;0m INFO 12-05 15:01:32 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4058658)[0;0m INFO 12-05 15:01:33 [default_loader.py:314] Loading weights took 0.88 seconds
[1;36m(EngineCore_DP0 pid=4058658)[0;0m INFO 12-05 15:01:34 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.393862 seconds
[1;36m(EngineCore_DP0 pid=4058658)[0;0m INFO 12-05 15:01:38 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4058658)[0;0m INFO 12-05 15:01:38 [backends.py:647] Dynamo bytecode transform time: 3.60 s
[1;36m(EngineCore_DP0 pid=4058658)[0;0m ERROR 12-05 15:01:38 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4058658)[0;0m ERROR 12-05 15:01:38 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4058658)[0;0m ERROR 12-05 15:01:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4058658)[0;0m ERROR 12-05 15:01:38 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4058658)[0;0m ERROR 12-05 15:01:38 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058658)[0;0m ERROR 12-05 15:01:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4058658)[0;0m ERROR 12-05 15:01:38 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4058658)[0;0m ERROR 12-05 15:01:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4058658)[0;0m ERROR 12-05 15:01:38 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4058658)[0;0m ERROR 12-05 15:01:38 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058658)[0;0m ERROR 12-05 15:01:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4058658)[0;0m ERROR 12-05 15:01:38 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4058658)[0;0m ERROR 12-05 15:01:38 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058658)[0;0m ERROR 12-05 15:01:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4058658)[0;0m ERROR 12-05 15:01:38 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4058658)[0;0m ERROR 12-05 15:01:38 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058658)[0;0m ERROR 12-05 15:01:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4058658)[0;0m ERROR 12-05 15:01:38 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4058658)[0;0m ERROR 12-05 15:01:38 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058658)[0;0m ERROR 12-05 15:01:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4058658)[0;0m ERROR 12-05 15:01:38 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4058658)[0;0m ERROR 12-05 15:01:38 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058658)[0;0m ERROR 12-05 15:01:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4058658)[0;0m ERROR 12-05 15:01:38 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4058658)[0;0m ERROR 12-05 15:01:38 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058658)[0;0m ERROR 12-05 15:01:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4058658)[0;0m ERROR 12-05 15:01:38 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4058658)[0;0m ERROR 12-05 15:01:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4058658)[0;0m ERROR 12-05 15:01:38 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4058658)[0;0m ERROR 12-05 15:01:38 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058658)[0;0m ERROR 12-05 15:01:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4058658)[0;0m ERROR 12-05 15:01:38 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4058658)[0;0m ERROR 12-05 15:01:38 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058658)[0;0m ERROR 12-05 15:01:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4058658)[0;0m ERROR 12-05 15:01:38 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4058658)[0;0m ERROR 12-05 15:01:38 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058658)[0;0m ERROR 12-05 15:01:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4058658)[0;0m ERROR 12-05 15:01:38 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4058658)[0;0m ERROR 12-05 15:01:38 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058658)[0;0m ERROR 12-05 15:01:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4058658)[0;0m ERROR 12-05 15:01:38 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4058658)[0;0m ERROR 12-05 15:01:38 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058658)[0;0m ERROR 12-05 15:01:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4058658)[0;0m ERROR 12-05 15:01:38 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4058658)[0;0m ERROR 12-05 15:01:38 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058658)[0;0m ERROR 12-05 15:01:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4058658)[0;0m ERROR 12-05 15:01:38 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4058658)[0;0m ERROR 12-05 15:01:38 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058658)[0;0m ERROR 12-05 15:01:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4058658)[0;0m ERROR 12-05 15:01:38 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4058658)[0;0m ERROR 12-05 15:01:38 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058658)[0;0m ERROR 12-05 15:01:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4058658)[0;0m ERROR 12-05 15:01:38 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4058658)[0;0m ERROR 12-05 15:01:38 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058658)[0;0m ERROR 12-05 15:01:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4058658)[0;0m ERROR 12-05 15:01:38 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4058658)[0;0m ERROR 12-05 15:01:38 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058658)[0;0m ERROR 12-05 15:01:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4058658)[0;0m ERROR 12-05 15:01:38 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4058658)[0;0m ERROR 12-05 15:01:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4058658)[0;0m ERROR 12-05 15:01:38 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4058658)[0;0m ERROR 12-05 15:01:38 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058658)[0;0m ERROR 12-05 15:01:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4058658)[0;0m ERROR 12-05 15:01:38 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4058658)[0;0m ERROR 12-05 15:01:38 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058658)[0;0m ERROR 12-05 15:01:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4058658)[0;0m ERROR 12-05 15:01:38 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4058658)[0;0m ERROR 12-05 15:01:38 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058658)[0;0m ERROR 12-05 15:01:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4058658)[0;0m ERROR 12-05 15:01:38 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4058658)[0;0m ERROR 12-05 15:01:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4058658)[0;0m ERROR 12-05 15:01:38 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4058658)[0;0m ERROR 12-05 15:01:38 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058658)[0;0m ERROR 12-05 15:01:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4058658)[0;0m ERROR 12-05 15:01:38 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4058658)[0;0m ERROR 12-05 15:01:38 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058658)[0;0m ERROR 12-05 15:01:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4058658)[0;0m ERROR 12-05 15:01:38 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4058658)[0;0m ERROR 12-05 15:01:38 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058658)[0;0m ERROR 12-05 15:01:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4058658)[0;0m ERROR 12-05 15:01:38 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4058658)[0;0m ERROR 12-05 15:01:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4058658)[0;0m ERROR 12-05 15:01:38 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4058658)[0;0m ERROR 12-05 15:01:38 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058658)[0;0m ERROR 12-05 15:01:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4058658)[0;0m ERROR 12-05 15:01:38 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4058658)[0;0m ERROR 12-05 15:01:38 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058658)[0;0m ERROR 12-05 15:01:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4058658)[0;0m ERROR 12-05 15:01:38 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4058658)[0;0m ERROR 12-05 15:01:38 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058658)[0;0m ERROR 12-05 15:01:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4058658)[0;0m ERROR 12-05 15:01:38 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4058658)[0;0m ERROR 12-05 15:01:38 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058658)[0;0m ERROR 12-05 15:01:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4058658)[0;0m ERROR 12-05 15:01:38 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4058658)[0;0m ERROR 12-05 15:01:38 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7fd960343080>' raised:
[1;36m(EngineCore_DP0 pid=4058658)[0;0m ERROR 12-05 15:01:38 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4058658)[0;0m ERROR 12-05 15:01:38 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4058658)[0;0m ERROR 12-05 15:01:38 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4058658)[0;0m ERROR 12-05 15:01:38 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:01:59 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:02:00 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:02:00 [model.py:1745] Using max model len 131072
INFO 12-05 15:02:00 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4058746)[0;0m INFO 12-05 15:02:07 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4058746)[0;0m INFO 12-05 15:02:08 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:51865 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4058746)[0;0m INFO 12-05 15:02:08 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4058746)[0;0m INFO 12-05 15:02:08 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4058746)[0;0m INFO 12-05 15:02:08 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4058746)[0;0m INFO 12-05 15:02:08 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4058746)[0;0m INFO 12-05 15:02:10 [default_loader.py:314] Loading weights took 0.88 seconds
[1;36m(EngineCore_DP0 pid=4058746)[0;0m INFO 12-05 15:02:10 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.409999 seconds
[1;36m(EngineCore_DP0 pid=4058746)[0;0m INFO 12-05 15:02:14 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4058746)[0;0m INFO 12-05 15:02:14 [backends.py:647] Dynamo bytecode transform time: 3.56 s
[1;36m(EngineCore_DP0 pid=4058746)[0;0m ERROR 12-05 15:02:14 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4058746)[0;0m ERROR 12-05 15:02:14 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4058746)[0;0m ERROR 12-05 15:02:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4058746)[0;0m ERROR 12-05 15:02:14 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4058746)[0;0m ERROR 12-05 15:02:14 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058746)[0;0m ERROR 12-05 15:02:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4058746)[0;0m ERROR 12-05 15:02:14 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4058746)[0;0m ERROR 12-05 15:02:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4058746)[0;0m ERROR 12-05 15:02:14 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4058746)[0;0m ERROR 12-05 15:02:14 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058746)[0;0m ERROR 12-05 15:02:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4058746)[0;0m ERROR 12-05 15:02:14 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4058746)[0;0m ERROR 12-05 15:02:14 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058746)[0;0m ERROR 12-05 15:02:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4058746)[0;0m ERROR 12-05 15:02:14 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4058746)[0;0m ERROR 12-05 15:02:14 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058746)[0;0m ERROR 12-05 15:02:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4058746)[0;0m ERROR 12-05 15:02:14 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4058746)[0;0m ERROR 12-05 15:02:14 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058746)[0;0m ERROR 12-05 15:02:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4058746)[0;0m ERROR 12-05 15:02:14 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4058746)[0;0m ERROR 12-05 15:02:14 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058746)[0;0m ERROR 12-05 15:02:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4058746)[0;0m ERROR 12-05 15:02:14 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4058746)[0;0m ERROR 12-05 15:02:14 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058746)[0;0m ERROR 12-05 15:02:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4058746)[0;0m ERROR 12-05 15:02:14 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4058746)[0;0m ERROR 12-05 15:02:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4058746)[0;0m ERROR 12-05 15:02:14 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4058746)[0;0m ERROR 12-05 15:02:14 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058746)[0;0m ERROR 12-05 15:02:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4058746)[0;0m ERROR 12-05 15:02:14 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4058746)[0;0m ERROR 12-05 15:02:14 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058746)[0;0m ERROR 12-05 15:02:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4058746)[0;0m ERROR 12-05 15:02:14 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4058746)[0;0m ERROR 12-05 15:02:14 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058746)[0;0m ERROR 12-05 15:02:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4058746)[0;0m ERROR 12-05 15:02:14 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4058746)[0;0m ERROR 12-05 15:02:14 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058746)[0;0m ERROR 12-05 15:02:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4058746)[0;0m ERROR 12-05 15:02:14 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4058746)[0;0m ERROR 12-05 15:02:14 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058746)[0;0m ERROR 12-05 15:02:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4058746)[0;0m ERROR 12-05 15:02:14 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4058746)[0;0m ERROR 12-05 15:02:14 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058746)[0;0m ERROR 12-05 15:02:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4058746)[0;0m ERROR 12-05 15:02:14 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4058746)[0;0m ERROR 12-05 15:02:14 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058746)[0;0m ERROR 12-05 15:02:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4058746)[0;0m ERROR 12-05 15:02:14 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4058746)[0;0m ERROR 12-05 15:02:14 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058746)[0;0m ERROR 12-05 15:02:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4058746)[0;0m ERROR 12-05 15:02:14 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4058746)[0;0m ERROR 12-05 15:02:14 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058746)[0;0m ERROR 12-05 15:02:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4058746)[0;0m ERROR 12-05 15:02:14 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4058746)[0;0m ERROR 12-05 15:02:14 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058746)[0;0m ERROR 12-05 15:02:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4058746)[0;0m ERROR 12-05 15:02:14 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4058746)[0;0m ERROR 12-05 15:02:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4058746)[0;0m ERROR 12-05 15:02:14 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4058746)[0;0m ERROR 12-05 15:02:14 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058746)[0;0m ERROR 12-05 15:02:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4058746)[0;0m ERROR 12-05 15:02:14 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4058746)[0;0m ERROR 12-05 15:02:14 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058746)[0;0m ERROR 12-05 15:02:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4058746)[0;0m ERROR 12-05 15:02:14 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4058746)[0;0m ERROR 12-05 15:02:14 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058746)[0;0m ERROR 12-05 15:02:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4058746)[0;0m ERROR 12-05 15:02:14 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4058746)[0;0m ERROR 12-05 15:02:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4058746)[0;0m ERROR 12-05 15:02:14 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4058746)[0;0m ERROR 12-05 15:02:14 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058746)[0;0m ERROR 12-05 15:02:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4058746)[0;0m ERROR 12-05 15:02:14 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4058746)[0;0m ERROR 12-05 15:02:14 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058746)[0;0m ERROR 12-05 15:02:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4058746)[0;0m ERROR 12-05 15:02:14 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4058746)[0;0m ERROR 12-05 15:02:14 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058746)[0;0m ERROR 12-05 15:02:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4058746)[0;0m ERROR 12-05 15:02:14 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4058746)[0;0m ERROR 12-05 15:02:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4058746)[0;0m ERROR 12-05 15:02:14 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4058746)[0;0m ERROR 12-05 15:02:14 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058746)[0;0m ERROR 12-05 15:02:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4058746)[0;0m ERROR 12-05 15:02:14 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4058746)[0;0m ERROR 12-05 15:02:14 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058746)[0;0m ERROR 12-05 15:02:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4058746)[0;0m ERROR 12-05 15:02:14 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4058746)[0;0m ERROR 12-05 15:02:14 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058746)[0;0m ERROR 12-05 15:02:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4058746)[0;0m ERROR 12-05 15:02:14 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4058746)[0;0m ERROR 12-05 15:02:14 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058746)[0;0m ERROR 12-05 15:02:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4058746)[0;0m ERROR 12-05 15:02:14 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4058746)[0;0m ERROR 12-05 15:02:14 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7ff61c25d640>' raised:
[1;36m(EngineCore_DP0 pid=4058746)[0;0m ERROR 12-05 15:02:14 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4058746)[0;0m ERROR 12-05 15:02:14 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4058746)[0;0m ERROR 12-05 15:02:14 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4058746)[0;0m ERROR 12-05 15:02:14 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:02:35 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:02:36 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:02:36 [model.py:1745] Using max model len 131072
INFO 12-05 15:02:36 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4058827)[0;0m INFO 12-05 15:02:43 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4058827)[0;0m INFO 12-05 15:02:43 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:51521 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4058827)[0;0m INFO 12-05 15:02:43 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4058827)[0;0m INFO 12-05 15:02:44 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4058827)[0;0m INFO 12-05 15:02:44 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4058827)[0;0m INFO 12-05 15:02:44 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4058827)[0;0m INFO 12-05 15:02:45 [default_loader.py:314] Loading weights took 0.88 seconds
[1;36m(EngineCore_DP0 pid=4058827)[0;0m INFO 12-05 15:02:46 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.415193 seconds
[1;36m(EngineCore_DP0 pid=4058827)[0;0m INFO 12-05 15:02:50 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4058827)[0;0m INFO 12-05 15:02:50 [backends.py:647] Dynamo bytecode transform time: 3.64 s
[1;36m(EngineCore_DP0 pid=4058827)[0;0m ERROR 12-05 15:02:50 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4058827)[0;0m ERROR 12-05 15:02:50 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4058827)[0;0m ERROR 12-05 15:02:50 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4058827)[0;0m ERROR 12-05 15:02:50 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4058827)[0;0m ERROR 12-05 15:02:50 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058827)[0;0m ERROR 12-05 15:02:50 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4058827)[0;0m ERROR 12-05 15:02:50 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4058827)[0;0m ERROR 12-05 15:02:50 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4058827)[0;0m ERROR 12-05 15:02:50 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4058827)[0;0m ERROR 12-05 15:02:50 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058827)[0;0m ERROR 12-05 15:02:50 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4058827)[0;0m ERROR 12-05 15:02:50 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4058827)[0;0m ERROR 12-05 15:02:50 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058827)[0;0m ERROR 12-05 15:02:50 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4058827)[0;0m ERROR 12-05 15:02:50 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4058827)[0;0m ERROR 12-05 15:02:50 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058827)[0;0m ERROR 12-05 15:02:50 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4058827)[0;0m ERROR 12-05 15:02:50 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4058827)[0;0m ERROR 12-05 15:02:50 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058827)[0;0m ERROR 12-05 15:02:50 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4058827)[0;0m ERROR 12-05 15:02:50 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4058827)[0;0m ERROR 12-05 15:02:50 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058827)[0;0m ERROR 12-05 15:02:50 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4058827)[0;0m ERROR 12-05 15:02:50 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4058827)[0;0m ERROR 12-05 15:02:50 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058827)[0;0m ERROR 12-05 15:02:50 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4058827)[0;0m ERROR 12-05 15:02:50 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4058827)[0;0m ERROR 12-05 15:02:50 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4058827)[0;0m ERROR 12-05 15:02:50 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4058827)[0;0m ERROR 12-05 15:02:50 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058827)[0;0m ERROR 12-05 15:02:50 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4058827)[0;0m ERROR 12-05 15:02:50 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4058827)[0;0m ERROR 12-05 15:02:50 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058827)[0;0m ERROR 12-05 15:02:50 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4058827)[0;0m ERROR 12-05 15:02:50 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4058827)[0;0m ERROR 12-05 15:02:50 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058827)[0;0m ERROR 12-05 15:02:50 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4058827)[0;0m ERROR 12-05 15:02:50 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4058827)[0;0m ERROR 12-05 15:02:50 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058827)[0;0m ERROR 12-05 15:02:50 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4058827)[0;0m ERROR 12-05 15:02:50 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4058827)[0;0m ERROR 12-05 15:02:50 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058827)[0;0m ERROR 12-05 15:02:50 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4058827)[0;0m ERROR 12-05 15:02:50 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4058827)[0;0m ERROR 12-05 15:02:50 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058827)[0;0m ERROR 12-05 15:02:50 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4058827)[0;0m ERROR 12-05 15:02:50 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4058827)[0;0m ERROR 12-05 15:02:50 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058827)[0;0m ERROR 12-05 15:02:50 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4058827)[0;0m ERROR 12-05 15:02:50 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4058827)[0;0m ERROR 12-05 15:02:50 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058827)[0;0m ERROR 12-05 15:02:50 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4058827)[0;0m ERROR 12-05 15:02:50 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4058827)[0;0m ERROR 12-05 15:02:50 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058827)[0;0m ERROR 12-05 15:02:50 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4058827)[0;0m ERROR 12-05 15:02:50 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4058827)[0;0m ERROR 12-05 15:02:50 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058827)[0;0m ERROR 12-05 15:02:50 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4058827)[0;0m ERROR 12-05 15:02:50 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4058827)[0;0m ERROR 12-05 15:02:50 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4058827)[0;0m ERROR 12-05 15:02:50 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4058827)[0;0m ERROR 12-05 15:02:50 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058827)[0;0m ERROR 12-05 15:02:50 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4058827)[0;0m ERROR 12-05 15:02:50 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4058827)[0;0m ERROR 12-05 15:02:50 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058827)[0;0m ERROR 12-05 15:02:50 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4058827)[0;0m ERROR 12-05 15:02:50 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4058827)[0;0m ERROR 12-05 15:02:50 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058827)[0;0m ERROR 12-05 15:02:50 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4058827)[0;0m ERROR 12-05 15:02:50 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4058827)[0;0m ERROR 12-05 15:02:50 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4058827)[0;0m ERROR 12-05 15:02:50 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4058827)[0;0m ERROR 12-05 15:02:50 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058827)[0;0m ERROR 12-05 15:02:50 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4058827)[0;0m ERROR 12-05 15:02:50 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4058827)[0;0m ERROR 12-05 15:02:50 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058827)[0;0m ERROR 12-05 15:02:50 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4058827)[0;0m ERROR 12-05 15:02:50 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4058827)[0;0m ERROR 12-05 15:02:50 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058827)[0;0m ERROR 12-05 15:02:50 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4058827)[0;0m ERROR 12-05 15:02:50 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4058827)[0;0m ERROR 12-05 15:02:50 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4058827)[0;0m ERROR 12-05 15:02:50 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4058827)[0;0m ERROR 12-05 15:02:50 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058827)[0;0m ERROR 12-05 15:02:50 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4058827)[0;0m ERROR 12-05 15:02:50 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4058827)[0;0m ERROR 12-05 15:02:50 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058827)[0;0m ERROR 12-05 15:02:50 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4058827)[0;0m ERROR 12-05 15:02:50 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4058827)[0;0m ERROR 12-05 15:02:50 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058827)[0;0m ERROR 12-05 15:02:50 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4058827)[0;0m ERROR 12-05 15:02:50 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4058827)[0;0m ERROR 12-05 15:02:50 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058827)[0;0m ERROR 12-05 15:02:50 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4058827)[0;0m ERROR 12-05 15:02:50 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4058827)[0;0m ERROR 12-05 15:02:50 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7efd0cfcbd40>' raised:
[1;36m(EngineCore_DP0 pid=4058827)[0;0m ERROR 12-05 15:02:50 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4058827)[0;0m ERROR 12-05 15:02:50 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4058827)[0;0m ERROR 12-05 15:02:50 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4058827)[0;0m ERROR 12-05 15:02:50 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:03:11 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:03:12 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:03:12 [model.py:1745] Using max model len 131072
INFO 12-05 15:03:12 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4058913)[0;0m INFO 12-05 15:03:20 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4058913)[0;0m INFO 12-05 15:03:21 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:41337 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4058913)[0;0m INFO 12-05 15:03:21 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4058913)[0;0m INFO 12-05 15:03:22 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4058913)[0;0m INFO 12-05 15:03:22 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4058913)[0;0m INFO 12-05 15:03:22 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4058913)[0;0m INFO 12-05 15:03:23 [default_loader.py:314] Loading weights took 0.88 seconds
[1;36m(EngineCore_DP0 pid=4058913)[0;0m INFO 12-05 15:03:24 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.392828 seconds
[1;36m(EngineCore_DP0 pid=4058913)[0;0m INFO 12-05 15:03:27 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4058913)[0;0m INFO 12-05 15:03:27 [backends.py:647] Dynamo bytecode transform time: 3.63 s
[1;36m(EngineCore_DP0 pid=4058913)[0;0m ERROR 12-05 15:03:28 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4058913)[0;0m ERROR 12-05 15:03:28 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4058913)[0;0m ERROR 12-05 15:03:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4058913)[0;0m ERROR 12-05 15:03:28 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4058913)[0;0m ERROR 12-05 15:03:28 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058913)[0;0m ERROR 12-05 15:03:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4058913)[0;0m ERROR 12-05 15:03:28 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4058913)[0;0m ERROR 12-05 15:03:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4058913)[0;0m ERROR 12-05 15:03:28 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4058913)[0;0m ERROR 12-05 15:03:28 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058913)[0;0m ERROR 12-05 15:03:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4058913)[0;0m ERROR 12-05 15:03:28 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4058913)[0;0m ERROR 12-05 15:03:28 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058913)[0;0m ERROR 12-05 15:03:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4058913)[0;0m ERROR 12-05 15:03:28 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4058913)[0;0m ERROR 12-05 15:03:28 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058913)[0;0m ERROR 12-05 15:03:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4058913)[0;0m ERROR 12-05 15:03:28 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4058913)[0;0m ERROR 12-05 15:03:28 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058913)[0;0m ERROR 12-05 15:03:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4058913)[0;0m ERROR 12-05 15:03:28 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4058913)[0;0m ERROR 12-05 15:03:28 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058913)[0;0m ERROR 12-05 15:03:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4058913)[0;0m ERROR 12-05 15:03:28 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4058913)[0;0m ERROR 12-05 15:03:28 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058913)[0;0m ERROR 12-05 15:03:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4058913)[0;0m ERROR 12-05 15:03:28 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4058913)[0;0m ERROR 12-05 15:03:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4058913)[0;0m ERROR 12-05 15:03:28 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4058913)[0;0m ERROR 12-05 15:03:28 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058913)[0;0m ERROR 12-05 15:03:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4058913)[0;0m ERROR 12-05 15:03:28 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4058913)[0;0m ERROR 12-05 15:03:28 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058913)[0;0m ERROR 12-05 15:03:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4058913)[0;0m ERROR 12-05 15:03:28 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4058913)[0;0m ERROR 12-05 15:03:28 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058913)[0;0m ERROR 12-05 15:03:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4058913)[0;0m ERROR 12-05 15:03:28 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4058913)[0;0m ERROR 12-05 15:03:28 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058913)[0;0m ERROR 12-05 15:03:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4058913)[0;0m ERROR 12-05 15:03:28 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4058913)[0;0m ERROR 12-05 15:03:28 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058913)[0;0m ERROR 12-05 15:03:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4058913)[0;0m ERROR 12-05 15:03:28 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4058913)[0;0m ERROR 12-05 15:03:28 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058913)[0;0m ERROR 12-05 15:03:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4058913)[0;0m ERROR 12-05 15:03:28 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4058913)[0;0m ERROR 12-05 15:03:28 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058913)[0;0m ERROR 12-05 15:03:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4058913)[0;0m ERROR 12-05 15:03:28 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4058913)[0;0m ERROR 12-05 15:03:28 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058913)[0;0m ERROR 12-05 15:03:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4058913)[0;0m ERROR 12-05 15:03:28 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4058913)[0;0m ERROR 12-05 15:03:28 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058913)[0;0m ERROR 12-05 15:03:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4058913)[0;0m ERROR 12-05 15:03:28 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4058913)[0;0m ERROR 12-05 15:03:28 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058913)[0;0m ERROR 12-05 15:03:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4058913)[0;0m ERROR 12-05 15:03:28 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4058913)[0;0m ERROR 12-05 15:03:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4058913)[0;0m ERROR 12-05 15:03:28 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4058913)[0;0m ERROR 12-05 15:03:28 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058913)[0;0m ERROR 12-05 15:03:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4058913)[0;0m ERROR 12-05 15:03:28 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4058913)[0;0m ERROR 12-05 15:03:28 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058913)[0;0m ERROR 12-05 15:03:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4058913)[0;0m ERROR 12-05 15:03:28 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4058913)[0;0m ERROR 12-05 15:03:28 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058913)[0;0m ERROR 12-05 15:03:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4058913)[0;0m ERROR 12-05 15:03:28 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4058913)[0;0m ERROR 12-05 15:03:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4058913)[0;0m ERROR 12-05 15:03:28 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4058913)[0;0m ERROR 12-05 15:03:28 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058913)[0;0m ERROR 12-05 15:03:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4058913)[0;0m ERROR 12-05 15:03:28 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4058913)[0;0m ERROR 12-05 15:03:28 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058913)[0;0m ERROR 12-05 15:03:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4058913)[0;0m ERROR 12-05 15:03:28 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4058913)[0;0m ERROR 12-05 15:03:28 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058913)[0;0m ERROR 12-05 15:03:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4058913)[0;0m ERROR 12-05 15:03:28 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4058913)[0;0m ERROR 12-05 15:03:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4058913)[0;0m ERROR 12-05 15:03:28 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4058913)[0;0m ERROR 12-05 15:03:28 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058913)[0;0m ERROR 12-05 15:03:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4058913)[0;0m ERROR 12-05 15:03:28 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4058913)[0;0m ERROR 12-05 15:03:28 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058913)[0;0m ERROR 12-05 15:03:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4058913)[0;0m ERROR 12-05 15:03:28 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4058913)[0;0m ERROR 12-05 15:03:28 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058913)[0;0m ERROR 12-05 15:03:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4058913)[0;0m ERROR 12-05 15:03:28 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4058913)[0;0m ERROR 12-05 15:03:28 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4058913)[0;0m ERROR 12-05 15:03:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4058913)[0;0m ERROR 12-05 15:03:28 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4058913)[0;0m ERROR 12-05 15:03:28 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7f06001ebe90>' raised:
[1;36m(EngineCore_DP0 pid=4058913)[0;0m ERROR 12-05 15:03:28 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4058913)[0;0m ERROR 12-05 15:03:28 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4058913)[0;0m ERROR 12-05 15:03:28 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4058913)[0;0m ERROR 12-05 15:03:28 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:03:49 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:03:49 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:03:49 [model.py:1745] Using max model len 131072
INFO 12-05 15:03:49 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4059000)[0;0m INFO 12-05 15:03:56 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4059000)[0;0m INFO 12-05 15:03:57 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:36517 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4059000)[0;0m INFO 12-05 15:03:57 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4059000)[0;0m INFO 12-05 15:03:57 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4059000)[0;0m INFO 12-05 15:03:58 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4059000)[0;0m INFO 12-05 15:03:58 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4059000)[0;0m INFO 12-05 15:03:59 [default_loader.py:314] Loading weights took 0.88 seconds
[1;36m(EngineCore_DP0 pid=4059000)[0;0m INFO 12-05 15:04:00 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.413635 seconds
[1;36m(EngineCore_DP0 pid=4059000)[0;0m INFO 12-05 15:04:03 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4059000)[0;0m INFO 12-05 15:04:03 [backends.py:647] Dynamo bytecode transform time: 3.60 s
[1;36m(EngineCore_DP0 pid=4059000)[0;0m ERROR 12-05 15:04:04 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4059000)[0;0m ERROR 12-05 15:04:04 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4059000)[0;0m ERROR 12-05 15:04:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4059000)[0;0m ERROR 12-05 15:04:04 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4059000)[0;0m ERROR 12-05 15:04:04 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059000)[0;0m ERROR 12-05 15:04:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4059000)[0;0m ERROR 12-05 15:04:04 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4059000)[0;0m ERROR 12-05 15:04:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4059000)[0;0m ERROR 12-05 15:04:04 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4059000)[0;0m ERROR 12-05 15:04:04 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059000)[0;0m ERROR 12-05 15:04:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4059000)[0;0m ERROR 12-05 15:04:04 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4059000)[0;0m ERROR 12-05 15:04:04 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059000)[0;0m ERROR 12-05 15:04:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4059000)[0;0m ERROR 12-05 15:04:04 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4059000)[0;0m ERROR 12-05 15:04:04 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059000)[0;0m ERROR 12-05 15:04:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4059000)[0;0m ERROR 12-05 15:04:04 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4059000)[0;0m ERROR 12-05 15:04:04 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059000)[0;0m ERROR 12-05 15:04:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4059000)[0;0m ERROR 12-05 15:04:04 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4059000)[0;0m ERROR 12-05 15:04:04 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059000)[0;0m ERROR 12-05 15:04:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4059000)[0;0m ERROR 12-05 15:04:04 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4059000)[0;0m ERROR 12-05 15:04:04 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059000)[0;0m ERROR 12-05 15:04:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4059000)[0;0m ERROR 12-05 15:04:04 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4059000)[0;0m ERROR 12-05 15:04:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4059000)[0;0m ERROR 12-05 15:04:04 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4059000)[0;0m ERROR 12-05 15:04:04 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059000)[0;0m ERROR 12-05 15:04:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4059000)[0;0m ERROR 12-05 15:04:04 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4059000)[0;0m ERROR 12-05 15:04:04 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059000)[0;0m ERROR 12-05 15:04:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4059000)[0;0m ERROR 12-05 15:04:04 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4059000)[0;0m ERROR 12-05 15:04:04 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059000)[0;0m ERROR 12-05 15:04:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4059000)[0;0m ERROR 12-05 15:04:04 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4059000)[0;0m ERROR 12-05 15:04:04 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059000)[0;0m ERROR 12-05 15:04:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4059000)[0;0m ERROR 12-05 15:04:04 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4059000)[0;0m ERROR 12-05 15:04:04 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059000)[0;0m ERROR 12-05 15:04:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4059000)[0;0m ERROR 12-05 15:04:04 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4059000)[0;0m ERROR 12-05 15:04:04 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059000)[0;0m ERROR 12-05 15:04:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4059000)[0;0m ERROR 12-05 15:04:04 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4059000)[0;0m ERROR 12-05 15:04:04 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059000)[0;0m ERROR 12-05 15:04:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4059000)[0;0m ERROR 12-05 15:04:04 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4059000)[0;0m ERROR 12-05 15:04:04 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059000)[0;0m ERROR 12-05 15:04:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4059000)[0;0m ERROR 12-05 15:04:04 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4059000)[0;0m ERROR 12-05 15:04:04 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059000)[0;0m ERROR 12-05 15:04:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4059000)[0;0m ERROR 12-05 15:04:04 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4059000)[0;0m ERROR 12-05 15:04:04 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059000)[0;0m ERROR 12-05 15:04:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4059000)[0;0m ERROR 12-05 15:04:04 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4059000)[0;0m ERROR 12-05 15:04:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4059000)[0;0m ERROR 12-05 15:04:04 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4059000)[0;0m ERROR 12-05 15:04:04 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059000)[0;0m ERROR 12-05 15:04:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4059000)[0;0m ERROR 12-05 15:04:04 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4059000)[0;0m ERROR 12-05 15:04:04 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059000)[0;0m ERROR 12-05 15:04:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4059000)[0;0m ERROR 12-05 15:04:04 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4059000)[0;0m ERROR 12-05 15:04:04 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059000)[0;0m ERROR 12-05 15:04:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4059000)[0;0m ERROR 12-05 15:04:04 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4059000)[0;0m ERROR 12-05 15:04:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4059000)[0;0m ERROR 12-05 15:04:04 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4059000)[0;0m ERROR 12-05 15:04:04 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059000)[0;0m ERROR 12-05 15:04:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4059000)[0;0m ERROR 12-05 15:04:04 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4059000)[0;0m ERROR 12-05 15:04:04 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059000)[0;0m ERROR 12-05 15:04:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4059000)[0;0m ERROR 12-05 15:04:04 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4059000)[0;0m ERROR 12-05 15:04:04 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059000)[0;0m ERROR 12-05 15:04:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4059000)[0;0m ERROR 12-05 15:04:04 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4059000)[0;0m ERROR 12-05 15:04:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4059000)[0;0m ERROR 12-05 15:04:04 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4059000)[0;0m ERROR 12-05 15:04:04 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059000)[0;0m ERROR 12-05 15:04:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4059000)[0;0m ERROR 12-05 15:04:04 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4059000)[0;0m ERROR 12-05 15:04:04 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059000)[0;0m ERROR 12-05 15:04:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4059000)[0;0m ERROR 12-05 15:04:04 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4059000)[0;0m ERROR 12-05 15:04:04 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059000)[0;0m ERROR 12-05 15:04:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4059000)[0;0m ERROR 12-05 15:04:04 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4059000)[0;0m ERROR 12-05 15:04:04 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059000)[0;0m ERROR 12-05 15:04:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4059000)[0;0m ERROR 12-05 15:04:04 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4059000)[0;0m ERROR 12-05 15:04:04 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7f2c183df860>' raised:
[1;36m(EngineCore_DP0 pid=4059000)[0;0m ERROR 12-05 15:04:04 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4059000)[0;0m ERROR 12-05 15:04:04 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4059000)[0;0m ERROR 12-05 15:04:04 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4059000)[0;0m ERROR 12-05 15:04:04 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:04:25 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:04:25 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:04:25 [model.py:1745] Using max model len 131072
INFO 12-05 15:04:25 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4059079)[0;0m INFO 12-05 15:04:33 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4059079)[0;0m INFO 12-05 15:04:33 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:41405 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4059079)[0;0m INFO 12-05 15:04:33 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4059079)[0;0m INFO 12-05 15:04:34 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4059079)[0;0m INFO 12-05 15:04:34 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4059079)[0;0m INFO 12-05 15:04:34 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4059079)[0;0m INFO 12-05 15:04:35 [default_loader.py:314] Loading weights took 0.87 seconds
[1;36m(EngineCore_DP0 pid=4059079)[0;0m INFO 12-05 15:04:36 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.423332 seconds
[1;36m(EngineCore_DP0 pid=4059079)[0;0m INFO 12-05 15:04:39 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4059079)[0;0m INFO 12-05 15:04:39 [backends.py:647] Dynamo bytecode transform time: 3.55 s
[1;36m(EngineCore_DP0 pid=4059079)[0;0m ERROR 12-05 15:04:40 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4059079)[0;0m ERROR 12-05 15:04:40 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4059079)[0;0m ERROR 12-05 15:04:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4059079)[0;0m ERROR 12-05 15:04:40 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4059079)[0;0m ERROR 12-05 15:04:40 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059079)[0;0m ERROR 12-05 15:04:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4059079)[0;0m ERROR 12-05 15:04:40 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4059079)[0;0m ERROR 12-05 15:04:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4059079)[0;0m ERROR 12-05 15:04:40 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4059079)[0;0m ERROR 12-05 15:04:40 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059079)[0;0m ERROR 12-05 15:04:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4059079)[0;0m ERROR 12-05 15:04:40 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4059079)[0;0m ERROR 12-05 15:04:40 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059079)[0;0m ERROR 12-05 15:04:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4059079)[0;0m ERROR 12-05 15:04:40 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4059079)[0;0m ERROR 12-05 15:04:40 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059079)[0;0m ERROR 12-05 15:04:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4059079)[0;0m ERROR 12-05 15:04:40 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4059079)[0;0m ERROR 12-05 15:04:40 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059079)[0;0m ERROR 12-05 15:04:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4059079)[0;0m ERROR 12-05 15:04:40 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4059079)[0;0m ERROR 12-05 15:04:40 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059079)[0;0m ERROR 12-05 15:04:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4059079)[0;0m ERROR 12-05 15:04:40 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4059079)[0;0m ERROR 12-05 15:04:40 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059079)[0;0m ERROR 12-05 15:04:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4059079)[0;0m ERROR 12-05 15:04:40 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4059079)[0;0m ERROR 12-05 15:04:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4059079)[0;0m ERROR 12-05 15:04:40 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4059079)[0;0m ERROR 12-05 15:04:40 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059079)[0;0m ERROR 12-05 15:04:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4059079)[0;0m ERROR 12-05 15:04:40 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4059079)[0;0m ERROR 12-05 15:04:40 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059079)[0;0m ERROR 12-05 15:04:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4059079)[0;0m ERROR 12-05 15:04:40 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4059079)[0;0m ERROR 12-05 15:04:40 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059079)[0;0m ERROR 12-05 15:04:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4059079)[0;0m ERROR 12-05 15:04:40 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4059079)[0;0m ERROR 12-05 15:04:40 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059079)[0;0m ERROR 12-05 15:04:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4059079)[0;0m ERROR 12-05 15:04:40 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4059079)[0;0m ERROR 12-05 15:04:40 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059079)[0;0m ERROR 12-05 15:04:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4059079)[0;0m ERROR 12-05 15:04:40 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4059079)[0;0m ERROR 12-05 15:04:40 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059079)[0;0m ERROR 12-05 15:04:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4059079)[0;0m ERROR 12-05 15:04:40 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4059079)[0;0m ERROR 12-05 15:04:40 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059079)[0;0m ERROR 12-05 15:04:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4059079)[0;0m ERROR 12-05 15:04:40 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4059079)[0;0m ERROR 12-05 15:04:40 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059079)[0;0m ERROR 12-05 15:04:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4059079)[0;0m ERROR 12-05 15:04:40 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4059079)[0;0m ERROR 12-05 15:04:40 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059079)[0;0m ERROR 12-05 15:04:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4059079)[0;0m ERROR 12-05 15:04:40 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4059079)[0;0m ERROR 12-05 15:04:40 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059079)[0;0m ERROR 12-05 15:04:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4059079)[0;0m ERROR 12-05 15:04:40 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4059079)[0;0m ERROR 12-05 15:04:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4059079)[0;0m ERROR 12-05 15:04:40 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4059079)[0;0m ERROR 12-05 15:04:40 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059079)[0;0m ERROR 12-05 15:04:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4059079)[0;0m ERROR 12-05 15:04:40 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4059079)[0;0m ERROR 12-05 15:04:40 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059079)[0;0m ERROR 12-05 15:04:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4059079)[0;0m ERROR 12-05 15:04:40 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4059079)[0;0m ERROR 12-05 15:04:40 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059079)[0;0m ERROR 12-05 15:04:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4059079)[0;0m ERROR 12-05 15:04:40 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4059079)[0;0m ERROR 12-05 15:04:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4059079)[0;0m ERROR 12-05 15:04:40 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4059079)[0;0m ERROR 12-05 15:04:40 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059079)[0;0m ERROR 12-05 15:04:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4059079)[0;0m ERROR 12-05 15:04:40 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4059079)[0;0m ERROR 12-05 15:04:40 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059079)[0;0m ERROR 12-05 15:04:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4059079)[0;0m ERROR 12-05 15:04:40 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4059079)[0;0m ERROR 12-05 15:04:40 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059079)[0;0m ERROR 12-05 15:04:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4059079)[0;0m ERROR 12-05 15:04:40 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4059079)[0;0m ERROR 12-05 15:04:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4059079)[0;0m ERROR 12-05 15:04:40 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4059079)[0;0m ERROR 12-05 15:04:40 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059079)[0;0m ERROR 12-05 15:04:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4059079)[0;0m ERROR 12-05 15:04:40 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4059079)[0;0m ERROR 12-05 15:04:40 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059079)[0;0m ERROR 12-05 15:04:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4059079)[0;0m ERROR 12-05 15:04:40 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4059079)[0;0m ERROR 12-05 15:04:40 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059079)[0;0m ERROR 12-05 15:04:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4059079)[0;0m ERROR 12-05 15:04:40 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4059079)[0;0m ERROR 12-05 15:04:40 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059079)[0;0m ERROR 12-05 15:04:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4059079)[0;0m ERROR 12-05 15:04:40 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4059079)[0;0m ERROR 12-05 15:04:40 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7f1f7c1e3f50>' raised:
[1;36m(EngineCore_DP0 pid=4059079)[0;0m ERROR 12-05 15:04:40 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4059079)[0;0m ERROR 12-05 15:04:40 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4059079)[0;0m ERROR 12-05 15:04:40 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4059079)[0;0m ERROR 12-05 15:04:40 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:05:01 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:05:01 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:05:01 [model.py:1745] Using max model len 131072
INFO 12-05 15:05:01 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4059171)[0;0m INFO 12-05 15:05:08 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4059171)[0;0m INFO 12-05 15:05:09 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:54233 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4059171)[0;0m INFO 12-05 15:05:09 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4059171)[0;0m INFO 12-05 15:05:10 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4059171)[0;0m INFO 12-05 15:05:10 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4059171)[0;0m INFO 12-05 15:05:10 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4059171)[0;0m INFO 12-05 15:05:11 [default_loader.py:314] Loading weights took 0.88 seconds
[1;36m(EngineCore_DP0 pid=4059171)[0;0m INFO 12-05 15:05:12 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.418021 seconds
[1;36m(EngineCore_DP0 pid=4059171)[0;0m INFO 12-05 15:05:15 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4059171)[0;0m INFO 12-05 15:05:15 [backends.py:647] Dynamo bytecode transform time: 3.59 s
[1;36m(EngineCore_DP0 pid=4059171)[0;0m ERROR 12-05 15:05:16 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4059171)[0;0m ERROR 12-05 15:05:16 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4059171)[0;0m ERROR 12-05 15:05:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4059171)[0;0m ERROR 12-05 15:05:16 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4059171)[0;0m ERROR 12-05 15:05:16 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059171)[0;0m ERROR 12-05 15:05:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4059171)[0;0m ERROR 12-05 15:05:16 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4059171)[0;0m ERROR 12-05 15:05:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4059171)[0;0m ERROR 12-05 15:05:16 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4059171)[0;0m ERROR 12-05 15:05:16 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059171)[0;0m ERROR 12-05 15:05:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4059171)[0;0m ERROR 12-05 15:05:16 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4059171)[0;0m ERROR 12-05 15:05:16 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059171)[0;0m ERROR 12-05 15:05:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4059171)[0;0m ERROR 12-05 15:05:16 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4059171)[0;0m ERROR 12-05 15:05:16 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059171)[0;0m ERROR 12-05 15:05:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4059171)[0;0m ERROR 12-05 15:05:16 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4059171)[0;0m ERROR 12-05 15:05:16 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059171)[0;0m ERROR 12-05 15:05:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4059171)[0;0m ERROR 12-05 15:05:16 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4059171)[0;0m ERROR 12-05 15:05:16 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059171)[0;0m ERROR 12-05 15:05:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4059171)[0;0m ERROR 12-05 15:05:16 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4059171)[0;0m ERROR 12-05 15:05:16 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059171)[0;0m ERROR 12-05 15:05:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4059171)[0;0m ERROR 12-05 15:05:16 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4059171)[0;0m ERROR 12-05 15:05:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4059171)[0;0m ERROR 12-05 15:05:16 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4059171)[0;0m ERROR 12-05 15:05:16 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059171)[0;0m ERROR 12-05 15:05:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4059171)[0;0m ERROR 12-05 15:05:16 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4059171)[0;0m ERROR 12-05 15:05:16 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059171)[0;0m ERROR 12-05 15:05:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4059171)[0;0m ERROR 12-05 15:05:16 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4059171)[0;0m ERROR 12-05 15:05:16 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059171)[0;0m ERROR 12-05 15:05:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4059171)[0;0m ERROR 12-05 15:05:16 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4059171)[0;0m ERROR 12-05 15:05:16 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059171)[0;0m ERROR 12-05 15:05:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4059171)[0;0m ERROR 12-05 15:05:16 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4059171)[0;0m ERROR 12-05 15:05:16 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059171)[0;0m ERROR 12-05 15:05:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4059171)[0;0m ERROR 12-05 15:05:16 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4059171)[0;0m ERROR 12-05 15:05:16 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059171)[0;0m ERROR 12-05 15:05:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4059171)[0;0m ERROR 12-05 15:05:16 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4059171)[0;0m ERROR 12-05 15:05:16 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059171)[0;0m ERROR 12-05 15:05:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4059171)[0;0m ERROR 12-05 15:05:16 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4059171)[0;0m ERROR 12-05 15:05:16 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059171)[0;0m ERROR 12-05 15:05:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4059171)[0;0m ERROR 12-05 15:05:16 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4059171)[0;0m ERROR 12-05 15:05:16 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059171)[0;0m ERROR 12-05 15:05:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4059171)[0;0m ERROR 12-05 15:05:16 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4059171)[0;0m ERROR 12-05 15:05:16 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059171)[0;0m ERROR 12-05 15:05:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4059171)[0;0m ERROR 12-05 15:05:16 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4059171)[0;0m ERROR 12-05 15:05:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4059171)[0;0m ERROR 12-05 15:05:16 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4059171)[0;0m ERROR 12-05 15:05:16 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059171)[0;0m ERROR 12-05 15:05:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4059171)[0;0m ERROR 12-05 15:05:16 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4059171)[0;0m ERROR 12-05 15:05:16 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059171)[0;0m ERROR 12-05 15:05:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4059171)[0;0m ERROR 12-05 15:05:16 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4059171)[0;0m ERROR 12-05 15:05:16 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059171)[0;0m ERROR 12-05 15:05:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4059171)[0;0m ERROR 12-05 15:05:16 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4059171)[0;0m ERROR 12-05 15:05:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4059171)[0;0m ERROR 12-05 15:05:16 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4059171)[0;0m ERROR 12-05 15:05:16 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059171)[0;0m ERROR 12-05 15:05:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4059171)[0;0m ERROR 12-05 15:05:16 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4059171)[0;0m ERROR 12-05 15:05:16 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059171)[0;0m ERROR 12-05 15:05:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4059171)[0;0m ERROR 12-05 15:05:16 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4059171)[0;0m ERROR 12-05 15:05:16 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059171)[0;0m ERROR 12-05 15:05:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4059171)[0;0m ERROR 12-05 15:05:16 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4059171)[0;0m ERROR 12-05 15:05:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4059171)[0;0m ERROR 12-05 15:05:16 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4059171)[0;0m ERROR 12-05 15:05:16 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059171)[0;0m ERROR 12-05 15:05:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4059171)[0;0m ERROR 12-05 15:05:16 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4059171)[0;0m ERROR 12-05 15:05:16 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059171)[0;0m ERROR 12-05 15:05:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4059171)[0;0m ERROR 12-05 15:05:16 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4059171)[0;0m ERROR 12-05 15:05:16 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059171)[0;0m ERROR 12-05 15:05:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4059171)[0;0m ERROR 12-05 15:05:16 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4059171)[0;0m ERROR 12-05 15:05:16 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059171)[0;0m ERROR 12-05 15:05:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4059171)[0;0m ERROR 12-05 15:05:16 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4059171)[0;0m ERROR 12-05 15:05:16 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7ff0d819d7f0>' raised:
[1;36m(EngineCore_DP0 pid=4059171)[0;0m ERROR 12-05 15:05:16 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4059171)[0;0m ERROR 12-05 15:05:16 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4059171)[0;0m ERROR 12-05 15:05:16 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4059171)[0;0m ERROR 12-05 15:05:16 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:05:37 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:05:37 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:05:37 [model.py:1745] Using max model len 131072
INFO 12-05 15:05:37 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4059271)[0;0m INFO 12-05 15:05:45 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4059271)[0;0m INFO 12-05 15:05:45 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:52189 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4059271)[0;0m INFO 12-05 15:05:45 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4059271)[0;0m INFO 12-05 15:05:46 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4059271)[0;0m INFO 12-05 15:05:46 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4059271)[0;0m INFO 12-05 15:05:46 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4059271)[0;0m INFO 12-05 15:05:47 [default_loader.py:314] Loading weights took 0.87 seconds
[1;36m(EngineCore_DP0 pid=4059271)[0;0m INFO 12-05 15:05:48 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.460973 seconds
[1;36m(EngineCore_DP0 pid=4059271)[0;0m INFO 12-05 15:05:52 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4059271)[0;0m INFO 12-05 15:05:52 [backends.py:647] Dynamo bytecode transform time: 3.62 s
[1;36m(EngineCore_DP0 pid=4059271)[0;0m ERROR 12-05 15:05:52 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4059271)[0;0m ERROR 12-05 15:05:52 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4059271)[0;0m ERROR 12-05 15:05:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4059271)[0;0m ERROR 12-05 15:05:52 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4059271)[0;0m ERROR 12-05 15:05:52 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059271)[0;0m ERROR 12-05 15:05:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4059271)[0;0m ERROR 12-05 15:05:52 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4059271)[0;0m ERROR 12-05 15:05:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4059271)[0;0m ERROR 12-05 15:05:52 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4059271)[0;0m ERROR 12-05 15:05:52 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059271)[0;0m ERROR 12-05 15:05:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4059271)[0;0m ERROR 12-05 15:05:52 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4059271)[0;0m ERROR 12-05 15:05:52 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059271)[0;0m ERROR 12-05 15:05:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4059271)[0;0m ERROR 12-05 15:05:52 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4059271)[0;0m ERROR 12-05 15:05:52 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059271)[0;0m ERROR 12-05 15:05:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4059271)[0;0m ERROR 12-05 15:05:52 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4059271)[0;0m ERROR 12-05 15:05:52 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059271)[0;0m ERROR 12-05 15:05:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4059271)[0;0m ERROR 12-05 15:05:52 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4059271)[0;0m ERROR 12-05 15:05:52 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059271)[0;0m ERROR 12-05 15:05:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4059271)[0;0m ERROR 12-05 15:05:52 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4059271)[0;0m ERROR 12-05 15:05:52 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059271)[0;0m ERROR 12-05 15:05:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4059271)[0;0m ERROR 12-05 15:05:52 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4059271)[0;0m ERROR 12-05 15:05:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4059271)[0;0m ERROR 12-05 15:05:52 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4059271)[0;0m ERROR 12-05 15:05:52 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059271)[0;0m ERROR 12-05 15:05:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4059271)[0;0m ERROR 12-05 15:05:52 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4059271)[0;0m ERROR 12-05 15:05:52 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059271)[0;0m ERROR 12-05 15:05:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4059271)[0;0m ERROR 12-05 15:05:52 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4059271)[0;0m ERROR 12-05 15:05:52 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059271)[0;0m ERROR 12-05 15:05:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4059271)[0;0m ERROR 12-05 15:05:52 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4059271)[0;0m ERROR 12-05 15:05:52 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059271)[0;0m ERROR 12-05 15:05:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4059271)[0;0m ERROR 12-05 15:05:52 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4059271)[0;0m ERROR 12-05 15:05:52 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059271)[0;0m ERROR 12-05 15:05:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4059271)[0;0m ERROR 12-05 15:05:52 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4059271)[0;0m ERROR 12-05 15:05:52 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059271)[0;0m ERROR 12-05 15:05:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4059271)[0;0m ERROR 12-05 15:05:52 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4059271)[0;0m ERROR 12-05 15:05:52 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059271)[0;0m ERROR 12-05 15:05:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4059271)[0;0m ERROR 12-05 15:05:52 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4059271)[0;0m ERROR 12-05 15:05:52 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059271)[0;0m ERROR 12-05 15:05:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4059271)[0;0m ERROR 12-05 15:05:52 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4059271)[0;0m ERROR 12-05 15:05:52 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059271)[0;0m ERROR 12-05 15:05:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4059271)[0;0m ERROR 12-05 15:05:52 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4059271)[0;0m ERROR 12-05 15:05:52 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059271)[0;0m ERROR 12-05 15:05:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4059271)[0;0m ERROR 12-05 15:05:52 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4059271)[0;0m ERROR 12-05 15:05:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4059271)[0;0m ERROR 12-05 15:05:52 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4059271)[0;0m ERROR 12-05 15:05:52 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059271)[0;0m ERROR 12-05 15:05:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4059271)[0;0m ERROR 12-05 15:05:52 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4059271)[0;0m ERROR 12-05 15:05:52 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059271)[0;0m ERROR 12-05 15:05:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4059271)[0;0m ERROR 12-05 15:05:52 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4059271)[0;0m ERROR 12-05 15:05:52 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059271)[0;0m ERROR 12-05 15:05:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4059271)[0;0m ERROR 12-05 15:05:52 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4059271)[0;0m ERROR 12-05 15:05:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4059271)[0;0m ERROR 12-05 15:05:52 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4059271)[0;0m ERROR 12-05 15:05:52 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059271)[0;0m ERROR 12-05 15:05:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4059271)[0;0m ERROR 12-05 15:05:52 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4059271)[0;0m ERROR 12-05 15:05:52 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059271)[0;0m ERROR 12-05 15:05:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4059271)[0;0m ERROR 12-05 15:05:52 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4059271)[0;0m ERROR 12-05 15:05:52 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059271)[0;0m ERROR 12-05 15:05:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4059271)[0;0m ERROR 12-05 15:05:52 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4059271)[0;0m ERROR 12-05 15:05:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4059271)[0;0m ERROR 12-05 15:05:52 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4059271)[0;0m ERROR 12-05 15:05:52 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059271)[0;0m ERROR 12-05 15:05:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4059271)[0;0m ERROR 12-05 15:05:52 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4059271)[0;0m ERROR 12-05 15:05:52 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059271)[0;0m ERROR 12-05 15:05:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4059271)[0;0m ERROR 12-05 15:05:52 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4059271)[0;0m ERROR 12-05 15:05:52 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059271)[0;0m ERROR 12-05 15:05:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4059271)[0;0m ERROR 12-05 15:05:52 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4059271)[0;0m ERROR 12-05 15:05:52 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059271)[0;0m ERROR 12-05 15:05:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4059271)[0;0m ERROR 12-05 15:05:52 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4059271)[0;0m ERROR 12-05 15:05:52 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7fb6e8193d40>' raised:
[1;36m(EngineCore_DP0 pid=4059271)[0;0m ERROR 12-05 15:05:52 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4059271)[0;0m ERROR 12-05 15:05:52 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4059271)[0;0m ERROR 12-05 15:05:52 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4059271)[0;0m ERROR 12-05 15:05:52 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:06:13 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:06:13 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:06:13 [model.py:1745] Using max model len 131072
INFO 12-05 15:06:13 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4059452)[0;0m INFO 12-05 15:06:21 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4059452)[0;0m INFO 12-05 15:06:21 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:47249 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4059452)[0;0m INFO 12-05 15:06:21 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4059452)[0;0m INFO 12-05 15:06:22 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4059452)[0;0m INFO 12-05 15:06:22 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4059452)[0;0m INFO 12-05 15:06:22 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4059452)[0;0m INFO 12-05 15:06:23 [default_loader.py:314] Loading weights took 0.87 seconds
[1;36m(EngineCore_DP0 pid=4059452)[0;0m INFO 12-05 15:06:24 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.405608 seconds
[1;36m(EngineCore_DP0 pid=4059452)[0;0m INFO 12-05 15:06:28 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4059452)[0;0m INFO 12-05 15:06:28 [backends.py:647] Dynamo bytecode transform time: 3.86 s
[1;36m(EngineCore_DP0 pid=4059452)[0;0m ERROR 12-05 15:06:28 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4059452)[0;0m ERROR 12-05 15:06:28 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4059452)[0;0m ERROR 12-05 15:06:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4059452)[0;0m ERROR 12-05 15:06:28 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4059452)[0;0m ERROR 12-05 15:06:28 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059452)[0;0m ERROR 12-05 15:06:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4059452)[0;0m ERROR 12-05 15:06:28 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4059452)[0;0m ERROR 12-05 15:06:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4059452)[0;0m ERROR 12-05 15:06:28 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4059452)[0;0m ERROR 12-05 15:06:28 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059452)[0;0m ERROR 12-05 15:06:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4059452)[0;0m ERROR 12-05 15:06:28 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4059452)[0;0m ERROR 12-05 15:06:28 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059452)[0;0m ERROR 12-05 15:06:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4059452)[0;0m ERROR 12-05 15:06:28 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4059452)[0;0m ERROR 12-05 15:06:28 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059452)[0;0m ERROR 12-05 15:06:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4059452)[0;0m ERROR 12-05 15:06:28 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4059452)[0;0m ERROR 12-05 15:06:28 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059452)[0;0m ERROR 12-05 15:06:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4059452)[0;0m ERROR 12-05 15:06:28 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4059452)[0;0m ERROR 12-05 15:06:28 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059452)[0;0m ERROR 12-05 15:06:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4059452)[0;0m ERROR 12-05 15:06:28 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4059452)[0;0m ERROR 12-05 15:06:28 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059452)[0;0m ERROR 12-05 15:06:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4059452)[0;0m ERROR 12-05 15:06:28 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4059452)[0;0m ERROR 12-05 15:06:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4059452)[0;0m ERROR 12-05 15:06:28 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4059452)[0;0m ERROR 12-05 15:06:28 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059452)[0;0m ERROR 12-05 15:06:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4059452)[0;0m ERROR 12-05 15:06:28 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4059452)[0;0m ERROR 12-05 15:06:28 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059452)[0;0m ERROR 12-05 15:06:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4059452)[0;0m ERROR 12-05 15:06:28 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4059452)[0;0m ERROR 12-05 15:06:28 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059452)[0;0m ERROR 12-05 15:06:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4059452)[0;0m ERROR 12-05 15:06:28 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4059452)[0;0m ERROR 12-05 15:06:28 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059452)[0;0m ERROR 12-05 15:06:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4059452)[0;0m ERROR 12-05 15:06:28 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4059452)[0;0m ERROR 12-05 15:06:28 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059452)[0;0m ERROR 12-05 15:06:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4059452)[0;0m ERROR 12-05 15:06:28 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4059452)[0;0m ERROR 12-05 15:06:28 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059452)[0;0m ERROR 12-05 15:06:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4059452)[0;0m ERROR 12-05 15:06:28 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4059452)[0;0m ERROR 12-05 15:06:28 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059452)[0;0m ERROR 12-05 15:06:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4059452)[0;0m ERROR 12-05 15:06:28 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4059452)[0;0m ERROR 12-05 15:06:28 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059452)[0;0m ERROR 12-05 15:06:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4059452)[0;0m ERROR 12-05 15:06:28 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4059452)[0;0m ERROR 12-05 15:06:28 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059452)[0;0m ERROR 12-05 15:06:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4059452)[0;0m ERROR 12-05 15:06:28 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4059452)[0;0m ERROR 12-05 15:06:28 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059452)[0;0m ERROR 12-05 15:06:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4059452)[0;0m ERROR 12-05 15:06:28 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4059452)[0;0m ERROR 12-05 15:06:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4059452)[0;0m ERROR 12-05 15:06:28 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4059452)[0;0m ERROR 12-05 15:06:28 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059452)[0;0m ERROR 12-05 15:06:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4059452)[0;0m ERROR 12-05 15:06:28 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4059452)[0;0m ERROR 12-05 15:06:28 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059452)[0;0m ERROR 12-05 15:06:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4059452)[0;0m ERROR 12-05 15:06:28 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4059452)[0;0m ERROR 12-05 15:06:28 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059452)[0;0m ERROR 12-05 15:06:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4059452)[0;0m ERROR 12-05 15:06:28 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4059452)[0;0m ERROR 12-05 15:06:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4059452)[0;0m ERROR 12-05 15:06:28 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4059452)[0;0m ERROR 12-05 15:06:28 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059452)[0;0m ERROR 12-05 15:06:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4059452)[0;0m ERROR 12-05 15:06:28 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4059452)[0;0m ERROR 12-05 15:06:28 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059452)[0;0m ERROR 12-05 15:06:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4059452)[0;0m ERROR 12-05 15:06:28 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4059452)[0;0m ERROR 12-05 15:06:28 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059452)[0;0m ERROR 12-05 15:06:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4059452)[0;0m ERROR 12-05 15:06:28 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4059452)[0;0m ERROR 12-05 15:06:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4059452)[0;0m ERROR 12-05 15:06:28 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4059452)[0;0m ERROR 12-05 15:06:28 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059452)[0;0m ERROR 12-05 15:06:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4059452)[0;0m ERROR 12-05 15:06:28 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4059452)[0;0m ERROR 12-05 15:06:28 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059452)[0;0m ERROR 12-05 15:06:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4059452)[0;0m ERROR 12-05 15:06:28 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4059452)[0;0m ERROR 12-05 15:06:28 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059452)[0;0m ERROR 12-05 15:06:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4059452)[0;0m ERROR 12-05 15:06:28 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4059452)[0;0m ERROR 12-05 15:06:28 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059452)[0;0m ERROR 12-05 15:06:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4059452)[0;0m ERROR 12-05 15:06:28 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4059452)[0;0m ERROR 12-05 15:06:28 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7f9128354530>' raised:
[1;36m(EngineCore_DP0 pid=4059452)[0;0m ERROR 12-05 15:06:28 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4059452)[0;0m ERROR 12-05 15:06:28 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4059452)[0;0m ERROR 12-05 15:06:28 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4059452)[0;0m ERROR 12-05 15:06:28 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:06:49 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:06:50 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:06:50 [model.py:1745] Using max model len 131072
INFO 12-05 15:06:50 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4059549)[0;0m INFO 12-05 15:06:57 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4059549)[0;0m INFO 12-05 15:06:58 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:37217 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4059549)[0;0m INFO 12-05 15:06:58 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4059549)[0;0m INFO 12-05 15:06:58 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4059549)[0;0m INFO 12-05 15:06:59 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4059549)[0;0m INFO 12-05 15:06:59 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4059549)[0;0m INFO 12-05 15:07:00 [default_loader.py:314] Loading weights took 0.88 seconds
[1;36m(EngineCore_DP0 pid=4059549)[0;0m INFO 12-05 15:07:00 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.446711 seconds
[1;36m(EngineCore_DP0 pid=4059549)[0;0m INFO 12-05 15:07:04 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4059549)[0;0m INFO 12-05 15:07:04 [backends.py:647] Dynamo bytecode transform time: 3.68 s
[1;36m(EngineCore_DP0 pid=4059549)[0;0m ERROR 12-05 15:07:04 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4059549)[0;0m ERROR 12-05 15:07:04 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4059549)[0;0m ERROR 12-05 15:07:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4059549)[0;0m ERROR 12-05 15:07:04 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4059549)[0;0m ERROR 12-05 15:07:04 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059549)[0;0m ERROR 12-05 15:07:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4059549)[0;0m ERROR 12-05 15:07:04 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4059549)[0;0m ERROR 12-05 15:07:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4059549)[0;0m ERROR 12-05 15:07:04 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4059549)[0;0m ERROR 12-05 15:07:04 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059549)[0;0m ERROR 12-05 15:07:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4059549)[0;0m ERROR 12-05 15:07:04 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4059549)[0;0m ERROR 12-05 15:07:04 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059549)[0;0m ERROR 12-05 15:07:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4059549)[0;0m ERROR 12-05 15:07:04 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4059549)[0;0m ERROR 12-05 15:07:04 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059549)[0;0m ERROR 12-05 15:07:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4059549)[0;0m ERROR 12-05 15:07:04 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4059549)[0;0m ERROR 12-05 15:07:04 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059549)[0;0m ERROR 12-05 15:07:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4059549)[0;0m ERROR 12-05 15:07:04 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4059549)[0;0m ERROR 12-05 15:07:04 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059549)[0;0m ERROR 12-05 15:07:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4059549)[0;0m ERROR 12-05 15:07:04 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4059549)[0;0m ERROR 12-05 15:07:04 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059549)[0;0m ERROR 12-05 15:07:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4059549)[0;0m ERROR 12-05 15:07:04 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4059549)[0;0m ERROR 12-05 15:07:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4059549)[0;0m ERROR 12-05 15:07:04 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4059549)[0;0m ERROR 12-05 15:07:04 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059549)[0;0m ERROR 12-05 15:07:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4059549)[0;0m ERROR 12-05 15:07:04 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4059549)[0;0m ERROR 12-05 15:07:04 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059549)[0;0m ERROR 12-05 15:07:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4059549)[0;0m ERROR 12-05 15:07:04 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4059549)[0;0m ERROR 12-05 15:07:04 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059549)[0;0m ERROR 12-05 15:07:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4059549)[0;0m ERROR 12-05 15:07:04 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4059549)[0;0m ERROR 12-05 15:07:04 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059549)[0;0m ERROR 12-05 15:07:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4059549)[0;0m ERROR 12-05 15:07:04 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4059549)[0;0m ERROR 12-05 15:07:04 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059549)[0;0m ERROR 12-05 15:07:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4059549)[0;0m ERROR 12-05 15:07:04 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4059549)[0;0m ERROR 12-05 15:07:04 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059549)[0;0m ERROR 12-05 15:07:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4059549)[0;0m ERROR 12-05 15:07:04 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4059549)[0;0m ERROR 12-05 15:07:04 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059549)[0;0m ERROR 12-05 15:07:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4059549)[0;0m ERROR 12-05 15:07:04 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4059549)[0;0m ERROR 12-05 15:07:04 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059549)[0;0m ERROR 12-05 15:07:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4059549)[0;0m ERROR 12-05 15:07:04 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4059549)[0;0m ERROR 12-05 15:07:04 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059549)[0;0m ERROR 12-05 15:07:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4059549)[0;0m ERROR 12-05 15:07:04 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4059549)[0;0m ERROR 12-05 15:07:04 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059549)[0;0m ERROR 12-05 15:07:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4059549)[0;0m ERROR 12-05 15:07:04 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4059549)[0;0m ERROR 12-05 15:07:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4059549)[0;0m ERROR 12-05 15:07:04 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4059549)[0;0m ERROR 12-05 15:07:04 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059549)[0;0m ERROR 12-05 15:07:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4059549)[0;0m ERROR 12-05 15:07:04 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4059549)[0;0m ERROR 12-05 15:07:04 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059549)[0;0m ERROR 12-05 15:07:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4059549)[0;0m ERROR 12-05 15:07:04 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4059549)[0;0m ERROR 12-05 15:07:04 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059549)[0;0m ERROR 12-05 15:07:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4059549)[0;0m ERROR 12-05 15:07:04 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4059549)[0;0m ERROR 12-05 15:07:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4059549)[0;0m ERROR 12-05 15:07:04 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4059549)[0;0m ERROR 12-05 15:07:04 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059549)[0;0m ERROR 12-05 15:07:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4059549)[0;0m ERROR 12-05 15:07:04 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4059549)[0;0m ERROR 12-05 15:07:04 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059549)[0;0m ERROR 12-05 15:07:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4059549)[0;0m ERROR 12-05 15:07:04 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4059549)[0;0m ERROR 12-05 15:07:04 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059549)[0;0m ERROR 12-05 15:07:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4059549)[0;0m ERROR 12-05 15:07:04 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4059549)[0;0m ERROR 12-05 15:07:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4059549)[0;0m ERROR 12-05 15:07:04 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4059549)[0;0m ERROR 12-05 15:07:04 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059549)[0;0m ERROR 12-05 15:07:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4059549)[0;0m ERROR 12-05 15:07:04 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4059549)[0;0m ERROR 12-05 15:07:04 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059549)[0;0m ERROR 12-05 15:07:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4059549)[0;0m ERROR 12-05 15:07:04 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4059549)[0;0m ERROR 12-05 15:07:04 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059549)[0;0m ERROR 12-05 15:07:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4059549)[0;0m ERROR 12-05 15:07:04 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4059549)[0;0m ERROR 12-05 15:07:04 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059549)[0;0m ERROR 12-05 15:07:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4059549)[0;0m ERROR 12-05 15:07:04 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4059549)[0;0m ERROR 12-05 15:07:04 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7fdad420bcb0>' raised:
[1;36m(EngineCore_DP0 pid=4059549)[0;0m ERROR 12-05 15:07:04 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4059549)[0;0m ERROR 12-05 15:07:04 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4059549)[0;0m ERROR 12-05 15:07:04 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4059549)[0;0m ERROR 12-05 15:07:04 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:07:26 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:07:26 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:07:26 [model.py:1745] Using max model len 131072
INFO 12-05 15:07:26 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4059626)[0;0m INFO 12-05 15:07:34 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4059626)[0;0m INFO 12-05 15:07:35 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:51517 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4059626)[0;0m INFO 12-05 15:07:35 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4059626)[0;0m INFO 12-05 15:07:36 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4059626)[0;0m INFO 12-05 15:07:36 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4059626)[0;0m INFO 12-05 15:07:36 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4059626)[0;0m INFO 12-05 15:07:37 [default_loader.py:314] Loading weights took 0.87 seconds
[1;36m(EngineCore_DP0 pid=4059626)[0;0m INFO 12-05 15:07:38 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.449703 seconds
[1;36m(EngineCore_DP0 pid=4059626)[0;0m INFO 12-05 15:07:42 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4059626)[0;0m INFO 12-05 15:07:42 [backends.py:647] Dynamo bytecode transform time: 3.65 s
[1;36m(EngineCore_DP0 pid=4059626)[0;0m ERROR 12-05 15:07:42 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4059626)[0;0m ERROR 12-05 15:07:42 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4059626)[0;0m ERROR 12-05 15:07:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4059626)[0;0m ERROR 12-05 15:07:42 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4059626)[0;0m ERROR 12-05 15:07:42 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059626)[0;0m ERROR 12-05 15:07:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4059626)[0;0m ERROR 12-05 15:07:42 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4059626)[0;0m ERROR 12-05 15:07:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4059626)[0;0m ERROR 12-05 15:07:42 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4059626)[0;0m ERROR 12-05 15:07:42 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059626)[0;0m ERROR 12-05 15:07:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4059626)[0;0m ERROR 12-05 15:07:42 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4059626)[0;0m ERROR 12-05 15:07:42 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059626)[0;0m ERROR 12-05 15:07:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4059626)[0;0m ERROR 12-05 15:07:42 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4059626)[0;0m ERROR 12-05 15:07:42 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059626)[0;0m ERROR 12-05 15:07:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4059626)[0;0m ERROR 12-05 15:07:42 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4059626)[0;0m ERROR 12-05 15:07:42 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059626)[0;0m ERROR 12-05 15:07:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4059626)[0;0m ERROR 12-05 15:07:42 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4059626)[0;0m ERROR 12-05 15:07:42 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059626)[0;0m ERROR 12-05 15:07:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4059626)[0;0m ERROR 12-05 15:07:42 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4059626)[0;0m ERROR 12-05 15:07:42 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059626)[0;0m ERROR 12-05 15:07:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4059626)[0;0m ERROR 12-05 15:07:42 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4059626)[0;0m ERROR 12-05 15:07:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4059626)[0;0m ERROR 12-05 15:07:42 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4059626)[0;0m ERROR 12-05 15:07:42 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059626)[0;0m ERROR 12-05 15:07:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4059626)[0;0m ERROR 12-05 15:07:42 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4059626)[0;0m ERROR 12-05 15:07:42 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059626)[0;0m ERROR 12-05 15:07:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4059626)[0;0m ERROR 12-05 15:07:42 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4059626)[0;0m ERROR 12-05 15:07:42 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059626)[0;0m ERROR 12-05 15:07:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4059626)[0;0m ERROR 12-05 15:07:42 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4059626)[0;0m ERROR 12-05 15:07:42 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059626)[0;0m ERROR 12-05 15:07:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4059626)[0;0m ERROR 12-05 15:07:42 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4059626)[0;0m ERROR 12-05 15:07:42 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059626)[0;0m ERROR 12-05 15:07:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4059626)[0;0m ERROR 12-05 15:07:42 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4059626)[0;0m ERROR 12-05 15:07:42 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059626)[0;0m ERROR 12-05 15:07:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4059626)[0;0m ERROR 12-05 15:07:42 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4059626)[0;0m ERROR 12-05 15:07:42 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059626)[0;0m ERROR 12-05 15:07:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4059626)[0;0m ERROR 12-05 15:07:42 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4059626)[0;0m ERROR 12-05 15:07:42 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059626)[0;0m ERROR 12-05 15:07:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4059626)[0;0m ERROR 12-05 15:07:42 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4059626)[0;0m ERROR 12-05 15:07:42 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059626)[0;0m ERROR 12-05 15:07:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4059626)[0;0m ERROR 12-05 15:07:42 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4059626)[0;0m ERROR 12-05 15:07:42 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059626)[0;0m ERROR 12-05 15:07:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4059626)[0;0m ERROR 12-05 15:07:42 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4059626)[0;0m ERROR 12-05 15:07:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4059626)[0;0m ERROR 12-05 15:07:42 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4059626)[0;0m ERROR 12-05 15:07:42 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059626)[0;0m ERROR 12-05 15:07:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4059626)[0;0m ERROR 12-05 15:07:42 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4059626)[0;0m ERROR 12-05 15:07:42 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059626)[0;0m ERROR 12-05 15:07:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4059626)[0;0m ERROR 12-05 15:07:42 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4059626)[0;0m ERROR 12-05 15:07:42 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059626)[0;0m ERROR 12-05 15:07:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4059626)[0;0m ERROR 12-05 15:07:42 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4059626)[0;0m ERROR 12-05 15:07:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4059626)[0;0m ERROR 12-05 15:07:42 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4059626)[0;0m ERROR 12-05 15:07:42 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059626)[0;0m ERROR 12-05 15:07:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4059626)[0;0m ERROR 12-05 15:07:42 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4059626)[0;0m ERROR 12-05 15:07:42 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059626)[0;0m ERROR 12-05 15:07:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4059626)[0;0m ERROR 12-05 15:07:42 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4059626)[0;0m ERROR 12-05 15:07:42 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059626)[0;0m ERROR 12-05 15:07:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4059626)[0;0m ERROR 12-05 15:07:42 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4059626)[0;0m ERROR 12-05 15:07:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4059626)[0;0m ERROR 12-05 15:07:42 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4059626)[0;0m ERROR 12-05 15:07:42 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059626)[0;0m ERROR 12-05 15:07:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4059626)[0;0m ERROR 12-05 15:07:42 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4059626)[0;0m ERROR 12-05 15:07:42 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059626)[0;0m ERROR 12-05 15:07:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4059626)[0;0m ERROR 12-05 15:07:42 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4059626)[0;0m ERROR 12-05 15:07:42 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059626)[0;0m ERROR 12-05 15:07:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4059626)[0;0m ERROR 12-05 15:07:42 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4059626)[0;0m ERROR 12-05 15:07:42 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059626)[0;0m ERROR 12-05 15:07:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4059626)[0;0m ERROR 12-05 15:07:42 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4059626)[0;0m ERROR 12-05 15:07:42 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7feb20268200>' raised:
[1;36m(EngineCore_DP0 pid=4059626)[0;0m ERROR 12-05 15:07:42 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4059626)[0;0m ERROR 12-05 15:07:42 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4059626)[0;0m ERROR 12-05 15:07:42 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4059626)[0;0m ERROR 12-05 15:07:42 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:08:03 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:08:03 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:08:03 [model.py:1745] Using max model len 131072
INFO 12-05 15:08:03 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4059714)[0;0m INFO 12-05 15:08:11 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4059714)[0;0m INFO 12-05 15:08:12 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:36875 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4059714)[0;0m INFO 12-05 15:08:12 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4059714)[0;0m INFO 12-05 15:08:12 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4059714)[0;0m INFO 12-05 15:08:12 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4059714)[0;0m INFO 12-05 15:08:12 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4059714)[0;0m INFO 12-05 15:08:13 [default_loader.py:314] Loading weights took 0.87 seconds
[1;36m(EngineCore_DP0 pid=4059714)[0;0m INFO 12-05 15:08:14 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.405954 seconds
[1;36m(EngineCore_DP0 pid=4059714)[0;0m INFO 12-05 15:08:18 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4059714)[0;0m INFO 12-05 15:08:18 [backends.py:647] Dynamo bytecode transform time: 3.57 s
[1;36m(EngineCore_DP0 pid=4059714)[0;0m ERROR 12-05 15:08:18 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4059714)[0;0m ERROR 12-05 15:08:18 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4059714)[0;0m ERROR 12-05 15:08:18 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4059714)[0;0m ERROR 12-05 15:08:18 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4059714)[0;0m ERROR 12-05 15:08:18 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059714)[0;0m ERROR 12-05 15:08:18 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4059714)[0;0m ERROR 12-05 15:08:18 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4059714)[0;0m ERROR 12-05 15:08:18 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4059714)[0;0m ERROR 12-05 15:08:18 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4059714)[0;0m ERROR 12-05 15:08:18 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059714)[0;0m ERROR 12-05 15:08:18 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4059714)[0;0m ERROR 12-05 15:08:18 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4059714)[0;0m ERROR 12-05 15:08:18 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059714)[0;0m ERROR 12-05 15:08:18 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4059714)[0;0m ERROR 12-05 15:08:18 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4059714)[0;0m ERROR 12-05 15:08:18 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059714)[0;0m ERROR 12-05 15:08:18 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4059714)[0;0m ERROR 12-05 15:08:18 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4059714)[0;0m ERROR 12-05 15:08:18 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059714)[0;0m ERROR 12-05 15:08:18 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4059714)[0;0m ERROR 12-05 15:08:18 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4059714)[0;0m ERROR 12-05 15:08:18 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059714)[0;0m ERROR 12-05 15:08:18 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4059714)[0;0m ERROR 12-05 15:08:18 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4059714)[0;0m ERROR 12-05 15:08:18 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059714)[0;0m ERROR 12-05 15:08:18 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4059714)[0;0m ERROR 12-05 15:08:18 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4059714)[0;0m ERROR 12-05 15:08:18 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4059714)[0;0m ERROR 12-05 15:08:18 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4059714)[0;0m ERROR 12-05 15:08:18 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059714)[0;0m ERROR 12-05 15:08:18 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4059714)[0;0m ERROR 12-05 15:08:18 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4059714)[0;0m ERROR 12-05 15:08:18 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059714)[0;0m ERROR 12-05 15:08:18 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4059714)[0;0m ERROR 12-05 15:08:18 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4059714)[0;0m ERROR 12-05 15:08:18 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059714)[0;0m ERROR 12-05 15:08:18 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4059714)[0;0m ERROR 12-05 15:08:18 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4059714)[0;0m ERROR 12-05 15:08:18 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059714)[0;0m ERROR 12-05 15:08:18 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4059714)[0;0m ERROR 12-05 15:08:18 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4059714)[0;0m ERROR 12-05 15:08:18 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059714)[0;0m ERROR 12-05 15:08:18 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4059714)[0;0m ERROR 12-05 15:08:18 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4059714)[0;0m ERROR 12-05 15:08:18 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059714)[0;0m ERROR 12-05 15:08:18 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4059714)[0;0m ERROR 12-05 15:08:18 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4059714)[0;0m ERROR 12-05 15:08:18 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059714)[0;0m ERROR 12-05 15:08:18 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4059714)[0;0m ERROR 12-05 15:08:18 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4059714)[0;0m ERROR 12-05 15:08:18 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059714)[0;0m ERROR 12-05 15:08:18 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4059714)[0;0m ERROR 12-05 15:08:18 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4059714)[0;0m ERROR 12-05 15:08:18 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059714)[0;0m ERROR 12-05 15:08:18 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4059714)[0;0m ERROR 12-05 15:08:18 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4059714)[0;0m ERROR 12-05 15:08:18 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059714)[0;0m ERROR 12-05 15:08:18 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4059714)[0;0m ERROR 12-05 15:08:18 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4059714)[0;0m ERROR 12-05 15:08:18 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4059714)[0;0m ERROR 12-05 15:08:18 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4059714)[0;0m ERROR 12-05 15:08:18 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059714)[0;0m ERROR 12-05 15:08:18 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4059714)[0;0m ERROR 12-05 15:08:18 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4059714)[0;0m ERROR 12-05 15:08:18 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059714)[0;0m ERROR 12-05 15:08:18 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4059714)[0;0m ERROR 12-05 15:08:18 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4059714)[0;0m ERROR 12-05 15:08:18 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059714)[0;0m ERROR 12-05 15:08:18 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4059714)[0;0m ERROR 12-05 15:08:18 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4059714)[0;0m ERROR 12-05 15:08:18 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4059714)[0;0m ERROR 12-05 15:08:18 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4059714)[0;0m ERROR 12-05 15:08:18 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059714)[0;0m ERROR 12-05 15:08:18 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4059714)[0;0m ERROR 12-05 15:08:18 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4059714)[0;0m ERROR 12-05 15:08:18 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059714)[0;0m ERROR 12-05 15:08:18 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4059714)[0;0m ERROR 12-05 15:08:18 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4059714)[0;0m ERROR 12-05 15:08:18 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059714)[0;0m ERROR 12-05 15:08:18 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4059714)[0;0m ERROR 12-05 15:08:18 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4059714)[0;0m ERROR 12-05 15:08:18 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4059714)[0;0m ERROR 12-05 15:08:18 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4059714)[0;0m ERROR 12-05 15:08:18 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059714)[0;0m ERROR 12-05 15:08:18 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4059714)[0;0m ERROR 12-05 15:08:18 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4059714)[0;0m ERROR 12-05 15:08:18 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059714)[0;0m ERROR 12-05 15:08:18 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4059714)[0;0m ERROR 12-05 15:08:18 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4059714)[0;0m ERROR 12-05 15:08:18 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059714)[0;0m ERROR 12-05 15:08:18 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4059714)[0;0m ERROR 12-05 15:08:18 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4059714)[0;0m ERROR 12-05 15:08:18 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059714)[0;0m ERROR 12-05 15:08:18 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4059714)[0;0m ERROR 12-05 15:08:18 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4059714)[0;0m ERROR 12-05 15:08:18 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7f6c742b9460>' raised:
[1;36m(EngineCore_DP0 pid=4059714)[0;0m ERROR 12-05 15:08:18 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4059714)[0;0m ERROR 12-05 15:08:18 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4059714)[0;0m ERROR 12-05 15:08:18 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4059714)[0;0m ERROR 12-05 15:08:18 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:08:39 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:08:39 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:08:39 [model.py:1745] Using max model len 131072
INFO 12-05 15:08:39 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4059794)[0;0m INFO 12-05 15:08:47 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4059794)[0;0m INFO 12-05 15:08:47 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:59153 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4059794)[0;0m INFO 12-05 15:08:47 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4059794)[0;0m INFO 12-05 15:08:48 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4059794)[0;0m INFO 12-05 15:08:48 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4059794)[0;0m INFO 12-05 15:08:48 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4059794)[0;0m INFO 12-05 15:08:49 [default_loader.py:314] Loading weights took 0.88 seconds
[1;36m(EngineCore_DP0 pid=4059794)[0;0m INFO 12-05 15:08:50 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.395832 seconds
[1;36m(EngineCore_DP0 pid=4059794)[0;0m INFO 12-05 15:08:54 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4059794)[0;0m INFO 12-05 15:08:54 [backends.py:647] Dynamo bytecode transform time: 3.60 s
[1;36m(EngineCore_DP0 pid=4059794)[0;0m ERROR 12-05 15:08:54 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4059794)[0;0m ERROR 12-05 15:08:54 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4059794)[0;0m ERROR 12-05 15:08:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4059794)[0;0m ERROR 12-05 15:08:54 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4059794)[0;0m ERROR 12-05 15:08:54 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059794)[0;0m ERROR 12-05 15:08:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4059794)[0;0m ERROR 12-05 15:08:54 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4059794)[0;0m ERROR 12-05 15:08:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4059794)[0;0m ERROR 12-05 15:08:54 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4059794)[0;0m ERROR 12-05 15:08:54 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059794)[0;0m ERROR 12-05 15:08:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4059794)[0;0m ERROR 12-05 15:08:54 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4059794)[0;0m ERROR 12-05 15:08:54 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059794)[0;0m ERROR 12-05 15:08:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4059794)[0;0m ERROR 12-05 15:08:54 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4059794)[0;0m ERROR 12-05 15:08:54 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059794)[0;0m ERROR 12-05 15:08:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4059794)[0;0m ERROR 12-05 15:08:54 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4059794)[0;0m ERROR 12-05 15:08:54 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059794)[0;0m ERROR 12-05 15:08:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4059794)[0;0m ERROR 12-05 15:08:54 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4059794)[0;0m ERROR 12-05 15:08:54 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059794)[0;0m ERROR 12-05 15:08:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4059794)[0;0m ERROR 12-05 15:08:54 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4059794)[0;0m ERROR 12-05 15:08:54 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059794)[0;0m ERROR 12-05 15:08:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4059794)[0;0m ERROR 12-05 15:08:54 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4059794)[0;0m ERROR 12-05 15:08:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4059794)[0;0m ERROR 12-05 15:08:54 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4059794)[0;0m ERROR 12-05 15:08:54 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059794)[0;0m ERROR 12-05 15:08:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4059794)[0;0m ERROR 12-05 15:08:54 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4059794)[0;0m ERROR 12-05 15:08:54 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059794)[0;0m ERROR 12-05 15:08:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4059794)[0;0m ERROR 12-05 15:08:54 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4059794)[0;0m ERROR 12-05 15:08:54 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059794)[0;0m ERROR 12-05 15:08:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4059794)[0;0m ERROR 12-05 15:08:54 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4059794)[0;0m ERROR 12-05 15:08:54 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059794)[0;0m ERROR 12-05 15:08:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4059794)[0;0m ERROR 12-05 15:08:54 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4059794)[0;0m ERROR 12-05 15:08:54 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059794)[0;0m ERROR 12-05 15:08:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4059794)[0;0m ERROR 12-05 15:08:54 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4059794)[0;0m ERROR 12-05 15:08:54 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059794)[0;0m ERROR 12-05 15:08:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4059794)[0;0m ERROR 12-05 15:08:54 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4059794)[0;0m ERROR 12-05 15:08:54 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059794)[0;0m ERROR 12-05 15:08:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4059794)[0;0m ERROR 12-05 15:08:54 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4059794)[0;0m ERROR 12-05 15:08:54 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059794)[0;0m ERROR 12-05 15:08:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4059794)[0;0m ERROR 12-05 15:08:54 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4059794)[0;0m ERROR 12-05 15:08:54 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059794)[0;0m ERROR 12-05 15:08:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4059794)[0;0m ERROR 12-05 15:08:54 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4059794)[0;0m ERROR 12-05 15:08:54 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059794)[0;0m ERROR 12-05 15:08:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4059794)[0;0m ERROR 12-05 15:08:54 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4059794)[0;0m ERROR 12-05 15:08:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4059794)[0;0m ERROR 12-05 15:08:54 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4059794)[0;0m ERROR 12-05 15:08:54 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059794)[0;0m ERROR 12-05 15:08:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4059794)[0;0m ERROR 12-05 15:08:54 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4059794)[0;0m ERROR 12-05 15:08:54 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059794)[0;0m ERROR 12-05 15:08:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4059794)[0;0m ERROR 12-05 15:08:54 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4059794)[0;0m ERROR 12-05 15:08:54 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059794)[0;0m ERROR 12-05 15:08:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4059794)[0;0m ERROR 12-05 15:08:54 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4059794)[0;0m ERROR 12-05 15:08:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4059794)[0;0m ERROR 12-05 15:08:54 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4059794)[0;0m ERROR 12-05 15:08:54 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059794)[0;0m ERROR 12-05 15:08:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4059794)[0;0m ERROR 12-05 15:08:54 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4059794)[0;0m ERROR 12-05 15:08:54 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059794)[0;0m ERROR 12-05 15:08:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4059794)[0;0m ERROR 12-05 15:08:54 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4059794)[0;0m ERROR 12-05 15:08:54 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059794)[0;0m ERROR 12-05 15:08:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4059794)[0;0m ERROR 12-05 15:08:54 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4059794)[0;0m ERROR 12-05 15:08:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4059794)[0;0m ERROR 12-05 15:08:54 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4059794)[0;0m ERROR 12-05 15:08:54 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059794)[0;0m ERROR 12-05 15:08:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4059794)[0;0m ERROR 12-05 15:08:54 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4059794)[0;0m ERROR 12-05 15:08:54 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059794)[0;0m ERROR 12-05 15:08:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4059794)[0;0m ERROR 12-05 15:08:54 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4059794)[0;0m ERROR 12-05 15:08:54 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059794)[0;0m ERROR 12-05 15:08:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4059794)[0;0m ERROR 12-05 15:08:54 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4059794)[0;0m ERROR 12-05 15:08:54 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059794)[0;0m ERROR 12-05 15:08:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4059794)[0;0m ERROR 12-05 15:08:54 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4059794)[0;0m ERROR 12-05 15:08:54 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7f23602f3d40>' raised:
[1;36m(EngineCore_DP0 pid=4059794)[0;0m ERROR 12-05 15:08:54 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4059794)[0;0m ERROR 12-05 15:08:54 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4059794)[0;0m ERROR 12-05 15:08:54 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4059794)[0;0m ERROR 12-05 15:08:54 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:09:15 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:09:15 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:09:15 [model.py:1745] Using max model len 131072
INFO 12-05 15:09:15 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4059880)[0;0m INFO 12-05 15:09:23 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4059880)[0;0m INFO 12-05 15:09:23 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:35459 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4059880)[0;0m INFO 12-05 15:09:23 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4059880)[0;0m INFO 12-05 15:09:24 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4059880)[0;0m INFO 12-05 15:09:24 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4059880)[0;0m INFO 12-05 15:09:24 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4059880)[0;0m INFO 12-05 15:09:25 [default_loader.py:314] Loading weights took 0.87 seconds
[1;36m(EngineCore_DP0 pid=4059880)[0;0m INFO 12-05 15:09:26 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.404812 seconds
[1;36m(EngineCore_DP0 pid=4059880)[0;0m INFO 12-05 15:09:29 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4059880)[0;0m INFO 12-05 15:09:29 [backends.py:647] Dynamo bytecode transform time: 3.58 s
[1;36m(EngineCore_DP0 pid=4059880)[0;0m ERROR 12-05 15:09:30 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4059880)[0;0m ERROR 12-05 15:09:30 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4059880)[0;0m ERROR 12-05 15:09:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4059880)[0;0m ERROR 12-05 15:09:30 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4059880)[0;0m ERROR 12-05 15:09:30 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059880)[0;0m ERROR 12-05 15:09:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4059880)[0;0m ERROR 12-05 15:09:30 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4059880)[0;0m ERROR 12-05 15:09:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4059880)[0;0m ERROR 12-05 15:09:30 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4059880)[0;0m ERROR 12-05 15:09:30 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059880)[0;0m ERROR 12-05 15:09:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4059880)[0;0m ERROR 12-05 15:09:30 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4059880)[0;0m ERROR 12-05 15:09:30 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059880)[0;0m ERROR 12-05 15:09:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4059880)[0;0m ERROR 12-05 15:09:30 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4059880)[0;0m ERROR 12-05 15:09:30 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059880)[0;0m ERROR 12-05 15:09:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4059880)[0;0m ERROR 12-05 15:09:30 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4059880)[0;0m ERROR 12-05 15:09:30 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059880)[0;0m ERROR 12-05 15:09:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4059880)[0;0m ERROR 12-05 15:09:30 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4059880)[0;0m ERROR 12-05 15:09:30 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059880)[0;0m ERROR 12-05 15:09:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4059880)[0;0m ERROR 12-05 15:09:30 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4059880)[0;0m ERROR 12-05 15:09:30 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059880)[0;0m ERROR 12-05 15:09:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4059880)[0;0m ERROR 12-05 15:09:30 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4059880)[0;0m ERROR 12-05 15:09:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4059880)[0;0m ERROR 12-05 15:09:30 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4059880)[0;0m ERROR 12-05 15:09:30 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059880)[0;0m ERROR 12-05 15:09:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4059880)[0;0m ERROR 12-05 15:09:30 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4059880)[0;0m ERROR 12-05 15:09:30 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059880)[0;0m ERROR 12-05 15:09:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4059880)[0;0m ERROR 12-05 15:09:30 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4059880)[0;0m ERROR 12-05 15:09:30 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059880)[0;0m ERROR 12-05 15:09:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4059880)[0;0m ERROR 12-05 15:09:30 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4059880)[0;0m ERROR 12-05 15:09:30 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059880)[0;0m ERROR 12-05 15:09:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4059880)[0;0m ERROR 12-05 15:09:30 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4059880)[0;0m ERROR 12-05 15:09:30 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059880)[0;0m ERROR 12-05 15:09:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4059880)[0;0m ERROR 12-05 15:09:30 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4059880)[0;0m ERROR 12-05 15:09:30 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059880)[0;0m ERROR 12-05 15:09:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4059880)[0;0m ERROR 12-05 15:09:30 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4059880)[0;0m ERROR 12-05 15:09:30 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059880)[0;0m ERROR 12-05 15:09:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4059880)[0;0m ERROR 12-05 15:09:30 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4059880)[0;0m ERROR 12-05 15:09:30 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059880)[0;0m ERROR 12-05 15:09:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4059880)[0;0m ERROR 12-05 15:09:30 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4059880)[0;0m ERROR 12-05 15:09:30 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059880)[0;0m ERROR 12-05 15:09:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4059880)[0;0m ERROR 12-05 15:09:30 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4059880)[0;0m ERROR 12-05 15:09:30 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059880)[0;0m ERROR 12-05 15:09:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4059880)[0;0m ERROR 12-05 15:09:30 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4059880)[0;0m ERROR 12-05 15:09:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4059880)[0;0m ERROR 12-05 15:09:30 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4059880)[0;0m ERROR 12-05 15:09:30 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059880)[0;0m ERROR 12-05 15:09:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4059880)[0;0m ERROR 12-05 15:09:30 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4059880)[0;0m ERROR 12-05 15:09:30 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059880)[0;0m ERROR 12-05 15:09:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4059880)[0;0m ERROR 12-05 15:09:30 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4059880)[0;0m ERROR 12-05 15:09:30 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059880)[0;0m ERROR 12-05 15:09:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4059880)[0;0m ERROR 12-05 15:09:30 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4059880)[0;0m ERROR 12-05 15:09:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4059880)[0;0m ERROR 12-05 15:09:30 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4059880)[0;0m ERROR 12-05 15:09:30 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059880)[0;0m ERROR 12-05 15:09:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4059880)[0;0m ERROR 12-05 15:09:30 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4059880)[0;0m ERROR 12-05 15:09:30 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059880)[0;0m ERROR 12-05 15:09:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4059880)[0;0m ERROR 12-05 15:09:30 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4059880)[0;0m ERROR 12-05 15:09:30 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059880)[0;0m ERROR 12-05 15:09:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4059880)[0;0m ERROR 12-05 15:09:30 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4059880)[0;0m ERROR 12-05 15:09:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4059880)[0;0m ERROR 12-05 15:09:30 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4059880)[0;0m ERROR 12-05 15:09:30 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059880)[0;0m ERROR 12-05 15:09:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4059880)[0;0m ERROR 12-05 15:09:30 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4059880)[0;0m ERROR 12-05 15:09:30 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059880)[0;0m ERROR 12-05 15:09:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4059880)[0;0m ERROR 12-05 15:09:30 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4059880)[0;0m ERROR 12-05 15:09:30 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059880)[0;0m ERROR 12-05 15:09:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4059880)[0;0m ERROR 12-05 15:09:30 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4059880)[0;0m ERROR 12-05 15:09:30 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059880)[0;0m ERROR 12-05 15:09:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4059880)[0;0m ERROR 12-05 15:09:30 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4059880)[0;0m ERROR 12-05 15:09:30 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7f236cd0a2d0>' raised:
[1;36m(EngineCore_DP0 pid=4059880)[0;0m ERROR 12-05 15:09:30 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4059880)[0;0m ERROR 12-05 15:09:30 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4059880)[0;0m ERROR 12-05 15:09:30 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4059880)[0;0m ERROR 12-05 15:09:30 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:09:51 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:09:51 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:09:51 [model.py:1745] Using max model len 131072
INFO 12-05 15:09:51 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4059970)[0;0m INFO 12-05 15:09:58 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4059970)[0;0m INFO 12-05 15:09:59 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:38197 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4059970)[0;0m INFO 12-05 15:09:59 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4059970)[0;0m INFO 12-05 15:09:59 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4059970)[0;0m INFO 12-05 15:10:00 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4059970)[0;0m INFO 12-05 15:10:00 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4059970)[0;0m INFO 12-05 15:10:01 [default_loader.py:314] Loading weights took 0.87 seconds
[1;36m(EngineCore_DP0 pid=4059970)[0;0m INFO 12-05 15:10:01 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.422833 seconds
[1;36m(EngineCore_DP0 pid=4059970)[0;0m INFO 12-05 15:10:05 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4059970)[0;0m INFO 12-05 15:10:05 [backends.py:647] Dynamo bytecode transform time: 3.59 s
[1;36m(EngineCore_DP0 pid=4059970)[0;0m ERROR 12-05 15:10:05 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4059970)[0;0m ERROR 12-05 15:10:05 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4059970)[0;0m ERROR 12-05 15:10:05 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4059970)[0;0m ERROR 12-05 15:10:05 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4059970)[0;0m ERROR 12-05 15:10:05 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059970)[0;0m ERROR 12-05 15:10:05 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4059970)[0;0m ERROR 12-05 15:10:05 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4059970)[0;0m ERROR 12-05 15:10:05 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4059970)[0;0m ERROR 12-05 15:10:05 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4059970)[0;0m ERROR 12-05 15:10:05 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059970)[0;0m ERROR 12-05 15:10:05 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4059970)[0;0m ERROR 12-05 15:10:05 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4059970)[0;0m ERROR 12-05 15:10:05 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059970)[0;0m ERROR 12-05 15:10:05 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4059970)[0;0m ERROR 12-05 15:10:05 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4059970)[0;0m ERROR 12-05 15:10:05 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059970)[0;0m ERROR 12-05 15:10:05 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4059970)[0;0m ERROR 12-05 15:10:05 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4059970)[0;0m ERROR 12-05 15:10:05 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059970)[0;0m ERROR 12-05 15:10:05 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4059970)[0;0m ERROR 12-05 15:10:05 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4059970)[0;0m ERROR 12-05 15:10:05 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059970)[0;0m ERROR 12-05 15:10:05 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4059970)[0;0m ERROR 12-05 15:10:05 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4059970)[0;0m ERROR 12-05 15:10:05 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059970)[0;0m ERROR 12-05 15:10:05 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4059970)[0;0m ERROR 12-05 15:10:05 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4059970)[0;0m ERROR 12-05 15:10:05 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4059970)[0;0m ERROR 12-05 15:10:05 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4059970)[0;0m ERROR 12-05 15:10:05 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059970)[0;0m ERROR 12-05 15:10:05 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4059970)[0;0m ERROR 12-05 15:10:05 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4059970)[0;0m ERROR 12-05 15:10:05 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059970)[0;0m ERROR 12-05 15:10:05 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4059970)[0;0m ERROR 12-05 15:10:05 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4059970)[0;0m ERROR 12-05 15:10:05 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059970)[0;0m ERROR 12-05 15:10:05 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4059970)[0;0m ERROR 12-05 15:10:05 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4059970)[0;0m ERROR 12-05 15:10:05 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059970)[0;0m ERROR 12-05 15:10:05 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4059970)[0;0m ERROR 12-05 15:10:05 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4059970)[0;0m ERROR 12-05 15:10:05 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059970)[0;0m ERROR 12-05 15:10:05 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4059970)[0;0m ERROR 12-05 15:10:05 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4059970)[0;0m ERROR 12-05 15:10:05 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059970)[0;0m ERROR 12-05 15:10:05 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4059970)[0;0m ERROR 12-05 15:10:05 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4059970)[0;0m ERROR 12-05 15:10:05 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059970)[0;0m ERROR 12-05 15:10:05 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4059970)[0;0m ERROR 12-05 15:10:05 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4059970)[0;0m ERROR 12-05 15:10:05 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059970)[0;0m ERROR 12-05 15:10:05 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4059970)[0;0m ERROR 12-05 15:10:05 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4059970)[0;0m ERROR 12-05 15:10:05 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059970)[0;0m ERROR 12-05 15:10:05 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4059970)[0;0m ERROR 12-05 15:10:05 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4059970)[0;0m ERROR 12-05 15:10:05 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059970)[0;0m ERROR 12-05 15:10:05 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4059970)[0;0m ERROR 12-05 15:10:05 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4059970)[0;0m ERROR 12-05 15:10:05 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4059970)[0;0m ERROR 12-05 15:10:05 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4059970)[0;0m ERROR 12-05 15:10:05 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059970)[0;0m ERROR 12-05 15:10:05 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4059970)[0;0m ERROR 12-05 15:10:05 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4059970)[0;0m ERROR 12-05 15:10:05 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059970)[0;0m ERROR 12-05 15:10:05 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4059970)[0;0m ERROR 12-05 15:10:05 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4059970)[0;0m ERROR 12-05 15:10:05 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059970)[0;0m ERROR 12-05 15:10:05 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4059970)[0;0m ERROR 12-05 15:10:05 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4059970)[0;0m ERROR 12-05 15:10:05 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4059970)[0;0m ERROR 12-05 15:10:05 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4059970)[0;0m ERROR 12-05 15:10:05 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059970)[0;0m ERROR 12-05 15:10:05 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4059970)[0;0m ERROR 12-05 15:10:05 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4059970)[0;0m ERROR 12-05 15:10:05 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059970)[0;0m ERROR 12-05 15:10:05 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4059970)[0;0m ERROR 12-05 15:10:05 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4059970)[0;0m ERROR 12-05 15:10:05 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059970)[0;0m ERROR 12-05 15:10:05 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4059970)[0;0m ERROR 12-05 15:10:05 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4059970)[0;0m ERROR 12-05 15:10:05 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4059970)[0;0m ERROR 12-05 15:10:05 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4059970)[0;0m ERROR 12-05 15:10:05 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059970)[0;0m ERROR 12-05 15:10:05 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4059970)[0;0m ERROR 12-05 15:10:05 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4059970)[0;0m ERROR 12-05 15:10:05 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059970)[0;0m ERROR 12-05 15:10:05 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4059970)[0;0m ERROR 12-05 15:10:05 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4059970)[0;0m ERROR 12-05 15:10:05 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059970)[0;0m ERROR 12-05 15:10:05 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4059970)[0;0m ERROR 12-05 15:10:05 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4059970)[0;0m ERROR 12-05 15:10:05 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4059970)[0;0m ERROR 12-05 15:10:05 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4059970)[0;0m ERROR 12-05 15:10:05 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4059970)[0;0m ERROR 12-05 15:10:05 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7f4d301250d0>' raised:
[1;36m(EngineCore_DP0 pid=4059970)[0;0m ERROR 12-05 15:10:05 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4059970)[0;0m ERROR 12-05 15:10:05 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4059970)[0;0m ERROR 12-05 15:10:05 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4059970)[0;0m ERROR 12-05 15:10:05 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:10:27 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:10:27 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:10:27 [model.py:1745] Using max model len 131072
INFO 12-05 15:10:27 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4060050)[0;0m INFO 12-05 15:10:34 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4060050)[0;0m INFO 12-05 15:10:35 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:37339 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4060050)[0;0m INFO 12-05 15:10:35 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4060050)[0;0m INFO 12-05 15:10:35 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4060050)[0;0m INFO 12-05 15:10:36 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4060050)[0;0m INFO 12-05 15:10:36 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4060050)[0;0m INFO 12-05 15:10:37 [default_loader.py:314] Loading weights took 0.87 seconds
[1;36m(EngineCore_DP0 pid=4060050)[0;0m INFO 12-05 15:10:38 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.460885 seconds
[1;36m(EngineCore_DP0 pid=4060050)[0;0m INFO 12-05 15:10:41 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4060050)[0;0m INFO 12-05 15:10:41 [backends.py:647] Dynamo bytecode transform time: 3.59 s
[1;36m(EngineCore_DP0 pid=4060050)[0;0m ERROR 12-05 15:10:42 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4060050)[0;0m ERROR 12-05 15:10:42 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4060050)[0;0m ERROR 12-05 15:10:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4060050)[0;0m ERROR 12-05 15:10:42 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4060050)[0;0m ERROR 12-05 15:10:42 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060050)[0;0m ERROR 12-05 15:10:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4060050)[0;0m ERROR 12-05 15:10:42 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4060050)[0;0m ERROR 12-05 15:10:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4060050)[0;0m ERROR 12-05 15:10:42 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4060050)[0;0m ERROR 12-05 15:10:42 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060050)[0;0m ERROR 12-05 15:10:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4060050)[0;0m ERROR 12-05 15:10:42 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4060050)[0;0m ERROR 12-05 15:10:42 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060050)[0;0m ERROR 12-05 15:10:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4060050)[0;0m ERROR 12-05 15:10:42 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4060050)[0;0m ERROR 12-05 15:10:42 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060050)[0;0m ERROR 12-05 15:10:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4060050)[0;0m ERROR 12-05 15:10:42 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4060050)[0;0m ERROR 12-05 15:10:42 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060050)[0;0m ERROR 12-05 15:10:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4060050)[0;0m ERROR 12-05 15:10:42 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4060050)[0;0m ERROR 12-05 15:10:42 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060050)[0;0m ERROR 12-05 15:10:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4060050)[0;0m ERROR 12-05 15:10:42 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4060050)[0;0m ERROR 12-05 15:10:42 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060050)[0;0m ERROR 12-05 15:10:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4060050)[0;0m ERROR 12-05 15:10:42 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4060050)[0;0m ERROR 12-05 15:10:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4060050)[0;0m ERROR 12-05 15:10:42 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4060050)[0;0m ERROR 12-05 15:10:42 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060050)[0;0m ERROR 12-05 15:10:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4060050)[0;0m ERROR 12-05 15:10:42 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4060050)[0;0m ERROR 12-05 15:10:42 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060050)[0;0m ERROR 12-05 15:10:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4060050)[0;0m ERROR 12-05 15:10:42 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4060050)[0;0m ERROR 12-05 15:10:42 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060050)[0;0m ERROR 12-05 15:10:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4060050)[0;0m ERROR 12-05 15:10:42 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4060050)[0;0m ERROR 12-05 15:10:42 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060050)[0;0m ERROR 12-05 15:10:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4060050)[0;0m ERROR 12-05 15:10:42 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4060050)[0;0m ERROR 12-05 15:10:42 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060050)[0;0m ERROR 12-05 15:10:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4060050)[0;0m ERROR 12-05 15:10:42 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4060050)[0;0m ERROR 12-05 15:10:42 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060050)[0;0m ERROR 12-05 15:10:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4060050)[0;0m ERROR 12-05 15:10:42 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4060050)[0;0m ERROR 12-05 15:10:42 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060050)[0;0m ERROR 12-05 15:10:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4060050)[0;0m ERROR 12-05 15:10:42 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4060050)[0;0m ERROR 12-05 15:10:42 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060050)[0;0m ERROR 12-05 15:10:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4060050)[0;0m ERROR 12-05 15:10:42 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4060050)[0;0m ERROR 12-05 15:10:42 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060050)[0;0m ERROR 12-05 15:10:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4060050)[0;0m ERROR 12-05 15:10:42 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4060050)[0;0m ERROR 12-05 15:10:42 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060050)[0;0m ERROR 12-05 15:10:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4060050)[0;0m ERROR 12-05 15:10:42 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4060050)[0;0m ERROR 12-05 15:10:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4060050)[0;0m ERROR 12-05 15:10:42 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4060050)[0;0m ERROR 12-05 15:10:42 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060050)[0;0m ERROR 12-05 15:10:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4060050)[0;0m ERROR 12-05 15:10:42 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4060050)[0;0m ERROR 12-05 15:10:42 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060050)[0;0m ERROR 12-05 15:10:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4060050)[0;0m ERROR 12-05 15:10:42 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4060050)[0;0m ERROR 12-05 15:10:42 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060050)[0;0m ERROR 12-05 15:10:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4060050)[0;0m ERROR 12-05 15:10:42 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4060050)[0;0m ERROR 12-05 15:10:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4060050)[0;0m ERROR 12-05 15:10:42 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4060050)[0;0m ERROR 12-05 15:10:42 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060050)[0;0m ERROR 12-05 15:10:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4060050)[0;0m ERROR 12-05 15:10:42 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4060050)[0;0m ERROR 12-05 15:10:42 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060050)[0;0m ERROR 12-05 15:10:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4060050)[0;0m ERROR 12-05 15:10:42 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4060050)[0;0m ERROR 12-05 15:10:42 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060050)[0;0m ERROR 12-05 15:10:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4060050)[0;0m ERROR 12-05 15:10:42 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4060050)[0;0m ERROR 12-05 15:10:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4060050)[0;0m ERROR 12-05 15:10:42 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4060050)[0;0m ERROR 12-05 15:10:42 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060050)[0;0m ERROR 12-05 15:10:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4060050)[0;0m ERROR 12-05 15:10:42 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4060050)[0;0m ERROR 12-05 15:10:42 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060050)[0;0m ERROR 12-05 15:10:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4060050)[0;0m ERROR 12-05 15:10:42 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4060050)[0;0m ERROR 12-05 15:10:42 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060050)[0;0m ERROR 12-05 15:10:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4060050)[0;0m ERROR 12-05 15:10:42 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4060050)[0;0m ERROR 12-05 15:10:42 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060050)[0;0m ERROR 12-05 15:10:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4060050)[0;0m ERROR 12-05 15:10:42 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4060050)[0;0m ERROR 12-05 15:10:42 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7f46082dbb00>' raised:
[1;36m(EngineCore_DP0 pid=4060050)[0;0m ERROR 12-05 15:10:42 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4060050)[0;0m ERROR 12-05 15:10:42 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4060050)[0;0m ERROR 12-05 15:10:42 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4060050)[0;0m ERROR 12-05 15:10:42 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:11:03 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:11:03 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:11:03 [model.py:1745] Using max model len 131072
INFO 12-05 15:11:03 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4060155)[0;0m INFO 12-05 15:11:10 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4060155)[0;0m INFO 12-05 15:11:11 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:48387 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4060155)[0;0m INFO 12-05 15:11:11 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4060155)[0;0m INFO 12-05 15:11:11 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4060155)[0;0m INFO 12-05 15:11:12 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4060155)[0;0m INFO 12-05 15:11:12 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4060155)[0;0m INFO 12-05 15:11:13 [default_loader.py:314] Loading weights took 0.87 seconds
[1;36m(EngineCore_DP0 pid=4060155)[0;0m INFO 12-05 15:11:13 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.429927 seconds
[1;36m(EngineCore_DP0 pid=4060155)[0;0m INFO 12-05 15:11:17 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4060155)[0;0m INFO 12-05 15:11:17 [backends.py:647] Dynamo bytecode transform time: 3.64 s
[1;36m(EngineCore_DP0 pid=4060155)[0;0m ERROR 12-05 15:11:17 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4060155)[0;0m ERROR 12-05 15:11:17 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4060155)[0;0m ERROR 12-05 15:11:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4060155)[0;0m ERROR 12-05 15:11:17 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4060155)[0;0m ERROR 12-05 15:11:17 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060155)[0;0m ERROR 12-05 15:11:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4060155)[0;0m ERROR 12-05 15:11:17 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4060155)[0;0m ERROR 12-05 15:11:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4060155)[0;0m ERROR 12-05 15:11:17 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4060155)[0;0m ERROR 12-05 15:11:17 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060155)[0;0m ERROR 12-05 15:11:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4060155)[0;0m ERROR 12-05 15:11:17 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4060155)[0;0m ERROR 12-05 15:11:17 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060155)[0;0m ERROR 12-05 15:11:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4060155)[0;0m ERROR 12-05 15:11:17 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4060155)[0;0m ERROR 12-05 15:11:17 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060155)[0;0m ERROR 12-05 15:11:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4060155)[0;0m ERROR 12-05 15:11:17 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4060155)[0;0m ERROR 12-05 15:11:17 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060155)[0;0m ERROR 12-05 15:11:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4060155)[0;0m ERROR 12-05 15:11:17 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4060155)[0;0m ERROR 12-05 15:11:17 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060155)[0;0m ERROR 12-05 15:11:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4060155)[0;0m ERROR 12-05 15:11:17 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4060155)[0;0m ERROR 12-05 15:11:17 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060155)[0;0m ERROR 12-05 15:11:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4060155)[0;0m ERROR 12-05 15:11:17 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4060155)[0;0m ERROR 12-05 15:11:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4060155)[0;0m ERROR 12-05 15:11:17 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4060155)[0;0m ERROR 12-05 15:11:17 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060155)[0;0m ERROR 12-05 15:11:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4060155)[0;0m ERROR 12-05 15:11:17 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4060155)[0;0m ERROR 12-05 15:11:17 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060155)[0;0m ERROR 12-05 15:11:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4060155)[0;0m ERROR 12-05 15:11:17 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4060155)[0;0m ERROR 12-05 15:11:17 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060155)[0;0m ERROR 12-05 15:11:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4060155)[0;0m ERROR 12-05 15:11:17 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4060155)[0;0m ERROR 12-05 15:11:17 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060155)[0;0m ERROR 12-05 15:11:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4060155)[0;0m ERROR 12-05 15:11:17 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4060155)[0;0m ERROR 12-05 15:11:17 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060155)[0;0m ERROR 12-05 15:11:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4060155)[0;0m ERROR 12-05 15:11:17 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4060155)[0;0m ERROR 12-05 15:11:17 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060155)[0;0m ERROR 12-05 15:11:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4060155)[0;0m ERROR 12-05 15:11:17 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4060155)[0;0m ERROR 12-05 15:11:17 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060155)[0;0m ERROR 12-05 15:11:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4060155)[0;0m ERROR 12-05 15:11:17 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4060155)[0;0m ERROR 12-05 15:11:17 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060155)[0;0m ERROR 12-05 15:11:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4060155)[0;0m ERROR 12-05 15:11:17 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4060155)[0;0m ERROR 12-05 15:11:17 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060155)[0;0m ERROR 12-05 15:11:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4060155)[0;0m ERROR 12-05 15:11:17 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4060155)[0;0m ERROR 12-05 15:11:17 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060155)[0;0m ERROR 12-05 15:11:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4060155)[0;0m ERROR 12-05 15:11:17 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4060155)[0;0m ERROR 12-05 15:11:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4060155)[0;0m ERROR 12-05 15:11:17 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4060155)[0;0m ERROR 12-05 15:11:17 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060155)[0;0m ERROR 12-05 15:11:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4060155)[0;0m ERROR 12-05 15:11:17 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4060155)[0;0m ERROR 12-05 15:11:17 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060155)[0;0m ERROR 12-05 15:11:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4060155)[0;0m ERROR 12-05 15:11:17 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4060155)[0;0m ERROR 12-05 15:11:17 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060155)[0;0m ERROR 12-05 15:11:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4060155)[0;0m ERROR 12-05 15:11:17 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4060155)[0;0m ERROR 12-05 15:11:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4060155)[0;0m ERROR 12-05 15:11:17 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4060155)[0;0m ERROR 12-05 15:11:17 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060155)[0;0m ERROR 12-05 15:11:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4060155)[0;0m ERROR 12-05 15:11:17 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4060155)[0;0m ERROR 12-05 15:11:17 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060155)[0;0m ERROR 12-05 15:11:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4060155)[0;0m ERROR 12-05 15:11:17 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4060155)[0;0m ERROR 12-05 15:11:17 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060155)[0;0m ERROR 12-05 15:11:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4060155)[0;0m ERROR 12-05 15:11:17 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4060155)[0;0m ERROR 12-05 15:11:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4060155)[0;0m ERROR 12-05 15:11:17 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4060155)[0;0m ERROR 12-05 15:11:17 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060155)[0;0m ERROR 12-05 15:11:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4060155)[0;0m ERROR 12-05 15:11:17 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4060155)[0;0m ERROR 12-05 15:11:17 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060155)[0;0m ERROR 12-05 15:11:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4060155)[0;0m ERROR 12-05 15:11:17 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4060155)[0;0m ERROR 12-05 15:11:17 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060155)[0;0m ERROR 12-05 15:11:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4060155)[0;0m ERROR 12-05 15:11:17 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4060155)[0;0m ERROR 12-05 15:11:17 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060155)[0;0m ERROR 12-05 15:11:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4060155)[0;0m ERROR 12-05 15:11:17 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4060155)[0;0m ERROR 12-05 15:11:17 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7f88bc191c10>' raised:
[1;36m(EngineCore_DP0 pid=4060155)[0;0m ERROR 12-05 15:11:17 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4060155)[0;0m ERROR 12-05 15:11:17 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4060155)[0;0m ERROR 12-05 15:11:17 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4060155)[0;0m ERROR 12-05 15:11:17 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:11:39 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:11:39 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:11:39 [model.py:1745] Using max model len 131072
INFO 12-05 15:11:39 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4060313)[0;0m INFO 12-05 15:11:47 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4060313)[0;0m INFO 12-05 15:11:48 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:35113 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4060313)[0;0m INFO 12-05 15:11:48 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4060313)[0;0m INFO 12-05 15:11:48 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4060313)[0;0m INFO 12-05 15:11:49 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4060313)[0;0m INFO 12-05 15:11:49 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4060313)[0;0m INFO 12-05 15:11:50 [default_loader.py:314] Loading weights took 0.87 seconds
[1;36m(EngineCore_DP0 pid=4060313)[0;0m INFO 12-05 15:11:50 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.393918 seconds
[1;36m(EngineCore_DP0 pid=4060313)[0;0m INFO 12-05 15:11:54 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4060313)[0;0m INFO 12-05 15:11:54 [backends.py:647] Dynamo bytecode transform time: 3.75 s
[1;36m(EngineCore_DP0 pid=4060313)[0;0m ERROR 12-05 15:11:55 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4060313)[0;0m ERROR 12-05 15:11:55 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4060313)[0;0m ERROR 12-05 15:11:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4060313)[0;0m ERROR 12-05 15:11:55 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4060313)[0;0m ERROR 12-05 15:11:55 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060313)[0;0m ERROR 12-05 15:11:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4060313)[0;0m ERROR 12-05 15:11:55 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4060313)[0;0m ERROR 12-05 15:11:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4060313)[0;0m ERROR 12-05 15:11:55 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4060313)[0;0m ERROR 12-05 15:11:55 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060313)[0;0m ERROR 12-05 15:11:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4060313)[0;0m ERROR 12-05 15:11:55 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4060313)[0;0m ERROR 12-05 15:11:55 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060313)[0;0m ERROR 12-05 15:11:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4060313)[0;0m ERROR 12-05 15:11:55 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4060313)[0;0m ERROR 12-05 15:11:55 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060313)[0;0m ERROR 12-05 15:11:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4060313)[0;0m ERROR 12-05 15:11:55 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4060313)[0;0m ERROR 12-05 15:11:55 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060313)[0;0m ERROR 12-05 15:11:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4060313)[0;0m ERROR 12-05 15:11:55 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4060313)[0;0m ERROR 12-05 15:11:55 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060313)[0;0m ERROR 12-05 15:11:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4060313)[0;0m ERROR 12-05 15:11:55 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4060313)[0;0m ERROR 12-05 15:11:55 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060313)[0;0m ERROR 12-05 15:11:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4060313)[0;0m ERROR 12-05 15:11:55 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4060313)[0;0m ERROR 12-05 15:11:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4060313)[0;0m ERROR 12-05 15:11:55 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4060313)[0;0m ERROR 12-05 15:11:55 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060313)[0;0m ERROR 12-05 15:11:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4060313)[0;0m ERROR 12-05 15:11:55 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4060313)[0;0m ERROR 12-05 15:11:55 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060313)[0;0m ERROR 12-05 15:11:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4060313)[0;0m ERROR 12-05 15:11:55 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4060313)[0;0m ERROR 12-05 15:11:55 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060313)[0;0m ERROR 12-05 15:11:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4060313)[0;0m ERROR 12-05 15:11:55 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4060313)[0;0m ERROR 12-05 15:11:55 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060313)[0;0m ERROR 12-05 15:11:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4060313)[0;0m ERROR 12-05 15:11:55 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4060313)[0;0m ERROR 12-05 15:11:55 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060313)[0;0m ERROR 12-05 15:11:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4060313)[0;0m ERROR 12-05 15:11:55 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4060313)[0;0m ERROR 12-05 15:11:55 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060313)[0;0m ERROR 12-05 15:11:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4060313)[0;0m ERROR 12-05 15:11:55 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4060313)[0;0m ERROR 12-05 15:11:55 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060313)[0;0m ERROR 12-05 15:11:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4060313)[0;0m ERROR 12-05 15:11:55 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4060313)[0;0m ERROR 12-05 15:11:55 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060313)[0;0m ERROR 12-05 15:11:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4060313)[0;0m ERROR 12-05 15:11:55 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4060313)[0;0m ERROR 12-05 15:11:55 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060313)[0;0m ERROR 12-05 15:11:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4060313)[0;0m ERROR 12-05 15:11:55 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4060313)[0;0m ERROR 12-05 15:11:55 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060313)[0;0m ERROR 12-05 15:11:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4060313)[0;0m ERROR 12-05 15:11:55 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4060313)[0;0m ERROR 12-05 15:11:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4060313)[0;0m ERROR 12-05 15:11:55 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4060313)[0;0m ERROR 12-05 15:11:55 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060313)[0;0m ERROR 12-05 15:11:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4060313)[0;0m ERROR 12-05 15:11:55 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4060313)[0;0m ERROR 12-05 15:11:55 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060313)[0;0m ERROR 12-05 15:11:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4060313)[0;0m ERROR 12-05 15:11:55 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4060313)[0;0m ERROR 12-05 15:11:55 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060313)[0;0m ERROR 12-05 15:11:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4060313)[0;0m ERROR 12-05 15:11:55 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4060313)[0;0m ERROR 12-05 15:11:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4060313)[0;0m ERROR 12-05 15:11:55 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4060313)[0;0m ERROR 12-05 15:11:55 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060313)[0;0m ERROR 12-05 15:11:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4060313)[0;0m ERROR 12-05 15:11:55 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4060313)[0;0m ERROR 12-05 15:11:55 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060313)[0;0m ERROR 12-05 15:11:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4060313)[0;0m ERROR 12-05 15:11:55 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4060313)[0;0m ERROR 12-05 15:11:55 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060313)[0;0m ERROR 12-05 15:11:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4060313)[0;0m ERROR 12-05 15:11:55 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4060313)[0;0m ERROR 12-05 15:11:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4060313)[0;0m ERROR 12-05 15:11:55 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4060313)[0;0m ERROR 12-05 15:11:55 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060313)[0;0m ERROR 12-05 15:11:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4060313)[0;0m ERROR 12-05 15:11:55 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4060313)[0;0m ERROR 12-05 15:11:55 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060313)[0;0m ERROR 12-05 15:11:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4060313)[0;0m ERROR 12-05 15:11:55 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4060313)[0;0m ERROR 12-05 15:11:55 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060313)[0;0m ERROR 12-05 15:11:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4060313)[0;0m ERROR 12-05 15:11:55 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4060313)[0;0m ERROR 12-05 15:11:55 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060313)[0;0m ERROR 12-05 15:11:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4060313)[0;0m ERROR 12-05 15:11:55 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4060313)[0;0m ERROR 12-05 15:11:55 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7fce20110500>' raised:
[1;36m(EngineCore_DP0 pid=4060313)[0;0m ERROR 12-05 15:11:55 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4060313)[0;0m ERROR 12-05 15:11:55 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4060313)[0;0m ERROR 12-05 15:11:55 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4060313)[0;0m ERROR 12-05 15:11:55 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:12:16 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:12:16 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:12:16 [model.py:1745] Using max model len 131072
INFO 12-05 15:12:16 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4060399)[0;0m INFO 12-05 15:12:25 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4060399)[0;0m INFO 12-05 15:12:26 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:58693 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4060399)[0;0m INFO 12-05 15:12:26 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4060399)[0;0m INFO 12-05 15:12:26 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4060399)[0;0m INFO 12-05 15:12:27 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4060399)[0;0m INFO 12-05 15:12:27 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4060399)[0;0m INFO 12-05 15:12:28 [default_loader.py:314] Loading weights took 0.88 seconds
[1;36m(EngineCore_DP0 pid=4060399)[0;0m INFO 12-05 15:12:28 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.421123 seconds
[1;36m(EngineCore_DP0 pid=4060399)[0;0m INFO 12-05 15:12:32 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4060399)[0;0m INFO 12-05 15:12:32 [backends.py:647] Dynamo bytecode transform time: 3.63 s
[1;36m(EngineCore_DP0 pid=4060399)[0;0m ERROR 12-05 15:12:32 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4060399)[0;0m ERROR 12-05 15:12:32 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4060399)[0;0m ERROR 12-05 15:12:32 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4060399)[0;0m ERROR 12-05 15:12:32 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4060399)[0;0m ERROR 12-05 15:12:32 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060399)[0;0m ERROR 12-05 15:12:32 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4060399)[0;0m ERROR 12-05 15:12:32 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4060399)[0;0m ERROR 12-05 15:12:32 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4060399)[0;0m ERROR 12-05 15:12:32 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4060399)[0;0m ERROR 12-05 15:12:32 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060399)[0;0m ERROR 12-05 15:12:32 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4060399)[0;0m ERROR 12-05 15:12:32 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4060399)[0;0m ERROR 12-05 15:12:32 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060399)[0;0m ERROR 12-05 15:12:32 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4060399)[0;0m ERROR 12-05 15:12:32 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4060399)[0;0m ERROR 12-05 15:12:32 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060399)[0;0m ERROR 12-05 15:12:32 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4060399)[0;0m ERROR 12-05 15:12:32 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4060399)[0;0m ERROR 12-05 15:12:32 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060399)[0;0m ERROR 12-05 15:12:32 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4060399)[0;0m ERROR 12-05 15:12:32 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4060399)[0;0m ERROR 12-05 15:12:32 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060399)[0;0m ERROR 12-05 15:12:32 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4060399)[0;0m ERROR 12-05 15:12:32 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4060399)[0;0m ERROR 12-05 15:12:32 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060399)[0;0m ERROR 12-05 15:12:32 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4060399)[0;0m ERROR 12-05 15:12:32 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4060399)[0;0m ERROR 12-05 15:12:32 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4060399)[0;0m ERROR 12-05 15:12:32 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4060399)[0;0m ERROR 12-05 15:12:32 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060399)[0;0m ERROR 12-05 15:12:32 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4060399)[0;0m ERROR 12-05 15:12:32 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4060399)[0;0m ERROR 12-05 15:12:32 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060399)[0;0m ERROR 12-05 15:12:32 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4060399)[0;0m ERROR 12-05 15:12:32 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4060399)[0;0m ERROR 12-05 15:12:32 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060399)[0;0m ERROR 12-05 15:12:32 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4060399)[0;0m ERROR 12-05 15:12:32 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4060399)[0;0m ERROR 12-05 15:12:32 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060399)[0;0m ERROR 12-05 15:12:32 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4060399)[0;0m ERROR 12-05 15:12:32 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4060399)[0;0m ERROR 12-05 15:12:32 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060399)[0;0m ERROR 12-05 15:12:32 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4060399)[0;0m ERROR 12-05 15:12:32 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4060399)[0;0m ERROR 12-05 15:12:32 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060399)[0;0m ERROR 12-05 15:12:32 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4060399)[0;0m ERROR 12-05 15:12:32 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4060399)[0;0m ERROR 12-05 15:12:32 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060399)[0;0m ERROR 12-05 15:12:32 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4060399)[0;0m ERROR 12-05 15:12:32 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4060399)[0;0m ERROR 12-05 15:12:32 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060399)[0;0m ERROR 12-05 15:12:32 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4060399)[0;0m ERROR 12-05 15:12:32 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4060399)[0;0m ERROR 12-05 15:12:32 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060399)[0;0m ERROR 12-05 15:12:32 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4060399)[0;0m ERROR 12-05 15:12:32 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4060399)[0;0m ERROR 12-05 15:12:32 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060399)[0;0m ERROR 12-05 15:12:32 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4060399)[0;0m ERROR 12-05 15:12:32 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4060399)[0;0m ERROR 12-05 15:12:32 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4060399)[0;0m ERROR 12-05 15:12:32 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4060399)[0;0m ERROR 12-05 15:12:32 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060399)[0;0m ERROR 12-05 15:12:32 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4060399)[0;0m ERROR 12-05 15:12:32 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4060399)[0;0m ERROR 12-05 15:12:32 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060399)[0;0m ERROR 12-05 15:12:32 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4060399)[0;0m ERROR 12-05 15:12:32 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4060399)[0;0m ERROR 12-05 15:12:32 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060399)[0;0m ERROR 12-05 15:12:32 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4060399)[0;0m ERROR 12-05 15:12:32 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4060399)[0;0m ERROR 12-05 15:12:32 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4060399)[0;0m ERROR 12-05 15:12:32 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4060399)[0;0m ERROR 12-05 15:12:32 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060399)[0;0m ERROR 12-05 15:12:32 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4060399)[0;0m ERROR 12-05 15:12:32 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4060399)[0;0m ERROR 12-05 15:12:32 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060399)[0;0m ERROR 12-05 15:12:32 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4060399)[0;0m ERROR 12-05 15:12:32 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4060399)[0;0m ERROR 12-05 15:12:32 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060399)[0;0m ERROR 12-05 15:12:32 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4060399)[0;0m ERROR 12-05 15:12:32 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4060399)[0;0m ERROR 12-05 15:12:32 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4060399)[0;0m ERROR 12-05 15:12:32 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4060399)[0;0m ERROR 12-05 15:12:32 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060399)[0;0m ERROR 12-05 15:12:32 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4060399)[0;0m ERROR 12-05 15:12:32 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4060399)[0;0m ERROR 12-05 15:12:32 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060399)[0;0m ERROR 12-05 15:12:32 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4060399)[0;0m ERROR 12-05 15:12:32 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4060399)[0;0m ERROR 12-05 15:12:32 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060399)[0;0m ERROR 12-05 15:12:32 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4060399)[0;0m ERROR 12-05 15:12:32 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4060399)[0;0m ERROR 12-05 15:12:32 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060399)[0;0m ERROR 12-05 15:12:32 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4060399)[0;0m ERROR 12-05 15:12:32 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4060399)[0;0m ERROR 12-05 15:12:32 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7fe644216cf0>' raised:
[1;36m(EngineCore_DP0 pid=4060399)[0;0m ERROR 12-05 15:12:32 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4060399)[0;0m ERROR 12-05 15:12:32 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4060399)[0;0m ERROR 12-05 15:12:32 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4060399)[0;0m ERROR 12-05 15:12:32 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:12:54 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:12:54 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:12:54 [model.py:1745] Using max model len 131072
INFO 12-05 15:12:54 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4060487)[0;0m INFO 12-05 15:13:01 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4060487)[0;0m INFO 12-05 15:13:02 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:36707 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4060487)[0;0m INFO 12-05 15:13:02 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4060487)[0;0m INFO 12-05 15:13:02 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4060487)[0;0m INFO 12-05 15:13:03 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4060487)[0;0m INFO 12-05 15:13:03 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4060487)[0;0m INFO 12-05 15:13:04 [default_loader.py:314] Loading weights took 0.87 seconds
[1;36m(EngineCore_DP0 pid=4060487)[0;0m INFO 12-05 15:13:04 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.443712 seconds
[1;36m(EngineCore_DP0 pid=4060487)[0;0m INFO 12-05 15:13:08 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4060487)[0;0m INFO 12-05 15:13:08 [backends.py:647] Dynamo bytecode transform time: 3.60 s
[1;36m(EngineCore_DP0 pid=4060487)[0;0m ERROR 12-05 15:13:08 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4060487)[0;0m ERROR 12-05 15:13:08 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4060487)[0;0m ERROR 12-05 15:13:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4060487)[0;0m ERROR 12-05 15:13:08 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4060487)[0;0m ERROR 12-05 15:13:08 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060487)[0;0m ERROR 12-05 15:13:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4060487)[0;0m ERROR 12-05 15:13:08 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4060487)[0;0m ERROR 12-05 15:13:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4060487)[0;0m ERROR 12-05 15:13:08 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4060487)[0;0m ERROR 12-05 15:13:08 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060487)[0;0m ERROR 12-05 15:13:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4060487)[0;0m ERROR 12-05 15:13:08 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4060487)[0;0m ERROR 12-05 15:13:08 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060487)[0;0m ERROR 12-05 15:13:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4060487)[0;0m ERROR 12-05 15:13:08 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4060487)[0;0m ERROR 12-05 15:13:08 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060487)[0;0m ERROR 12-05 15:13:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4060487)[0;0m ERROR 12-05 15:13:08 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4060487)[0;0m ERROR 12-05 15:13:08 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060487)[0;0m ERROR 12-05 15:13:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4060487)[0;0m ERROR 12-05 15:13:08 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4060487)[0;0m ERROR 12-05 15:13:08 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060487)[0;0m ERROR 12-05 15:13:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4060487)[0;0m ERROR 12-05 15:13:08 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4060487)[0;0m ERROR 12-05 15:13:08 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060487)[0;0m ERROR 12-05 15:13:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4060487)[0;0m ERROR 12-05 15:13:08 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4060487)[0;0m ERROR 12-05 15:13:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4060487)[0;0m ERROR 12-05 15:13:08 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4060487)[0;0m ERROR 12-05 15:13:08 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060487)[0;0m ERROR 12-05 15:13:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4060487)[0;0m ERROR 12-05 15:13:08 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4060487)[0;0m ERROR 12-05 15:13:08 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060487)[0;0m ERROR 12-05 15:13:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4060487)[0;0m ERROR 12-05 15:13:08 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4060487)[0;0m ERROR 12-05 15:13:08 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060487)[0;0m ERROR 12-05 15:13:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4060487)[0;0m ERROR 12-05 15:13:08 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4060487)[0;0m ERROR 12-05 15:13:08 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060487)[0;0m ERROR 12-05 15:13:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4060487)[0;0m ERROR 12-05 15:13:08 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4060487)[0;0m ERROR 12-05 15:13:08 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060487)[0;0m ERROR 12-05 15:13:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4060487)[0;0m ERROR 12-05 15:13:08 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4060487)[0;0m ERROR 12-05 15:13:08 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060487)[0;0m ERROR 12-05 15:13:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4060487)[0;0m ERROR 12-05 15:13:08 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4060487)[0;0m ERROR 12-05 15:13:08 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060487)[0;0m ERROR 12-05 15:13:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4060487)[0;0m ERROR 12-05 15:13:08 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4060487)[0;0m ERROR 12-05 15:13:08 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060487)[0;0m ERROR 12-05 15:13:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4060487)[0;0m ERROR 12-05 15:13:08 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4060487)[0;0m ERROR 12-05 15:13:08 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060487)[0;0m ERROR 12-05 15:13:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4060487)[0;0m ERROR 12-05 15:13:08 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4060487)[0;0m ERROR 12-05 15:13:08 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060487)[0;0m ERROR 12-05 15:13:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4060487)[0;0m ERROR 12-05 15:13:08 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4060487)[0;0m ERROR 12-05 15:13:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4060487)[0;0m ERROR 12-05 15:13:08 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4060487)[0;0m ERROR 12-05 15:13:08 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060487)[0;0m ERROR 12-05 15:13:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4060487)[0;0m ERROR 12-05 15:13:08 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4060487)[0;0m ERROR 12-05 15:13:08 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060487)[0;0m ERROR 12-05 15:13:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4060487)[0;0m ERROR 12-05 15:13:08 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4060487)[0;0m ERROR 12-05 15:13:08 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060487)[0;0m ERROR 12-05 15:13:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4060487)[0;0m ERROR 12-05 15:13:08 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4060487)[0;0m ERROR 12-05 15:13:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4060487)[0;0m ERROR 12-05 15:13:08 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4060487)[0;0m ERROR 12-05 15:13:08 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060487)[0;0m ERROR 12-05 15:13:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4060487)[0;0m ERROR 12-05 15:13:08 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4060487)[0;0m ERROR 12-05 15:13:08 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060487)[0;0m ERROR 12-05 15:13:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4060487)[0;0m ERROR 12-05 15:13:08 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4060487)[0;0m ERROR 12-05 15:13:08 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060487)[0;0m ERROR 12-05 15:13:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4060487)[0;0m ERROR 12-05 15:13:08 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4060487)[0;0m ERROR 12-05 15:13:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4060487)[0;0m ERROR 12-05 15:13:08 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4060487)[0;0m ERROR 12-05 15:13:08 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060487)[0;0m ERROR 12-05 15:13:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4060487)[0;0m ERROR 12-05 15:13:08 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4060487)[0;0m ERROR 12-05 15:13:08 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060487)[0;0m ERROR 12-05 15:13:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4060487)[0;0m ERROR 12-05 15:13:08 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4060487)[0;0m ERROR 12-05 15:13:08 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060487)[0;0m ERROR 12-05 15:13:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4060487)[0;0m ERROR 12-05 15:13:08 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4060487)[0;0m ERROR 12-05 15:13:08 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060487)[0;0m ERROR 12-05 15:13:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4060487)[0;0m ERROR 12-05 15:13:08 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4060487)[0;0m ERROR 12-05 15:13:08 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7f368c285b20>' raised:
[1;36m(EngineCore_DP0 pid=4060487)[0;0m ERROR 12-05 15:13:08 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4060487)[0;0m ERROR 12-05 15:13:08 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4060487)[0;0m ERROR 12-05 15:13:08 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4060487)[0;0m ERROR 12-05 15:13:08 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:13:30 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:13:30 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:13:30 [model.py:1745] Using max model len 131072
INFO 12-05 15:13:30 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4060566)[0;0m INFO 12-05 15:13:37 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4060566)[0;0m INFO 12-05 15:13:38 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:35319 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4060566)[0;0m INFO 12-05 15:13:38 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4060566)[0;0m INFO 12-05 15:13:38 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4060566)[0;0m INFO 12-05 15:13:39 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4060566)[0;0m INFO 12-05 15:13:39 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4060566)[0;0m INFO 12-05 15:13:40 [default_loader.py:314] Loading weights took 0.87 seconds
[1;36m(EngineCore_DP0 pid=4060566)[0;0m INFO 12-05 15:13:41 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.442533 seconds
[1;36m(EngineCore_DP0 pid=4060566)[0;0m INFO 12-05 15:13:44 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4060566)[0;0m INFO 12-05 15:13:44 [backends.py:647] Dynamo bytecode transform time: 3.65 s
[1;36m(EngineCore_DP0 pid=4060566)[0;0m ERROR 12-05 15:13:45 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4060566)[0;0m ERROR 12-05 15:13:45 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4060566)[0;0m ERROR 12-05 15:13:45 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4060566)[0;0m ERROR 12-05 15:13:45 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4060566)[0;0m ERROR 12-05 15:13:45 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060566)[0;0m ERROR 12-05 15:13:45 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4060566)[0;0m ERROR 12-05 15:13:45 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4060566)[0;0m ERROR 12-05 15:13:45 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4060566)[0;0m ERROR 12-05 15:13:45 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4060566)[0;0m ERROR 12-05 15:13:45 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060566)[0;0m ERROR 12-05 15:13:45 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4060566)[0;0m ERROR 12-05 15:13:45 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4060566)[0;0m ERROR 12-05 15:13:45 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060566)[0;0m ERROR 12-05 15:13:45 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4060566)[0;0m ERROR 12-05 15:13:45 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4060566)[0;0m ERROR 12-05 15:13:45 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060566)[0;0m ERROR 12-05 15:13:45 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4060566)[0;0m ERROR 12-05 15:13:45 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4060566)[0;0m ERROR 12-05 15:13:45 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060566)[0;0m ERROR 12-05 15:13:45 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4060566)[0;0m ERROR 12-05 15:13:45 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4060566)[0;0m ERROR 12-05 15:13:45 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060566)[0;0m ERROR 12-05 15:13:45 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4060566)[0;0m ERROR 12-05 15:13:45 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4060566)[0;0m ERROR 12-05 15:13:45 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060566)[0;0m ERROR 12-05 15:13:45 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4060566)[0;0m ERROR 12-05 15:13:45 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4060566)[0;0m ERROR 12-05 15:13:45 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4060566)[0;0m ERROR 12-05 15:13:45 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4060566)[0;0m ERROR 12-05 15:13:45 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060566)[0;0m ERROR 12-05 15:13:45 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4060566)[0;0m ERROR 12-05 15:13:45 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4060566)[0;0m ERROR 12-05 15:13:45 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060566)[0;0m ERROR 12-05 15:13:45 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4060566)[0;0m ERROR 12-05 15:13:45 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4060566)[0;0m ERROR 12-05 15:13:45 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060566)[0;0m ERROR 12-05 15:13:45 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4060566)[0;0m ERROR 12-05 15:13:45 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4060566)[0;0m ERROR 12-05 15:13:45 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060566)[0;0m ERROR 12-05 15:13:45 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4060566)[0;0m ERROR 12-05 15:13:45 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4060566)[0;0m ERROR 12-05 15:13:45 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060566)[0;0m ERROR 12-05 15:13:45 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4060566)[0;0m ERROR 12-05 15:13:45 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4060566)[0;0m ERROR 12-05 15:13:45 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060566)[0;0m ERROR 12-05 15:13:45 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4060566)[0;0m ERROR 12-05 15:13:45 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4060566)[0;0m ERROR 12-05 15:13:45 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060566)[0;0m ERROR 12-05 15:13:45 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4060566)[0;0m ERROR 12-05 15:13:45 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4060566)[0;0m ERROR 12-05 15:13:45 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060566)[0;0m ERROR 12-05 15:13:45 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4060566)[0;0m ERROR 12-05 15:13:45 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4060566)[0;0m ERROR 12-05 15:13:45 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060566)[0;0m ERROR 12-05 15:13:45 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4060566)[0;0m ERROR 12-05 15:13:45 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4060566)[0;0m ERROR 12-05 15:13:45 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060566)[0;0m ERROR 12-05 15:13:45 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4060566)[0;0m ERROR 12-05 15:13:45 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4060566)[0;0m ERROR 12-05 15:13:45 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4060566)[0;0m ERROR 12-05 15:13:45 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4060566)[0;0m ERROR 12-05 15:13:45 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060566)[0;0m ERROR 12-05 15:13:45 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4060566)[0;0m ERROR 12-05 15:13:45 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4060566)[0;0m ERROR 12-05 15:13:45 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060566)[0;0m ERROR 12-05 15:13:45 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4060566)[0;0m ERROR 12-05 15:13:45 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4060566)[0;0m ERROR 12-05 15:13:45 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060566)[0;0m ERROR 12-05 15:13:45 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4060566)[0;0m ERROR 12-05 15:13:45 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4060566)[0;0m ERROR 12-05 15:13:45 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4060566)[0;0m ERROR 12-05 15:13:45 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4060566)[0;0m ERROR 12-05 15:13:45 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060566)[0;0m ERROR 12-05 15:13:45 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4060566)[0;0m ERROR 12-05 15:13:45 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4060566)[0;0m ERROR 12-05 15:13:45 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060566)[0;0m ERROR 12-05 15:13:45 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4060566)[0;0m ERROR 12-05 15:13:45 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4060566)[0;0m ERROR 12-05 15:13:45 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060566)[0;0m ERROR 12-05 15:13:45 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4060566)[0;0m ERROR 12-05 15:13:45 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4060566)[0;0m ERROR 12-05 15:13:45 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4060566)[0;0m ERROR 12-05 15:13:45 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4060566)[0;0m ERROR 12-05 15:13:45 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060566)[0;0m ERROR 12-05 15:13:45 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4060566)[0;0m ERROR 12-05 15:13:45 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4060566)[0;0m ERROR 12-05 15:13:45 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060566)[0;0m ERROR 12-05 15:13:45 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4060566)[0;0m ERROR 12-05 15:13:45 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4060566)[0;0m ERROR 12-05 15:13:45 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060566)[0;0m ERROR 12-05 15:13:45 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4060566)[0;0m ERROR 12-05 15:13:45 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4060566)[0;0m ERROR 12-05 15:13:45 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060566)[0;0m ERROR 12-05 15:13:45 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4060566)[0;0m ERROR 12-05 15:13:45 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4060566)[0;0m ERROR 12-05 15:13:45 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7f63e0139100>' raised:
[1;36m(EngineCore_DP0 pid=4060566)[0;0m ERROR 12-05 15:13:45 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4060566)[0;0m ERROR 12-05 15:13:45 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4060566)[0;0m ERROR 12-05 15:13:45 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4060566)[0;0m ERROR 12-05 15:13:45 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:14:06 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:14:06 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:14:06 [model.py:1745] Using max model len 131072
INFO 12-05 15:14:06 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4060652)[0;0m INFO 12-05 15:14:14 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4060652)[0;0m INFO 12-05 15:14:14 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:57271 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4060652)[0;0m INFO 12-05 15:14:14 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4060652)[0;0m INFO 12-05 15:14:15 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4060652)[0;0m INFO 12-05 15:14:15 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4060652)[0;0m INFO 12-05 15:14:15 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4060652)[0;0m INFO 12-05 15:14:16 [default_loader.py:314] Loading weights took 0.87 seconds
[1;36m(EngineCore_DP0 pid=4060652)[0;0m INFO 12-05 15:14:17 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.432018 seconds
[1;36m(EngineCore_DP0 pid=4060652)[0;0m INFO 12-05 15:14:21 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4060652)[0;0m INFO 12-05 15:14:21 [backends.py:647] Dynamo bytecode transform time: 3.67 s
[1;36m(EngineCore_DP0 pid=4060652)[0;0m ERROR 12-05 15:14:21 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4060652)[0;0m ERROR 12-05 15:14:21 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4060652)[0;0m ERROR 12-05 15:14:21 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4060652)[0;0m ERROR 12-05 15:14:21 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4060652)[0;0m ERROR 12-05 15:14:21 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060652)[0;0m ERROR 12-05 15:14:21 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4060652)[0;0m ERROR 12-05 15:14:21 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4060652)[0;0m ERROR 12-05 15:14:21 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4060652)[0;0m ERROR 12-05 15:14:21 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4060652)[0;0m ERROR 12-05 15:14:21 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060652)[0;0m ERROR 12-05 15:14:21 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4060652)[0;0m ERROR 12-05 15:14:21 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4060652)[0;0m ERROR 12-05 15:14:21 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060652)[0;0m ERROR 12-05 15:14:21 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4060652)[0;0m ERROR 12-05 15:14:21 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4060652)[0;0m ERROR 12-05 15:14:21 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060652)[0;0m ERROR 12-05 15:14:21 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4060652)[0;0m ERROR 12-05 15:14:21 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4060652)[0;0m ERROR 12-05 15:14:21 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060652)[0;0m ERROR 12-05 15:14:21 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4060652)[0;0m ERROR 12-05 15:14:21 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4060652)[0;0m ERROR 12-05 15:14:21 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060652)[0;0m ERROR 12-05 15:14:21 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4060652)[0;0m ERROR 12-05 15:14:21 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4060652)[0;0m ERROR 12-05 15:14:21 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060652)[0;0m ERROR 12-05 15:14:21 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4060652)[0;0m ERROR 12-05 15:14:21 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4060652)[0;0m ERROR 12-05 15:14:21 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4060652)[0;0m ERROR 12-05 15:14:21 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4060652)[0;0m ERROR 12-05 15:14:21 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060652)[0;0m ERROR 12-05 15:14:21 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4060652)[0;0m ERROR 12-05 15:14:21 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4060652)[0;0m ERROR 12-05 15:14:21 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060652)[0;0m ERROR 12-05 15:14:21 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4060652)[0;0m ERROR 12-05 15:14:21 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4060652)[0;0m ERROR 12-05 15:14:21 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060652)[0;0m ERROR 12-05 15:14:21 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4060652)[0;0m ERROR 12-05 15:14:21 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4060652)[0;0m ERROR 12-05 15:14:21 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060652)[0;0m ERROR 12-05 15:14:21 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4060652)[0;0m ERROR 12-05 15:14:21 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4060652)[0;0m ERROR 12-05 15:14:21 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060652)[0;0m ERROR 12-05 15:14:21 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4060652)[0;0m ERROR 12-05 15:14:21 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4060652)[0;0m ERROR 12-05 15:14:21 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060652)[0;0m ERROR 12-05 15:14:21 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4060652)[0;0m ERROR 12-05 15:14:21 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4060652)[0;0m ERROR 12-05 15:14:21 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060652)[0;0m ERROR 12-05 15:14:21 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4060652)[0;0m ERROR 12-05 15:14:21 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4060652)[0;0m ERROR 12-05 15:14:21 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060652)[0;0m ERROR 12-05 15:14:21 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4060652)[0;0m ERROR 12-05 15:14:21 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4060652)[0;0m ERROR 12-05 15:14:21 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060652)[0;0m ERROR 12-05 15:14:21 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4060652)[0;0m ERROR 12-05 15:14:21 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4060652)[0;0m ERROR 12-05 15:14:21 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060652)[0;0m ERROR 12-05 15:14:21 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4060652)[0;0m ERROR 12-05 15:14:21 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4060652)[0;0m ERROR 12-05 15:14:21 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4060652)[0;0m ERROR 12-05 15:14:21 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4060652)[0;0m ERROR 12-05 15:14:21 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060652)[0;0m ERROR 12-05 15:14:21 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4060652)[0;0m ERROR 12-05 15:14:21 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4060652)[0;0m ERROR 12-05 15:14:21 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060652)[0;0m ERROR 12-05 15:14:21 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4060652)[0;0m ERROR 12-05 15:14:21 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4060652)[0;0m ERROR 12-05 15:14:21 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060652)[0;0m ERROR 12-05 15:14:21 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4060652)[0;0m ERROR 12-05 15:14:21 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4060652)[0;0m ERROR 12-05 15:14:21 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4060652)[0;0m ERROR 12-05 15:14:21 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4060652)[0;0m ERROR 12-05 15:14:21 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060652)[0;0m ERROR 12-05 15:14:21 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4060652)[0;0m ERROR 12-05 15:14:21 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4060652)[0;0m ERROR 12-05 15:14:21 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060652)[0;0m ERROR 12-05 15:14:21 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4060652)[0;0m ERROR 12-05 15:14:21 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4060652)[0;0m ERROR 12-05 15:14:21 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060652)[0;0m ERROR 12-05 15:14:21 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4060652)[0;0m ERROR 12-05 15:14:21 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4060652)[0;0m ERROR 12-05 15:14:21 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4060652)[0;0m ERROR 12-05 15:14:21 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4060652)[0;0m ERROR 12-05 15:14:21 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060652)[0;0m ERROR 12-05 15:14:21 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4060652)[0;0m ERROR 12-05 15:14:21 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4060652)[0;0m ERROR 12-05 15:14:21 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060652)[0;0m ERROR 12-05 15:14:21 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4060652)[0;0m ERROR 12-05 15:14:21 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4060652)[0;0m ERROR 12-05 15:14:21 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060652)[0;0m ERROR 12-05 15:14:21 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4060652)[0;0m ERROR 12-05 15:14:21 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4060652)[0;0m ERROR 12-05 15:14:21 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060652)[0;0m ERROR 12-05 15:14:21 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4060652)[0;0m ERROR 12-05 15:14:21 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4060652)[0;0m ERROR 12-05 15:14:21 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7f7fb021d2e0>' raised:
[1;36m(EngineCore_DP0 pid=4060652)[0;0m ERROR 12-05 15:14:21 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4060652)[0;0m ERROR 12-05 15:14:21 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4060652)[0;0m ERROR 12-05 15:14:21 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4060652)[0;0m ERROR 12-05 15:14:21 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:14:42 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:14:42 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:14:42 [model.py:1745] Using max model len 131072
INFO 12-05 15:14:42 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4060734)[0;0m INFO 12-05 15:14:50 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4060734)[0;0m INFO 12-05 15:14:51 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:47207 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4060734)[0;0m INFO 12-05 15:14:51 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4060734)[0;0m INFO 12-05 15:14:51 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4060734)[0;0m INFO 12-05 15:14:51 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4060734)[0;0m INFO 12-05 15:14:51 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4060734)[0;0m INFO 12-05 15:14:53 [default_loader.py:314] Loading weights took 0.87 seconds
[1;36m(EngineCore_DP0 pid=4060734)[0;0m INFO 12-05 15:14:53 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.418357 seconds
[1;36m(EngineCore_DP0 pid=4060734)[0;0m INFO 12-05 15:14:57 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4060734)[0;0m INFO 12-05 15:14:57 [backends.py:647] Dynamo bytecode transform time: 3.60 s
[1;36m(EngineCore_DP0 pid=4060734)[0;0m ERROR 12-05 15:14:57 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4060734)[0;0m ERROR 12-05 15:14:57 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4060734)[0;0m ERROR 12-05 15:14:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4060734)[0;0m ERROR 12-05 15:14:57 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4060734)[0;0m ERROR 12-05 15:14:57 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060734)[0;0m ERROR 12-05 15:14:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4060734)[0;0m ERROR 12-05 15:14:57 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4060734)[0;0m ERROR 12-05 15:14:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4060734)[0;0m ERROR 12-05 15:14:57 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4060734)[0;0m ERROR 12-05 15:14:57 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060734)[0;0m ERROR 12-05 15:14:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4060734)[0;0m ERROR 12-05 15:14:57 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4060734)[0;0m ERROR 12-05 15:14:57 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060734)[0;0m ERROR 12-05 15:14:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4060734)[0;0m ERROR 12-05 15:14:57 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4060734)[0;0m ERROR 12-05 15:14:57 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060734)[0;0m ERROR 12-05 15:14:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4060734)[0;0m ERROR 12-05 15:14:57 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4060734)[0;0m ERROR 12-05 15:14:57 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060734)[0;0m ERROR 12-05 15:14:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4060734)[0;0m ERROR 12-05 15:14:57 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4060734)[0;0m ERROR 12-05 15:14:57 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060734)[0;0m ERROR 12-05 15:14:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4060734)[0;0m ERROR 12-05 15:14:57 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4060734)[0;0m ERROR 12-05 15:14:57 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060734)[0;0m ERROR 12-05 15:14:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4060734)[0;0m ERROR 12-05 15:14:57 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4060734)[0;0m ERROR 12-05 15:14:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4060734)[0;0m ERROR 12-05 15:14:57 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4060734)[0;0m ERROR 12-05 15:14:57 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060734)[0;0m ERROR 12-05 15:14:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4060734)[0;0m ERROR 12-05 15:14:57 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4060734)[0;0m ERROR 12-05 15:14:57 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060734)[0;0m ERROR 12-05 15:14:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4060734)[0;0m ERROR 12-05 15:14:57 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4060734)[0;0m ERROR 12-05 15:14:57 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060734)[0;0m ERROR 12-05 15:14:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4060734)[0;0m ERROR 12-05 15:14:57 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4060734)[0;0m ERROR 12-05 15:14:57 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060734)[0;0m ERROR 12-05 15:14:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4060734)[0;0m ERROR 12-05 15:14:57 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4060734)[0;0m ERROR 12-05 15:14:57 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060734)[0;0m ERROR 12-05 15:14:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4060734)[0;0m ERROR 12-05 15:14:57 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4060734)[0;0m ERROR 12-05 15:14:57 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060734)[0;0m ERROR 12-05 15:14:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4060734)[0;0m ERROR 12-05 15:14:57 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4060734)[0;0m ERROR 12-05 15:14:57 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060734)[0;0m ERROR 12-05 15:14:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4060734)[0;0m ERROR 12-05 15:14:57 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4060734)[0;0m ERROR 12-05 15:14:57 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060734)[0;0m ERROR 12-05 15:14:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4060734)[0;0m ERROR 12-05 15:14:57 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4060734)[0;0m ERROR 12-05 15:14:57 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060734)[0;0m ERROR 12-05 15:14:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4060734)[0;0m ERROR 12-05 15:14:57 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4060734)[0;0m ERROR 12-05 15:14:57 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060734)[0;0m ERROR 12-05 15:14:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4060734)[0;0m ERROR 12-05 15:14:57 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4060734)[0;0m ERROR 12-05 15:14:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4060734)[0;0m ERROR 12-05 15:14:57 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4060734)[0;0m ERROR 12-05 15:14:57 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060734)[0;0m ERROR 12-05 15:14:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4060734)[0;0m ERROR 12-05 15:14:57 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4060734)[0;0m ERROR 12-05 15:14:57 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060734)[0;0m ERROR 12-05 15:14:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4060734)[0;0m ERROR 12-05 15:14:57 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4060734)[0;0m ERROR 12-05 15:14:57 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060734)[0;0m ERROR 12-05 15:14:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4060734)[0;0m ERROR 12-05 15:14:57 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4060734)[0;0m ERROR 12-05 15:14:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4060734)[0;0m ERROR 12-05 15:14:57 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4060734)[0;0m ERROR 12-05 15:14:57 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060734)[0;0m ERROR 12-05 15:14:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4060734)[0;0m ERROR 12-05 15:14:57 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4060734)[0;0m ERROR 12-05 15:14:57 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060734)[0;0m ERROR 12-05 15:14:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4060734)[0;0m ERROR 12-05 15:14:57 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4060734)[0;0m ERROR 12-05 15:14:57 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060734)[0;0m ERROR 12-05 15:14:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4060734)[0;0m ERROR 12-05 15:14:57 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4060734)[0;0m ERROR 12-05 15:14:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4060734)[0;0m ERROR 12-05 15:14:57 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4060734)[0;0m ERROR 12-05 15:14:57 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060734)[0;0m ERROR 12-05 15:14:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4060734)[0;0m ERROR 12-05 15:14:57 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4060734)[0;0m ERROR 12-05 15:14:57 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060734)[0;0m ERROR 12-05 15:14:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4060734)[0;0m ERROR 12-05 15:14:57 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4060734)[0;0m ERROR 12-05 15:14:57 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060734)[0;0m ERROR 12-05 15:14:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4060734)[0;0m ERROR 12-05 15:14:57 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4060734)[0;0m ERROR 12-05 15:14:57 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060734)[0;0m ERROR 12-05 15:14:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4060734)[0;0m ERROR 12-05 15:14:57 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4060734)[0;0m ERROR 12-05 15:14:57 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7f2cac1fefc0>' raised:
[1;36m(EngineCore_DP0 pid=4060734)[0;0m ERROR 12-05 15:14:57 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4060734)[0;0m ERROR 12-05 15:14:57 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4060734)[0;0m ERROR 12-05 15:14:57 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4060734)[0;0m ERROR 12-05 15:14:57 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:15:18 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:15:19 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:15:19 [model.py:1745] Using max model len 131072
INFO 12-05 15:15:19 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4060820)[0;0m INFO 12-05 15:15:26 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4060820)[0;0m INFO 12-05 15:15:27 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:58147 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4060820)[0;0m INFO 12-05 15:15:27 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4060820)[0;0m INFO 12-05 15:15:27 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4060820)[0;0m INFO 12-05 15:15:28 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4060820)[0;0m INFO 12-05 15:15:28 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4060820)[0;0m INFO 12-05 15:15:29 [default_loader.py:314] Loading weights took 0.87 seconds
[1;36m(EngineCore_DP0 pid=4060820)[0;0m INFO 12-05 15:15:29 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.457949 seconds
[1;36m(EngineCore_DP0 pid=4060820)[0;0m INFO 12-05 15:15:33 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4060820)[0;0m INFO 12-05 15:15:33 [backends.py:647] Dynamo bytecode transform time: 3.59 s
[1;36m(EngineCore_DP0 pid=4060820)[0;0m ERROR 12-05 15:15:33 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4060820)[0;0m ERROR 12-05 15:15:33 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4060820)[0;0m ERROR 12-05 15:15:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4060820)[0;0m ERROR 12-05 15:15:33 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4060820)[0;0m ERROR 12-05 15:15:33 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060820)[0;0m ERROR 12-05 15:15:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4060820)[0;0m ERROR 12-05 15:15:33 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4060820)[0;0m ERROR 12-05 15:15:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4060820)[0;0m ERROR 12-05 15:15:33 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4060820)[0;0m ERROR 12-05 15:15:33 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060820)[0;0m ERROR 12-05 15:15:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4060820)[0;0m ERROR 12-05 15:15:33 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4060820)[0;0m ERROR 12-05 15:15:33 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060820)[0;0m ERROR 12-05 15:15:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4060820)[0;0m ERROR 12-05 15:15:33 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4060820)[0;0m ERROR 12-05 15:15:33 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060820)[0;0m ERROR 12-05 15:15:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4060820)[0;0m ERROR 12-05 15:15:33 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4060820)[0;0m ERROR 12-05 15:15:33 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060820)[0;0m ERROR 12-05 15:15:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4060820)[0;0m ERROR 12-05 15:15:33 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4060820)[0;0m ERROR 12-05 15:15:33 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060820)[0;0m ERROR 12-05 15:15:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4060820)[0;0m ERROR 12-05 15:15:33 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4060820)[0;0m ERROR 12-05 15:15:33 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060820)[0;0m ERROR 12-05 15:15:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4060820)[0;0m ERROR 12-05 15:15:33 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4060820)[0;0m ERROR 12-05 15:15:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4060820)[0;0m ERROR 12-05 15:15:33 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4060820)[0;0m ERROR 12-05 15:15:33 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060820)[0;0m ERROR 12-05 15:15:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4060820)[0;0m ERROR 12-05 15:15:33 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4060820)[0;0m ERROR 12-05 15:15:33 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060820)[0;0m ERROR 12-05 15:15:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4060820)[0;0m ERROR 12-05 15:15:33 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4060820)[0;0m ERROR 12-05 15:15:33 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060820)[0;0m ERROR 12-05 15:15:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4060820)[0;0m ERROR 12-05 15:15:33 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4060820)[0;0m ERROR 12-05 15:15:33 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060820)[0;0m ERROR 12-05 15:15:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4060820)[0;0m ERROR 12-05 15:15:33 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4060820)[0;0m ERROR 12-05 15:15:33 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060820)[0;0m ERROR 12-05 15:15:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4060820)[0;0m ERROR 12-05 15:15:33 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4060820)[0;0m ERROR 12-05 15:15:33 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060820)[0;0m ERROR 12-05 15:15:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4060820)[0;0m ERROR 12-05 15:15:33 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4060820)[0;0m ERROR 12-05 15:15:33 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060820)[0;0m ERROR 12-05 15:15:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4060820)[0;0m ERROR 12-05 15:15:33 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4060820)[0;0m ERROR 12-05 15:15:33 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060820)[0;0m ERROR 12-05 15:15:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4060820)[0;0m ERROR 12-05 15:15:33 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4060820)[0;0m ERROR 12-05 15:15:33 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060820)[0;0m ERROR 12-05 15:15:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4060820)[0;0m ERROR 12-05 15:15:33 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4060820)[0;0m ERROR 12-05 15:15:33 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060820)[0;0m ERROR 12-05 15:15:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4060820)[0;0m ERROR 12-05 15:15:33 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4060820)[0;0m ERROR 12-05 15:15:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4060820)[0;0m ERROR 12-05 15:15:33 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4060820)[0;0m ERROR 12-05 15:15:33 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060820)[0;0m ERROR 12-05 15:15:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4060820)[0;0m ERROR 12-05 15:15:33 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4060820)[0;0m ERROR 12-05 15:15:33 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060820)[0;0m ERROR 12-05 15:15:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4060820)[0;0m ERROR 12-05 15:15:33 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4060820)[0;0m ERROR 12-05 15:15:33 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060820)[0;0m ERROR 12-05 15:15:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4060820)[0;0m ERROR 12-05 15:15:33 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4060820)[0;0m ERROR 12-05 15:15:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4060820)[0;0m ERROR 12-05 15:15:33 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4060820)[0;0m ERROR 12-05 15:15:33 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060820)[0;0m ERROR 12-05 15:15:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4060820)[0;0m ERROR 12-05 15:15:33 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4060820)[0;0m ERROR 12-05 15:15:33 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060820)[0;0m ERROR 12-05 15:15:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4060820)[0;0m ERROR 12-05 15:15:33 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4060820)[0;0m ERROR 12-05 15:15:33 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060820)[0;0m ERROR 12-05 15:15:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4060820)[0;0m ERROR 12-05 15:15:33 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4060820)[0;0m ERROR 12-05 15:15:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4060820)[0;0m ERROR 12-05 15:15:33 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4060820)[0;0m ERROR 12-05 15:15:33 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060820)[0;0m ERROR 12-05 15:15:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4060820)[0;0m ERROR 12-05 15:15:33 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4060820)[0;0m ERROR 12-05 15:15:33 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060820)[0;0m ERROR 12-05 15:15:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4060820)[0;0m ERROR 12-05 15:15:33 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4060820)[0;0m ERROR 12-05 15:15:33 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060820)[0;0m ERROR 12-05 15:15:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4060820)[0;0m ERROR 12-05 15:15:33 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4060820)[0;0m ERROR 12-05 15:15:33 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060820)[0;0m ERROR 12-05 15:15:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4060820)[0;0m ERROR 12-05 15:15:33 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4060820)[0;0m ERROR 12-05 15:15:33 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7f872b6bb110>' raised:
[1;36m(EngineCore_DP0 pid=4060820)[0;0m ERROR 12-05 15:15:33 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4060820)[0;0m ERROR 12-05 15:15:33 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4060820)[0;0m ERROR 12-05 15:15:33 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4060820)[0;0m ERROR 12-05 15:15:33 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:15:55 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:15:55 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:15:55 [model.py:1745] Using max model len 131072
INFO 12-05 15:15:55 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4060908)[0;0m INFO 12-05 15:16:02 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4060908)[0;0m INFO 12-05 15:16:03 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:34439 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4060908)[0;0m INFO 12-05 15:16:03 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4060908)[0;0m INFO 12-05 15:16:03 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4060908)[0;0m INFO 12-05 15:16:04 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4060908)[0;0m INFO 12-05 15:16:04 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4060908)[0;0m INFO 12-05 15:16:05 [default_loader.py:314] Loading weights took 0.87 seconds
[1;36m(EngineCore_DP0 pid=4060908)[0;0m INFO 12-05 15:16:05 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.434991 seconds
[1;36m(EngineCore_DP0 pid=4060908)[0;0m INFO 12-05 15:16:09 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4060908)[0;0m INFO 12-05 15:16:09 [backends.py:647] Dynamo bytecode transform time: 3.59 s
[1;36m(EngineCore_DP0 pid=4060908)[0;0m ERROR 12-05 15:16:09 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4060908)[0;0m ERROR 12-05 15:16:09 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4060908)[0;0m ERROR 12-05 15:16:09 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4060908)[0;0m ERROR 12-05 15:16:09 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4060908)[0;0m ERROR 12-05 15:16:09 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060908)[0;0m ERROR 12-05 15:16:09 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4060908)[0;0m ERROR 12-05 15:16:09 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4060908)[0;0m ERROR 12-05 15:16:09 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4060908)[0;0m ERROR 12-05 15:16:09 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4060908)[0;0m ERROR 12-05 15:16:09 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060908)[0;0m ERROR 12-05 15:16:09 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4060908)[0;0m ERROR 12-05 15:16:09 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4060908)[0;0m ERROR 12-05 15:16:09 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060908)[0;0m ERROR 12-05 15:16:09 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4060908)[0;0m ERROR 12-05 15:16:09 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4060908)[0;0m ERROR 12-05 15:16:09 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060908)[0;0m ERROR 12-05 15:16:09 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4060908)[0;0m ERROR 12-05 15:16:09 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4060908)[0;0m ERROR 12-05 15:16:09 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060908)[0;0m ERROR 12-05 15:16:09 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4060908)[0;0m ERROR 12-05 15:16:09 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4060908)[0;0m ERROR 12-05 15:16:09 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060908)[0;0m ERROR 12-05 15:16:09 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4060908)[0;0m ERROR 12-05 15:16:09 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4060908)[0;0m ERROR 12-05 15:16:09 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060908)[0;0m ERROR 12-05 15:16:09 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4060908)[0;0m ERROR 12-05 15:16:09 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4060908)[0;0m ERROR 12-05 15:16:09 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4060908)[0;0m ERROR 12-05 15:16:09 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4060908)[0;0m ERROR 12-05 15:16:09 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060908)[0;0m ERROR 12-05 15:16:09 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4060908)[0;0m ERROR 12-05 15:16:09 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4060908)[0;0m ERROR 12-05 15:16:09 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060908)[0;0m ERROR 12-05 15:16:09 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4060908)[0;0m ERROR 12-05 15:16:09 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4060908)[0;0m ERROR 12-05 15:16:09 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060908)[0;0m ERROR 12-05 15:16:09 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4060908)[0;0m ERROR 12-05 15:16:09 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4060908)[0;0m ERROR 12-05 15:16:09 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060908)[0;0m ERROR 12-05 15:16:09 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4060908)[0;0m ERROR 12-05 15:16:09 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4060908)[0;0m ERROR 12-05 15:16:09 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060908)[0;0m ERROR 12-05 15:16:09 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4060908)[0;0m ERROR 12-05 15:16:09 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4060908)[0;0m ERROR 12-05 15:16:09 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060908)[0;0m ERROR 12-05 15:16:09 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4060908)[0;0m ERROR 12-05 15:16:09 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4060908)[0;0m ERROR 12-05 15:16:09 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060908)[0;0m ERROR 12-05 15:16:09 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4060908)[0;0m ERROR 12-05 15:16:09 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4060908)[0;0m ERROR 12-05 15:16:09 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060908)[0;0m ERROR 12-05 15:16:09 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4060908)[0;0m ERROR 12-05 15:16:09 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4060908)[0;0m ERROR 12-05 15:16:09 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060908)[0;0m ERROR 12-05 15:16:09 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4060908)[0;0m ERROR 12-05 15:16:09 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4060908)[0;0m ERROR 12-05 15:16:09 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060908)[0;0m ERROR 12-05 15:16:09 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4060908)[0;0m ERROR 12-05 15:16:09 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4060908)[0;0m ERROR 12-05 15:16:09 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4060908)[0;0m ERROR 12-05 15:16:09 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4060908)[0;0m ERROR 12-05 15:16:09 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060908)[0;0m ERROR 12-05 15:16:09 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4060908)[0;0m ERROR 12-05 15:16:09 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4060908)[0;0m ERROR 12-05 15:16:09 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060908)[0;0m ERROR 12-05 15:16:09 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4060908)[0;0m ERROR 12-05 15:16:09 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4060908)[0;0m ERROR 12-05 15:16:09 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060908)[0;0m ERROR 12-05 15:16:09 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4060908)[0;0m ERROR 12-05 15:16:09 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4060908)[0;0m ERROR 12-05 15:16:09 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4060908)[0;0m ERROR 12-05 15:16:09 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4060908)[0;0m ERROR 12-05 15:16:09 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060908)[0;0m ERROR 12-05 15:16:09 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4060908)[0;0m ERROR 12-05 15:16:09 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4060908)[0;0m ERROR 12-05 15:16:09 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060908)[0;0m ERROR 12-05 15:16:09 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4060908)[0;0m ERROR 12-05 15:16:09 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4060908)[0;0m ERROR 12-05 15:16:09 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060908)[0;0m ERROR 12-05 15:16:09 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4060908)[0;0m ERROR 12-05 15:16:09 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4060908)[0;0m ERROR 12-05 15:16:09 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4060908)[0;0m ERROR 12-05 15:16:09 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4060908)[0;0m ERROR 12-05 15:16:09 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060908)[0;0m ERROR 12-05 15:16:09 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4060908)[0;0m ERROR 12-05 15:16:09 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4060908)[0;0m ERROR 12-05 15:16:09 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060908)[0;0m ERROR 12-05 15:16:09 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4060908)[0;0m ERROR 12-05 15:16:09 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4060908)[0;0m ERROR 12-05 15:16:09 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060908)[0;0m ERROR 12-05 15:16:09 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4060908)[0;0m ERROR 12-05 15:16:09 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4060908)[0;0m ERROR 12-05 15:16:09 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4060908)[0;0m ERROR 12-05 15:16:09 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4060908)[0;0m ERROR 12-05 15:16:09 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4060908)[0;0m ERROR 12-05 15:16:09 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7f6a91deeb40>' raised:
[1;36m(EngineCore_DP0 pid=4060908)[0;0m ERROR 12-05 15:16:09 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4060908)[0;0m ERROR 12-05 15:16:09 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4060908)[0;0m ERROR 12-05 15:16:09 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4060908)[0;0m ERROR 12-05 15:16:09 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:16:31 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:16:31 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:16:31 [model.py:1745] Using max model len 131072
INFO 12-05 15:16:31 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4061085)[0;0m INFO 12-05 15:16:39 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4061085)[0;0m INFO 12-05 15:16:40 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:48003 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4061085)[0;0m INFO 12-05 15:16:40 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4061085)[0;0m INFO 12-05 15:16:40 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4061085)[0;0m INFO 12-05 15:16:41 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4061085)[0;0m INFO 12-05 15:16:41 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4061085)[0;0m INFO 12-05 15:16:42 [default_loader.py:314] Loading weights took 0.88 seconds
[1;36m(EngineCore_DP0 pid=4061085)[0;0m INFO 12-05 15:16:42 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.424749 seconds
[1;36m(EngineCore_DP0 pid=4061085)[0;0m INFO 12-05 15:16:46 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4061085)[0;0m INFO 12-05 15:16:46 [backends.py:647] Dynamo bytecode transform time: 3.66 s
[1;36m(EngineCore_DP0 pid=4061085)[0;0m ERROR 12-05 15:16:46 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4061085)[0;0m ERROR 12-05 15:16:46 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4061085)[0;0m ERROR 12-05 15:16:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4061085)[0;0m ERROR 12-05 15:16:46 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4061085)[0;0m ERROR 12-05 15:16:46 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061085)[0;0m ERROR 12-05 15:16:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4061085)[0;0m ERROR 12-05 15:16:46 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4061085)[0;0m ERROR 12-05 15:16:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4061085)[0;0m ERROR 12-05 15:16:46 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4061085)[0;0m ERROR 12-05 15:16:46 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061085)[0;0m ERROR 12-05 15:16:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4061085)[0;0m ERROR 12-05 15:16:46 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4061085)[0;0m ERROR 12-05 15:16:46 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061085)[0;0m ERROR 12-05 15:16:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4061085)[0;0m ERROR 12-05 15:16:46 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4061085)[0;0m ERROR 12-05 15:16:46 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061085)[0;0m ERROR 12-05 15:16:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4061085)[0;0m ERROR 12-05 15:16:46 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4061085)[0;0m ERROR 12-05 15:16:46 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061085)[0;0m ERROR 12-05 15:16:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4061085)[0;0m ERROR 12-05 15:16:46 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4061085)[0;0m ERROR 12-05 15:16:46 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061085)[0;0m ERROR 12-05 15:16:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4061085)[0;0m ERROR 12-05 15:16:46 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4061085)[0;0m ERROR 12-05 15:16:46 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061085)[0;0m ERROR 12-05 15:16:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4061085)[0;0m ERROR 12-05 15:16:46 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4061085)[0;0m ERROR 12-05 15:16:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4061085)[0;0m ERROR 12-05 15:16:46 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4061085)[0;0m ERROR 12-05 15:16:46 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061085)[0;0m ERROR 12-05 15:16:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4061085)[0;0m ERROR 12-05 15:16:46 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4061085)[0;0m ERROR 12-05 15:16:46 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061085)[0;0m ERROR 12-05 15:16:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4061085)[0;0m ERROR 12-05 15:16:46 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4061085)[0;0m ERROR 12-05 15:16:46 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061085)[0;0m ERROR 12-05 15:16:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4061085)[0;0m ERROR 12-05 15:16:46 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4061085)[0;0m ERROR 12-05 15:16:46 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061085)[0;0m ERROR 12-05 15:16:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4061085)[0;0m ERROR 12-05 15:16:46 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4061085)[0;0m ERROR 12-05 15:16:46 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061085)[0;0m ERROR 12-05 15:16:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4061085)[0;0m ERROR 12-05 15:16:46 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4061085)[0;0m ERROR 12-05 15:16:46 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061085)[0;0m ERROR 12-05 15:16:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4061085)[0;0m ERROR 12-05 15:16:46 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4061085)[0;0m ERROR 12-05 15:16:46 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061085)[0;0m ERROR 12-05 15:16:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4061085)[0;0m ERROR 12-05 15:16:46 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4061085)[0;0m ERROR 12-05 15:16:46 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061085)[0;0m ERROR 12-05 15:16:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4061085)[0;0m ERROR 12-05 15:16:46 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4061085)[0;0m ERROR 12-05 15:16:46 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061085)[0;0m ERROR 12-05 15:16:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4061085)[0;0m ERROR 12-05 15:16:46 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4061085)[0;0m ERROR 12-05 15:16:46 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061085)[0;0m ERROR 12-05 15:16:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4061085)[0;0m ERROR 12-05 15:16:46 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4061085)[0;0m ERROR 12-05 15:16:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4061085)[0;0m ERROR 12-05 15:16:46 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4061085)[0;0m ERROR 12-05 15:16:46 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061085)[0;0m ERROR 12-05 15:16:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4061085)[0;0m ERROR 12-05 15:16:46 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4061085)[0;0m ERROR 12-05 15:16:46 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061085)[0;0m ERROR 12-05 15:16:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4061085)[0;0m ERROR 12-05 15:16:46 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4061085)[0;0m ERROR 12-05 15:16:46 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061085)[0;0m ERROR 12-05 15:16:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4061085)[0;0m ERROR 12-05 15:16:46 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4061085)[0;0m ERROR 12-05 15:16:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4061085)[0;0m ERROR 12-05 15:16:46 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4061085)[0;0m ERROR 12-05 15:16:46 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061085)[0;0m ERROR 12-05 15:16:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4061085)[0;0m ERROR 12-05 15:16:46 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4061085)[0;0m ERROR 12-05 15:16:46 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061085)[0;0m ERROR 12-05 15:16:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4061085)[0;0m ERROR 12-05 15:16:46 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4061085)[0;0m ERROR 12-05 15:16:46 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061085)[0;0m ERROR 12-05 15:16:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4061085)[0;0m ERROR 12-05 15:16:46 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4061085)[0;0m ERROR 12-05 15:16:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4061085)[0;0m ERROR 12-05 15:16:46 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4061085)[0;0m ERROR 12-05 15:16:46 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061085)[0;0m ERROR 12-05 15:16:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4061085)[0;0m ERROR 12-05 15:16:46 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4061085)[0;0m ERROR 12-05 15:16:46 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061085)[0;0m ERROR 12-05 15:16:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4061085)[0;0m ERROR 12-05 15:16:46 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4061085)[0;0m ERROR 12-05 15:16:46 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061085)[0;0m ERROR 12-05 15:16:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4061085)[0;0m ERROR 12-05 15:16:46 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4061085)[0;0m ERROR 12-05 15:16:46 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061085)[0;0m ERROR 12-05 15:16:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4061085)[0;0m ERROR 12-05 15:16:46 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4061085)[0;0m ERROR 12-05 15:16:46 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7f71c4118170>' raised:
[1;36m(EngineCore_DP0 pid=4061085)[0;0m ERROR 12-05 15:16:46 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4061085)[0;0m ERROR 12-05 15:16:46 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4061085)[0;0m ERROR 12-05 15:16:46 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4061085)[0;0m ERROR 12-05 15:16:46 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:17:08 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:17:08 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:17:08 [model.py:1745] Using max model len 131072
INFO 12-05 15:17:08 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4061169)[0;0m INFO 12-05 15:17:15 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4061169)[0;0m INFO 12-05 15:17:16 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:44701 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4061169)[0;0m INFO 12-05 15:17:16 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4061169)[0;0m INFO 12-05 15:17:16 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4061169)[0;0m INFO 12-05 15:17:17 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4061169)[0;0m INFO 12-05 15:17:17 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4061169)[0;0m INFO 12-05 15:17:18 [default_loader.py:314] Loading weights took 0.87 seconds
[1;36m(EngineCore_DP0 pid=4061169)[0;0m INFO 12-05 15:17:18 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.408493 seconds
[1;36m(EngineCore_DP0 pid=4061169)[0;0m INFO 12-05 15:17:22 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4061169)[0;0m INFO 12-05 15:17:22 [backends.py:647] Dynamo bytecode transform time: 3.60 s
[1;36m(EngineCore_DP0 pid=4061169)[0;0m ERROR 12-05 15:17:22 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4061169)[0;0m ERROR 12-05 15:17:22 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4061169)[0;0m ERROR 12-05 15:17:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4061169)[0;0m ERROR 12-05 15:17:22 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4061169)[0;0m ERROR 12-05 15:17:22 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061169)[0;0m ERROR 12-05 15:17:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4061169)[0;0m ERROR 12-05 15:17:22 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4061169)[0;0m ERROR 12-05 15:17:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4061169)[0;0m ERROR 12-05 15:17:22 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4061169)[0;0m ERROR 12-05 15:17:22 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061169)[0;0m ERROR 12-05 15:17:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4061169)[0;0m ERROR 12-05 15:17:22 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4061169)[0;0m ERROR 12-05 15:17:22 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061169)[0;0m ERROR 12-05 15:17:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4061169)[0;0m ERROR 12-05 15:17:22 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4061169)[0;0m ERROR 12-05 15:17:22 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061169)[0;0m ERROR 12-05 15:17:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4061169)[0;0m ERROR 12-05 15:17:22 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4061169)[0;0m ERROR 12-05 15:17:22 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061169)[0;0m ERROR 12-05 15:17:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4061169)[0;0m ERROR 12-05 15:17:22 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4061169)[0;0m ERROR 12-05 15:17:22 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061169)[0;0m ERROR 12-05 15:17:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4061169)[0;0m ERROR 12-05 15:17:22 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4061169)[0;0m ERROR 12-05 15:17:22 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061169)[0;0m ERROR 12-05 15:17:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4061169)[0;0m ERROR 12-05 15:17:22 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4061169)[0;0m ERROR 12-05 15:17:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4061169)[0;0m ERROR 12-05 15:17:22 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4061169)[0;0m ERROR 12-05 15:17:22 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061169)[0;0m ERROR 12-05 15:17:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4061169)[0;0m ERROR 12-05 15:17:22 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4061169)[0;0m ERROR 12-05 15:17:22 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061169)[0;0m ERROR 12-05 15:17:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4061169)[0;0m ERROR 12-05 15:17:22 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4061169)[0;0m ERROR 12-05 15:17:22 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061169)[0;0m ERROR 12-05 15:17:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4061169)[0;0m ERROR 12-05 15:17:22 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4061169)[0;0m ERROR 12-05 15:17:22 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061169)[0;0m ERROR 12-05 15:17:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4061169)[0;0m ERROR 12-05 15:17:22 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4061169)[0;0m ERROR 12-05 15:17:22 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061169)[0;0m ERROR 12-05 15:17:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4061169)[0;0m ERROR 12-05 15:17:22 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4061169)[0;0m ERROR 12-05 15:17:22 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061169)[0;0m ERROR 12-05 15:17:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4061169)[0;0m ERROR 12-05 15:17:22 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4061169)[0;0m ERROR 12-05 15:17:22 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061169)[0;0m ERROR 12-05 15:17:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4061169)[0;0m ERROR 12-05 15:17:22 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4061169)[0;0m ERROR 12-05 15:17:22 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061169)[0;0m ERROR 12-05 15:17:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4061169)[0;0m ERROR 12-05 15:17:22 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4061169)[0;0m ERROR 12-05 15:17:22 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061169)[0;0m ERROR 12-05 15:17:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4061169)[0;0m ERROR 12-05 15:17:22 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4061169)[0;0m ERROR 12-05 15:17:22 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061169)[0;0m ERROR 12-05 15:17:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4061169)[0;0m ERROR 12-05 15:17:22 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4061169)[0;0m ERROR 12-05 15:17:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4061169)[0;0m ERROR 12-05 15:17:22 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4061169)[0;0m ERROR 12-05 15:17:22 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061169)[0;0m ERROR 12-05 15:17:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4061169)[0;0m ERROR 12-05 15:17:22 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4061169)[0;0m ERROR 12-05 15:17:22 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061169)[0;0m ERROR 12-05 15:17:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4061169)[0;0m ERROR 12-05 15:17:22 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4061169)[0;0m ERROR 12-05 15:17:22 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061169)[0;0m ERROR 12-05 15:17:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4061169)[0;0m ERROR 12-05 15:17:22 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4061169)[0;0m ERROR 12-05 15:17:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4061169)[0;0m ERROR 12-05 15:17:22 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4061169)[0;0m ERROR 12-05 15:17:22 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061169)[0;0m ERROR 12-05 15:17:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4061169)[0;0m ERROR 12-05 15:17:22 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4061169)[0;0m ERROR 12-05 15:17:22 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061169)[0;0m ERROR 12-05 15:17:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4061169)[0;0m ERROR 12-05 15:17:22 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4061169)[0;0m ERROR 12-05 15:17:22 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061169)[0;0m ERROR 12-05 15:17:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4061169)[0;0m ERROR 12-05 15:17:22 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4061169)[0;0m ERROR 12-05 15:17:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4061169)[0;0m ERROR 12-05 15:17:22 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4061169)[0;0m ERROR 12-05 15:17:22 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061169)[0;0m ERROR 12-05 15:17:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4061169)[0;0m ERROR 12-05 15:17:22 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4061169)[0;0m ERROR 12-05 15:17:22 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061169)[0;0m ERROR 12-05 15:17:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4061169)[0;0m ERROR 12-05 15:17:22 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4061169)[0;0m ERROR 12-05 15:17:22 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061169)[0;0m ERROR 12-05 15:17:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4061169)[0;0m ERROR 12-05 15:17:22 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4061169)[0;0m ERROR 12-05 15:17:22 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061169)[0;0m ERROR 12-05 15:17:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4061169)[0;0m ERROR 12-05 15:17:22 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4061169)[0;0m ERROR 12-05 15:17:22 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7f52e82e5df0>' raised:
[1;36m(EngineCore_DP0 pid=4061169)[0;0m ERROR 12-05 15:17:22 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4061169)[0;0m ERROR 12-05 15:17:22 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4061169)[0;0m ERROR 12-05 15:17:22 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4061169)[0;0m ERROR 12-05 15:17:22 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:17:44 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:17:44 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:17:44 [model.py:1745] Using max model len 131072
INFO 12-05 15:17:44 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4061258)[0;0m INFO 12-05 15:17:51 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4061258)[0;0m INFO 12-05 15:17:52 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:42809 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4061258)[0;0m INFO 12-05 15:17:52 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4061258)[0;0m INFO 12-05 15:17:52 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4061258)[0;0m INFO 12-05 15:17:53 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4061258)[0;0m INFO 12-05 15:17:53 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4061258)[0;0m INFO 12-05 15:17:54 [default_loader.py:314] Loading weights took 0.87 seconds
[1;36m(EngineCore_DP0 pid=4061258)[0;0m INFO 12-05 15:17:54 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.447091 seconds
[1;36m(EngineCore_DP0 pid=4061258)[0;0m INFO 12-05 15:17:58 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4061258)[0;0m INFO 12-05 15:17:58 [backends.py:647] Dynamo bytecode transform time: 3.63 s
[1;36m(EngineCore_DP0 pid=4061258)[0;0m ERROR 12-05 15:17:59 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4061258)[0;0m ERROR 12-05 15:17:59 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4061258)[0;0m ERROR 12-05 15:17:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4061258)[0;0m ERROR 12-05 15:17:59 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4061258)[0;0m ERROR 12-05 15:17:59 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061258)[0;0m ERROR 12-05 15:17:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4061258)[0;0m ERROR 12-05 15:17:59 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4061258)[0;0m ERROR 12-05 15:17:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4061258)[0;0m ERROR 12-05 15:17:59 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4061258)[0;0m ERROR 12-05 15:17:59 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061258)[0;0m ERROR 12-05 15:17:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4061258)[0;0m ERROR 12-05 15:17:59 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4061258)[0;0m ERROR 12-05 15:17:59 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061258)[0;0m ERROR 12-05 15:17:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4061258)[0;0m ERROR 12-05 15:17:59 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4061258)[0;0m ERROR 12-05 15:17:59 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061258)[0;0m ERROR 12-05 15:17:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4061258)[0;0m ERROR 12-05 15:17:59 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4061258)[0;0m ERROR 12-05 15:17:59 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061258)[0;0m ERROR 12-05 15:17:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4061258)[0;0m ERROR 12-05 15:17:59 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4061258)[0;0m ERROR 12-05 15:17:59 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061258)[0;0m ERROR 12-05 15:17:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4061258)[0;0m ERROR 12-05 15:17:59 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4061258)[0;0m ERROR 12-05 15:17:59 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061258)[0;0m ERROR 12-05 15:17:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4061258)[0;0m ERROR 12-05 15:17:59 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4061258)[0;0m ERROR 12-05 15:17:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4061258)[0;0m ERROR 12-05 15:17:59 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4061258)[0;0m ERROR 12-05 15:17:59 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061258)[0;0m ERROR 12-05 15:17:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4061258)[0;0m ERROR 12-05 15:17:59 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4061258)[0;0m ERROR 12-05 15:17:59 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061258)[0;0m ERROR 12-05 15:17:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4061258)[0;0m ERROR 12-05 15:17:59 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4061258)[0;0m ERROR 12-05 15:17:59 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061258)[0;0m ERROR 12-05 15:17:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4061258)[0;0m ERROR 12-05 15:17:59 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4061258)[0;0m ERROR 12-05 15:17:59 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061258)[0;0m ERROR 12-05 15:17:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4061258)[0;0m ERROR 12-05 15:17:59 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4061258)[0;0m ERROR 12-05 15:17:59 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061258)[0;0m ERROR 12-05 15:17:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4061258)[0;0m ERROR 12-05 15:17:59 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4061258)[0;0m ERROR 12-05 15:17:59 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061258)[0;0m ERROR 12-05 15:17:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4061258)[0;0m ERROR 12-05 15:17:59 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4061258)[0;0m ERROR 12-05 15:17:59 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061258)[0;0m ERROR 12-05 15:17:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4061258)[0;0m ERROR 12-05 15:17:59 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4061258)[0;0m ERROR 12-05 15:17:59 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061258)[0;0m ERROR 12-05 15:17:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4061258)[0;0m ERROR 12-05 15:17:59 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4061258)[0;0m ERROR 12-05 15:17:59 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061258)[0;0m ERROR 12-05 15:17:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4061258)[0;0m ERROR 12-05 15:17:59 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4061258)[0;0m ERROR 12-05 15:17:59 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061258)[0;0m ERROR 12-05 15:17:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4061258)[0;0m ERROR 12-05 15:17:59 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4061258)[0;0m ERROR 12-05 15:17:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4061258)[0;0m ERROR 12-05 15:17:59 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4061258)[0;0m ERROR 12-05 15:17:59 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061258)[0;0m ERROR 12-05 15:17:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4061258)[0;0m ERROR 12-05 15:17:59 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4061258)[0;0m ERROR 12-05 15:17:59 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061258)[0;0m ERROR 12-05 15:17:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4061258)[0;0m ERROR 12-05 15:17:59 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4061258)[0;0m ERROR 12-05 15:17:59 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061258)[0;0m ERROR 12-05 15:17:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4061258)[0;0m ERROR 12-05 15:17:59 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4061258)[0;0m ERROR 12-05 15:17:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4061258)[0;0m ERROR 12-05 15:17:59 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4061258)[0;0m ERROR 12-05 15:17:59 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061258)[0;0m ERROR 12-05 15:17:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4061258)[0;0m ERROR 12-05 15:17:59 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4061258)[0;0m ERROR 12-05 15:17:59 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061258)[0;0m ERROR 12-05 15:17:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4061258)[0;0m ERROR 12-05 15:17:59 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4061258)[0;0m ERROR 12-05 15:17:59 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061258)[0;0m ERROR 12-05 15:17:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4061258)[0;0m ERROR 12-05 15:17:59 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4061258)[0;0m ERROR 12-05 15:17:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4061258)[0;0m ERROR 12-05 15:17:59 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4061258)[0;0m ERROR 12-05 15:17:59 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061258)[0;0m ERROR 12-05 15:17:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4061258)[0;0m ERROR 12-05 15:17:59 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4061258)[0;0m ERROR 12-05 15:17:59 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061258)[0;0m ERROR 12-05 15:17:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4061258)[0;0m ERROR 12-05 15:17:59 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4061258)[0;0m ERROR 12-05 15:17:59 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061258)[0;0m ERROR 12-05 15:17:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4061258)[0;0m ERROR 12-05 15:17:59 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4061258)[0;0m ERROR 12-05 15:17:59 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061258)[0;0m ERROR 12-05 15:17:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4061258)[0;0m ERROR 12-05 15:17:59 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4061258)[0;0m ERROR 12-05 15:17:59 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7f4aa13346e0>' raised:
[1;36m(EngineCore_DP0 pid=4061258)[0;0m ERROR 12-05 15:17:59 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4061258)[0;0m ERROR 12-05 15:17:59 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4061258)[0;0m ERROR 12-05 15:17:59 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4061258)[0;0m ERROR 12-05 15:17:59 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:18:20 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:18:20 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:18:20 [model.py:1745] Using max model len 131072
INFO 12-05 15:18:20 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4061334)[0;0m INFO 12-05 15:18:27 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4061334)[0;0m INFO 12-05 15:18:28 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:40383 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4061334)[0;0m INFO 12-05 15:18:28 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4061334)[0;0m INFO 12-05 15:18:28 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4061334)[0;0m INFO 12-05 15:18:29 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4061334)[0;0m INFO 12-05 15:18:29 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4061334)[0;0m INFO 12-05 15:18:30 [default_loader.py:314] Loading weights took 0.88 seconds
[1;36m(EngineCore_DP0 pid=4061334)[0;0m INFO 12-05 15:18:31 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.417987 seconds
[1;36m(EngineCore_DP0 pid=4061334)[0;0m INFO 12-05 15:18:34 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4061334)[0;0m INFO 12-05 15:18:34 [backends.py:647] Dynamo bytecode transform time: 3.65 s
[1;36m(EngineCore_DP0 pid=4061334)[0;0m ERROR 12-05 15:18:35 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4061334)[0;0m ERROR 12-05 15:18:35 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4061334)[0;0m ERROR 12-05 15:18:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4061334)[0;0m ERROR 12-05 15:18:35 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4061334)[0;0m ERROR 12-05 15:18:35 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061334)[0;0m ERROR 12-05 15:18:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4061334)[0;0m ERROR 12-05 15:18:35 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4061334)[0;0m ERROR 12-05 15:18:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4061334)[0;0m ERROR 12-05 15:18:35 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4061334)[0;0m ERROR 12-05 15:18:35 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061334)[0;0m ERROR 12-05 15:18:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4061334)[0;0m ERROR 12-05 15:18:35 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4061334)[0;0m ERROR 12-05 15:18:35 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061334)[0;0m ERROR 12-05 15:18:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4061334)[0;0m ERROR 12-05 15:18:35 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4061334)[0;0m ERROR 12-05 15:18:35 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061334)[0;0m ERROR 12-05 15:18:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4061334)[0;0m ERROR 12-05 15:18:35 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4061334)[0;0m ERROR 12-05 15:18:35 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061334)[0;0m ERROR 12-05 15:18:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4061334)[0;0m ERROR 12-05 15:18:35 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4061334)[0;0m ERROR 12-05 15:18:35 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061334)[0;0m ERROR 12-05 15:18:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4061334)[0;0m ERROR 12-05 15:18:35 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4061334)[0;0m ERROR 12-05 15:18:35 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061334)[0;0m ERROR 12-05 15:18:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4061334)[0;0m ERROR 12-05 15:18:35 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4061334)[0;0m ERROR 12-05 15:18:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4061334)[0;0m ERROR 12-05 15:18:35 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4061334)[0;0m ERROR 12-05 15:18:35 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061334)[0;0m ERROR 12-05 15:18:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4061334)[0;0m ERROR 12-05 15:18:35 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4061334)[0;0m ERROR 12-05 15:18:35 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061334)[0;0m ERROR 12-05 15:18:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4061334)[0;0m ERROR 12-05 15:18:35 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4061334)[0;0m ERROR 12-05 15:18:35 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061334)[0;0m ERROR 12-05 15:18:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4061334)[0;0m ERROR 12-05 15:18:35 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4061334)[0;0m ERROR 12-05 15:18:35 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061334)[0;0m ERROR 12-05 15:18:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4061334)[0;0m ERROR 12-05 15:18:35 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4061334)[0;0m ERROR 12-05 15:18:35 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061334)[0;0m ERROR 12-05 15:18:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4061334)[0;0m ERROR 12-05 15:18:35 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4061334)[0;0m ERROR 12-05 15:18:35 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061334)[0;0m ERROR 12-05 15:18:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4061334)[0;0m ERROR 12-05 15:18:35 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4061334)[0;0m ERROR 12-05 15:18:35 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061334)[0;0m ERROR 12-05 15:18:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4061334)[0;0m ERROR 12-05 15:18:35 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4061334)[0;0m ERROR 12-05 15:18:35 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061334)[0;0m ERROR 12-05 15:18:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4061334)[0;0m ERROR 12-05 15:18:35 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4061334)[0;0m ERROR 12-05 15:18:35 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061334)[0;0m ERROR 12-05 15:18:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4061334)[0;0m ERROR 12-05 15:18:35 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4061334)[0;0m ERROR 12-05 15:18:35 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061334)[0;0m ERROR 12-05 15:18:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4061334)[0;0m ERROR 12-05 15:18:35 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4061334)[0;0m ERROR 12-05 15:18:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4061334)[0;0m ERROR 12-05 15:18:35 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4061334)[0;0m ERROR 12-05 15:18:35 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061334)[0;0m ERROR 12-05 15:18:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4061334)[0;0m ERROR 12-05 15:18:35 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4061334)[0;0m ERROR 12-05 15:18:35 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061334)[0;0m ERROR 12-05 15:18:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4061334)[0;0m ERROR 12-05 15:18:35 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4061334)[0;0m ERROR 12-05 15:18:35 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061334)[0;0m ERROR 12-05 15:18:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4061334)[0;0m ERROR 12-05 15:18:35 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4061334)[0;0m ERROR 12-05 15:18:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4061334)[0;0m ERROR 12-05 15:18:35 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4061334)[0;0m ERROR 12-05 15:18:35 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061334)[0;0m ERROR 12-05 15:18:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4061334)[0;0m ERROR 12-05 15:18:35 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4061334)[0;0m ERROR 12-05 15:18:35 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061334)[0;0m ERROR 12-05 15:18:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4061334)[0;0m ERROR 12-05 15:18:35 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4061334)[0;0m ERROR 12-05 15:18:35 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061334)[0;0m ERROR 12-05 15:18:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4061334)[0;0m ERROR 12-05 15:18:35 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4061334)[0;0m ERROR 12-05 15:18:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4061334)[0;0m ERROR 12-05 15:18:35 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4061334)[0;0m ERROR 12-05 15:18:35 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061334)[0;0m ERROR 12-05 15:18:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4061334)[0;0m ERROR 12-05 15:18:35 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4061334)[0;0m ERROR 12-05 15:18:35 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061334)[0;0m ERROR 12-05 15:18:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4061334)[0;0m ERROR 12-05 15:18:35 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4061334)[0;0m ERROR 12-05 15:18:35 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061334)[0;0m ERROR 12-05 15:18:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4061334)[0;0m ERROR 12-05 15:18:35 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4061334)[0;0m ERROR 12-05 15:18:35 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061334)[0;0m ERROR 12-05 15:18:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4061334)[0;0m ERROR 12-05 15:18:35 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4061334)[0;0m ERROR 12-05 15:18:35 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7f6a2c15bbf0>' raised:
[1;36m(EngineCore_DP0 pid=4061334)[0;0m ERROR 12-05 15:18:35 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4061334)[0;0m ERROR 12-05 15:18:35 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4061334)[0;0m ERROR 12-05 15:18:35 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4061334)[0;0m ERROR 12-05 15:18:35 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:18:56 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:18:56 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:18:56 [model.py:1745] Using max model len 131072
INFO 12-05 15:18:56 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4061423)[0;0m INFO 12-05 15:19:03 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4061423)[0;0m INFO 12-05 15:19:04 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:52463 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4061423)[0;0m INFO 12-05 15:19:04 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4061423)[0;0m INFO 12-05 15:19:05 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4061423)[0;0m INFO 12-05 15:19:05 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4061423)[0;0m INFO 12-05 15:19:05 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4061423)[0;0m INFO 12-05 15:19:06 [default_loader.py:314] Loading weights took 0.87 seconds
[1;36m(EngineCore_DP0 pid=4061423)[0;0m INFO 12-05 15:19:07 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.409257 seconds
[1;36m(EngineCore_DP0 pid=4061423)[0;0m INFO 12-05 15:19:10 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4061423)[0;0m INFO 12-05 15:19:10 [backends.py:647] Dynamo bytecode transform time: 3.63 s
[1;36m(EngineCore_DP0 pid=4061423)[0;0m ERROR 12-05 15:19:11 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4061423)[0;0m ERROR 12-05 15:19:11 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4061423)[0;0m ERROR 12-05 15:19:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4061423)[0;0m ERROR 12-05 15:19:11 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4061423)[0;0m ERROR 12-05 15:19:11 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061423)[0;0m ERROR 12-05 15:19:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4061423)[0;0m ERROR 12-05 15:19:11 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4061423)[0;0m ERROR 12-05 15:19:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4061423)[0;0m ERROR 12-05 15:19:11 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4061423)[0;0m ERROR 12-05 15:19:11 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061423)[0;0m ERROR 12-05 15:19:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4061423)[0;0m ERROR 12-05 15:19:11 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4061423)[0;0m ERROR 12-05 15:19:11 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061423)[0;0m ERROR 12-05 15:19:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4061423)[0;0m ERROR 12-05 15:19:11 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4061423)[0;0m ERROR 12-05 15:19:11 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061423)[0;0m ERROR 12-05 15:19:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4061423)[0;0m ERROR 12-05 15:19:11 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4061423)[0;0m ERROR 12-05 15:19:11 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061423)[0;0m ERROR 12-05 15:19:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4061423)[0;0m ERROR 12-05 15:19:11 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4061423)[0;0m ERROR 12-05 15:19:11 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061423)[0;0m ERROR 12-05 15:19:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4061423)[0;0m ERROR 12-05 15:19:11 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4061423)[0;0m ERROR 12-05 15:19:11 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061423)[0;0m ERROR 12-05 15:19:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4061423)[0;0m ERROR 12-05 15:19:11 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4061423)[0;0m ERROR 12-05 15:19:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4061423)[0;0m ERROR 12-05 15:19:11 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4061423)[0;0m ERROR 12-05 15:19:11 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061423)[0;0m ERROR 12-05 15:19:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4061423)[0;0m ERROR 12-05 15:19:11 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4061423)[0;0m ERROR 12-05 15:19:11 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061423)[0;0m ERROR 12-05 15:19:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4061423)[0;0m ERROR 12-05 15:19:11 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4061423)[0;0m ERROR 12-05 15:19:11 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061423)[0;0m ERROR 12-05 15:19:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4061423)[0;0m ERROR 12-05 15:19:11 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4061423)[0;0m ERROR 12-05 15:19:11 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061423)[0;0m ERROR 12-05 15:19:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4061423)[0;0m ERROR 12-05 15:19:11 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4061423)[0;0m ERROR 12-05 15:19:11 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061423)[0;0m ERROR 12-05 15:19:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4061423)[0;0m ERROR 12-05 15:19:11 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4061423)[0;0m ERROR 12-05 15:19:11 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061423)[0;0m ERROR 12-05 15:19:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4061423)[0;0m ERROR 12-05 15:19:11 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4061423)[0;0m ERROR 12-05 15:19:11 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061423)[0;0m ERROR 12-05 15:19:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4061423)[0;0m ERROR 12-05 15:19:11 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4061423)[0;0m ERROR 12-05 15:19:11 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061423)[0;0m ERROR 12-05 15:19:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4061423)[0;0m ERROR 12-05 15:19:11 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4061423)[0;0m ERROR 12-05 15:19:11 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061423)[0;0m ERROR 12-05 15:19:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4061423)[0;0m ERROR 12-05 15:19:11 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4061423)[0;0m ERROR 12-05 15:19:11 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061423)[0;0m ERROR 12-05 15:19:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4061423)[0;0m ERROR 12-05 15:19:11 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4061423)[0;0m ERROR 12-05 15:19:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4061423)[0;0m ERROR 12-05 15:19:11 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4061423)[0;0m ERROR 12-05 15:19:11 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061423)[0;0m ERROR 12-05 15:19:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4061423)[0;0m ERROR 12-05 15:19:11 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4061423)[0;0m ERROR 12-05 15:19:11 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061423)[0;0m ERROR 12-05 15:19:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4061423)[0;0m ERROR 12-05 15:19:11 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4061423)[0;0m ERROR 12-05 15:19:11 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061423)[0;0m ERROR 12-05 15:19:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4061423)[0;0m ERROR 12-05 15:19:11 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4061423)[0;0m ERROR 12-05 15:19:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4061423)[0;0m ERROR 12-05 15:19:11 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4061423)[0;0m ERROR 12-05 15:19:11 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061423)[0;0m ERROR 12-05 15:19:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4061423)[0;0m ERROR 12-05 15:19:11 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4061423)[0;0m ERROR 12-05 15:19:11 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061423)[0;0m ERROR 12-05 15:19:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4061423)[0;0m ERROR 12-05 15:19:11 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4061423)[0;0m ERROR 12-05 15:19:11 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061423)[0;0m ERROR 12-05 15:19:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4061423)[0;0m ERROR 12-05 15:19:11 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4061423)[0;0m ERROR 12-05 15:19:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4061423)[0;0m ERROR 12-05 15:19:11 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4061423)[0;0m ERROR 12-05 15:19:11 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061423)[0;0m ERROR 12-05 15:19:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4061423)[0;0m ERROR 12-05 15:19:11 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4061423)[0;0m ERROR 12-05 15:19:11 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061423)[0;0m ERROR 12-05 15:19:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4061423)[0;0m ERROR 12-05 15:19:11 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4061423)[0;0m ERROR 12-05 15:19:11 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061423)[0;0m ERROR 12-05 15:19:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4061423)[0;0m ERROR 12-05 15:19:11 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4061423)[0;0m ERROR 12-05 15:19:11 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061423)[0;0m ERROR 12-05 15:19:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4061423)[0;0m ERROR 12-05 15:19:11 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4061423)[0;0m ERROR 12-05 15:19:11 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7facfc257c20>' raised:
[1;36m(EngineCore_DP0 pid=4061423)[0;0m ERROR 12-05 15:19:11 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4061423)[0;0m ERROR 12-05 15:19:11 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4061423)[0;0m ERROR 12-05 15:19:11 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4061423)[0;0m ERROR 12-05 15:19:11 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:19:32 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:19:32 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:19:32 [model.py:1745] Using max model len 131072
INFO 12-05 15:19:32 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4061507)[0;0m INFO 12-05 15:19:39 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4061507)[0;0m INFO 12-05 15:19:40 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:34543 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4061507)[0;0m INFO 12-05 15:19:40 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4061507)[0;0m INFO 12-05 15:19:41 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4061507)[0;0m INFO 12-05 15:19:41 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4061507)[0;0m INFO 12-05 15:19:41 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4061507)[0;0m INFO 12-05 15:19:42 [default_loader.py:314] Loading weights took 0.87 seconds
[1;36m(EngineCore_DP0 pid=4061507)[0;0m INFO 12-05 15:19:43 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.392425 seconds
[1;36m(EngineCore_DP0 pid=4061507)[0;0m INFO 12-05 15:19:46 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4061507)[0;0m INFO 12-05 15:19:46 [backends.py:647] Dynamo bytecode transform time: 3.60 s
[1;36m(EngineCore_DP0 pid=4061507)[0;0m ERROR 12-05 15:19:47 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4061507)[0;0m ERROR 12-05 15:19:47 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4061507)[0;0m ERROR 12-05 15:19:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4061507)[0;0m ERROR 12-05 15:19:47 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4061507)[0;0m ERROR 12-05 15:19:47 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061507)[0;0m ERROR 12-05 15:19:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4061507)[0;0m ERROR 12-05 15:19:47 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4061507)[0;0m ERROR 12-05 15:19:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4061507)[0;0m ERROR 12-05 15:19:47 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4061507)[0;0m ERROR 12-05 15:19:47 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061507)[0;0m ERROR 12-05 15:19:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4061507)[0;0m ERROR 12-05 15:19:47 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4061507)[0;0m ERROR 12-05 15:19:47 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061507)[0;0m ERROR 12-05 15:19:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4061507)[0;0m ERROR 12-05 15:19:47 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4061507)[0;0m ERROR 12-05 15:19:47 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061507)[0;0m ERROR 12-05 15:19:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4061507)[0;0m ERROR 12-05 15:19:47 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4061507)[0;0m ERROR 12-05 15:19:47 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061507)[0;0m ERROR 12-05 15:19:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4061507)[0;0m ERROR 12-05 15:19:47 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4061507)[0;0m ERROR 12-05 15:19:47 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061507)[0;0m ERROR 12-05 15:19:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4061507)[0;0m ERROR 12-05 15:19:47 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4061507)[0;0m ERROR 12-05 15:19:47 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061507)[0;0m ERROR 12-05 15:19:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4061507)[0;0m ERROR 12-05 15:19:47 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4061507)[0;0m ERROR 12-05 15:19:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4061507)[0;0m ERROR 12-05 15:19:47 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4061507)[0;0m ERROR 12-05 15:19:47 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061507)[0;0m ERROR 12-05 15:19:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4061507)[0;0m ERROR 12-05 15:19:47 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4061507)[0;0m ERROR 12-05 15:19:47 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061507)[0;0m ERROR 12-05 15:19:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4061507)[0;0m ERROR 12-05 15:19:47 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4061507)[0;0m ERROR 12-05 15:19:47 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061507)[0;0m ERROR 12-05 15:19:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4061507)[0;0m ERROR 12-05 15:19:47 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4061507)[0;0m ERROR 12-05 15:19:47 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061507)[0;0m ERROR 12-05 15:19:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4061507)[0;0m ERROR 12-05 15:19:47 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4061507)[0;0m ERROR 12-05 15:19:47 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061507)[0;0m ERROR 12-05 15:19:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4061507)[0;0m ERROR 12-05 15:19:47 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4061507)[0;0m ERROR 12-05 15:19:47 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061507)[0;0m ERROR 12-05 15:19:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4061507)[0;0m ERROR 12-05 15:19:47 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4061507)[0;0m ERROR 12-05 15:19:47 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061507)[0;0m ERROR 12-05 15:19:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4061507)[0;0m ERROR 12-05 15:19:47 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4061507)[0;0m ERROR 12-05 15:19:47 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061507)[0;0m ERROR 12-05 15:19:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4061507)[0;0m ERROR 12-05 15:19:47 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4061507)[0;0m ERROR 12-05 15:19:47 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061507)[0;0m ERROR 12-05 15:19:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4061507)[0;0m ERROR 12-05 15:19:47 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4061507)[0;0m ERROR 12-05 15:19:47 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061507)[0;0m ERROR 12-05 15:19:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4061507)[0;0m ERROR 12-05 15:19:47 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4061507)[0;0m ERROR 12-05 15:19:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4061507)[0;0m ERROR 12-05 15:19:47 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4061507)[0;0m ERROR 12-05 15:19:47 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061507)[0;0m ERROR 12-05 15:19:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4061507)[0;0m ERROR 12-05 15:19:47 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4061507)[0;0m ERROR 12-05 15:19:47 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061507)[0;0m ERROR 12-05 15:19:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4061507)[0;0m ERROR 12-05 15:19:47 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4061507)[0;0m ERROR 12-05 15:19:47 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061507)[0;0m ERROR 12-05 15:19:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4061507)[0;0m ERROR 12-05 15:19:47 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4061507)[0;0m ERROR 12-05 15:19:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4061507)[0;0m ERROR 12-05 15:19:47 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4061507)[0;0m ERROR 12-05 15:19:47 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061507)[0;0m ERROR 12-05 15:19:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4061507)[0;0m ERROR 12-05 15:19:47 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4061507)[0;0m ERROR 12-05 15:19:47 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061507)[0;0m ERROR 12-05 15:19:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4061507)[0;0m ERROR 12-05 15:19:47 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4061507)[0;0m ERROR 12-05 15:19:47 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061507)[0;0m ERROR 12-05 15:19:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4061507)[0;0m ERROR 12-05 15:19:47 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4061507)[0;0m ERROR 12-05 15:19:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4061507)[0;0m ERROR 12-05 15:19:47 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4061507)[0;0m ERROR 12-05 15:19:47 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061507)[0;0m ERROR 12-05 15:19:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4061507)[0;0m ERROR 12-05 15:19:47 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4061507)[0;0m ERROR 12-05 15:19:47 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061507)[0;0m ERROR 12-05 15:19:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4061507)[0;0m ERROR 12-05 15:19:47 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4061507)[0;0m ERROR 12-05 15:19:47 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061507)[0;0m ERROR 12-05 15:19:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4061507)[0;0m ERROR 12-05 15:19:47 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4061507)[0;0m ERROR 12-05 15:19:47 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061507)[0;0m ERROR 12-05 15:19:47 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4061507)[0;0m ERROR 12-05 15:19:47 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4061507)[0;0m ERROR 12-05 15:19:47 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7fb23c075970>' raised:
[1;36m(EngineCore_DP0 pid=4061507)[0;0m ERROR 12-05 15:19:47 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4061507)[0;0m ERROR 12-05 15:19:47 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4061507)[0;0m ERROR 12-05 15:19:47 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4061507)[0;0m ERROR 12-05 15:19:47 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:20:08 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:20:08 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:20:08 [model.py:1745] Using max model len 131072
INFO 12-05 15:20:08 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4061594)[0;0m INFO 12-05 15:20:16 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4061594)[0;0m INFO 12-05 15:20:16 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:37777 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4061594)[0;0m INFO 12-05 15:20:16 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4061594)[0;0m INFO 12-05 15:20:17 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4061594)[0;0m INFO 12-05 15:20:17 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4061594)[0;0m INFO 12-05 15:20:17 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4061594)[0;0m INFO 12-05 15:20:18 [default_loader.py:314] Loading weights took 0.87 seconds
[1;36m(EngineCore_DP0 pid=4061594)[0;0m INFO 12-05 15:20:19 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.440348 seconds
[1;36m(EngineCore_DP0 pid=4061594)[0;0m INFO 12-05 15:20:23 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4061594)[0;0m INFO 12-05 15:20:23 [backends.py:647] Dynamo bytecode transform time: 3.65 s
[1;36m(EngineCore_DP0 pid=4061594)[0;0m ERROR 12-05 15:20:23 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4061594)[0;0m ERROR 12-05 15:20:23 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4061594)[0;0m ERROR 12-05 15:20:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4061594)[0;0m ERROR 12-05 15:20:23 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4061594)[0;0m ERROR 12-05 15:20:23 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061594)[0;0m ERROR 12-05 15:20:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4061594)[0;0m ERROR 12-05 15:20:23 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4061594)[0;0m ERROR 12-05 15:20:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4061594)[0;0m ERROR 12-05 15:20:23 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4061594)[0;0m ERROR 12-05 15:20:23 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061594)[0;0m ERROR 12-05 15:20:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4061594)[0;0m ERROR 12-05 15:20:23 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4061594)[0;0m ERROR 12-05 15:20:23 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061594)[0;0m ERROR 12-05 15:20:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4061594)[0;0m ERROR 12-05 15:20:23 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4061594)[0;0m ERROR 12-05 15:20:23 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061594)[0;0m ERROR 12-05 15:20:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4061594)[0;0m ERROR 12-05 15:20:23 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4061594)[0;0m ERROR 12-05 15:20:23 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061594)[0;0m ERROR 12-05 15:20:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4061594)[0;0m ERROR 12-05 15:20:23 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4061594)[0;0m ERROR 12-05 15:20:23 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061594)[0;0m ERROR 12-05 15:20:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4061594)[0;0m ERROR 12-05 15:20:23 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4061594)[0;0m ERROR 12-05 15:20:23 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061594)[0;0m ERROR 12-05 15:20:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4061594)[0;0m ERROR 12-05 15:20:23 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4061594)[0;0m ERROR 12-05 15:20:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4061594)[0;0m ERROR 12-05 15:20:23 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4061594)[0;0m ERROR 12-05 15:20:23 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061594)[0;0m ERROR 12-05 15:20:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4061594)[0;0m ERROR 12-05 15:20:23 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4061594)[0;0m ERROR 12-05 15:20:23 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061594)[0;0m ERROR 12-05 15:20:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4061594)[0;0m ERROR 12-05 15:20:23 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4061594)[0;0m ERROR 12-05 15:20:23 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061594)[0;0m ERROR 12-05 15:20:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4061594)[0;0m ERROR 12-05 15:20:23 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4061594)[0;0m ERROR 12-05 15:20:23 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061594)[0;0m ERROR 12-05 15:20:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4061594)[0;0m ERROR 12-05 15:20:23 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4061594)[0;0m ERROR 12-05 15:20:23 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061594)[0;0m ERROR 12-05 15:20:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4061594)[0;0m ERROR 12-05 15:20:23 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4061594)[0;0m ERROR 12-05 15:20:23 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061594)[0;0m ERROR 12-05 15:20:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4061594)[0;0m ERROR 12-05 15:20:23 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4061594)[0;0m ERROR 12-05 15:20:23 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061594)[0;0m ERROR 12-05 15:20:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4061594)[0;0m ERROR 12-05 15:20:23 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4061594)[0;0m ERROR 12-05 15:20:23 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061594)[0;0m ERROR 12-05 15:20:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4061594)[0;0m ERROR 12-05 15:20:23 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4061594)[0;0m ERROR 12-05 15:20:23 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061594)[0;0m ERROR 12-05 15:20:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4061594)[0;0m ERROR 12-05 15:20:23 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4061594)[0;0m ERROR 12-05 15:20:23 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061594)[0;0m ERROR 12-05 15:20:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4061594)[0;0m ERROR 12-05 15:20:23 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4061594)[0;0m ERROR 12-05 15:20:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4061594)[0;0m ERROR 12-05 15:20:23 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4061594)[0;0m ERROR 12-05 15:20:23 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061594)[0;0m ERROR 12-05 15:20:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4061594)[0;0m ERROR 12-05 15:20:23 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4061594)[0;0m ERROR 12-05 15:20:23 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061594)[0;0m ERROR 12-05 15:20:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4061594)[0;0m ERROR 12-05 15:20:23 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4061594)[0;0m ERROR 12-05 15:20:23 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061594)[0;0m ERROR 12-05 15:20:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4061594)[0;0m ERROR 12-05 15:20:23 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4061594)[0;0m ERROR 12-05 15:20:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4061594)[0;0m ERROR 12-05 15:20:23 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4061594)[0;0m ERROR 12-05 15:20:23 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061594)[0;0m ERROR 12-05 15:20:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4061594)[0;0m ERROR 12-05 15:20:23 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4061594)[0;0m ERROR 12-05 15:20:23 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061594)[0;0m ERROR 12-05 15:20:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4061594)[0;0m ERROR 12-05 15:20:23 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4061594)[0;0m ERROR 12-05 15:20:23 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061594)[0;0m ERROR 12-05 15:20:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4061594)[0;0m ERROR 12-05 15:20:23 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4061594)[0;0m ERROR 12-05 15:20:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4061594)[0;0m ERROR 12-05 15:20:23 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4061594)[0;0m ERROR 12-05 15:20:23 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061594)[0;0m ERROR 12-05 15:20:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4061594)[0;0m ERROR 12-05 15:20:23 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4061594)[0;0m ERROR 12-05 15:20:23 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061594)[0;0m ERROR 12-05 15:20:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4061594)[0;0m ERROR 12-05 15:20:23 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4061594)[0;0m ERROR 12-05 15:20:23 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061594)[0;0m ERROR 12-05 15:20:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4061594)[0;0m ERROR 12-05 15:20:23 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4061594)[0;0m ERROR 12-05 15:20:23 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061594)[0;0m ERROR 12-05 15:20:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4061594)[0;0m ERROR 12-05 15:20:23 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4061594)[0;0m ERROR 12-05 15:20:23 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7f806c154c80>' raised:
[1;36m(EngineCore_DP0 pid=4061594)[0;0m ERROR 12-05 15:20:23 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4061594)[0;0m ERROR 12-05 15:20:23 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4061594)[0;0m ERROR 12-05 15:20:23 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4061594)[0;0m ERROR 12-05 15:20:23 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:20:44 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:20:44 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:20:44 [model.py:1745] Using max model len 131072
INFO 12-05 15:20:44 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4061683)[0;0m INFO 12-05 15:20:52 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4061683)[0;0m INFO 12-05 15:20:52 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:34893 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4061683)[0;0m INFO 12-05 15:20:52 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4061683)[0;0m INFO 12-05 15:20:53 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4061683)[0;0m INFO 12-05 15:20:53 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4061683)[0;0m INFO 12-05 15:20:53 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4061683)[0;0m INFO 12-05 15:20:54 [default_loader.py:314] Loading weights took 0.87 seconds
[1;36m(EngineCore_DP0 pid=4061683)[0;0m INFO 12-05 15:20:55 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.394361 seconds
[1;36m(EngineCore_DP0 pid=4061683)[0;0m INFO 12-05 15:20:59 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4061683)[0;0m INFO 12-05 15:20:59 [backends.py:647] Dynamo bytecode transform time: 3.62 s
[1;36m(EngineCore_DP0 pid=4061683)[0;0m ERROR 12-05 15:20:59 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4061683)[0;0m ERROR 12-05 15:20:59 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4061683)[0;0m ERROR 12-05 15:20:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4061683)[0;0m ERROR 12-05 15:20:59 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4061683)[0;0m ERROR 12-05 15:20:59 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061683)[0;0m ERROR 12-05 15:20:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4061683)[0;0m ERROR 12-05 15:20:59 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4061683)[0;0m ERROR 12-05 15:20:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4061683)[0;0m ERROR 12-05 15:20:59 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4061683)[0;0m ERROR 12-05 15:20:59 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061683)[0;0m ERROR 12-05 15:20:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4061683)[0;0m ERROR 12-05 15:20:59 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4061683)[0;0m ERROR 12-05 15:20:59 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061683)[0;0m ERROR 12-05 15:20:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4061683)[0;0m ERROR 12-05 15:20:59 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4061683)[0;0m ERROR 12-05 15:20:59 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061683)[0;0m ERROR 12-05 15:20:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4061683)[0;0m ERROR 12-05 15:20:59 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4061683)[0;0m ERROR 12-05 15:20:59 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061683)[0;0m ERROR 12-05 15:20:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4061683)[0;0m ERROR 12-05 15:20:59 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4061683)[0;0m ERROR 12-05 15:20:59 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061683)[0;0m ERROR 12-05 15:20:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4061683)[0;0m ERROR 12-05 15:20:59 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4061683)[0;0m ERROR 12-05 15:20:59 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061683)[0;0m ERROR 12-05 15:20:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4061683)[0;0m ERROR 12-05 15:20:59 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4061683)[0;0m ERROR 12-05 15:20:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4061683)[0;0m ERROR 12-05 15:20:59 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4061683)[0;0m ERROR 12-05 15:20:59 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061683)[0;0m ERROR 12-05 15:20:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4061683)[0;0m ERROR 12-05 15:20:59 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4061683)[0;0m ERROR 12-05 15:20:59 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061683)[0;0m ERROR 12-05 15:20:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4061683)[0;0m ERROR 12-05 15:20:59 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4061683)[0;0m ERROR 12-05 15:20:59 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061683)[0;0m ERROR 12-05 15:20:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4061683)[0;0m ERROR 12-05 15:20:59 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4061683)[0;0m ERROR 12-05 15:20:59 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061683)[0;0m ERROR 12-05 15:20:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4061683)[0;0m ERROR 12-05 15:20:59 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4061683)[0;0m ERROR 12-05 15:20:59 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061683)[0;0m ERROR 12-05 15:20:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4061683)[0;0m ERROR 12-05 15:20:59 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4061683)[0;0m ERROR 12-05 15:20:59 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061683)[0;0m ERROR 12-05 15:20:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4061683)[0;0m ERROR 12-05 15:20:59 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4061683)[0;0m ERROR 12-05 15:20:59 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061683)[0;0m ERROR 12-05 15:20:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4061683)[0;0m ERROR 12-05 15:20:59 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4061683)[0;0m ERROR 12-05 15:20:59 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061683)[0;0m ERROR 12-05 15:20:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4061683)[0;0m ERROR 12-05 15:20:59 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4061683)[0;0m ERROR 12-05 15:20:59 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061683)[0;0m ERROR 12-05 15:20:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4061683)[0;0m ERROR 12-05 15:20:59 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4061683)[0;0m ERROR 12-05 15:20:59 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061683)[0;0m ERROR 12-05 15:20:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4061683)[0;0m ERROR 12-05 15:20:59 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4061683)[0;0m ERROR 12-05 15:20:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4061683)[0;0m ERROR 12-05 15:20:59 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4061683)[0;0m ERROR 12-05 15:20:59 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061683)[0;0m ERROR 12-05 15:20:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4061683)[0;0m ERROR 12-05 15:20:59 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4061683)[0;0m ERROR 12-05 15:20:59 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061683)[0;0m ERROR 12-05 15:20:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4061683)[0;0m ERROR 12-05 15:20:59 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4061683)[0;0m ERROR 12-05 15:20:59 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061683)[0;0m ERROR 12-05 15:20:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4061683)[0;0m ERROR 12-05 15:20:59 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4061683)[0;0m ERROR 12-05 15:20:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4061683)[0;0m ERROR 12-05 15:20:59 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4061683)[0;0m ERROR 12-05 15:20:59 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061683)[0;0m ERROR 12-05 15:20:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4061683)[0;0m ERROR 12-05 15:20:59 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4061683)[0;0m ERROR 12-05 15:20:59 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061683)[0;0m ERROR 12-05 15:20:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4061683)[0;0m ERROR 12-05 15:20:59 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4061683)[0;0m ERROR 12-05 15:20:59 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061683)[0;0m ERROR 12-05 15:20:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4061683)[0;0m ERROR 12-05 15:20:59 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4061683)[0;0m ERROR 12-05 15:20:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4061683)[0;0m ERROR 12-05 15:20:59 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4061683)[0;0m ERROR 12-05 15:20:59 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061683)[0;0m ERROR 12-05 15:20:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4061683)[0;0m ERROR 12-05 15:20:59 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4061683)[0;0m ERROR 12-05 15:20:59 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061683)[0;0m ERROR 12-05 15:20:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4061683)[0;0m ERROR 12-05 15:20:59 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4061683)[0;0m ERROR 12-05 15:20:59 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061683)[0;0m ERROR 12-05 15:20:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4061683)[0;0m ERROR 12-05 15:20:59 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4061683)[0;0m ERROR 12-05 15:20:59 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061683)[0;0m ERROR 12-05 15:20:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4061683)[0;0m ERROR 12-05 15:20:59 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4061683)[0;0m ERROR 12-05 15:20:59 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7fa604108d70>' raised:
[1;36m(EngineCore_DP0 pid=4061683)[0;0m ERROR 12-05 15:20:59 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4061683)[0;0m ERROR 12-05 15:20:59 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4061683)[0;0m ERROR 12-05 15:20:59 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4061683)[0;0m ERROR 12-05 15:20:59 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:21:20 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:21:20 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:21:20 [model.py:1745] Using max model len 131072
INFO 12-05 15:21:20 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4061855)[0;0m INFO 12-05 15:21:28 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4061855)[0;0m INFO 12-05 15:21:28 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:41827 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4061855)[0;0m INFO 12-05 15:21:28 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4061855)[0;0m INFO 12-05 15:21:29 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4061855)[0;0m INFO 12-05 15:21:29 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4061855)[0;0m INFO 12-05 15:21:29 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4061855)[0;0m INFO 12-05 15:21:30 [default_loader.py:314] Loading weights took 0.87 seconds
[1;36m(EngineCore_DP0 pid=4061855)[0;0m INFO 12-05 15:21:31 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.444936 seconds
[1;36m(EngineCore_DP0 pid=4061855)[0;0m INFO 12-05 15:21:35 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4061855)[0;0m INFO 12-05 15:21:35 [backends.py:647] Dynamo bytecode transform time: 3.80 s
[1;36m(EngineCore_DP0 pid=4061855)[0;0m ERROR 12-05 15:21:35 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4061855)[0;0m ERROR 12-05 15:21:35 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4061855)[0;0m ERROR 12-05 15:21:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4061855)[0;0m ERROR 12-05 15:21:35 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4061855)[0;0m ERROR 12-05 15:21:35 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061855)[0;0m ERROR 12-05 15:21:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4061855)[0;0m ERROR 12-05 15:21:35 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4061855)[0;0m ERROR 12-05 15:21:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4061855)[0;0m ERROR 12-05 15:21:35 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4061855)[0;0m ERROR 12-05 15:21:35 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061855)[0;0m ERROR 12-05 15:21:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4061855)[0;0m ERROR 12-05 15:21:35 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4061855)[0;0m ERROR 12-05 15:21:35 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061855)[0;0m ERROR 12-05 15:21:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4061855)[0;0m ERROR 12-05 15:21:35 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4061855)[0;0m ERROR 12-05 15:21:35 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061855)[0;0m ERROR 12-05 15:21:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4061855)[0;0m ERROR 12-05 15:21:35 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4061855)[0;0m ERROR 12-05 15:21:35 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061855)[0;0m ERROR 12-05 15:21:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4061855)[0;0m ERROR 12-05 15:21:35 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4061855)[0;0m ERROR 12-05 15:21:35 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061855)[0;0m ERROR 12-05 15:21:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4061855)[0;0m ERROR 12-05 15:21:35 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4061855)[0;0m ERROR 12-05 15:21:35 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061855)[0;0m ERROR 12-05 15:21:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4061855)[0;0m ERROR 12-05 15:21:35 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4061855)[0;0m ERROR 12-05 15:21:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4061855)[0;0m ERROR 12-05 15:21:35 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4061855)[0;0m ERROR 12-05 15:21:35 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061855)[0;0m ERROR 12-05 15:21:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4061855)[0;0m ERROR 12-05 15:21:35 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4061855)[0;0m ERROR 12-05 15:21:35 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061855)[0;0m ERROR 12-05 15:21:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4061855)[0;0m ERROR 12-05 15:21:35 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4061855)[0;0m ERROR 12-05 15:21:35 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061855)[0;0m ERROR 12-05 15:21:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4061855)[0;0m ERROR 12-05 15:21:35 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4061855)[0;0m ERROR 12-05 15:21:35 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061855)[0;0m ERROR 12-05 15:21:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4061855)[0;0m ERROR 12-05 15:21:35 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4061855)[0;0m ERROR 12-05 15:21:35 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061855)[0;0m ERROR 12-05 15:21:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4061855)[0;0m ERROR 12-05 15:21:35 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4061855)[0;0m ERROR 12-05 15:21:35 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061855)[0;0m ERROR 12-05 15:21:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4061855)[0;0m ERROR 12-05 15:21:35 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4061855)[0;0m ERROR 12-05 15:21:35 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061855)[0;0m ERROR 12-05 15:21:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4061855)[0;0m ERROR 12-05 15:21:35 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4061855)[0;0m ERROR 12-05 15:21:35 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061855)[0;0m ERROR 12-05 15:21:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4061855)[0;0m ERROR 12-05 15:21:35 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4061855)[0;0m ERROR 12-05 15:21:35 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061855)[0;0m ERROR 12-05 15:21:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4061855)[0;0m ERROR 12-05 15:21:35 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4061855)[0;0m ERROR 12-05 15:21:35 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061855)[0;0m ERROR 12-05 15:21:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4061855)[0;0m ERROR 12-05 15:21:35 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4061855)[0;0m ERROR 12-05 15:21:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4061855)[0;0m ERROR 12-05 15:21:35 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4061855)[0;0m ERROR 12-05 15:21:35 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061855)[0;0m ERROR 12-05 15:21:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4061855)[0;0m ERROR 12-05 15:21:35 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4061855)[0;0m ERROR 12-05 15:21:35 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061855)[0;0m ERROR 12-05 15:21:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4061855)[0;0m ERROR 12-05 15:21:35 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4061855)[0;0m ERROR 12-05 15:21:35 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061855)[0;0m ERROR 12-05 15:21:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4061855)[0;0m ERROR 12-05 15:21:35 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4061855)[0;0m ERROR 12-05 15:21:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4061855)[0;0m ERROR 12-05 15:21:35 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4061855)[0;0m ERROR 12-05 15:21:35 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061855)[0;0m ERROR 12-05 15:21:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4061855)[0;0m ERROR 12-05 15:21:35 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4061855)[0;0m ERROR 12-05 15:21:35 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061855)[0;0m ERROR 12-05 15:21:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4061855)[0;0m ERROR 12-05 15:21:35 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4061855)[0;0m ERROR 12-05 15:21:35 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061855)[0;0m ERROR 12-05 15:21:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4061855)[0;0m ERROR 12-05 15:21:35 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4061855)[0;0m ERROR 12-05 15:21:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4061855)[0;0m ERROR 12-05 15:21:35 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4061855)[0;0m ERROR 12-05 15:21:35 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061855)[0;0m ERROR 12-05 15:21:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4061855)[0;0m ERROR 12-05 15:21:35 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4061855)[0;0m ERROR 12-05 15:21:35 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061855)[0;0m ERROR 12-05 15:21:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4061855)[0;0m ERROR 12-05 15:21:35 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4061855)[0;0m ERROR 12-05 15:21:35 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061855)[0;0m ERROR 12-05 15:21:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4061855)[0;0m ERROR 12-05 15:21:35 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4061855)[0;0m ERROR 12-05 15:21:35 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061855)[0;0m ERROR 12-05 15:21:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4061855)[0;0m ERROR 12-05 15:21:35 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4061855)[0;0m ERROR 12-05 15:21:35 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7f45fc1e7020>' raised:
[1;36m(EngineCore_DP0 pid=4061855)[0;0m ERROR 12-05 15:21:35 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4061855)[0;0m ERROR 12-05 15:21:35 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4061855)[0;0m ERROR 12-05 15:21:35 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4061855)[0;0m ERROR 12-05 15:21:35 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:21:56 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:21:57 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:21:57 [model.py:1745] Using max model len 131072
INFO 12-05 15:21:57 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4061943)[0;0m INFO 12-05 15:22:04 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4061943)[0;0m INFO 12-05 15:22:04 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:42393 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4061943)[0;0m INFO 12-05 15:22:04 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4061943)[0;0m INFO 12-05 15:22:05 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4061943)[0;0m INFO 12-05 15:22:05 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4061943)[0;0m INFO 12-05 15:22:05 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4061943)[0;0m INFO 12-05 15:22:06 [default_loader.py:314] Loading weights took 0.87 seconds
[1;36m(EngineCore_DP0 pid=4061943)[0;0m INFO 12-05 15:22:07 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.411712 seconds
[1;36m(EngineCore_DP0 pid=4061943)[0;0m INFO 12-05 15:22:11 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4061943)[0;0m INFO 12-05 15:22:11 [backends.py:647] Dynamo bytecode transform time: 3.61 s
[1;36m(EngineCore_DP0 pid=4061943)[0;0m ERROR 12-05 15:22:11 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4061943)[0;0m ERROR 12-05 15:22:11 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4061943)[0;0m ERROR 12-05 15:22:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4061943)[0;0m ERROR 12-05 15:22:11 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4061943)[0;0m ERROR 12-05 15:22:11 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061943)[0;0m ERROR 12-05 15:22:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4061943)[0;0m ERROR 12-05 15:22:11 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4061943)[0;0m ERROR 12-05 15:22:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4061943)[0;0m ERROR 12-05 15:22:11 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4061943)[0;0m ERROR 12-05 15:22:11 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061943)[0;0m ERROR 12-05 15:22:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4061943)[0;0m ERROR 12-05 15:22:11 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4061943)[0;0m ERROR 12-05 15:22:11 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061943)[0;0m ERROR 12-05 15:22:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4061943)[0;0m ERROR 12-05 15:22:11 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4061943)[0;0m ERROR 12-05 15:22:11 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061943)[0;0m ERROR 12-05 15:22:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4061943)[0;0m ERROR 12-05 15:22:11 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4061943)[0;0m ERROR 12-05 15:22:11 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061943)[0;0m ERROR 12-05 15:22:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4061943)[0;0m ERROR 12-05 15:22:11 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4061943)[0;0m ERROR 12-05 15:22:11 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061943)[0;0m ERROR 12-05 15:22:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4061943)[0;0m ERROR 12-05 15:22:11 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4061943)[0;0m ERROR 12-05 15:22:11 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061943)[0;0m ERROR 12-05 15:22:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4061943)[0;0m ERROR 12-05 15:22:11 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4061943)[0;0m ERROR 12-05 15:22:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4061943)[0;0m ERROR 12-05 15:22:11 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4061943)[0;0m ERROR 12-05 15:22:11 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061943)[0;0m ERROR 12-05 15:22:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4061943)[0;0m ERROR 12-05 15:22:11 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4061943)[0;0m ERROR 12-05 15:22:11 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061943)[0;0m ERROR 12-05 15:22:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4061943)[0;0m ERROR 12-05 15:22:11 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4061943)[0;0m ERROR 12-05 15:22:11 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061943)[0;0m ERROR 12-05 15:22:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4061943)[0;0m ERROR 12-05 15:22:11 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4061943)[0;0m ERROR 12-05 15:22:11 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061943)[0;0m ERROR 12-05 15:22:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4061943)[0;0m ERROR 12-05 15:22:11 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4061943)[0;0m ERROR 12-05 15:22:11 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061943)[0;0m ERROR 12-05 15:22:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4061943)[0;0m ERROR 12-05 15:22:11 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4061943)[0;0m ERROR 12-05 15:22:11 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061943)[0;0m ERROR 12-05 15:22:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4061943)[0;0m ERROR 12-05 15:22:11 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4061943)[0;0m ERROR 12-05 15:22:11 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061943)[0;0m ERROR 12-05 15:22:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4061943)[0;0m ERROR 12-05 15:22:11 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4061943)[0;0m ERROR 12-05 15:22:11 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061943)[0;0m ERROR 12-05 15:22:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4061943)[0;0m ERROR 12-05 15:22:11 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4061943)[0;0m ERROR 12-05 15:22:11 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061943)[0;0m ERROR 12-05 15:22:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4061943)[0;0m ERROR 12-05 15:22:11 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4061943)[0;0m ERROR 12-05 15:22:11 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061943)[0;0m ERROR 12-05 15:22:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4061943)[0;0m ERROR 12-05 15:22:11 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4061943)[0;0m ERROR 12-05 15:22:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4061943)[0;0m ERROR 12-05 15:22:11 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4061943)[0;0m ERROR 12-05 15:22:11 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061943)[0;0m ERROR 12-05 15:22:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4061943)[0;0m ERROR 12-05 15:22:11 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4061943)[0;0m ERROR 12-05 15:22:11 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061943)[0;0m ERROR 12-05 15:22:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4061943)[0;0m ERROR 12-05 15:22:11 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4061943)[0;0m ERROR 12-05 15:22:11 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061943)[0;0m ERROR 12-05 15:22:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4061943)[0;0m ERROR 12-05 15:22:11 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4061943)[0;0m ERROR 12-05 15:22:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4061943)[0;0m ERROR 12-05 15:22:11 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4061943)[0;0m ERROR 12-05 15:22:11 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061943)[0;0m ERROR 12-05 15:22:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4061943)[0;0m ERROR 12-05 15:22:11 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4061943)[0;0m ERROR 12-05 15:22:11 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061943)[0;0m ERROR 12-05 15:22:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4061943)[0;0m ERROR 12-05 15:22:11 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4061943)[0;0m ERROR 12-05 15:22:11 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061943)[0;0m ERROR 12-05 15:22:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4061943)[0;0m ERROR 12-05 15:22:11 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4061943)[0;0m ERROR 12-05 15:22:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4061943)[0;0m ERROR 12-05 15:22:11 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4061943)[0;0m ERROR 12-05 15:22:11 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061943)[0;0m ERROR 12-05 15:22:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4061943)[0;0m ERROR 12-05 15:22:11 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4061943)[0;0m ERROR 12-05 15:22:11 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061943)[0;0m ERROR 12-05 15:22:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4061943)[0;0m ERROR 12-05 15:22:11 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4061943)[0;0m ERROR 12-05 15:22:11 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061943)[0;0m ERROR 12-05 15:22:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4061943)[0;0m ERROR 12-05 15:22:11 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4061943)[0;0m ERROR 12-05 15:22:11 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4061943)[0;0m ERROR 12-05 15:22:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4061943)[0;0m ERROR 12-05 15:22:11 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4061943)[0;0m ERROR 12-05 15:22:11 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7f0c7033c8c0>' raised:
[1;36m(EngineCore_DP0 pid=4061943)[0;0m ERROR 12-05 15:22:11 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4061943)[0;0m ERROR 12-05 15:22:11 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4061943)[0;0m ERROR 12-05 15:22:11 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4061943)[0;0m ERROR 12-05 15:22:11 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:22:32 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:22:32 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:22:32 [model.py:1745] Using max model len 131072
INFO 12-05 15:22:32 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4062024)[0;0m INFO 12-05 15:22:41 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4062024)[0;0m INFO 12-05 15:22:42 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:41767 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4062024)[0;0m INFO 12-05 15:22:42 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4062024)[0;0m INFO 12-05 15:22:42 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4062024)[0;0m INFO 12-05 15:22:43 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4062024)[0;0m INFO 12-05 15:22:43 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4062024)[0;0m INFO 12-05 15:22:44 [default_loader.py:314] Loading weights took 0.87 seconds
[1;36m(EngineCore_DP0 pid=4062024)[0;0m INFO 12-05 15:22:44 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.400148 seconds
[1;36m(EngineCore_DP0 pid=4062024)[0;0m INFO 12-05 15:22:48 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4062024)[0;0m INFO 12-05 15:22:48 [backends.py:647] Dynamo bytecode transform time: 3.67 s
[1;36m(EngineCore_DP0 pid=4062024)[0;0m ERROR 12-05 15:22:48 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4062024)[0;0m ERROR 12-05 15:22:48 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4062024)[0;0m ERROR 12-05 15:22:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4062024)[0;0m ERROR 12-05 15:22:48 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4062024)[0;0m ERROR 12-05 15:22:48 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062024)[0;0m ERROR 12-05 15:22:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4062024)[0;0m ERROR 12-05 15:22:48 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4062024)[0;0m ERROR 12-05 15:22:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4062024)[0;0m ERROR 12-05 15:22:48 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4062024)[0;0m ERROR 12-05 15:22:48 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062024)[0;0m ERROR 12-05 15:22:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4062024)[0;0m ERROR 12-05 15:22:48 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4062024)[0;0m ERROR 12-05 15:22:48 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062024)[0;0m ERROR 12-05 15:22:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4062024)[0;0m ERROR 12-05 15:22:48 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4062024)[0;0m ERROR 12-05 15:22:48 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062024)[0;0m ERROR 12-05 15:22:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4062024)[0;0m ERROR 12-05 15:22:48 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4062024)[0;0m ERROR 12-05 15:22:48 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062024)[0;0m ERROR 12-05 15:22:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4062024)[0;0m ERROR 12-05 15:22:48 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4062024)[0;0m ERROR 12-05 15:22:48 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062024)[0;0m ERROR 12-05 15:22:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4062024)[0;0m ERROR 12-05 15:22:48 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4062024)[0;0m ERROR 12-05 15:22:48 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062024)[0;0m ERROR 12-05 15:22:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4062024)[0;0m ERROR 12-05 15:22:48 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4062024)[0;0m ERROR 12-05 15:22:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4062024)[0;0m ERROR 12-05 15:22:48 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4062024)[0;0m ERROR 12-05 15:22:48 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062024)[0;0m ERROR 12-05 15:22:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4062024)[0;0m ERROR 12-05 15:22:48 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4062024)[0;0m ERROR 12-05 15:22:48 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062024)[0;0m ERROR 12-05 15:22:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4062024)[0;0m ERROR 12-05 15:22:48 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4062024)[0;0m ERROR 12-05 15:22:48 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062024)[0;0m ERROR 12-05 15:22:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4062024)[0;0m ERROR 12-05 15:22:48 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4062024)[0;0m ERROR 12-05 15:22:48 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062024)[0;0m ERROR 12-05 15:22:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4062024)[0;0m ERROR 12-05 15:22:48 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4062024)[0;0m ERROR 12-05 15:22:48 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062024)[0;0m ERROR 12-05 15:22:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4062024)[0;0m ERROR 12-05 15:22:48 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4062024)[0;0m ERROR 12-05 15:22:48 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062024)[0;0m ERROR 12-05 15:22:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4062024)[0;0m ERROR 12-05 15:22:48 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4062024)[0;0m ERROR 12-05 15:22:48 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062024)[0;0m ERROR 12-05 15:22:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4062024)[0;0m ERROR 12-05 15:22:48 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4062024)[0;0m ERROR 12-05 15:22:48 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062024)[0;0m ERROR 12-05 15:22:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4062024)[0;0m ERROR 12-05 15:22:48 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4062024)[0;0m ERROR 12-05 15:22:48 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062024)[0;0m ERROR 12-05 15:22:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4062024)[0;0m ERROR 12-05 15:22:48 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4062024)[0;0m ERROR 12-05 15:22:48 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062024)[0;0m ERROR 12-05 15:22:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4062024)[0;0m ERROR 12-05 15:22:48 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4062024)[0;0m ERROR 12-05 15:22:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4062024)[0;0m ERROR 12-05 15:22:48 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4062024)[0;0m ERROR 12-05 15:22:48 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062024)[0;0m ERROR 12-05 15:22:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4062024)[0;0m ERROR 12-05 15:22:48 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4062024)[0;0m ERROR 12-05 15:22:48 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062024)[0;0m ERROR 12-05 15:22:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4062024)[0;0m ERROR 12-05 15:22:48 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4062024)[0;0m ERROR 12-05 15:22:48 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062024)[0;0m ERROR 12-05 15:22:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4062024)[0;0m ERROR 12-05 15:22:48 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4062024)[0;0m ERROR 12-05 15:22:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4062024)[0;0m ERROR 12-05 15:22:48 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4062024)[0;0m ERROR 12-05 15:22:48 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062024)[0;0m ERROR 12-05 15:22:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4062024)[0;0m ERROR 12-05 15:22:48 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4062024)[0;0m ERROR 12-05 15:22:48 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062024)[0;0m ERROR 12-05 15:22:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4062024)[0;0m ERROR 12-05 15:22:48 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4062024)[0;0m ERROR 12-05 15:22:48 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062024)[0;0m ERROR 12-05 15:22:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4062024)[0;0m ERROR 12-05 15:22:48 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4062024)[0;0m ERROR 12-05 15:22:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4062024)[0;0m ERROR 12-05 15:22:48 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4062024)[0;0m ERROR 12-05 15:22:48 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062024)[0;0m ERROR 12-05 15:22:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4062024)[0;0m ERROR 12-05 15:22:48 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4062024)[0;0m ERROR 12-05 15:22:48 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062024)[0;0m ERROR 12-05 15:22:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4062024)[0;0m ERROR 12-05 15:22:48 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4062024)[0;0m ERROR 12-05 15:22:48 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062024)[0;0m ERROR 12-05 15:22:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4062024)[0;0m ERROR 12-05 15:22:48 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4062024)[0;0m ERROR 12-05 15:22:48 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062024)[0;0m ERROR 12-05 15:22:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4062024)[0;0m ERROR 12-05 15:22:48 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4062024)[0;0m ERROR 12-05 15:22:48 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7fcf8c292360>' raised:
[1;36m(EngineCore_DP0 pid=4062024)[0;0m ERROR 12-05 15:22:48 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4062024)[0;0m ERROR 12-05 15:22:48 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4062024)[0;0m ERROR 12-05 15:22:48 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4062024)[0;0m ERROR 12-05 15:22:48 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:23:10 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:23:10 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:23:10 [model.py:1745] Using max model len 131072
INFO 12-05 15:23:10 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4062108)[0;0m INFO 12-05 15:23:17 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4062108)[0;0m INFO 12-05 15:23:18 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:43333 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4062108)[0;0m INFO 12-05 15:23:18 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4062108)[0;0m INFO 12-05 15:23:18 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4062108)[0;0m INFO 12-05 15:23:19 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4062108)[0;0m INFO 12-05 15:23:19 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4062108)[0;0m INFO 12-05 15:23:20 [default_loader.py:314] Loading weights took 0.87 seconds
[1;36m(EngineCore_DP0 pid=4062108)[0;0m INFO 12-05 15:23:20 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.428303 seconds
[1;36m(EngineCore_DP0 pid=4062108)[0;0m INFO 12-05 15:23:24 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4062108)[0;0m INFO 12-05 15:23:24 [backends.py:647] Dynamo bytecode transform time: 3.64 s
[1;36m(EngineCore_DP0 pid=4062108)[0;0m ERROR 12-05 15:23:25 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4062108)[0;0m ERROR 12-05 15:23:25 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4062108)[0;0m ERROR 12-05 15:23:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4062108)[0;0m ERROR 12-05 15:23:25 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4062108)[0;0m ERROR 12-05 15:23:25 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062108)[0;0m ERROR 12-05 15:23:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4062108)[0;0m ERROR 12-05 15:23:25 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4062108)[0;0m ERROR 12-05 15:23:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4062108)[0;0m ERROR 12-05 15:23:25 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4062108)[0;0m ERROR 12-05 15:23:25 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062108)[0;0m ERROR 12-05 15:23:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4062108)[0;0m ERROR 12-05 15:23:25 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4062108)[0;0m ERROR 12-05 15:23:25 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062108)[0;0m ERROR 12-05 15:23:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4062108)[0;0m ERROR 12-05 15:23:25 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4062108)[0;0m ERROR 12-05 15:23:25 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062108)[0;0m ERROR 12-05 15:23:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4062108)[0;0m ERROR 12-05 15:23:25 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4062108)[0;0m ERROR 12-05 15:23:25 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062108)[0;0m ERROR 12-05 15:23:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4062108)[0;0m ERROR 12-05 15:23:25 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4062108)[0;0m ERROR 12-05 15:23:25 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062108)[0;0m ERROR 12-05 15:23:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4062108)[0;0m ERROR 12-05 15:23:25 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4062108)[0;0m ERROR 12-05 15:23:25 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062108)[0;0m ERROR 12-05 15:23:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4062108)[0;0m ERROR 12-05 15:23:25 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4062108)[0;0m ERROR 12-05 15:23:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4062108)[0;0m ERROR 12-05 15:23:25 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4062108)[0;0m ERROR 12-05 15:23:25 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062108)[0;0m ERROR 12-05 15:23:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4062108)[0;0m ERROR 12-05 15:23:25 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4062108)[0;0m ERROR 12-05 15:23:25 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062108)[0;0m ERROR 12-05 15:23:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4062108)[0;0m ERROR 12-05 15:23:25 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4062108)[0;0m ERROR 12-05 15:23:25 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062108)[0;0m ERROR 12-05 15:23:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4062108)[0;0m ERROR 12-05 15:23:25 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4062108)[0;0m ERROR 12-05 15:23:25 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062108)[0;0m ERROR 12-05 15:23:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4062108)[0;0m ERROR 12-05 15:23:25 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4062108)[0;0m ERROR 12-05 15:23:25 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062108)[0;0m ERROR 12-05 15:23:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4062108)[0;0m ERROR 12-05 15:23:25 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4062108)[0;0m ERROR 12-05 15:23:25 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062108)[0;0m ERROR 12-05 15:23:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4062108)[0;0m ERROR 12-05 15:23:25 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4062108)[0;0m ERROR 12-05 15:23:25 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062108)[0;0m ERROR 12-05 15:23:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4062108)[0;0m ERROR 12-05 15:23:25 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4062108)[0;0m ERROR 12-05 15:23:25 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062108)[0;0m ERROR 12-05 15:23:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4062108)[0;0m ERROR 12-05 15:23:25 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4062108)[0;0m ERROR 12-05 15:23:25 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062108)[0;0m ERROR 12-05 15:23:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4062108)[0;0m ERROR 12-05 15:23:25 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4062108)[0;0m ERROR 12-05 15:23:25 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062108)[0;0m ERROR 12-05 15:23:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4062108)[0;0m ERROR 12-05 15:23:25 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4062108)[0;0m ERROR 12-05 15:23:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4062108)[0;0m ERROR 12-05 15:23:25 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4062108)[0;0m ERROR 12-05 15:23:25 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062108)[0;0m ERROR 12-05 15:23:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4062108)[0;0m ERROR 12-05 15:23:25 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4062108)[0;0m ERROR 12-05 15:23:25 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062108)[0;0m ERROR 12-05 15:23:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4062108)[0;0m ERROR 12-05 15:23:25 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4062108)[0;0m ERROR 12-05 15:23:25 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062108)[0;0m ERROR 12-05 15:23:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4062108)[0;0m ERROR 12-05 15:23:25 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4062108)[0;0m ERROR 12-05 15:23:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4062108)[0;0m ERROR 12-05 15:23:25 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4062108)[0;0m ERROR 12-05 15:23:25 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062108)[0;0m ERROR 12-05 15:23:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4062108)[0;0m ERROR 12-05 15:23:25 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4062108)[0;0m ERROR 12-05 15:23:25 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062108)[0;0m ERROR 12-05 15:23:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4062108)[0;0m ERROR 12-05 15:23:25 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4062108)[0;0m ERROR 12-05 15:23:25 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062108)[0;0m ERROR 12-05 15:23:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4062108)[0;0m ERROR 12-05 15:23:25 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4062108)[0;0m ERROR 12-05 15:23:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4062108)[0;0m ERROR 12-05 15:23:25 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4062108)[0;0m ERROR 12-05 15:23:25 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062108)[0;0m ERROR 12-05 15:23:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4062108)[0;0m ERROR 12-05 15:23:25 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4062108)[0;0m ERROR 12-05 15:23:25 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062108)[0;0m ERROR 12-05 15:23:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4062108)[0;0m ERROR 12-05 15:23:25 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4062108)[0;0m ERROR 12-05 15:23:25 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062108)[0;0m ERROR 12-05 15:23:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4062108)[0;0m ERROR 12-05 15:23:25 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4062108)[0;0m ERROR 12-05 15:23:25 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062108)[0;0m ERROR 12-05 15:23:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4062108)[0;0m ERROR 12-05 15:23:25 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4062108)[0;0m ERROR 12-05 15:23:25 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7f20201204a0>' raised:
[1;36m(EngineCore_DP0 pid=4062108)[0;0m ERROR 12-05 15:23:25 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4062108)[0;0m ERROR 12-05 15:23:25 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4062108)[0;0m ERROR 12-05 15:23:25 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4062108)[0;0m ERROR 12-05 15:23:25 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:23:46 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:23:46 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:23:46 [model.py:1745] Using max model len 131072
INFO 12-05 15:23:46 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4062197)[0;0m INFO 12-05 15:23:53 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4062197)[0;0m INFO 12-05 15:23:54 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:56517 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4062197)[0;0m INFO 12-05 15:23:54 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4062197)[0;0m INFO 12-05 15:23:55 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4062197)[0;0m INFO 12-05 15:23:55 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4062197)[0;0m INFO 12-05 15:23:55 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4062197)[0;0m INFO 12-05 15:23:56 [default_loader.py:314] Loading weights took 0.87 seconds
[1;36m(EngineCore_DP0 pid=4062197)[0;0m INFO 12-05 15:23:57 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.397525 seconds
[1;36m(EngineCore_DP0 pid=4062197)[0;0m INFO 12-05 15:24:00 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4062197)[0;0m INFO 12-05 15:24:00 [backends.py:647] Dynamo bytecode transform time: 3.59 s
[1;36m(EngineCore_DP0 pid=4062197)[0;0m ERROR 12-05 15:24:01 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4062197)[0;0m ERROR 12-05 15:24:01 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4062197)[0;0m ERROR 12-05 15:24:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4062197)[0;0m ERROR 12-05 15:24:01 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4062197)[0;0m ERROR 12-05 15:24:01 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062197)[0;0m ERROR 12-05 15:24:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4062197)[0;0m ERROR 12-05 15:24:01 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4062197)[0;0m ERROR 12-05 15:24:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4062197)[0;0m ERROR 12-05 15:24:01 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4062197)[0;0m ERROR 12-05 15:24:01 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062197)[0;0m ERROR 12-05 15:24:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4062197)[0;0m ERROR 12-05 15:24:01 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4062197)[0;0m ERROR 12-05 15:24:01 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062197)[0;0m ERROR 12-05 15:24:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4062197)[0;0m ERROR 12-05 15:24:01 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4062197)[0;0m ERROR 12-05 15:24:01 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062197)[0;0m ERROR 12-05 15:24:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4062197)[0;0m ERROR 12-05 15:24:01 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4062197)[0;0m ERROR 12-05 15:24:01 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062197)[0;0m ERROR 12-05 15:24:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4062197)[0;0m ERROR 12-05 15:24:01 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4062197)[0;0m ERROR 12-05 15:24:01 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062197)[0;0m ERROR 12-05 15:24:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4062197)[0;0m ERROR 12-05 15:24:01 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4062197)[0;0m ERROR 12-05 15:24:01 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062197)[0;0m ERROR 12-05 15:24:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4062197)[0;0m ERROR 12-05 15:24:01 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4062197)[0;0m ERROR 12-05 15:24:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4062197)[0;0m ERROR 12-05 15:24:01 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4062197)[0;0m ERROR 12-05 15:24:01 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062197)[0;0m ERROR 12-05 15:24:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4062197)[0;0m ERROR 12-05 15:24:01 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4062197)[0;0m ERROR 12-05 15:24:01 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062197)[0;0m ERROR 12-05 15:24:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4062197)[0;0m ERROR 12-05 15:24:01 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4062197)[0;0m ERROR 12-05 15:24:01 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062197)[0;0m ERROR 12-05 15:24:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4062197)[0;0m ERROR 12-05 15:24:01 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4062197)[0;0m ERROR 12-05 15:24:01 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062197)[0;0m ERROR 12-05 15:24:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4062197)[0;0m ERROR 12-05 15:24:01 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4062197)[0;0m ERROR 12-05 15:24:01 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062197)[0;0m ERROR 12-05 15:24:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4062197)[0;0m ERROR 12-05 15:24:01 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4062197)[0;0m ERROR 12-05 15:24:01 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062197)[0;0m ERROR 12-05 15:24:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4062197)[0;0m ERROR 12-05 15:24:01 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4062197)[0;0m ERROR 12-05 15:24:01 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062197)[0;0m ERROR 12-05 15:24:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4062197)[0;0m ERROR 12-05 15:24:01 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4062197)[0;0m ERROR 12-05 15:24:01 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062197)[0;0m ERROR 12-05 15:24:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4062197)[0;0m ERROR 12-05 15:24:01 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4062197)[0;0m ERROR 12-05 15:24:01 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062197)[0;0m ERROR 12-05 15:24:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4062197)[0;0m ERROR 12-05 15:24:01 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4062197)[0;0m ERROR 12-05 15:24:01 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062197)[0;0m ERROR 12-05 15:24:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4062197)[0;0m ERROR 12-05 15:24:01 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4062197)[0;0m ERROR 12-05 15:24:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4062197)[0;0m ERROR 12-05 15:24:01 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4062197)[0;0m ERROR 12-05 15:24:01 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062197)[0;0m ERROR 12-05 15:24:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4062197)[0;0m ERROR 12-05 15:24:01 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4062197)[0;0m ERROR 12-05 15:24:01 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062197)[0;0m ERROR 12-05 15:24:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4062197)[0;0m ERROR 12-05 15:24:01 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4062197)[0;0m ERROR 12-05 15:24:01 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062197)[0;0m ERROR 12-05 15:24:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4062197)[0;0m ERROR 12-05 15:24:01 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4062197)[0;0m ERROR 12-05 15:24:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4062197)[0;0m ERROR 12-05 15:24:01 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4062197)[0;0m ERROR 12-05 15:24:01 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062197)[0;0m ERROR 12-05 15:24:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4062197)[0;0m ERROR 12-05 15:24:01 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4062197)[0;0m ERROR 12-05 15:24:01 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062197)[0;0m ERROR 12-05 15:24:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4062197)[0;0m ERROR 12-05 15:24:01 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4062197)[0;0m ERROR 12-05 15:24:01 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062197)[0;0m ERROR 12-05 15:24:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4062197)[0;0m ERROR 12-05 15:24:01 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4062197)[0;0m ERROR 12-05 15:24:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4062197)[0;0m ERROR 12-05 15:24:01 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4062197)[0;0m ERROR 12-05 15:24:01 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062197)[0;0m ERROR 12-05 15:24:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4062197)[0;0m ERROR 12-05 15:24:01 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4062197)[0;0m ERROR 12-05 15:24:01 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062197)[0;0m ERROR 12-05 15:24:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4062197)[0;0m ERROR 12-05 15:24:01 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4062197)[0;0m ERROR 12-05 15:24:01 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062197)[0;0m ERROR 12-05 15:24:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4062197)[0;0m ERROR 12-05 15:24:01 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4062197)[0;0m ERROR 12-05 15:24:01 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062197)[0;0m ERROR 12-05 15:24:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4062197)[0;0m ERROR 12-05 15:24:01 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4062197)[0;0m ERROR 12-05 15:24:01 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7f85d035b0e0>' raised:
[1;36m(EngineCore_DP0 pid=4062197)[0;0m ERROR 12-05 15:24:01 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4062197)[0;0m ERROR 12-05 15:24:01 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4062197)[0;0m ERROR 12-05 15:24:01 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4062197)[0;0m ERROR 12-05 15:24:01 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:24:22 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:24:22 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:24:22 [model.py:1745] Using max model len 131072
INFO 12-05 15:24:22 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4062276)[0;0m INFO 12-05 15:24:29 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4062276)[0;0m INFO 12-05 15:24:30 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:52083 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4062276)[0;0m INFO 12-05 15:24:30 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4062276)[0;0m INFO 12-05 15:24:30 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4062276)[0;0m INFO 12-05 15:24:31 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4062276)[0;0m INFO 12-05 15:24:31 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4062276)[0;0m INFO 12-05 15:24:32 [default_loader.py:314] Loading weights took 0.88 seconds
[1;36m(EngineCore_DP0 pid=4062276)[0;0m INFO 12-05 15:24:32 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.398663 seconds
[1;36m(EngineCore_DP0 pid=4062276)[0;0m INFO 12-05 15:24:36 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4062276)[0;0m INFO 12-05 15:24:36 [backends.py:647] Dynamo bytecode transform time: 3.63 s
[1;36m(EngineCore_DP0 pid=4062276)[0;0m ERROR 12-05 15:24:37 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4062276)[0;0m ERROR 12-05 15:24:37 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4062276)[0;0m ERROR 12-05 15:24:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4062276)[0;0m ERROR 12-05 15:24:37 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4062276)[0;0m ERROR 12-05 15:24:37 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062276)[0;0m ERROR 12-05 15:24:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4062276)[0;0m ERROR 12-05 15:24:37 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4062276)[0;0m ERROR 12-05 15:24:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4062276)[0;0m ERROR 12-05 15:24:37 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4062276)[0;0m ERROR 12-05 15:24:37 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062276)[0;0m ERROR 12-05 15:24:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4062276)[0;0m ERROR 12-05 15:24:37 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4062276)[0;0m ERROR 12-05 15:24:37 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062276)[0;0m ERROR 12-05 15:24:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4062276)[0;0m ERROR 12-05 15:24:37 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4062276)[0;0m ERROR 12-05 15:24:37 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062276)[0;0m ERROR 12-05 15:24:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4062276)[0;0m ERROR 12-05 15:24:37 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4062276)[0;0m ERROR 12-05 15:24:37 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062276)[0;0m ERROR 12-05 15:24:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4062276)[0;0m ERROR 12-05 15:24:37 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4062276)[0;0m ERROR 12-05 15:24:37 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062276)[0;0m ERROR 12-05 15:24:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4062276)[0;0m ERROR 12-05 15:24:37 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4062276)[0;0m ERROR 12-05 15:24:37 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062276)[0;0m ERROR 12-05 15:24:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4062276)[0;0m ERROR 12-05 15:24:37 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4062276)[0;0m ERROR 12-05 15:24:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4062276)[0;0m ERROR 12-05 15:24:37 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4062276)[0;0m ERROR 12-05 15:24:37 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062276)[0;0m ERROR 12-05 15:24:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4062276)[0;0m ERROR 12-05 15:24:37 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4062276)[0;0m ERROR 12-05 15:24:37 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062276)[0;0m ERROR 12-05 15:24:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4062276)[0;0m ERROR 12-05 15:24:37 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4062276)[0;0m ERROR 12-05 15:24:37 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062276)[0;0m ERROR 12-05 15:24:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4062276)[0;0m ERROR 12-05 15:24:37 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4062276)[0;0m ERROR 12-05 15:24:37 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062276)[0;0m ERROR 12-05 15:24:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4062276)[0;0m ERROR 12-05 15:24:37 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4062276)[0;0m ERROR 12-05 15:24:37 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062276)[0;0m ERROR 12-05 15:24:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4062276)[0;0m ERROR 12-05 15:24:37 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4062276)[0;0m ERROR 12-05 15:24:37 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062276)[0;0m ERROR 12-05 15:24:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4062276)[0;0m ERROR 12-05 15:24:37 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4062276)[0;0m ERROR 12-05 15:24:37 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062276)[0;0m ERROR 12-05 15:24:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4062276)[0;0m ERROR 12-05 15:24:37 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4062276)[0;0m ERROR 12-05 15:24:37 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062276)[0;0m ERROR 12-05 15:24:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4062276)[0;0m ERROR 12-05 15:24:37 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4062276)[0;0m ERROR 12-05 15:24:37 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062276)[0;0m ERROR 12-05 15:24:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4062276)[0;0m ERROR 12-05 15:24:37 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4062276)[0;0m ERROR 12-05 15:24:37 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062276)[0;0m ERROR 12-05 15:24:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4062276)[0;0m ERROR 12-05 15:24:37 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4062276)[0;0m ERROR 12-05 15:24:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4062276)[0;0m ERROR 12-05 15:24:37 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4062276)[0;0m ERROR 12-05 15:24:37 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062276)[0;0m ERROR 12-05 15:24:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4062276)[0;0m ERROR 12-05 15:24:37 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4062276)[0;0m ERROR 12-05 15:24:37 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062276)[0;0m ERROR 12-05 15:24:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4062276)[0;0m ERROR 12-05 15:24:37 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4062276)[0;0m ERROR 12-05 15:24:37 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062276)[0;0m ERROR 12-05 15:24:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4062276)[0;0m ERROR 12-05 15:24:37 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4062276)[0;0m ERROR 12-05 15:24:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4062276)[0;0m ERROR 12-05 15:24:37 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4062276)[0;0m ERROR 12-05 15:24:37 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062276)[0;0m ERROR 12-05 15:24:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4062276)[0;0m ERROR 12-05 15:24:37 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4062276)[0;0m ERROR 12-05 15:24:37 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062276)[0;0m ERROR 12-05 15:24:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4062276)[0;0m ERROR 12-05 15:24:37 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4062276)[0;0m ERROR 12-05 15:24:37 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062276)[0;0m ERROR 12-05 15:24:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4062276)[0;0m ERROR 12-05 15:24:37 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4062276)[0;0m ERROR 12-05 15:24:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4062276)[0;0m ERROR 12-05 15:24:37 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4062276)[0;0m ERROR 12-05 15:24:37 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062276)[0;0m ERROR 12-05 15:24:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4062276)[0;0m ERROR 12-05 15:24:37 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4062276)[0;0m ERROR 12-05 15:24:37 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062276)[0;0m ERROR 12-05 15:24:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4062276)[0;0m ERROR 12-05 15:24:37 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4062276)[0;0m ERROR 12-05 15:24:37 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062276)[0;0m ERROR 12-05 15:24:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4062276)[0;0m ERROR 12-05 15:24:37 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4062276)[0;0m ERROR 12-05 15:24:37 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062276)[0;0m ERROR 12-05 15:24:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4062276)[0;0m ERROR 12-05 15:24:37 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4062276)[0;0m ERROR 12-05 15:24:37 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7f95c43d38f0>' raised:
[1;36m(EngineCore_DP0 pid=4062276)[0;0m ERROR 12-05 15:24:37 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4062276)[0;0m ERROR 12-05 15:24:37 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4062276)[0;0m ERROR 12-05 15:24:37 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4062276)[0;0m ERROR 12-05 15:24:37 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:24:58 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:24:58 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:24:58 [model.py:1745] Using max model len 131072
INFO 12-05 15:24:58 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4062365)[0;0m INFO 12-05 15:25:05 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4062365)[0;0m INFO 12-05 15:25:06 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:45941 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4062365)[0;0m INFO 12-05 15:25:06 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4062365)[0;0m INFO 12-05 15:25:07 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4062365)[0;0m INFO 12-05 15:25:07 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4062365)[0;0m INFO 12-05 15:25:07 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4062365)[0;0m INFO 12-05 15:25:08 [default_loader.py:314] Loading weights took 0.87 seconds
[1;36m(EngineCore_DP0 pid=4062365)[0;0m INFO 12-05 15:25:09 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.392598 seconds
[1;36m(EngineCore_DP0 pid=4062365)[0;0m INFO 12-05 15:25:12 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4062365)[0;0m INFO 12-05 15:25:12 [backends.py:647] Dynamo bytecode transform time: 3.64 s
[1;36m(EngineCore_DP0 pid=4062365)[0;0m ERROR 12-05 15:25:13 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4062365)[0;0m ERROR 12-05 15:25:13 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4062365)[0;0m ERROR 12-05 15:25:13 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4062365)[0;0m ERROR 12-05 15:25:13 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4062365)[0;0m ERROR 12-05 15:25:13 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062365)[0;0m ERROR 12-05 15:25:13 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4062365)[0;0m ERROR 12-05 15:25:13 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4062365)[0;0m ERROR 12-05 15:25:13 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4062365)[0;0m ERROR 12-05 15:25:13 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4062365)[0;0m ERROR 12-05 15:25:13 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062365)[0;0m ERROR 12-05 15:25:13 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4062365)[0;0m ERROR 12-05 15:25:13 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4062365)[0;0m ERROR 12-05 15:25:13 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062365)[0;0m ERROR 12-05 15:25:13 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4062365)[0;0m ERROR 12-05 15:25:13 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4062365)[0;0m ERROR 12-05 15:25:13 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062365)[0;0m ERROR 12-05 15:25:13 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4062365)[0;0m ERROR 12-05 15:25:13 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4062365)[0;0m ERROR 12-05 15:25:13 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062365)[0;0m ERROR 12-05 15:25:13 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4062365)[0;0m ERROR 12-05 15:25:13 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4062365)[0;0m ERROR 12-05 15:25:13 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062365)[0;0m ERROR 12-05 15:25:13 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4062365)[0;0m ERROR 12-05 15:25:13 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4062365)[0;0m ERROR 12-05 15:25:13 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062365)[0;0m ERROR 12-05 15:25:13 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4062365)[0;0m ERROR 12-05 15:25:13 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4062365)[0;0m ERROR 12-05 15:25:13 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4062365)[0;0m ERROR 12-05 15:25:13 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4062365)[0;0m ERROR 12-05 15:25:13 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062365)[0;0m ERROR 12-05 15:25:13 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4062365)[0;0m ERROR 12-05 15:25:13 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4062365)[0;0m ERROR 12-05 15:25:13 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062365)[0;0m ERROR 12-05 15:25:13 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4062365)[0;0m ERROR 12-05 15:25:13 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4062365)[0;0m ERROR 12-05 15:25:13 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062365)[0;0m ERROR 12-05 15:25:13 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4062365)[0;0m ERROR 12-05 15:25:13 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4062365)[0;0m ERROR 12-05 15:25:13 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062365)[0;0m ERROR 12-05 15:25:13 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4062365)[0;0m ERROR 12-05 15:25:13 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4062365)[0;0m ERROR 12-05 15:25:13 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062365)[0;0m ERROR 12-05 15:25:13 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4062365)[0;0m ERROR 12-05 15:25:13 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4062365)[0;0m ERROR 12-05 15:25:13 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062365)[0;0m ERROR 12-05 15:25:13 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4062365)[0;0m ERROR 12-05 15:25:13 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4062365)[0;0m ERROR 12-05 15:25:13 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062365)[0;0m ERROR 12-05 15:25:13 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4062365)[0;0m ERROR 12-05 15:25:13 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4062365)[0;0m ERROR 12-05 15:25:13 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062365)[0;0m ERROR 12-05 15:25:13 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4062365)[0;0m ERROR 12-05 15:25:13 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4062365)[0;0m ERROR 12-05 15:25:13 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062365)[0;0m ERROR 12-05 15:25:13 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4062365)[0;0m ERROR 12-05 15:25:13 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4062365)[0;0m ERROR 12-05 15:25:13 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062365)[0;0m ERROR 12-05 15:25:13 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4062365)[0;0m ERROR 12-05 15:25:13 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4062365)[0;0m ERROR 12-05 15:25:13 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4062365)[0;0m ERROR 12-05 15:25:13 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4062365)[0;0m ERROR 12-05 15:25:13 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062365)[0;0m ERROR 12-05 15:25:13 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4062365)[0;0m ERROR 12-05 15:25:13 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4062365)[0;0m ERROR 12-05 15:25:13 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062365)[0;0m ERROR 12-05 15:25:13 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4062365)[0;0m ERROR 12-05 15:25:13 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4062365)[0;0m ERROR 12-05 15:25:13 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062365)[0;0m ERROR 12-05 15:25:13 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4062365)[0;0m ERROR 12-05 15:25:13 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4062365)[0;0m ERROR 12-05 15:25:13 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4062365)[0;0m ERROR 12-05 15:25:13 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4062365)[0;0m ERROR 12-05 15:25:13 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062365)[0;0m ERROR 12-05 15:25:13 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4062365)[0;0m ERROR 12-05 15:25:13 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4062365)[0;0m ERROR 12-05 15:25:13 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062365)[0;0m ERROR 12-05 15:25:13 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4062365)[0;0m ERROR 12-05 15:25:13 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4062365)[0;0m ERROR 12-05 15:25:13 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062365)[0;0m ERROR 12-05 15:25:13 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4062365)[0;0m ERROR 12-05 15:25:13 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4062365)[0;0m ERROR 12-05 15:25:13 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4062365)[0;0m ERROR 12-05 15:25:13 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4062365)[0;0m ERROR 12-05 15:25:13 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062365)[0;0m ERROR 12-05 15:25:13 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4062365)[0;0m ERROR 12-05 15:25:13 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4062365)[0;0m ERROR 12-05 15:25:13 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062365)[0;0m ERROR 12-05 15:25:13 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4062365)[0;0m ERROR 12-05 15:25:13 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4062365)[0;0m ERROR 12-05 15:25:13 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062365)[0;0m ERROR 12-05 15:25:13 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4062365)[0;0m ERROR 12-05 15:25:13 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4062365)[0;0m ERROR 12-05 15:25:13 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062365)[0;0m ERROR 12-05 15:25:13 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4062365)[0;0m ERROR 12-05 15:25:13 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4062365)[0;0m ERROR 12-05 15:25:13 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7fef600f9460>' raised:
[1;36m(EngineCore_DP0 pid=4062365)[0;0m ERROR 12-05 15:25:13 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4062365)[0;0m ERROR 12-05 15:25:13 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4062365)[0;0m ERROR 12-05 15:25:13 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4062365)[0;0m ERROR 12-05 15:25:13 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:25:34 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:25:34 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:25:34 [model.py:1745] Using max model len 131072
INFO 12-05 15:25:34 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4062446)[0;0m INFO 12-05 15:25:41 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4062446)[0;0m INFO 12-05 15:25:42 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:36115 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4062446)[0;0m INFO 12-05 15:25:42 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4062446)[0;0m INFO 12-05 15:25:42 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4062446)[0;0m INFO 12-05 15:25:43 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4062446)[0;0m INFO 12-05 15:25:43 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4062446)[0;0m INFO 12-05 15:25:44 [default_loader.py:314] Loading weights took 0.88 seconds
[1;36m(EngineCore_DP0 pid=4062446)[0;0m INFO 12-05 15:25:44 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.440131 seconds
[1;36m(EngineCore_DP0 pid=4062446)[0;0m INFO 12-05 15:25:48 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4062446)[0;0m INFO 12-05 15:25:48 [backends.py:647] Dynamo bytecode transform time: 3.61 s
[1;36m(EngineCore_DP0 pid=4062446)[0;0m ERROR 12-05 15:25:48 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4062446)[0;0m ERROR 12-05 15:25:48 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4062446)[0;0m ERROR 12-05 15:25:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4062446)[0;0m ERROR 12-05 15:25:48 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4062446)[0;0m ERROR 12-05 15:25:48 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062446)[0;0m ERROR 12-05 15:25:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4062446)[0;0m ERROR 12-05 15:25:48 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4062446)[0;0m ERROR 12-05 15:25:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4062446)[0;0m ERROR 12-05 15:25:48 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4062446)[0;0m ERROR 12-05 15:25:48 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062446)[0;0m ERROR 12-05 15:25:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4062446)[0;0m ERROR 12-05 15:25:48 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4062446)[0;0m ERROR 12-05 15:25:48 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062446)[0;0m ERROR 12-05 15:25:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4062446)[0;0m ERROR 12-05 15:25:48 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4062446)[0;0m ERROR 12-05 15:25:48 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062446)[0;0m ERROR 12-05 15:25:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4062446)[0;0m ERROR 12-05 15:25:48 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4062446)[0;0m ERROR 12-05 15:25:48 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062446)[0;0m ERROR 12-05 15:25:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4062446)[0;0m ERROR 12-05 15:25:48 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4062446)[0;0m ERROR 12-05 15:25:48 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062446)[0;0m ERROR 12-05 15:25:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4062446)[0;0m ERROR 12-05 15:25:48 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4062446)[0;0m ERROR 12-05 15:25:48 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062446)[0;0m ERROR 12-05 15:25:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4062446)[0;0m ERROR 12-05 15:25:48 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4062446)[0;0m ERROR 12-05 15:25:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4062446)[0;0m ERROR 12-05 15:25:48 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4062446)[0;0m ERROR 12-05 15:25:48 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062446)[0;0m ERROR 12-05 15:25:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4062446)[0;0m ERROR 12-05 15:25:48 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4062446)[0;0m ERROR 12-05 15:25:48 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062446)[0;0m ERROR 12-05 15:25:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4062446)[0;0m ERROR 12-05 15:25:48 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4062446)[0;0m ERROR 12-05 15:25:48 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062446)[0;0m ERROR 12-05 15:25:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4062446)[0;0m ERROR 12-05 15:25:48 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4062446)[0;0m ERROR 12-05 15:25:48 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062446)[0;0m ERROR 12-05 15:25:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4062446)[0;0m ERROR 12-05 15:25:48 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4062446)[0;0m ERROR 12-05 15:25:48 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062446)[0;0m ERROR 12-05 15:25:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4062446)[0;0m ERROR 12-05 15:25:48 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4062446)[0;0m ERROR 12-05 15:25:48 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062446)[0;0m ERROR 12-05 15:25:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4062446)[0;0m ERROR 12-05 15:25:48 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4062446)[0;0m ERROR 12-05 15:25:48 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062446)[0;0m ERROR 12-05 15:25:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4062446)[0;0m ERROR 12-05 15:25:48 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4062446)[0;0m ERROR 12-05 15:25:48 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062446)[0;0m ERROR 12-05 15:25:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4062446)[0;0m ERROR 12-05 15:25:48 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4062446)[0;0m ERROR 12-05 15:25:48 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062446)[0;0m ERROR 12-05 15:25:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4062446)[0;0m ERROR 12-05 15:25:48 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4062446)[0;0m ERROR 12-05 15:25:48 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062446)[0;0m ERROR 12-05 15:25:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4062446)[0;0m ERROR 12-05 15:25:48 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4062446)[0;0m ERROR 12-05 15:25:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4062446)[0;0m ERROR 12-05 15:25:48 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4062446)[0;0m ERROR 12-05 15:25:48 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062446)[0;0m ERROR 12-05 15:25:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4062446)[0;0m ERROR 12-05 15:25:48 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4062446)[0;0m ERROR 12-05 15:25:48 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062446)[0;0m ERROR 12-05 15:25:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4062446)[0;0m ERROR 12-05 15:25:48 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4062446)[0;0m ERROR 12-05 15:25:48 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062446)[0;0m ERROR 12-05 15:25:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4062446)[0;0m ERROR 12-05 15:25:48 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4062446)[0;0m ERROR 12-05 15:25:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4062446)[0;0m ERROR 12-05 15:25:48 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4062446)[0;0m ERROR 12-05 15:25:48 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062446)[0;0m ERROR 12-05 15:25:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4062446)[0;0m ERROR 12-05 15:25:48 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4062446)[0;0m ERROR 12-05 15:25:48 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062446)[0;0m ERROR 12-05 15:25:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4062446)[0;0m ERROR 12-05 15:25:48 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4062446)[0;0m ERROR 12-05 15:25:48 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062446)[0;0m ERROR 12-05 15:25:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4062446)[0;0m ERROR 12-05 15:25:48 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4062446)[0;0m ERROR 12-05 15:25:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4062446)[0;0m ERROR 12-05 15:25:48 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4062446)[0;0m ERROR 12-05 15:25:48 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062446)[0;0m ERROR 12-05 15:25:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4062446)[0;0m ERROR 12-05 15:25:48 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4062446)[0;0m ERROR 12-05 15:25:48 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062446)[0;0m ERROR 12-05 15:25:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4062446)[0;0m ERROR 12-05 15:25:48 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4062446)[0;0m ERROR 12-05 15:25:48 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062446)[0;0m ERROR 12-05 15:25:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4062446)[0;0m ERROR 12-05 15:25:48 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4062446)[0;0m ERROR 12-05 15:25:48 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062446)[0;0m ERROR 12-05 15:25:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4062446)[0;0m ERROR 12-05 15:25:48 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4062446)[0;0m ERROR 12-05 15:25:48 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7f6c197cb0b0>' raised:
[1;36m(EngineCore_DP0 pid=4062446)[0;0m ERROR 12-05 15:25:48 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4062446)[0;0m ERROR 12-05 15:25:48 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4062446)[0;0m ERROR 12-05 15:25:48 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4062446)[0;0m ERROR 12-05 15:25:48 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:26:10 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:26:10 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:26:10 [model.py:1745] Using max model len 131072
INFO 12-05 15:26:10 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4062630)[0;0m INFO 12-05 15:26:17 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4062630)[0;0m INFO 12-05 15:26:18 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:38345 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4062630)[0;0m INFO 12-05 15:26:18 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4062630)[0;0m INFO 12-05 15:26:19 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4062630)[0;0m INFO 12-05 15:26:19 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4062630)[0;0m INFO 12-05 15:26:19 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4062630)[0;0m INFO 12-05 15:26:20 [default_loader.py:314] Loading weights took 0.87 seconds
[1;36m(EngineCore_DP0 pid=4062630)[0;0m INFO 12-05 15:26:21 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.426657 seconds
[1;36m(EngineCore_DP0 pid=4062630)[0;0m INFO 12-05 15:26:24 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4062630)[0;0m INFO 12-05 15:26:24 [backends.py:647] Dynamo bytecode transform time: 3.57 s
[1;36m(EngineCore_DP0 pid=4062630)[0;0m ERROR 12-05 15:26:25 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4062630)[0;0m ERROR 12-05 15:26:25 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4062630)[0;0m ERROR 12-05 15:26:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4062630)[0;0m ERROR 12-05 15:26:25 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4062630)[0;0m ERROR 12-05 15:26:25 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062630)[0;0m ERROR 12-05 15:26:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4062630)[0;0m ERROR 12-05 15:26:25 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4062630)[0;0m ERROR 12-05 15:26:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4062630)[0;0m ERROR 12-05 15:26:25 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4062630)[0;0m ERROR 12-05 15:26:25 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062630)[0;0m ERROR 12-05 15:26:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4062630)[0;0m ERROR 12-05 15:26:25 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4062630)[0;0m ERROR 12-05 15:26:25 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062630)[0;0m ERROR 12-05 15:26:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4062630)[0;0m ERROR 12-05 15:26:25 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4062630)[0;0m ERROR 12-05 15:26:25 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062630)[0;0m ERROR 12-05 15:26:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4062630)[0;0m ERROR 12-05 15:26:25 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4062630)[0;0m ERROR 12-05 15:26:25 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062630)[0;0m ERROR 12-05 15:26:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4062630)[0;0m ERROR 12-05 15:26:25 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4062630)[0;0m ERROR 12-05 15:26:25 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062630)[0;0m ERROR 12-05 15:26:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4062630)[0;0m ERROR 12-05 15:26:25 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4062630)[0;0m ERROR 12-05 15:26:25 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062630)[0;0m ERROR 12-05 15:26:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4062630)[0;0m ERROR 12-05 15:26:25 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4062630)[0;0m ERROR 12-05 15:26:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4062630)[0;0m ERROR 12-05 15:26:25 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4062630)[0;0m ERROR 12-05 15:26:25 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062630)[0;0m ERROR 12-05 15:26:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4062630)[0;0m ERROR 12-05 15:26:25 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4062630)[0;0m ERROR 12-05 15:26:25 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062630)[0;0m ERROR 12-05 15:26:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4062630)[0;0m ERROR 12-05 15:26:25 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4062630)[0;0m ERROR 12-05 15:26:25 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062630)[0;0m ERROR 12-05 15:26:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4062630)[0;0m ERROR 12-05 15:26:25 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4062630)[0;0m ERROR 12-05 15:26:25 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062630)[0;0m ERROR 12-05 15:26:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4062630)[0;0m ERROR 12-05 15:26:25 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4062630)[0;0m ERROR 12-05 15:26:25 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062630)[0;0m ERROR 12-05 15:26:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4062630)[0;0m ERROR 12-05 15:26:25 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4062630)[0;0m ERROR 12-05 15:26:25 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062630)[0;0m ERROR 12-05 15:26:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4062630)[0;0m ERROR 12-05 15:26:25 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4062630)[0;0m ERROR 12-05 15:26:25 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062630)[0;0m ERROR 12-05 15:26:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4062630)[0;0m ERROR 12-05 15:26:25 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4062630)[0;0m ERROR 12-05 15:26:25 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062630)[0;0m ERROR 12-05 15:26:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4062630)[0;0m ERROR 12-05 15:26:25 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4062630)[0;0m ERROR 12-05 15:26:25 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062630)[0;0m ERROR 12-05 15:26:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4062630)[0;0m ERROR 12-05 15:26:25 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4062630)[0;0m ERROR 12-05 15:26:25 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062630)[0;0m ERROR 12-05 15:26:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4062630)[0;0m ERROR 12-05 15:26:25 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4062630)[0;0m ERROR 12-05 15:26:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4062630)[0;0m ERROR 12-05 15:26:25 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4062630)[0;0m ERROR 12-05 15:26:25 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062630)[0;0m ERROR 12-05 15:26:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4062630)[0;0m ERROR 12-05 15:26:25 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4062630)[0;0m ERROR 12-05 15:26:25 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062630)[0;0m ERROR 12-05 15:26:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4062630)[0;0m ERROR 12-05 15:26:25 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4062630)[0;0m ERROR 12-05 15:26:25 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062630)[0;0m ERROR 12-05 15:26:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4062630)[0;0m ERROR 12-05 15:26:25 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4062630)[0;0m ERROR 12-05 15:26:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4062630)[0;0m ERROR 12-05 15:26:25 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4062630)[0;0m ERROR 12-05 15:26:25 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062630)[0;0m ERROR 12-05 15:26:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4062630)[0;0m ERROR 12-05 15:26:25 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4062630)[0;0m ERROR 12-05 15:26:25 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062630)[0;0m ERROR 12-05 15:26:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4062630)[0;0m ERROR 12-05 15:26:25 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4062630)[0;0m ERROR 12-05 15:26:25 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062630)[0;0m ERROR 12-05 15:26:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4062630)[0;0m ERROR 12-05 15:26:25 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4062630)[0;0m ERROR 12-05 15:26:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4062630)[0;0m ERROR 12-05 15:26:25 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4062630)[0;0m ERROR 12-05 15:26:25 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062630)[0;0m ERROR 12-05 15:26:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4062630)[0;0m ERROR 12-05 15:26:25 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4062630)[0;0m ERROR 12-05 15:26:25 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062630)[0;0m ERROR 12-05 15:26:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4062630)[0;0m ERROR 12-05 15:26:25 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4062630)[0;0m ERROR 12-05 15:26:25 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062630)[0;0m ERROR 12-05 15:26:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4062630)[0;0m ERROR 12-05 15:26:25 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4062630)[0;0m ERROR 12-05 15:26:25 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062630)[0;0m ERROR 12-05 15:26:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4062630)[0;0m ERROR 12-05 15:26:25 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4062630)[0;0m ERROR 12-05 15:26:25 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7f8f102bd010>' raised:
[1;36m(EngineCore_DP0 pid=4062630)[0;0m ERROR 12-05 15:26:25 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4062630)[0;0m ERROR 12-05 15:26:25 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4062630)[0;0m ERROR 12-05 15:26:25 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4062630)[0;0m ERROR 12-05 15:26:25 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:26:46 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:26:46 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:26:46 [model.py:1745] Using max model len 131072
INFO 12-05 15:26:46 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4062718)[0;0m INFO 12-05 15:26:53 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4062718)[0;0m INFO 12-05 15:26:54 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:48559 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4062718)[0;0m INFO 12-05 15:26:54 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4062718)[0;0m INFO 12-05 15:26:54 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4062718)[0;0m INFO 12-05 15:26:55 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4062718)[0;0m INFO 12-05 15:26:55 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4062718)[0;0m INFO 12-05 15:26:56 [default_loader.py:314] Loading weights took 0.87 seconds
[1;36m(EngineCore_DP0 pid=4062718)[0;0m INFO 12-05 15:26:56 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.407952 seconds
[1;36m(EngineCore_DP0 pid=4062718)[0;0m INFO 12-05 15:27:00 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4062718)[0;0m INFO 12-05 15:27:00 [backends.py:647] Dynamo bytecode transform time: 3.59 s
[1;36m(EngineCore_DP0 pid=4062718)[0;0m ERROR 12-05 15:27:00 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4062718)[0;0m ERROR 12-05 15:27:00 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4062718)[0;0m ERROR 12-05 15:27:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4062718)[0;0m ERROR 12-05 15:27:00 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4062718)[0;0m ERROR 12-05 15:27:00 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062718)[0;0m ERROR 12-05 15:27:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4062718)[0;0m ERROR 12-05 15:27:00 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4062718)[0;0m ERROR 12-05 15:27:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4062718)[0;0m ERROR 12-05 15:27:00 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4062718)[0;0m ERROR 12-05 15:27:00 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062718)[0;0m ERROR 12-05 15:27:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4062718)[0;0m ERROR 12-05 15:27:00 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4062718)[0;0m ERROR 12-05 15:27:00 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062718)[0;0m ERROR 12-05 15:27:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4062718)[0;0m ERROR 12-05 15:27:00 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4062718)[0;0m ERROR 12-05 15:27:00 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062718)[0;0m ERROR 12-05 15:27:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4062718)[0;0m ERROR 12-05 15:27:00 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4062718)[0;0m ERROR 12-05 15:27:00 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062718)[0;0m ERROR 12-05 15:27:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4062718)[0;0m ERROR 12-05 15:27:00 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4062718)[0;0m ERROR 12-05 15:27:00 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062718)[0;0m ERROR 12-05 15:27:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4062718)[0;0m ERROR 12-05 15:27:00 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4062718)[0;0m ERROR 12-05 15:27:00 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062718)[0;0m ERROR 12-05 15:27:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4062718)[0;0m ERROR 12-05 15:27:00 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4062718)[0;0m ERROR 12-05 15:27:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4062718)[0;0m ERROR 12-05 15:27:00 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4062718)[0;0m ERROR 12-05 15:27:00 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062718)[0;0m ERROR 12-05 15:27:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4062718)[0;0m ERROR 12-05 15:27:00 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4062718)[0;0m ERROR 12-05 15:27:00 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062718)[0;0m ERROR 12-05 15:27:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4062718)[0;0m ERROR 12-05 15:27:00 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4062718)[0;0m ERROR 12-05 15:27:00 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062718)[0;0m ERROR 12-05 15:27:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4062718)[0;0m ERROR 12-05 15:27:00 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4062718)[0;0m ERROR 12-05 15:27:00 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062718)[0;0m ERROR 12-05 15:27:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4062718)[0;0m ERROR 12-05 15:27:00 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4062718)[0;0m ERROR 12-05 15:27:00 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062718)[0;0m ERROR 12-05 15:27:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4062718)[0;0m ERROR 12-05 15:27:00 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4062718)[0;0m ERROR 12-05 15:27:00 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062718)[0;0m ERROR 12-05 15:27:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4062718)[0;0m ERROR 12-05 15:27:00 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4062718)[0;0m ERROR 12-05 15:27:00 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062718)[0;0m ERROR 12-05 15:27:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4062718)[0;0m ERROR 12-05 15:27:00 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4062718)[0;0m ERROR 12-05 15:27:00 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062718)[0;0m ERROR 12-05 15:27:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4062718)[0;0m ERROR 12-05 15:27:00 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4062718)[0;0m ERROR 12-05 15:27:00 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062718)[0;0m ERROR 12-05 15:27:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4062718)[0;0m ERROR 12-05 15:27:00 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4062718)[0;0m ERROR 12-05 15:27:00 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062718)[0;0m ERROR 12-05 15:27:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4062718)[0;0m ERROR 12-05 15:27:00 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4062718)[0;0m ERROR 12-05 15:27:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4062718)[0;0m ERROR 12-05 15:27:00 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4062718)[0;0m ERROR 12-05 15:27:00 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062718)[0;0m ERROR 12-05 15:27:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4062718)[0;0m ERROR 12-05 15:27:00 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4062718)[0;0m ERROR 12-05 15:27:00 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062718)[0;0m ERROR 12-05 15:27:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4062718)[0;0m ERROR 12-05 15:27:00 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4062718)[0;0m ERROR 12-05 15:27:00 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062718)[0;0m ERROR 12-05 15:27:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4062718)[0;0m ERROR 12-05 15:27:00 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4062718)[0;0m ERROR 12-05 15:27:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4062718)[0;0m ERROR 12-05 15:27:00 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4062718)[0;0m ERROR 12-05 15:27:00 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062718)[0;0m ERROR 12-05 15:27:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4062718)[0;0m ERROR 12-05 15:27:00 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4062718)[0;0m ERROR 12-05 15:27:00 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062718)[0;0m ERROR 12-05 15:27:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4062718)[0;0m ERROR 12-05 15:27:00 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4062718)[0;0m ERROR 12-05 15:27:00 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062718)[0;0m ERROR 12-05 15:27:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4062718)[0;0m ERROR 12-05 15:27:00 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4062718)[0;0m ERROR 12-05 15:27:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4062718)[0;0m ERROR 12-05 15:27:00 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4062718)[0;0m ERROR 12-05 15:27:00 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062718)[0;0m ERROR 12-05 15:27:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4062718)[0;0m ERROR 12-05 15:27:00 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4062718)[0;0m ERROR 12-05 15:27:00 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062718)[0;0m ERROR 12-05 15:27:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4062718)[0;0m ERROR 12-05 15:27:00 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4062718)[0;0m ERROR 12-05 15:27:00 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062718)[0;0m ERROR 12-05 15:27:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4062718)[0;0m ERROR 12-05 15:27:00 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4062718)[0;0m ERROR 12-05 15:27:00 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062718)[0;0m ERROR 12-05 15:27:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4062718)[0;0m ERROR 12-05 15:27:00 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4062718)[0;0m ERROR 12-05 15:27:00 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7f9a28088aa0>' raised:
[1;36m(EngineCore_DP0 pid=4062718)[0;0m ERROR 12-05 15:27:00 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4062718)[0;0m ERROR 12-05 15:27:00 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4062718)[0;0m ERROR 12-05 15:27:00 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4062718)[0;0m ERROR 12-05 15:27:00 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:27:22 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:27:22 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:27:22 [model.py:1745] Using max model len 131072
INFO 12-05 15:27:22 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4062796)[0;0m INFO 12-05 15:27:29 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4062796)[0;0m INFO 12-05 15:27:30 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:53947 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4062796)[0;0m INFO 12-05 15:27:30 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4062796)[0;0m INFO 12-05 15:27:30 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4062796)[0;0m INFO 12-05 15:27:31 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4062796)[0;0m INFO 12-05 15:27:31 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4062796)[0;0m INFO 12-05 15:27:32 [default_loader.py:314] Loading weights took 0.88 seconds
[1;36m(EngineCore_DP0 pid=4062796)[0;0m INFO 12-05 15:27:32 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.452224 seconds
[1;36m(EngineCore_DP0 pid=4062796)[0;0m INFO 12-05 15:27:36 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4062796)[0;0m INFO 12-05 15:27:36 [backends.py:647] Dynamo bytecode transform time: 3.61 s
[1;36m(EngineCore_DP0 pid=4062796)[0;0m ERROR 12-05 15:27:36 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4062796)[0;0m ERROR 12-05 15:27:36 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4062796)[0;0m ERROR 12-05 15:27:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4062796)[0;0m ERROR 12-05 15:27:36 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4062796)[0;0m ERROR 12-05 15:27:36 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062796)[0;0m ERROR 12-05 15:27:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4062796)[0;0m ERROR 12-05 15:27:36 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4062796)[0;0m ERROR 12-05 15:27:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4062796)[0;0m ERROR 12-05 15:27:36 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4062796)[0;0m ERROR 12-05 15:27:36 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062796)[0;0m ERROR 12-05 15:27:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4062796)[0;0m ERROR 12-05 15:27:36 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4062796)[0;0m ERROR 12-05 15:27:36 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062796)[0;0m ERROR 12-05 15:27:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4062796)[0;0m ERROR 12-05 15:27:36 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4062796)[0;0m ERROR 12-05 15:27:36 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062796)[0;0m ERROR 12-05 15:27:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4062796)[0;0m ERROR 12-05 15:27:36 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4062796)[0;0m ERROR 12-05 15:27:36 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062796)[0;0m ERROR 12-05 15:27:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4062796)[0;0m ERROR 12-05 15:27:36 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4062796)[0;0m ERROR 12-05 15:27:36 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062796)[0;0m ERROR 12-05 15:27:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4062796)[0;0m ERROR 12-05 15:27:36 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4062796)[0;0m ERROR 12-05 15:27:36 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062796)[0;0m ERROR 12-05 15:27:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4062796)[0;0m ERROR 12-05 15:27:36 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4062796)[0;0m ERROR 12-05 15:27:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4062796)[0;0m ERROR 12-05 15:27:36 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4062796)[0;0m ERROR 12-05 15:27:36 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062796)[0;0m ERROR 12-05 15:27:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4062796)[0;0m ERROR 12-05 15:27:36 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4062796)[0;0m ERROR 12-05 15:27:36 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062796)[0;0m ERROR 12-05 15:27:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4062796)[0;0m ERROR 12-05 15:27:36 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4062796)[0;0m ERROR 12-05 15:27:36 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062796)[0;0m ERROR 12-05 15:27:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4062796)[0;0m ERROR 12-05 15:27:36 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4062796)[0;0m ERROR 12-05 15:27:36 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062796)[0;0m ERROR 12-05 15:27:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4062796)[0;0m ERROR 12-05 15:27:36 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4062796)[0;0m ERROR 12-05 15:27:36 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062796)[0;0m ERROR 12-05 15:27:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4062796)[0;0m ERROR 12-05 15:27:36 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4062796)[0;0m ERROR 12-05 15:27:36 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062796)[0;0m ERROR 12-05 15:27:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4062796)[0;0m ERROR 12-05 15:27:36 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4062796)[0;0m ERROR 12-05 15:27:36 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062796)[0;0m ERROR 12-05 15:27:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4062796)[0;0m ERROR 12-05 15:27:36 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4062796)[0;0m ERROR 12-05 15:27:36 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062796)[0;0m ERROR 12-05 15:27:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4062796)[0;0m ERROR 12-05 15:27:36 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4062796)[0;0m ERROR 12-05 15:27:36 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062796)[0;0m ERROR 12-05 15:27:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4062796)[0;0m ERROR 12-05 15:27:36 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4062796)[0;0m ERROR 12-05 15:27:36 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062796)[0;0m ERROR 12-05 15:27:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4062796)[0;0m ERROR 12-05 15:27:36 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4062796)[0;0m ERROR 12-05 15:27:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4062796)[0;0m ERROR 12-05 15:27:36 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4062796)[0;0m ERROR 12-05 15:27:36 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062796)[0;0m ERROR 12-05 15:27:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4062796)[0;0m ERROR 12-05 15:27:36 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4062796)[0;0m ERROR 12-05 15:27:36 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062796)[0;0m ERROR 12-05 15:27:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4062796)[0;0m ERROR 12-05 15:27:36 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4062796)[0;0m ERROR 12-05 15:27:36 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062796)[0;0m ERROR 12-05 15:27:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4062796)[0;0m ERROR 12-05 15:27:36 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4062796)[0;0m ERROR 12-05 15:27:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4062796)[0;0m ERROR 12-05 15:27:36 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4062796)[0;0m ERROR 12-05 15:27:36 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062796)[0;0m ERROR 12-05 15:27:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4062796)[0;0m ERROR 12-05 15:27:36 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4062796)[0;0m ERROR 12-05 15:27:36 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062796)[0;0m ERROR 12-05 15:27:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4062796)[0;0m ERROR 12-05 15:27:36 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4062796)[0;0m ERROR 12-05 15:27:36 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062796)[0;0m ERROR 12-05 15:27:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4062796)[0;0m ERROR 12-05 15:27:36 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4062796)[0;0m ERROR 12-05 15:27:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4062796)[0;0m ERROR 12-05 15:27:36 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4062796)[0;0m ERROR 12-05 15:27:36 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062796)[0;0m ERROR 12-05 15:27:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4062796)[0;0m ERROR 12-05 15:27:36 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4062796)[0;0m ERROR 12-05 15:27:36 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062796)[0;0m ERROR 12-05 15:27:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4062796)[0;0m ERROR 12-05 15:27:36 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4062796)[0;0m ERROR 12-05 15:27:36 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062796)[0;0m ERROR 12-05 15:27:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4062796)[0;0m ERROR 12-05 15:27:36 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4062796)[0;0m ERROR 12-05 15:27:36 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062796)[0;0m ERROR 12-05 15:27:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4062796)[0;0m ERROR 12-05 15:27:36 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4062796)[0;0m ERROR 12-05 15:27:36 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7f2a94221f10>' raised:
[1;36m(EngineCore_DP0 pid=4062796)[0;0m ERROR 12-05 15:27:36 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4062796)[0;0m ERROR 12-05 15:27:36 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4062796)[0;0m ERROR 12-05 15:27:36 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4062796)[0;0m ERROR 12-05 15:27:36 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:27:58 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:27:58 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:27:58 [model.py:1745] Using max model len 131072
INFO 12-05 15:27:58 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4062883)[0;0m INFO 12-05 15:28:05 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4062883)[0;0m INFO 12-05 15:28:06 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:36723 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4062883)[0;0m INFO 12-05 15:28:06 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4062883)[0;0m INFO 12-05 15:28:06 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4062883)[0;0m INFO 12-05 15:28:07 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4062883)[0;0m INFO 12-05 15:28:07 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4062883)[0;0m INFO 12-05 15:28:08 [default_loader.py:314] Loading weights took 0.87 seconds
[1;36m(EngineCore_DP0 pid=4062883)[0;0m INFO 12-05 15:28:08 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.395432 seconds
[1;36m(EngineCore_DP0 pid=4062883)[0;0m INFO 12-05 15:28:12 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4062883)[0;0m INFO 12-05 15:28:12 [backends.py:647] Dynamo bytecode transform time: 3.60 s
[1;36m(EngineCore_DP0 pid=4062883)[0;0m ERROR 12-05 15:28:12 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4062883)[0;0m ERROR 12-05 15:28:12 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4062883)[0;0m ERROR 12-05 15:28:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4062883)[0;0m ERROR 12-05 15:28:12 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4062883)[0;0m ERROR 12-05 15:28:12 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062883)[0;0m ERROR 12-05 15:28:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4062883)[0;0m ERROR 12-05 15:28:12 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4062883)[0;0m ERROR 12-05 15:28:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4062883)[0;0m ERROR 12-05 15:28:12 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4062883)[0;0m ERROR 12-05 15:28:12 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062883)[0;0m ERROR 12-05 15:28:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4062883)[0;0m ERROR 12-05 15:28:12 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4062883)[0;0m ERROR 12-05 15:28:12 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062883)[0;0m ERROR 12-05 15:28:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4062883)[0;0m ERROR 12-05 15:28:12 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4062883)[0;0m ERROR 12-05 15:28:12 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062883)[0;0m ERROR 12-05 15:28:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4062883)[0;0m ERROR 12-05 15:28:12 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4062883)[0;0m ERROR 12-05 15:28:12 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062883)[0;0m ERROR 12-05 15:28:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4062883)[0;0m ERROR 12-05 15:28:12 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4062883)[0;0m ERROR 12-05 15:28:12 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062883)[0;0m ERROR 12-05 15:28:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4062883)[0;0m ERROR 12-05 15:28:12 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4062883)[0;0m ERROR 12-05 15:28:12 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062883)[0;0m ERROR 12-05 15:28:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4062883)[0;0m ERROR 12-05 15:28:12 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4062883)[0;0m ERROR 12-05 15:28:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4062883)[0;0m ERROR 12-05 15:28:12 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4062883)[0;0m ERROR 12-05 15:28:12 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062883)[0;0m ERROR 12-05 15:28:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4062883)[0;0m ERROR 12-05 15:28:12 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4062883)[0;0m ERROR 12-05 15:28:12 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062883)[0;0m ERROR 12-05 15:28:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4062883)[0;0m ERROR 12-05 15:28:12 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4062883)[0;0m ERROR 12-05 15:28:12 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062883)[0;0m ERROR 12-05 15:28:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4062883)[0;0m ERROR 12-05 15:28:12 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4062883)[0;0m ERROR 12-05 15:28:12 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062883)[0;0m ERROR 12-05 15:28:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4062883)[0;0m ERROR 12-05 15:28:12 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4062883)[0;0m ERROR 12-05 15:28:12 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062883)[0;0m ERROR 12-05 15:28:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4062883)[0;0m ERROR 12-05 15:28:12 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4062883)[0;0m ERROR 12-05 15:28:12 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062883)[0;0m ERROR 12-05 15:28:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4062883)[0;0m ERROR 12-05 15:28:12 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4062883)[0;0m ERROR 12-05 15:28:12 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062883)[0;0m ERROR 12-05 15:28:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4062883)[0;0m ERROR 12-05 15:28:12 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4062883)[0;0m ERROR 12-05 15:28:12 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062883)[0;0m ERROR 12-05 15:28:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4062883)[0;0m ERROR 12-05 15:28:12 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4062883)[0;0m ERROR 12-05 15:28:12 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062883)[0;0m ERROR 12-05 15:28:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4062883)[0;0m ERROR 12-05 15:28:12 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4062883)[0;0m ERROR 12-05 15:28:12 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062883)[0;0m ERROR 12-05 15:28:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4062883)[0;0m ERROR 12-05 15:28:12 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4062883)[0;0m ERROR 12-05 15:28:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4062883)[0;0m ERROR 12-05 15:28:12 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4062883)[0;0m ERROR 12-05 15:28:12 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062883)[0;0m ERROR 12-05 15:28:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4062883)[0;0m ERROR 12-05 15:28:12 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4062883)[0;0m ERROR 12-05 15:28:12 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062883)[0;0m ERROR 12-05 15:28:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4062883)[0;0m ERROR 12-05 15:28:12 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4062883)[0;0m ERROR 12-05 15:28:12 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062883)[0;0m ERROR 12-05 15:28:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4062883)[0;0m ERROR 12-05 15:28:12 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4062883)[0;0m ERROR 12-05 15:28:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4062883)[0;0m ERROR 12-05 15:28:12 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4062883)[0;0m ERROR 12-05 15:28:12 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062883)[0;0m ERROR 12-05 15:28:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4062883)[0;0m ERROR 12-05 15:28:12 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4062883)[0;0m ERROR 12-05 15:28:12 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062883)[0;0m ERROR 12-05 15:28:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4062883)[0;0m ERROR 12-05 15:28:12 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4062883)[0;0m ERROR 12-05 15:28:12 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062883)[0;0m ERROR 12-05 15:28:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4062883)[0;0m ERROR 12-05 15:28:12 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4062883)[0;0m ERROR 12-05 15:28:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4062883)[0;0m ERROR 12-05 15:28:12 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4062883)[0;0m ERROR 12-05 15:28:12 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062883)[0;0m ERROR 12-05 15:28:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4062883)[0;0m ERROR 12-05 15:28:12 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4062883)[0;0m ERROR 12-05 15:28:12 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062883)[0;0m ERROR 12-05 15:28:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4062883)[0;0m ERROR 12-05 15:28:12 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4062883)[0;0m ERROR 12-05 15:28:12 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062883)[0;0m ERROR 12-05 15:28:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4062883)[0;0m ERROR 12-05 15:28:12 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4062883)[0;0m ERROR 12-05 15:28:12 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062883)[0;0m ERROR 12-05 15:28:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4062883)[0;0m ERROR 12-05 15:28:12 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4062883)[0;0m ERROR 12-05 15:28:12 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7f38001284a0>' raised:
[1;36m(EngineCore_DP0 pid=4062883)[0;0m ERROR 12-05 15:28:12 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4062883)[0;0m ERROR 12-05 15:28:12 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4062883)[0;0m ERROR 12-05 15:28:12 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4062883)[0;0m ERROR 12-05 15:28:12 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:28:34 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:28:34 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:28:34 [model.py:1745] Using max model len 131072
INFO 12-05 15:28:34 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4062963)[0;0m INFO 12-05 15:28:41 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4062963)[0;0m INFO 12-05 15:28:42 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:33047 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4062963)[0;0m INFO 12-05 15:28:42 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4062963)[0;0m INFO 12-05 15:28:42 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4062963)[0;0m INFO 12-05 15:28:43 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4062963)[0;0m INFO 12-05 15:28:43 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4062963)[0;0m INFO 12-05 15:28:44 [default_loader.py:314] Loading weights took 0.87 seconds
[1;36m(EngineCore_DP0 pid=4062963)[0;0m INFO 12-05 15:28:44 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.373951 seconds
[1;36m(EngineCore_DP0 pid=4062963)[0;0m INFO 12-05 15:28:48 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4062963)[0;0m INFO 12-05 15:28:48 [backends.py:647] Dynamo bytecode transform time: 3.58 s
[1;36m(EngineCore_DP0 pid=4062963)[0;0m ERROR 12-05 15:28:48 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4062963)[0;0m ERROR 12-05 15:28:48 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4062963)[0;0m ERROR 12-05 15:28:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4062963)[0;0m ERROR 12-05 15:28:48 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4062963)[0;0m ERROR 12-05 15:28:48 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062963)[0;0m ERROR 12-05 15:28:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4062963)[0;0m ERROR 12-05 15:28:48 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4062963)[0;0m ERROR 12-05 15:28:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4062963)[0;0m ERROR 12-05 15:28:48 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4062963)[0;0m ERROR 12-05 15:28:48 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062963)[0;0m ERROR 12-05 15:28:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4062963)[0;0m ERROR 12-05 15:28:48 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4062963)[0;0m ERROR 12-05 15:28:48 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062963)[0;0m ERROR 12-05 15:28:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4062963)[0;0m ERROR 12-05 15:28:48 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4062963)[0;0m ERROR 12-05 15:28:48 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062963)[0;0m ERROR 12-05 15:28:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4062963)[0;0m ERROR 12-05 15:28:48 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4062963)[0;0m ERROR 12-05 15:28:48 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062963)[0;0m ERROR 12-05 15:28:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4062963)[0;0m ERROR 12-05 15:28:48 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4062963)[0;0m ERROR 12-05 15:28:48 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062963)[0;0m ERROR 12-05 15:28:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4062963)[0;0m ERROR 12-05 15:28:48 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4062963)[0;0m ERROR 12-05 15:28:48 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062963)[0;0m ERROR 12-05 15:28:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4062963)[0;0m ERROR 12-05 15:28:48 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4062963)[0;0m ERROR 12-05 15:28:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4062963)[0;0m ERROR 12-05 15:28:48 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4062963)[0;0m ERROR 12-05 15:28:48 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062963)[0;0m ERROR 12-05 15:28:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4062963)[0;0m ERROR 12-05 15:28:48 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4062963)[0;0m ERROR 12-05 15:28:48 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062963)[0;0m ERROR 12-05 15:28:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4062963)[0;0m ERROR 12-05 15:28:48 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4062963)[0;0m ERROR 12-05 15:28:48 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062963)[0;0m ERROR 12-05 15:28:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4062963)[0;0m ERROR 12-05 15:28:48 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4062963)[0;0m ERROR 12-05 15:28:48 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062963)[0;0m ERROR 12-05 15:28:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4062963)[0;0m ERROR 12-05 15:28:48 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4062963)[0;0m ERROR 12-05 15:28:48 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062963)[0;0m ERROR 12-05 15:28:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4062963)[0;0m ERROR 12-05 15:28:48 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4062963)[0;0m ERROR 12-05 15:28:48 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062963)[0;0m ERROR 12-05 15:28:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4062963)[0;0m ERROR 12-05 15:28:48 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4062963)[0;0m ERROR 12-05 15:28:48 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062963)[0;0m ERROR 12-05 15:28:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4062963)[0;0m ERROR 12-05 15:28:48 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4062963)[0;0m ERROR 12-05 15:28:48 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062963)[0;0m ERROR 12-05 15:28:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4062963)[0;0m ERROR 12-05 15:28:48 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4062963)[0;0m ERROR 12-05 15:28:48 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062963)[0;0m ERROR 12-05 15:28:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4062963)[0;0m ERROR 12-05 15:28:48 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4062963)[0;0m ERROR 12-05 15:28:48 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062963)[0;0m ERROR 12-05 15:28:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4062963)[0;0m ERROR 12-05 15:28:48 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4062963)[0;0m ERROR 12-05 15:28:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4062963)[0;0m ERROR 12-05 15:28:48 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4062963)[0;0m ERROR 12-05 15:28:48 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062963)[0;0m ERROR 12-05 15:28:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4062963)[0;0m ERROR 12-05 15:28:48 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4062963)[0;0m ERROR 12-05 15:28:48 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062963)[0;0m ERROR 12-05 15:28:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4062963)[0;0m ERROR 12-05 15:28:48 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4062963)[0;0m ERROR 12-05 15:28:48 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062963)[0;0m ERROR 12-05 15:28:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4062963)[0;0m ERROR 12-05 15:28:48 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4062963)[0;0m ERROR 12-05 15:28:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4062963)[0;0m ERROR 12-05 15:28:48 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4062963)[0;0m ERROR 12-05 15:28:48 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062963)[0;0m ERROR 12-05 15:28:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4062963)[0;0m ERROR 12-05 15:28:48 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4062963)[0;0m ERROR 12-05 15:28:48 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062963)[0;0m ERROR 12-05 15:28:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4062963)[0;0m ERROR 12-05 15:28:48 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4062963)[0;0m ERROR 12-05 15:28:48 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062963)[0;0m ERROR 12-05 15:28:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4062963)[0;0m ERROR 12-05 15:28:48 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4062963)[0;0m ERROR 12-05 15:28:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4062963)[0;0m ERROR 12-05 15:28:48 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4062963)[0;0m ERROR 12-05 15:28:48 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062963)[0;0m ERROR 12-05 15:28:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4062963)[0;0m ERROR 12-05 15:28:48 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4062963)[0;0m ERROR 12-05 15:28:48 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062963)[0;0m ERROR 12-05 15:28:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4062963)[0;0m ERROR 12-05 15:28:48 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4062963)[0;0m ERROR 12-05 15:28:48 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062963)[0;0m ERROR 12-05 15:28:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4062963)[0;0m ERROR 12-05 15:28:48 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4062963)[0;0m ERROR 12-05 15:28:48 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4062963)[0;0m ERROR 12-05 15:28:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4062963)[0;0m ERROR 12-05 15:28:48 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4062963)[0;0m ERROR 12-05 15:28:48 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7f1da0257b90>' raised:
[1;36m(EngineCore_DP0 pid=4062963)[0;0m ERROR 12-05 15:28:48 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4062963)[0;0m ERROR 12-05 15:28:48 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4062963)[0;0m ERROR 12-05 15:28:48 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4062963)[0;0m ERROR 12-05 15:28:48 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:29:10 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:29:10 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:29:10 [model.py:1745] Using max model len 131072
INFO 12-05 15:29:10 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4063049)[0;0m INFO 12-05 15:29:17 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4063049)[0;0m INFO 12-05 15:29:18 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:55437 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4063049)[0;0m INFO 12-05 15:29:18 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4063049)[0;0m INFO 12-05 15:29:19 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4063049)[0;0m INFO 12-05 15:29:19 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4063049)[0;0m INFO 12-05 15:29:19 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4063049)[0;0m INFO 12-05 15:29:20 [default_loader.py:314] Loading weights took 0.87 seconds
[1;36m(EngineCore_DP0 pid=4063049)[0;0m INFO 12-05 15:29:21 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.387442 seconds
[1;36m(EngineCore_DP0 pid=4063049)[0;0m INFO 12-05 15:29:24 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4063049)[0;0m INFO 12-05 15:29:24 [backends.py:647] Dynamo bytecode transform time: 3.61 s
[1;36m(EngineCore_DP0 pid=4063049)[0;0m ERROR 12-05 15:29:25 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4063049)[0;0m ERROR 12-05 15:29:25 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4063049)[0;0m ERROR 12-05 15:29:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4063049)[0;0m ERROR 12-05 15:29:25 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4063049)[0;0m ERROR 12-05 15:29:25 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063049)[0;0m ERROR 12-05 15:29:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4063049)[0;0m ERROR 12-05 15:29:25 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4063049)[0;0m ERROR 12-05 15:29:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4063049)[0;0m ERROR 12-05 15:29:25 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4063049)[0;0m ERROR 12-05 15:29:25 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063049)[0;0m ERROR 12-05 15:29:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4063049)[0;0m ERROR 12-05 15:29:25 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4063049)[0;0m ERROR 12-05 15:29:25 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063049)[0;0m ERROR 12-05 15:29:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4063049)[0;0m ERROR 12-05 15:29:25 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4063049)[0;0m ERROR 12-05 15:29:25 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063049)[0;0m ERROR 12-05 15:29:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4063049)[0;0m ERROR 12-05 15:29:25 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4063049)[0;0m ERROR 12-05 15:29:25 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063049)[0;0m ERROR 12-05 15:29:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4063049)[0;0m ERROR 12-05 15:29:25 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4063049)[0;0m ERROR 12-05 15:29:25 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063049)[0;0m ERROR 12-05 15:29:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4063049)[0;0m ERROR 12-05 15:29:25 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4063049)[0;0m ERROR 12-05 15:29:25 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063049)[0;0m ERROR 12-05 15:29:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4063049)[0;0m ERROR 12-05 15:29:25 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4063049)[0;0m ERROR 12-05 15:29:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4063049)[0;0m ERROR 12-05 15:29:25 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4063049)[0;0m ERROR 12-05 15:29:25 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063049)[0;0m ERROR 12-05 15:29:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4063049)[0;0m ERROR 12-05 15:29:25 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4063049)[0;0m ERROR 12-05 15:29:25 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063049)[0;0m ERROR 12-05 15:29:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4063049)[0;0m ERROR 12-05 15:29:25 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4063049)[0;0m ERROR 12-05 15:29:25 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063049)[0;0m ERROR 12-05 15:29:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4063049)[0;0m ERROR 12-05 15:29:25 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4063049)[0;0m ERROR 12-05 15:29:25 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063049)[0;0m ERROR 12-05 15:29:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4063049)[0;0m ERROR 12-05 15:29:25 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4063049)[0;0m ERROR 12-05 15:29:25 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063049)[0;0m ERROR 12-05 15:29:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4063049)[0;0m ERROR 12-05 15:29:25 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4063049)[0;0m ERROR 12-05 15:29:25 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063049)[0;0m ERROR 12-05 15:29:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4063049)[0;0m ERROR 12-05 15:29:25 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4063049)[0;0m ERROR 12-05 15:29:25 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063049)[0;0m ERROR 12-05 15:29:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4063049)[0;0m ERROR 12-05 15:29:25 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4063049)[0;0m ERROR 12-05 15:29:25 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063049)[0;0m ERROR 12-05 15:29:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4063049)[0;0m ERROR 12-05 15:29:25 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4063049)[0;0m ERROR 12-05 15:29:25 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063049)[0;0m ERROR 12-05 15:29:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4063049)[0;0m ERROR 12-05 15:29:25 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4063049)[0;0m ERROR 12-05 15:29:25 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063049)[0;0m ERROR 12-05 15:29:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4063049)[0;0m ERROR 12-05 15:29:25 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4063049)[0;0m ERROR 12-05 15:29:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4063049)[0;0m ERROR 12-05 15:29:25 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4063049)[0;0m ERROR 12-05 15:29:25 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063049)[0;0m ERROR 12-05 15:29:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4063049)[0;0m ERROR 12-05 15:29:25 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4063049)[0;0m ERROR 12-05 15:29:25 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063049)[0;0m ERROR 12-05 15:29:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4063049)[0;0m ERROR 12-05 15:29:25 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4063049)[0;0m ERROR 12-05 15:29:25 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063049)[0;0m ERROR 12-05 15:29:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4063049)[0;0m ERROR 12-05 15:29:25 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4063049)[0;0m ERROR 12-05 15:29:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4063049)[0;0m ERROR 12-05 15:29:25 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4063049)[0;0m ERROR 12-05 15:29:25 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063049)[0;0m ERROR 12-05 15:29:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4063049)[0;0m ERROR 12-05 15:29:25 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4063049)[0;0m ERROR 12-05 15:29:25 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063049)[0;0m ERROR 12-05 15:29:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4063049)[0;0m ERROR 12-05 15:29:25 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4063049)[0;0m ERROR 12-05 15:29:25 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063049)[0;0m ERROR 12-05 15:29:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4063049)[0;0m ERROR 12-05 15:29:25 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4063049)[0;0m ERROR 12-05 15:29:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4063049)[0;0m ERROR 12-05 15:29:25 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4063049)[0;0m ERROR 12-05 15:29:25 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063049)[0;0m ERROR 12-05 15:29:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4063049)[0;0m ERROR 12-05 15:29:25 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4063049)[0;0m ERROR 12-05 15:29:25 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063049)[0;0m ERROR 12-05 15:29:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4063049)[0;0m ERROR 12-05 15:29:25 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4063049)[0;0m ERROR 12-05 15:29:25 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063049)[0;0m ERROR 12-05 15:29:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4063049)[0;0m ERROR 12-05 15:29:25 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4063049)[0;0m ERROR 12-05 15:29:25 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063049)[0;0m ERROR 12-05 15:29:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4063049)[0;0m ERROR 12-05 15:29:25 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4063049)[0;0m ERROR 12-05 15:29:25 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7f13541b5ee0>' raised:
[1;36m(EngineCore_DP0 pid=4063049)[0;0m ERROR 12-05 15:29:25 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4063049)[0;0m ERROR 12-05 15:29:25 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4063049)[0;0m ERROR 12-05 15:29:25 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4063049)[0;0m ERROR 12-05 15:29:25 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:29:46 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:29:46 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:29:46 [model.py:1745] Using max model len 131072
INFO 12-05 15:29:46 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4063138)[0;0m INFO 12-05 15:29:53 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4063138)[0;0m INFO 12-05 15:29:54 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:56687 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4063138)[0;0m INFO 12-05 15:29:54 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4063138)[0;0m INFO 12-05 15:29:55 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4063138)[0;0m INFO 12-05 15:29:55 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4063138)[0;0m INFO 12-05 15:29:55 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4063138)[0;0m INFO 12-05 15:29:56 [default_loader.py:314] Loading weights took 0.87 seconds
[1;36m(EngineCore_DP0 pid=4063138)[0;0m INFO 12-05 15:29:57 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.409895 seconds
[1;36m(EngineCore_DP0 pid=4063138)[0;0m INFO 12-05 15:30:00 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4063138)[0;0m INFO 12-05 15:30:00 [backends.py:647] Dynamo bytecode transform time: 3.58 s
[1;36m(EngineCore_DP0 pid=4063138)[0;0m ERROR 12-05 15:30:01 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4063138)[0;0m ERROR 12-05 15:30:01 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4063138)[0;0m ERROR 12-05 15:30:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4063138)[0;0m ERROR 12-05 15:30:01 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4063138)[0;0m ERROR 12-05 15:30:01 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063138)[0;0m ERROR 12-05 15:30:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4063138)[0;0m ERROR 12-05 15:30:01 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4063138)[0;0m ERROR 12-05 15:30:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4063138)[0;0m ERROR 12-05 15:30:01 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4063138)[0;0m ERROR 12-05 15:30:01 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063138)[0;0m ERROR 12-05 15:30:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4063138)[0;0m ERROR 12-05 15:30:01 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4063138)[0;0m ERROR 12-05 15:30:01 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063138)[0;0m ERROR 12-05 15:30:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4063138)[0;0m ERROR 12-05 15:30:01 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4063138)[0;0m ERROR 12-05 15:30:01 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063138)[0;0m ERROR 12-05 15:30:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4063138)[0;0m ERROR 12-05 15:30:01 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4063138)[0;0m ERROR 12-05 15:30:01 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063138)[0;0m ERROR 12-05 15:30:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4063138)[0;0m ERROR 12-05 15:30:01 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4063138)[0;0m ERROR 12-05 15:30:01 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063138)[0;0m ERROR 12-05 15:30:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4063138)[0;0m ERROR 12-05 15:30:01 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4063138)[0;0m ERROR 12-05 15:30:01 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063138)[0;0m ERROR 12-05 15:30:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4063138)[0;0m ERROR 12-05 15:30:01 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4063138)[0;0m ERROR 12-05 15:30:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4063138)[0;0m ERROR 12-05 15:30:01 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4063138)[0;0m ERROR 12-05 15:30:01 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063138)[0;0m ERROR 12-05 15:30:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4063138)[0;0m ERROR 12-05 15:30:01 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4063138)[0;0m ERROR 12-05 15:30:01 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063138)[0;0m ERROR 12-05 15:30:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4063138)[0;0m ERROR 12-05 15:30:01 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4063138)[0;0m ERROR 12-05 15:30:01 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063138)[0;0m ERROR 12-05 15:30:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4063138)[0;0m ERROR 12-05 15:30:01 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4063138)[0;0m ERROR 12-05 15:30:01 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063138)[0;0m ERROR 12-05 15:30:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4063138)[0;0m ERROR 12-05 15:30:01 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4063138)[0;0m ERROR 12-05 15:30:01 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063138)[0;0m ERROR 12-05 15:30:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4063138)[0;0m ERROR 12-05 15:30:01 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4063138)[0;0m ERROR 12-05 15:30:01 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063138)[0;0m ERROR 12-05 15:30:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4063138)[0;0m ERROR 12-05 15:30:01 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4063138)[0;0m ERROR 12-05 15:30:01 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063138)[0;0m ERROR 12-05 15:30:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4063138)[0;0m ERROR 12-05 15:30:01 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4063138)[0;0m ERROR 12-05 15:30:01 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063138)[0;0m ERROR 12-05 15:30:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4063138)[0;0m ERROR 12-05 15:30:01 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4063138)[0;0m ERROR 12-05 15:30:01 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063138)[0;0m ERROR 12-05 15:30:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4063138)[0;0m ERROR 12-05 15:30:01 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4063138)[0;0m ERROR 12-05 15:30:01 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063138)[0;0m ERROR 12-05 15:30:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4063138)[0;0m ERROR 12-05 15:30:01 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4063138)[0;0m ERROR 12-05 15:30:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4063138)[0;0m ERROR 12-05 15:30:01 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4063138)[0;0m ERROR 12-05 15:30:01 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063138)[0;0m ERROR 12-05 15:30:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4063138)[0;0m ERROR 12-05 15:30:01 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4063138)[0;0m ERROR 12-05 15:30:01 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063138)[0;0m ERROR 12-05 15:30:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4063138)[0;0m ERROR 12-05 15:30:01 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4063138)[0;0m ERROR 12-05 15:30:01 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063138)[0;0m ERROR 12-05 15:30:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4063138)[0;0m ERROR 12-05 15:30:01 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4063138)[0;0m ERROR 12-05 15:30:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4063138)[0;0m ERROR 12-05 15:30:01 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4063138)[0;0m ERROR 12-05 15:30:01 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063138)[0;0m ERROR 12-05 15:30:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4063138)[0;0m ERROR 12-05 15:30:01 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4063138)[0;0m ERROR 12-05 15:30:01 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063138)[0;0m ERROR 12-05 15:30:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4063138)[0;0m ERROR 12-05 15:30:01 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4063138)[0;0m ERROR 12-05 15:30:01 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063138)[0;0m ERROR 12-05 15:30:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4063138)[0;0m ERROR 12-05 15:30:01 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4063138)[0;0m ERROR 12-05 15:30:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4063138)[0;0m ERROR 12-05 15:30:01 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4063138)[0;0m ERROR 12-05 15:30:01 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063138)[0;0m ERROR 12-05 15:30:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4063138)[0;0m ERROR 12-05 15:30:01 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4063138)[0;0m ERROR 12-05 15:30:01 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063138)[0;0m ERROR 12-05 15:30:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4063138)[0;0m ERROR 12-05 15:30:01 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4063138)[0;0m ERROR 12-05 15:30:01 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063138)[0;0m ERROR 12-05 15:30:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4063138)[0;0m ERROR 12-05 15:30:01 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4063138)[0;0m ERROR 12-05 15:30:01 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063138)[0;0m ERROR 12-05 15:30:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4063138)[0;0m ERROR 12-05 15:30:01 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4063138)[0;0m ERROR 12-05 15:30:01 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7f8e9c123d10>' raised:
[1;36m(EngineCore_DP0 pid=4063138)[0;0m ERROR 12-05 15:30:01 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4063138)[0;0m ERROR 12-05 15:30:01 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4063138)[0;0m ERROR 12-05 15:30:01 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4063138)[0;0m ERROR 12-05 15:30:01 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:30:22 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:30:22 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:30:22 [model.py:1745] Using max model len 131072
INFO 12-05 15:30:22 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4063216)[0;0m INFO 12-05 15:30:29 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4063216)[0;0m INFO 12-05 15:30:30 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:36819 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4063216)[0;0m INFO 12-05 15:30:30 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4063216)[0;0m INFO 12-05 15:30:30 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4063216)[0;0m INFO 12-05 15:30:31 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4063216)[0;0m INFO 12-05 15:30:31 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4063216)[0;0m INFO 12-05 15:30:32 [default_loader.py:314] Loading weights took 0.87 seconds
[1;36m(EngineCore_DP0 pid=4063216)[0;0m INFO 12-05 15:30:32 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.406389 seconds
[1;36m(EngineCore_DP0 pid=4063216)[0;0m INFO 12-05 15:30:36 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4063216)[0;0m INFO 12-05 15:30:36 [backends.py:647] Dynamo bytecode transform time: 3.62 s
[1;36m(EngineCore_DP0 pid=4063216)[0;0m ERROR 12-05 15:30:37 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4063216)[0;0m ERROR 12-05 15:30:37 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4063216)[0;0m ERROR 12-05 15:30:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4063216)[0;0m ERROR 12-05 15:30:37 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4063216)[0;0m ERROR 12-05 15:30:37 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063216)[0;0m ERROR 12-05 15:30:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4063216)[0;0m ERROR 12-05 15:30:37 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4063216)[0;0m ERROR 12-05 15:30:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4063216)[0;0m ERROR 12-05 15:30:37 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4063216)[0;0m ERROR 12-05 15:30:37 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063216)[0;0m ERROR 12-05 15:30:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4063216)[0;0m ERROR 12-05 15:30:37 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4063216)[0;0m ERROR 12-05 15:30:37 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063216)[0;0m ERROR 12-05 15:30:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4063216)[0;0m ERROR 12-05 15:30:37 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4063216)[0;0m ERROR 12-05 15:30:37 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063216)[0;0m ERROR 12-05 15:30:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4063216)[0;0m ERROR 12-05 15:30:37 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4063216)[0;0m ERROR 12-05 15:30:37 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063216)[0;0m ERROR 12-05 15:30:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4063216)[0;0m ERROR 12-05 15:30:37 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4063216)[0;0m ERROR 12-05 15:30:37 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063216)[0;0m ERROR 12-05 15:30:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4063216)[0;0m ERROR 12-05 15:30:37 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4063216)[0;0m ERROR 12-05 15:30:37 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063216)[0;0m ERROR 12-05 15:30:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4063216)[0;0m ERROR 12-05 15:30:37 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4063216)[0;0m ERROR 12-05 15:30:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4063216)[0;0m ERROR 12-05 15:30:37 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4063216)[0;0m ERROR 12-05 15:30:37 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063216)[0;0m ERROR 12-05 15:30:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4063216)[0;0m ERROR 12-05 15:30:37 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4063216)[0;0m ERROR 12-05 15:30:37 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063216)[0;0m ERROR 12-05 15:30:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4063216)[0;0m ERROR 12-05 15:30:37 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4063216)[0;0m ERROR 12-05 15:30:37 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063216)[0;0m ERROR 12-05 15:30:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4063216)[0;0m ERROR 12-05 15:30:37 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4063216)[0;0m ERROR 12-05 15:30:37 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063216)[0;0m ERROR 12-05 15:30:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4063216)[0;0m ERROR 12-05 15:30:37 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4063216)[0;0m ERROR 12-05 15:30:37 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063216)[0;0m ERROR 12-05 15:30:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4063216)[0;0m ERROR 12-05 15:30:37 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4063216)[0;0m ERROR 12-05 15:30:37 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063216)[0;0m ERROR 12-05 15:30:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4063216)[0;0m ERROR 12-05 15:30:37 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4063216)[0;0m ERROR 12-05 15:30:37 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063216)[0;0m ERROR 12-05 15:30:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4063216)[0;0m ERROR 12-05 15:30:37 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4063216)[0;0m ERROR 12-05 15:30:37 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063216)[0;0m ERROR 12-05 15:30:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4063216)[0;0m ERROR 12-05 15:30:37 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4063216)[0;0m ERROR 12-05 15:30:37 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063216)[0;0m ERROR 12-05 15:30:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4063216)[0;0m ERROR 12-05 15:30:37 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4063216)[0;0m ERROR 12-05 15:30:37 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063216)[0;0m ERROR 12-05 15:30:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4063216)[0;0m ERROR 12-05 15:30:37 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4063216)[0;0m ERROR 12-05 15:30:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4063216)[0;0m ERROR 12-05 15:30:37 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4063216)[0;0m ERROR 12-05 15:30:37 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063216)[0;0m ERROR 12-05 15:30:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4063216)[0;0m ERROR 12-05 15:30:37 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4063216)[0;0m ERROR 12-05 15:30:37 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063216)[0;0m ERROR 12-05 15:30:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4063216)[0;0m ERROR 12-05 15:30:37 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4063216)[0;0m ERROR 12-05 15:30:37 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063216)[0;0m ERROR 12-05 15:30:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4063216)[0;0m ERROR 12-05 15:30:37 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4063216)[0;0m ERROR 12-05 15:30:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4063216)[0;0m ERROR 12-05 15:30:37 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4063216)[0;0m ERROR 12-05 15:30:37 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063216)[0;0m ERROR 12-05 15:30:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4063216)[0;0m ERROR 12-05 15:30:37 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4063216)[0;0m ERROR 12-05 15:30:37 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063216)[0;0m ERROR 12-05 15:30:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4063216)[0;0m ERROR 12-05 15:30:37 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4063216)[0;0m ERROR 12-05 15:30:37 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063216)[0;0m ERROR 12-05 15:30:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4063216)[0;0m ERROR 12-05 15:30:37 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4063216)[0;0m ERROR 12-05 15:30:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4063216)[0;0m ERROR 12-05 15:30:37 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4063216)[0;0m ERROR 12-05 15:30:37 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063216)[0;0m ERROR 12-05 15:30:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4063216)[0;0m ERROR 12-05 15:30:37 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4063216)[0;0m ERROR 12-05 15:30:37 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063216)[0;0m ERROR 12-05 15:30:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4063216)[0;0m ERROR 12-05 15:30:37 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4063216)[0;0m ERROR 12-05 15:30:37 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063216)[0;0m ERROR 12-05 15:30:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4063216)[0;0m ERROR 12-05 15:30:37 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4063216)[0;0m ERROR 12-05 15:30:37 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063216)[0;0m ERROR 12-05 15:30:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4063216)[0;0m ERROR 12-05 15:30:37 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4063216)[0;0m ERROR 12-05 15:30:37 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7f89b8380140>' raised:
[1;36m(EngineCore_DP0 pid=4063216)[0;0m ERROR 12-05 15:30:37 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4063216)[0;0m ERROR 12-05 15:30:37 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4063216)[0;0m ERROR 12-05 15:30:37 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4063216)[0;0m ERROR 12-05 15:30:37 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:30:58 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:30:58 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:30:58 [model.py:1745] Using max model len 131072
INFO 12-05 15:30:58 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4063303)[0;0m INFO 12-05 15:31:06 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4063303)[0;0m INFO 12-05 15:31:07 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:57715 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4063303)[0;0m INFO 12-05 15:31:07 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4063303)[0;0m INFO 12-05 15:31:07 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4063303)[0;0m INFO 12-05 15:31:08 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4063303)[0;0m INFO 12-05 15:31:08 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4063303)[0;0m INFO 12-05 15:31:09 [default_loader.py:314] Loading weights took 0.88 seconds
[1;36m(EngineCore_DP0 pid=4063303)[0;0m INFO 12-05 15:31:09 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.396656 seconds
[1;36m(EngineCore_DP0 pid=4063303)[0;0m INFO 12-05 15:31:13 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4063303)[0;0m INFO 12-05 15:31:13 [backends.py:647] Dynamo bytecode transform time: 3.63 s
[1;36m(EngineCore_DP0 pid=4063303)[0;0m ERROR 12-05 15:31:13 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4063303)[0;0m ERROR 12-05 15:31:13 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4063303)[0;0m ERROR 12-05 15:31:13 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4063303)[0;0m ERROR 12-05 15:31:13 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4063303)[0;0m ERROR 12-05 15:31:13 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063303)[0;0m ERROR 12-05 15:31:13 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4063303)[0;0m ERROR 12-05 15:31:13 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4063303)[0;0m ERROR 12-05 15:31:13 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4063303)[0;0m ERROR 12-05 15:31:13 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4063303)[0;0m ERROR 12-05 15:31:13 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063303)[0;0m ERROR 12-05 15:31:13 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4063303)[0;0m ERROR 12-05 15:31:13 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4063303)[0;0m ERROR 12-05 15:31:13 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063303)[0;0m ERROR 12-05 15:31:13 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4063303)[0;0m ERROR 12-05 15:31:13 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4063303)[0;0m ERROR 12-05 15:31:13 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063303)[0;0m ERROR 12-05 15:31:13 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4063303)[0;0m ERROR 12-05 15:31:13 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4063303)[0;0m ERROR 12-05 15:31:13 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063303)[0;0m ERROR 12-05 15:31:13 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4063303)[0;0m ERROR 12-05 15:31:13 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4063303)[0;0m ERROR 12-05 15:31:13 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063303)[0;0m ERROR 12-05 15:31:13 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4063303)[0;0m ERROR 12-05 15:31:13 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4063303)[0;0m ERROR 12-05 15:31:13 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063303)[0;0m ERROR 12-05 15:31:13 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4063303)[0;0m ERROR 12-05 15:31:13 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4063303)[0;0m ERROR 12-05 15:31:13 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4063303)[0;0m ERROR 12-05 15:31:13 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4063303)[0;0m ERROR 12-05 15:31:13 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063303)[0;0m ERROR 12-05 15:31:13 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4063303)[0;0m ERROR 12-05 15:31:13 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4063303)[0;0m ERROR 12-05 15:31:13 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063303)[0;0m ERROR 12-05 15:31:13 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4063303)[0;0m ERROR 12-05 15:31:13 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4063303)[0;0m ERROR 12-05 15:31:13 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063303)[0;0m ERROR 12-05 15:31:13 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4063303)[0;0m ERROR 12-05 15:31:13 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4063303)[0;0m ERROR 12-05 15:31:13 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063303)[0;0m ERROR 12-05 15:31:13 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4063303)[0;0m ERROR 12-05 15:31:13 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4063303)[0;0m ERROR 12-05 15:31:13 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063303)[0;0m ERROR 12-05 15:31:13 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4063303)[0;0m ERROR 12-05 15:31:13 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4063303)[0;0m ERROR 12-05 15:31:13 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063303)[0;0m ERROR 12-05 15:31:13 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4063303)[0;0m ERROR 12-05 15:31:13 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4063303)[0;0m ERROR 12-05 15:31:13 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063303)[0;0m ERROR 12-05 15:31:13 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4063303)[0;0m ERROR 12-05 15:31:13 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4063303)[0;0m ERROR 12-05 15:31:13 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063303)[0;0m ERROR 12-05 15:31:13 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4063303)[0;0m ERROR 12-05 15:31:13 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4063303)[0;0m ERROR 12-05 15:31:13 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063303)[0;0m ERROR 12-05 15:31:13 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4063303)[0;0m ERROR 12-05 15:31:13 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4063303)[0;0m ERROR 12-05 15:31:13 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063303)[0;0m ERROR 12-05 15:31:13 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4063303)[0;0m ERROR 12-05 15:31:13 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4063303)[0;0m ERROR 12-05 15:31:13 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4063303)[0;0m ERROR 12-05 15:31:13 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4063303)[0;0m ERROR 12-05 15:31:13 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063303)[0;0m ERROR 12-05 15:31:13 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4063303)[0;0m ERROR 12-05 15:31:13 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4063303)[0;0m ERROR 12-05 15:31:13 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063303)[0;0m ERROR 12-05 15:31:13 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4063303)[0;0m ERROR 12-05 15:31:13 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4063303)[0;0m ERROR 12-05 15:31:13 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063303)[0;0m ERROR 12-05 15:31:13 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4063303)[0;0m ERROR 12-05 15:31:13 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4063303)[0;0m ERROR 12-05 15:31:13 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4063303)[0;0m ERROR 12-05 15:31:13 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4063303)[0;0m ERROR 12-05 15:31:13 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063303)[0;0m ERROR 12-05 15:31:13 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4063303)[0;0m ERROR 12-05 15:31:13 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4063303)[0;0m ERROR 12-05 15:31:13 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063303)[0;0m ERROR 12-05 15:31:13 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4063303)[0;0m ERROR 12-05 15:31:13 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4063303)[0;0m ERROR 12-05 15:31:13 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063303)[0;0m ERROR 12-05 15:31:13 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4063303)[0;0m ERROR 12-05 15:31:13 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4063303)[0;0m ERROR 12-05 15:31:13 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4063303)[0;0m ERROR 12-05 15:31:13 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4063303)[0;0m ERROR 12-05 15:31:13 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063303)[0;0m ERROR 12-05 15:31:13 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4063303)[0;0m ERROR 12-05 15:31:13 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4063303)[0;0m ERROR 12-05 15:31:13 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063303)[0;0m ERROR 12-05 15:31:13 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4063303)[0;0m ERROR 12-05 15:31:13 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4063303)[0;0m ERROR 12-05 15:31:13 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063303)[0;0m ERROR 12-05 15:31:13 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4063303)[0;0m ERROR 12-05 15:31:13 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4063303)[0;0m ERROR 12-05 15:31:13 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063303)[0;0m ERROR 12-05 15:31:13 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4063303)[0;0m ERROR 12-05 15:31:13 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4063303)[0;0m ERROR 12-05 15:31:13 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7f14da1b2f90>' raised:
[1;36m(EngineCore_DP0 pid=4063303)[0;0m ERROR 12-05 15:31:13 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4063303)[0;0m ERROR 12-05 15:31:13 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4063303)[0;0m ERROR 12-05 15:31:13 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4063303)[0;0m ERROR 12-05 15:31:13 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:31:35 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:31:35 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:31:35 [model.py:1745] Using max model len 131072
INFO 12-05 15:31:35 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4063480)[0;0m INFO 12-05 15:31:44 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4063480)[0;0m INFO 12-05 15:31:45 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:54337 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4063480)[0;0m INFO 12-05 15:31:45 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4063480)[0;0m INFO 12-05 15:31:45 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4063480)[0;0m INFO 12-05 15:31:46 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4063480)[0;0m INFO 12-05 15:31:46 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4063480)[0;0m INFO 12-05 15:31:47 [default_loader.py:314] Loading weights took 0.88 seconds
[1;36m(EngineCore_DP0 pid=4063480)[0;0m INFO 12-05 15:31:47 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.424205 seconds
[1;36m(EngineCore_DP0 pid=4063480)[0;0m INFO 12-05 15:31:51 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4063480)[0;0m INFO 12-05 15:31:51 [backends.py:647] Dynamo bytecode transform time: 3.64 s
[1;36m(EngineCore_DP0 pid=4063480)[0;0m ERROR 12-05 15:31:52 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4063480)[0;0m ERROR 12-05 15:31:52 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4063480)[0;0m ERROR 12-05 15:31:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4063480)[0;0m ERROR 12-05 15:31:52 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4063480)[0;0m ERROR 12-05 15:31:52 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063480)[0;0m ERROR 12-05 15:31:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4063480)[0;0m ERROR 12-05 15:31:52 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4063480)[0;0m ERROR 12-05 15:31:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4063480)[0;0m ERROR 12-05 15:31:52 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4063480)[0;0m ERROR 12-05 15:31:52 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063480)[0;0m ERROR 12-05 15:31:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4063480)[0;0m ERROR 12-05 15:31:52 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4063480)[0;0m ERROR 12-05 15:31:52 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063480)[0;0m ERROR 12-05 15:31:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4063480)[0;0m ERROR 12-05 15:31:52 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4063480)[0;0m ERROR 12-05 15:31:52 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063480)[0;0m ERROR 12-05 15:31:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4063480)[0;0m ERROR 12-05 15:31:52 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4063480)[0;0m ERROR 12-05 15:31:52 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063480)[0;0m ERROR 12-05 15:31:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4063480)[0;0m ERROR 12-05 15:31:52 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4063480)[0;0m ERROR 12-05 15:31:52 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063480)[0;0m ERROR 12-05 15:31:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4063480)[0;0m ERROR 12-05 15:31:52 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4063480)[0;0m ERROR 12-05 15:31:52 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063480)[0;0m ERROR 12-05 15:31:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4063480)[0;0m ERROR 12-05 15:31:52 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4063480)[0;0m ERROR 12-05 15:31:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4063480)[0;0m ERROR 12-05 15:31:52 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4063480)[0;0m ERROR 12-05 15:31:52 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063480)[0;0m ERROR 12-05 15:31:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4063480)[0;0m ERROR 12-05 15:31:52 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4063480)[0;0m ERROR 12-05 15:31:52 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063480)[0;0m ERROR 12-05 15:31:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4063480)[0;0m ERROR 12-05 15:31:52 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4063480)[0;0m ERROR 12-05 15:31:52 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063480)[0;0m ERROR 12-05 15:31:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4063480)[0;0m ERROR 12-05 15:31:52 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4063480)[0;0m ERROR 12-05 15:31:52 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063480)[0;0m ERROR 12-05 15:31:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4063480)[0;0m ERROR 12-05 15:31:52 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4063480)[0;0m ERROR 12-05 15:31:52 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063480)[0;0m ERROR 12-05 15:31:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4063480)[0;0m ERROR 12-05 15:31:52 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4063480)[0;0m ERROR 12-05 15:31:52 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063480)[0;0m ERROR 12-05 15:31:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4063480)[0;0m ERROR 12-05 15:31:52 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4063480)[0;0m ERROR 12-05 15:31:52 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063480)[0;0m ERROR 12-05 15:31:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4063480)[0;0m ERROR 12-05 15:31:52 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4063480)[0;0m ERROR 12-05 15:31:52 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063480)[0;0m ERROR 12-05 15:31:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4063480)[0;0m ERROR 12-05 15:31:52 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4063480)[0;0m ERROR 12-05 15:31:52 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063480)[0;0m ERROR 12-05 15:31:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4063480)[0;0m ERROR 12-05 15:31:52 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4063480)[0;0m ERROR 12-05 15:31:52 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063480)[0;0m ERROR 12-05 15:31:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4063480)[0;0m ERROR 12-05 15:31:52 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4063480)[0;0m ERROR 12-05 15:31:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4063480)[0;0m ERROR 12-05 15:31:52 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4063480)[0;0m ERROR 12-05 15:31:52 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063480)[0;0m ERROR 12-05 15:31:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4063480)[0;0m ERROR 12-05 15:31:52 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4063480)[0;0m ERROR 12-05 15:31:52 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063480)[0;0m ERROR 12-05 15:31:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4063480)[0;0m ERROR 12-05 15:31:52 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4063480)[0;0m ERROR 12-05 15:31:52 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063480)[0;0m ERROR 12-05 15:31:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4063480)[0;0m ERROR 12-05 15:31:52 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4063480)[0;0m ERROR 12-05 15:31:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4063480)[0;0m ERROR 12-05 15:31:52 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4063480)[0;0m ERROR 12-05 15:31:52 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063480)[0;0m ERROR 12-05 15:31:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4063480)[0;0m ERROR 12-05 15:31:52 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4063480)[0;0m ERROR 12-05 15:31:52 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063480)[0;0m ERROR 12-05 15:31:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4063480)[0;0m ERROR 12-05 15:31:52 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4063480)[0;0m ERROR 12-05 15:31:52 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063480)[0;0m ERROR 12-05 15:31:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4063480)[0;0m ERROR 12-05 15:31:52 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4063480)[0;0m ERROR 12-05 15:31:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4063480)[0;0m ERROR 12-05 15:31:52 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4063480)[0;0m ERROR 12-05 15:31:52 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063480)[0;0m ERROR 12-05 15:31:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4063480)[0;0m ERROR 12-05 15:31:52 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4063480)[0;0m ERROR 12-05 15:31:52 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063480)[0;0m ERROR 12-05 15:31:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4063480)[0;0m ERROR 12-05 15:31:52 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4063480)[0;0m ERROR 12-05 15:31:52 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063480)[0;0m ERROR 12-05 15:31:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4063480)[0;0m ERROR 12-05 15:31:52 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4063480)[0;0m ERROR 12-05 15:31:52 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063480)[0;0m ERROR 12-05 15:31:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4063480)[0;0m ERROR 12-05 15:31:52 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4063480)[0;0m ERROR 12-05 15:31:52 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7f9220108e30>' raised:
[1;36m(EngineCore_DP0 pid=4063480)[0;0m ERROR 12-05 15:31:52 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4063480)[0;0m ERROR 12-05 15:31:52 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4063480)[0;0m ERROR 12-05 15:31:52 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4063480)[0;0m ERROR 12-05 15:31:52 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:32:13 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:32:13 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:32:13 [model.py:1745] Using max model len 131072
INFO 12-05 15:32:13 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4063565)[0;0m INFO 12-05 15:32:21 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4063565)[0;0m INFO 12-05 15:32:22 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:41651 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4063565)[0;0m INFO 12-05 15:32:22 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4063565)[0;0m INFO 12-05 15:32:22 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4063565)[0;0m INFO 12-05 15:32:22 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4063565)[0;0m INFO 12-05 15:32:22 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4063565)[0;0m INFO 12-05 15:32:24 [default_loader.py:314] Loading weights took 0.87 seconds
[1;36m(EngineCore_DP0 pid=4063565)[0;0m INFO 12-05 15:32:24 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.432066 seconds
[1;36m(EngineCore_DP0 pid=4063565)[0;0m INFO 12-05 15:32:28 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4063565)[0;0m INFO 12-05 15:32:28 [backends.py:647] Dynamo bytecode transform time: 3.89 s
[1;36m(EngineCore_DP0 pid=4063565)[0;0m ERROR 12-05 15:32:28 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4063565)[0;0m ERROR 12-05 15:32:28 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4063565)[0;0m ERROR 12-05 15:32:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4063565)[0;0m ERROR 12-05 15:32:28 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4063565)[0;0m ERROR 12-05 15:32:28 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063565)[0;0m ERROR 12-05 15:32:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4063565)[0;0m ERROR 12-05 15:32:28 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4063565)[0;0m ERROR 12-05 15:32:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4063565)[0;0m ERROR 12-05 15:32:28 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4063565)[0;0m ERROR 12-05 15:32:28 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063565)[0;0m ERROR 12-05 15:32:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4063565)[0;0m ERROR 12-05 15:32:28 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4063565)[0;0m ERROR 12-05 15:32:28 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063565)[0;0m ERROR 12-05 15:32:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4063565)[0;0m ERROR 12-05 15:32:28 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4063565)[0;0m ERROR 12-05 15:32:28 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063565)[0;0m ERROR 12-05 15:32:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4063565)[0;0m ERROR 12-05 15:32:28 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4063565)[0;0m ERROR 12-05 15:32:28 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063565)[0;0m ERROR 12-05 15:32:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4063565)[0;0m ERROR 12-05 15:32:28 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4063565)[0;0m ERROR 12-05 15:32:28 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063565)[0;0m ERROR 12-05 15:32:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4063565)[0;0m ERROR 12-05 15:32:28 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4063565)[0;0m ERROR 12-05 15:32:28 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063565)[0;0m ERROR 12-05 15:32:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4063565)[0;0m ERROR 12-05 15:32:28 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4063565)[0;0m ERROR 12-05 15:32:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4063565)[0;0m ERROR 12-05 15:32:28 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4063565)[0;0m ERROR 12-05 15:32:28 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063565)[0;0m ERROR 12-05 15:32:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4063565)[0;0m ERROR 12-05 15:32:28 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4063565)[0;0m ERROR 12-05 15:32:28 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063565)[0;0m ERROR 12-05 15:32:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4063565)[0;0m ERROR 12-05 15:32:28 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4063565)[0;0m ERROR 12-05 15:32:28 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063565)[0;0m ERROR 12-05 15:32:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4063565)[0;0m ERROR 12-05 15:32:28 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4063565)[0;0m ERROR 12-05 15:32:28 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063565)[0;0m ERROR 12-05 15:32:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4063565)[0;0m ERROR 12-05 15:32:28 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4063565)[0;0m ERROR 12-05 15:32:28 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063565)[0;0m ERROR 12-05 15:32:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4063565)[0;0m ERROR 12-05 15:32:28 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4063565)[0;0m ERROR 12-05 15:32:28 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063565)[0;0m ERROR 12-05 15:32:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4063565)[0;0m ERROR 12-05 15:32:28 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4063565)[0;0m ERROR 12-05 15:32:28 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063565)[0;0m ERROR 12-05 15:32:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4063565)[0;0m ERROR 12-05 15:32:28 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4063565)[0;0m ERROR 12-05 15:32:28 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063565)[0;0m ERROR 12-05 15:32:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4063565)[0;0m ERROR 12-05 15:32:28 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4063565)[0;0m ERROR 12-05 15:32:28 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063565)[0;0m ERROR 12-05 15:32:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4063565)[0;0m ERROR 12-05 15:32:28 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4063565)[0;0m ERROR 12-05 15:32:28 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063565)[0;0m ERROR 12-05 15:32:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4063565)[0;0m ERROR 12-05 15:32:28 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4063565)[0;0m ERROR 12-05 15:32:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4063565)[0;0m ERROR 12-05 15:32:28 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4063565)[0;0m ERROR 12-05 15:32:28 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063565)[0;0m ERROR 12-05 15:32:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4063565)[0;0m ERROR 12-05 15:32:28 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4063565)[0;0m ERROR 12-05 15:32:28 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063565)[0;0m ERROR 12-05 15:32:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4063565)[0;0m ERROR 12-05 15:32:28 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4063565)[0;0m ERROR 12-05 15:32:28 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063565)[0;0m ERROR 12-05 15:32:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4063565)[0;0m ERROR 12-05 15:32:28 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4063565)[0;0m ERROR 12-05 15:32:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4063565)[0;0m ERROR 12-05 15:32:28 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4063565)[0;0m ERROR 12-05 15:32:28 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063565)[0;0m ERROR 12-05 15:32:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4063565)[0;0m ERROR 12-05 15:32:28 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4063565)[0;0m ERROR 12-05 15:32:28 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063565)[0;0m ERROR 12-05 15:32:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4063565)[0;0m ERROR 12-05 15:32:28 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4063565)[0;0m ERROR 12-05 15:32:28 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063565)[0;0m ERROR 12-05 15:32:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4063565)[0;0m ERROR 12-05 15:32:28 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4063565)[0;0m ERROR 12-05 15:32:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4063565)[0;0m ERROR 12-05 15:32:28 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4063565)[0;0m ERROR 12-05 15:32:28 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063565)[0;0m ERROR 12-05 15:32:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4063565)[0;0m ERROR 12-05 15:32:28 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4063565)[0;0m ERROR 12-05 15:32:28 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063565)[0;0m ERROR 12-05 15:32:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4063565)[0;0m ERROR 12-05 15:32:28 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4063565)[0;0m ERROR 12-05 15:32:28 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063565)[0;0m ERROR 12-05 15:32:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4063565)[0;0m ERROR 12-05 15:32:28 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4063565)[0;0m ERROR 12-05 15:32:28 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4063565)[0;0m ERROR 12-05 15:32:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4063565)[0;0m ERROR 12-05 15:32:28 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4063565)[0;0m ERROR 12-05 15:32:28 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7f70282c97c0>' raised:
[1;36m(EngineCore_DP0 pid=4063565)[0;0m ERROR 12-05 15:32:28 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4063565)[0;0m ERROR 12-05 15:32:28 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4063565)[0;0m ERROR 12-05 15:32:28 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4063565)[0;0m ERROR 12-05 15:32:28 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:32:50 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:32:50 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:32:50 [model.py:1745] Using max model len 131072
INFO 12-05 15:32:50 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {'EngineCore_DP0': 1}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:33:11 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:33:11 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:33:11 [model.py:1745] Using max model len 131072
INFO 12-05 15:33:11 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {'EngineCore_DP0': 1}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:33:31 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:33:31 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:33:31 [model.py:1745] Using max model len 131072
INFO 12-05 15:33:31 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {'EngineCore_DP0': 1}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:33:52 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:33:52 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:33:52 [model.py:1745] Using max model len 131072
INFO 12-05 15:33:52 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {'EngineCore_DP0': 1}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:34:13 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:34:13 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:34:13 [model.py:1745] Using max model len 131072
INFO 12-05 15:34:13 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {'EngineCore_DP0': 1}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:34:34 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:34:34 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:34:34 [model.py:1745] Using max model len 131072
INFO 12-05 15:34:34 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {'EngineCore_DP0': 1}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:34:54 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:34:55 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:34:55 [model.py:1745] Using max model len 131072
INFO 12-05 15:34:55 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {'EngineCore_DP0': 1}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:35:15 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:35:15 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:35:15 [model.py:1745] Using max model len 131072
INFO 12-05 15:35:15 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {'EngineCore_DP0': 1}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:35:36 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:35:36 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:35:36 [model.py:1745] Using max model len 131072
INFO 12-05 15:35:36 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {'EngineCore_DP0': 1}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:35:57 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:35:57 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:35:57 [model.py:1745] Using max model len 131072
INFO 12-05 15:35:57 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {'EngineCore_DP0': 1}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:36:17 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:36:18 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:36:18 [model.py:1745] Using max model len 131072
INFO 12-05 15:36:18 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {'EngineCore_DP0': 1}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:36:38 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:36:38 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:36:38 [model.py:1745] Using max model len 131072
INFO 12-05 15:36:38 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {'EngineCore_DP0': 1}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:36:59 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:36:59 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:36:59 [model.py:1745] Using max model len 131072
INFO 12-05 15:36:59 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {'EngineCore_DP0': 1}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:37:20 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:37:20 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:37:20 [model.py:1745] Using max model len 131072
INFO 12-05 15:37:20 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {'EngineCore_DP0': 1}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:37:41 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:37:41 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:37:41 [model.py:1745] Using max model len 131072
INFO 12-05 15:37:41 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {'EngineCore_DP0': 1}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:38:01 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:38:02 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:38:02 [model.py:1745] Using max model len 131072
INFO 12-05 15:38:02 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {'EngineCore_DP0': 1}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:38:22 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:38:22 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:38:22 [model.py:1745] Using max model len 131072
INFO 12-05 15:38:22 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {'EngineCore_DP0': 1}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:38:43 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:38:43 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:38:43 [model.py:1745] Using max model len 131072
INFO 12-05 15:38:43 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {'EngineCore_DP0': 1}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:39:04 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:39:04 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:39:04 [model.py:1745] Using max model len 131072
INFO 12-05 15:39:04 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {'EngineCore_DP0': 1}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:39:25 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:39:25 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:39:25 [model.py:1745] Using max model len 131072
INFO 12-05 15:39:25 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {'EngineCore_DP0': 1}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:39:45 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:39:45 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:39:45 [model.py:1745] Using max model len 131072
INFO 12-05 15:39:45 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {'EngineCore_DP0': 1}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:40:06 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:40:06 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:40:06 [model.py:1745] Using max model len 131072
INFO 12-05 15:40:06 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {'EngineCore_DP0': 1}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:40:27 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:40:27 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:40:27 [model.py:1745] Using max model len 131072
INFO 12-05 15:40:27 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {'EngineCore_DP0': 1}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:40:48 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:40:48 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:40:48 [model.py:1745] Using max model len 131072
INFO 12-05 15:40:48 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {'EngineCore_DP0': 1}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:41:08 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:41:09 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:41:09 [model.py:1745] Using max model len 131072
INFO 12-05 15:41:09 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {'EngineCore_DP0': 1}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:41:29 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:41:29 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:41:29 [model.py:1745] Using max model len 131072
INFO 12-05 15:41:29 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {'EngineCore_DP0': 1}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:41:50 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:41:50 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:41:50 [model.py:1745] Using max model len 131072
INFO 12-05 15:41:50 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {'EngineCore_DP0': 1}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:42:11 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:42:11 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:42:11 [model.py:1745] Using max model len 131072
INFO 12-05 15:42:11 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {'EngineCore_DP0': 1}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:42:32 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:42:33 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:42:33 [model.py:1745] Using max model len 131072
INFO 12-05 15:42:33 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {'EngineCore_DP0': 1}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:42:53 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:42:53 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:42:53 [model.py:1745] Using max model len 131072
INFO 12-05 15:42:53 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {'EngineCore_DP0': 1}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:43:14 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:43:14 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:43:14 [model.py:1745] Using max model len 131072
INFO 12-05 15:43:14 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {'EngineCore_DP0': 1}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:43:35 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:43:35 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:43:35 [model.py:1745] Using max model len 131072
INFO 12-05 15:43:35 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {'EngineCore_DP0': 1}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:43:56 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:43:56 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:43:56 [model.py:1745] Using max model len 131072
INFO 12-05 15:43:56 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {'EngineCore_DP0': 1}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:44:16 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:44:17 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:44:17 [model.py:1745] Using max model len 131072
INFO 12-05 15:44:17 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {'EngineCore_DP0': 1}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:44:37 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:44:37 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:44:37 [model.py:1745] Using max model len 131072
INFO 12-05 15:44:37 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {'EngineCore_DP0': 1}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:44:58 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:44:58 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:44:58 [model.py:1745] Using max model len 131072
INFO 12-05 15:44:58 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {'EngineCore_DP0': 1}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:45:19 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:45:19 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:45:19 [model.py:1745] Using max model len 131072
INFO 12-05 15:45:19 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {'EngineCore_DP0': 1}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:45:40 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:45:40 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:45:40 [model.py:1745] Using max model len 131072
INFO 12-05 15:45:40 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {'EngineCore_DP0': 1}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:46:00 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:46:00 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:46:00 [model.py:1745] Using max model len 131072
INFO 12-05 15:46:00 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {'EngineCore_DP0': 1}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:46:21 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:46:21 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:46:21 [model.py:1745] Using max model len 131072
INFO 12-05 15:46:21 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {'EngineCore_DP0': 1}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:46:42 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:46:42 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:46:42 [model.py:1745] Using max model len 131072
INFO 12-05 15:46:42 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {'EngineCore_DP0': 1}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:47:03 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:47:03 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:47:03 [model.py:1745] Using max model len 131072
INFO 12-05 15:47:03 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {'EngineCore_DP0': 1}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:47:24 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:47:24 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:47:24 [model.py:1745] Using max model len 131072
INFO 12-05 15:47:24 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {'EngineCore_DP0': 1}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:47:44 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:47:44 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:47:44 [model.py:1745] Using max model len 131072
INFO 12-05 15:47:44 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:48:05 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:48:05 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:48:05 [model.py:1745] Using max model len 131072
INFO 12-05 15:48:05 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {'EngineCore_DP0': 1}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:48:26 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:48:26 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:48:26 [model.py:1745] Using max model len 131072
INFO 12-05 15:48:26 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {'EngineCore_DP0': 1}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:48:47 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:48:47 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:48:47 [model.py:1745] Using max model len 131072
INFO 12-05 15:48:47 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {'EngineCore_DP0': 1}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:49:07 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:49:08 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:49:08 [model.py:1745] Using max model len 131072
INFO 12-05 15:49:08 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {'EngineCore_DP0': 1}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:49:28 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:49:28 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:49:28 [model.py:1745] Using max model len 131072
INFO 12-05 15:49:28 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {'EngineCore_DP0': 1}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:49:49 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:49:49 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:49:49 [model.py:1745] Using max model len 131072
INFO 12-05 15:49:49 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {'EngineCore_DP0': 1}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:50:10 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:50:10 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:50:10 [model.py:1745] Using max model len 131072
INFO 12-05 15:50:10 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {'EngineCore_DP0': 1}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:50:31 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:50:31 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:50:31 [model.py:1745] Using max model len 131072
INFO 12-05 15:50:31 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {'EngineCore_DP0': 1}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:50:51 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:50:51 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:50:51 [model.py:1745] Using max model len 131072
INFO 12-05 15:50:51 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {'EngineCore_DP0': 1}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:51:12 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:51:12 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:51:12 [model.py:1745] Using max model len 131072
INFO 12-05 15:51:12 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {'EngineCore_DP0': 1}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:51:33 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:51:33 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:51:33 [model.py:1745] Using max model len 131072
INFO 12-05 15:51:33 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {'EngineCore_DP0': 1}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:51:54 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:51:54 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:51:54 [model.py:1745] Using max model len 131072
INFO 12-05 15:51:54 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {'EngineCore_DP0': 1}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:52:15 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:52:15 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:52:15 [model.py:1745] Using max model len 131072
INFO 12-05 15:52:15 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {'EngineCore_DP0': 1}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:52:35 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:52:36 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:52:36 [model.py:1745] Using max model len 131072
INFO 12-05 15:52:36 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {'EngineCore_DP0': 1}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:52:56 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:52:56 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:52:56 [model.py:1745] Using max model len 131072
INFO 12-05 15:52:56 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {'EngineCore_DP0': 1}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:53:17 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:53:17 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:53:17 [model.py:1745] Using max model len 131072
INFO 12-05 15:53:17 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {'EngineCore_DP0': 1}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:53:38 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:53:38 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:53:38 [model.py:1745] Using max model len 131072
INFO 12-05 15:53:38 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {'EngineCore_DP0': 1}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:53:59 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:53:59 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:53:59 [model.py:1745] Using max model len 131072
INFO 12-05 15:53:59 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {'EngineCore_DP0': 1}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:54:19 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:54:20 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:54:20 [model.py:1745] Using max model len 131072
INFO 12-05 15:54:20 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {'EngineCore_DP0': 1}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:54:40 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:54:40 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:54:40 [model.py:1745] Using max model len 131072
INFO 12-05 15:54:40 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {'EngineCore_DP0': 1}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:55:01 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:55:01 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:55:01 [model.py:1745] Using max model len 131072
INFO 12-05 15:55:01 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {'EngineCore_DP0': 1}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:55:22 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:55:22 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:55:22 [model.py:1745] Using max model len 131072
INFO 12-05 15:55:22 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {'EngineCore_DP0': 1}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:55:43 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:55:43 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:55:43 [model.py:1745] Using max model len 131072
INFO 12-05 15:55:43 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {'EngineCore_DP0': 1}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:56:03 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:56:04 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:56:04 [model.py:1745] Using max model len 131072
INFO 12-05 15:56:04 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {'EngineCore_DP0': 1}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:56:24 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:56:24 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:56:24 [model.py:1745] Using max model len 131072
INFO 12-05 15:56:24 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {'EngineCore_DP0': 1}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:56:45 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:56:45 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:56:45 [model.py:1745] Using max model len 131072
INFO 12-05 15:56:45 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {'EngineCore_DP0': 1}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:57:06 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:57:06 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:57:06 [model.py:1745] Using max model len 131072
INFO 12-05 15:57:06 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {'EngineCore_DP0': 1}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:57:27 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:57:27 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:57:27 [model.py:1745] Using max model len 131072
INFO 12-05 15:57:27 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {'EngineCore_DP0': 1}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:57:48 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:57:48 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:57:48 [model.py:1745] Using max model len 131072
INFO 12-05 15:57:48 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {'EngineCore_DP0': 1}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:58:08 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:58:08 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:58:08 [model.py:1745] Using max model len 131072
INFO 12-05 15:58:08 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {'EngineCore_DP0': 1}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:58:29 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:58:29 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:58:29 [model.py:1745] Using max model len 131072
INFO 12-05 15:58:29 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {'EngineCore_DP0': 1}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:58:50 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:58:50 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:58:50 [model.py:1745] Using max model len 131072
INFO 12-05 15:58:50 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {'EngineCore_DP0': 1}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:59:11 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:59:11 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:59:11 [model.py:1745] Using max model len 131072
INFO 12-05 15:59:11 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {'EngineCore_DP0': 1}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:59:32 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:59:32 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:59:32 [model.py:1745] Using max model len 131072
INFO 12-05 15:59:32 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {'EngineCore_DP0': 1}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 15:59:52 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 15:59:53 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 15:59:53 [model.py:1745] Using max model len 131072
INFO 12-05 15:59:53 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {'EngineCore_DP0': 1}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 16:00:13 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 16:00:13 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 16:00:13 [model.py:1745] Using max model len 131072
INFO 12-05 16:00:13 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {'EngineCore_DP0': 1}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 16:00:34 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 16:00:34 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 16:00:34 [model.py:1745] Using max model len 131072
INFO 12-05 16:00:34 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {'EngineCore_DP0': 1}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 16:00:55 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 16:00:55 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 16:00:55 [model.py:1745] Using max model len 131072
INFO 12-05 16:00:55 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {'EngineCore_DP0': 1}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 16:01:16 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 16:01:16 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 16:01:16 [model.py:1745] Using max model len 131072
INFO 12-05 16:01:16 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {'EngineCore_DP0': 1}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 16:01:36 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 16:01:36 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 16:01:36 [model.py:1745] Using max model len 131072
INFO 12-05 16:01:36 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {'EngineCore_DP0': 1}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 16:01:57 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 16:01:57 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 16:01:57 [model.py:1745] Using max model len 131072
INFO 12-05 16:01:57 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {'EngineCore_DP0': 1}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 16:02:18 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 16:02:18 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 16:02:18 [model.py:1745] Using max model len 131072
INFO 12-05 16:02:18 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {'EngineCore_DP0': 1}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 16:02:39 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 16:02:39 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 16:02:39 [model.py:1745] Using max model len 131072
INFO 12-05 16:02:39 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {'EngineCore_DP0': 1}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 16:03:00 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 16:03:00 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 16:03:00 [model.py:1745] Using max model len 131072
INFO 12-05 16:03:00 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {'EngineCore_DP0': 1}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 16:03:20 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 16:03:20 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 16:03:20 [model.py:1745] Using max model len 131072
INFO 12-05 16:03:20 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {'EngineCore_DP0': 1}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 16:03:41 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 16:03:41 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 16:03:41 [model.py:1745] Using max model len 131072
INFO 12-05 16:03:41 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {'EngineCore_DP0': 1}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 16:04:02 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 16:04:02 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 16:04:02 [model.py:1745] Using max model len 131072
INFO 12-05 16:04:02 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {'EngineCore_DP0': 1}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 16:04:23 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 16:04:23 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 16:04:23 [model.py:1745] Using max model len 131072
INFO 12-05 16:04:23 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {'EngineCore_DP0': 1}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 16:04:43 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 16:04:44 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 16:04:44 [model.py:1745] Using max model len 131072
INFO 12-05 16:04:44 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {'EngineCore_DP0': 1}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 16:05:04 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 16:05:04 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 16:05:04 [model.py:1745] Using max model len 131072
INFO 12-05 16:05:04 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {'EngineCore_DP0': 1}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 16:05:25 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 16:05:25 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 16:05:25 [model.py:1745] Using max model len 131072
INFO 12-05 16:05:25 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {'EngineCore_DP0': 1}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 16:05:46 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 16:05:46 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 16:05:46 [model.py:1745] Using max model len 131072
INFO 12-05 16:05:46 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {'EngineCore_DP0': 1}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 16:06:07 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 16:06:07 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 16:06:07 [model.py:1745] Using max model len 131072
INFO 12-05 16:06:07 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {'EngineCore_DP0': 1}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 16:06:28 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 16:06:28 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 16:06:28 [model.py:1745] Using max model len 131072
INFO 12-05 16:06:28 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {'EngineCore_DP0': 1}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 16:06:48 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 16:06:49 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 16:06:49 [model.py:1745] Using max model len 131072
INFO 12-05 16:06:49 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {'EngineCore_DP0': 1}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 16:07:09 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 16:07:09 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 16:07:09 [model.py:1745] Using max model len 131072
INFO 12-05 16:07:09 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4065440)[0;0m INFO 12-05 16:07:19 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4065440)[0;0m INFO 12-05 16:07:20 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:44963 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4065440)[0;0m INFO 12-05 16:07:20 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4065440)[0;0m INFO 12-05 16:07:21 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4065440)[0;0m INFO 12-05 16:07:21 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4065440)[0;0m INFO 12-05 16:07:21 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4065440)[0;0m INFO 12-05 16:07:22 [default_loader.py:314] Loading weights took 0.88 seconds
[1;36m(EngineCore_DP0 pid=4065440)[0;0m INFO 12-05 16:07:23 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.523843 seconds
[1;36m(EngineCore_DP0 pid=4065440)[0;0m INFO 12-05 16:07:27 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4065440)[0;0m INFO 12-05 16:07:27 [backends.py:647] Dynamo bytecode transform time: 4.24 s
[1;36m(EngineCore_DP0 pid=4065440)[0;0m ERROR 12-05 16:07:27 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4065440)[0;0m ERROR 12-05 16:07:27 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4065440)[0;0m ERROR 12-05 16:07:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4065440)[0;0m ERROR 12-05 16:07:27 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4065440)[0;0m ERROR 12-05 16:07:27 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065440)[0;0m ERROR 12-05 16:07:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4065440)[0;0m ERROR 12-05 16:07:27 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4065440)[0;0m ERROR 12-05 16:07:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4065440)[0;0m ERROR 12-05 16:07:27 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4065440)[0;0m ERROR 12-05 16:07:27 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065440)[0;0m ERROR 12-05 16:07:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4065440)[0;0m ERROR 12-05 16:07:27 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4065440)[0;0m ERROR 12-05 16:07:27 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065440)[0;0m ERROR 12-05 16:07:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4065440)[0;0m ERROR 12-05 16:07:27 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4065440)[0;0m ERROR 12-05 16:07:27 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065440)[0;0m ERROR 12-05 16:07:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4065440)[0;0m ERROR 12-05 16:07:27 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4065440)[0;0m ERROR 12-05 16:07:27 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065440)[0;0m ERROR 12-05 16:07:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4065440)[0;0m ERROR 12-05 16:07:27 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4065440)[0;0m ERROR 12-05 16:07:27 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065440)[0;0m ERROR 12-05 16:07:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4065440)[0;0m ERROR 12-05 16:07:27 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4065440)[0;0m ERROR 12-05 16:07:27 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065440)[0;0m ERROR 12-05 16:07:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4065440)[0;0m ERROR 12-05 16:07:27 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4065440)[0;0m ERROR 12-05 16:07:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4065440)[0;0m ERROR 12-05 16:07:27 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4065440)[0;0m ERROR 12-05 16:07:27 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065440)[0;0m ERROR 12-05 16:07:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4065440)[0;0m ERROR 12-05 16:07:27 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4065440)[0;0m ERROR 12-05 16:07:27 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065440)[0;0m ERROR 12-05 16:07:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4065440)[0;0m ERROR 12-05 16:07:27 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4065440)[0;0m ERROR 12-05 16:07:27 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065440)[0;0m ERROR 12-05 16:07:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4065440)[0;0m ERROR 12-05 16:07:27 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4065440)[0;0m ERROR 12-05 16:07:27 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065440)[0;0m ERROR 12-05 16:07:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4065440)[0;0m ERROR 12-05 16:07:27 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4065440)[0;0m ERROR 12-05 16:07:27 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065440)[0;0m ERROR 12-05 16:07:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4065440)[0;0m ERROR 12-05 16:07:27 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4065440)[0;0m ERROR 12-05 16:07:27 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065440)[0;0m ERROR 12-05 16:07:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4065440)[0;0m ERROR 12-05 16:07:27 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4065440)[0;0m ERROR 12-05 16:07:27 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065440)[0;0m ERROR 12-05 16:07:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4065440)[0;0m ERROR 12-05 16:07:27 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4065440)[0;0m ERROR 12-05 16:07:27 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065440)[0;0m ERROR 12-05 16:07:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4065440)[0;0m ERROR 12-05 16:07:27 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4065440)[0;0m ERROR 12-05 16:07:27 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065440)[0;0m ERROR 12-05 16:07:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4065440)[0;0m ERROR 12-05 16:07:27 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4065440)[0;0m ERROR 12-05 16:07:27 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065440)[0;0m ERROR 12-05 16:07:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4065440)[0;0m ERROR 12-05 16:07:27 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4065440)[0;0m ERROR 12-05 16:07:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4065440)[0;0m ERROR 12-05 16:07:27 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4065440)[0;0m ERROR 12-05 16:07:27 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065440)[0;0m ERROR 12-05 16:07:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4065440)[0;0m ERROR 12-05 16:07:27 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4065440)[0;0m ERROR 12-05 16:07:27 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065440)[0;0m ERROR 12-05 16:07:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4065440)[0;0m ERROR 12-05 16:07:27 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4065440)[0;0m ERROR 12-05 16:07:27 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065440)[0;0m ERROR 12-05 16:07:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4065440)[0;0m ERROR 12-05 16:07:27 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4065440)[0;0m ERROR 12-05 16:07:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4065440)[0;0m ERROR 12-05 16:07:27 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4065440)[0;0m ERROR 12-05 16:07:27 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065440)[0;0m ERROR 12-05 16:07:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4065440)[0;0m ERROR 12-05 16:07:27 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4065440)[0;0m ERROR 12-05 16:07:27 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065440)[0;0m ERROR 12-05 16:07:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4065440)[0;0m ERROR 12-05 16:07:27 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4065440)[0;0m ERROR 12-05 16:07:27 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065440)[0;0m ERROR 12-05 16:07:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4065440)[0;0m ERROR 12-05 16:07:27 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4065440)[0;0m ERROR 12-05 16:07:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4065440)[0;0m ERROR 12-05 16:07:27 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4065440)[0;0m ERROR 12-05 16:07:27 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065440)[0;0m ERROR 12-05 16:07:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4065440)[0;0m ERROR 12-05 16:07:27 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4065440)[0;0m ERROR 12-05 16:07:27 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065440)[0;0m ERROR 12-05 16:07:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4065440)[0;0m ERROR 12-05 16:07:27 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4065440)[0;0m ERROR 12-05 16:07:27 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065440)[0;0m ERROR 12-05 16:07:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4065440)[0;0m ERROR 12-05 16:07:27 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4065440)[0;0m ERROR 12-05 16:07:27 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065440)[0;0m ERROR 12-05 16:07:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4065440)[0;0m ERROR 12-05 16:07:27 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4065440)[0;0m ERROR 12-05 16:07:27 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7fa170166720>' raised:
[1;36m(EngineCore_DP0 pid=4065440)[0;0m ERROR 12-05 16:07:27 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4065440)[0;0m ERROR 12-05 16:07:27 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4065440)[0;0m ERROR 12-05 16:07:27 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4065440)[0;0m ERROR 12-05 16:07:27 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 16:07:49 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 16:07:49 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 16:07:49 [model.py:1745] Using max model len 131072
INFO 12-05 16:07:49 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4065529)[0;0m INFO 12-05 16:07:56 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4065529)[0;0m INFO 12-05 16:07:57 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:33575 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4065529)[0;0m INFO 12-05 16:07:57 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4065529)[0;0m INFO 12-05 16:07:57 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4065529)[0;0m INFO 12-05 16:07:58 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4065529)[0;0m INFO 12-05 16:07:58 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4065529)[0;0m INFO 12-05 16:07:59 [default_loader.py:314] Loading weights took 0.87 seconds
[1;36m(EngineCore_DP0 pid=4065529)[0;0m INFO 12-05 16:07:59 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.402845 seconds
[1;36m(EngineCore_DP0 pid=4065529)[0;0m INFO 12-05 16:08:03 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4065529)[0;0m INFO 12-05 16:08:03 [backends.py:647] Dynamo bytecode transform time: 3.69 s
[1;36m(EngineCore_DP0 pid=4065529)[0;0m ERROR 12-05 16:08:04 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4065529)[0;0m ERROR 12-05 16:08:04 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4065529)[0;0m ERROR 12-05 16:08:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4065529)[0;0m ERROR 12-05 16:08:04 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4065529)[0;0m ERROR 12-05 16:08:04 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065529)[0;0m ERROR 12-05 16:08:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4065529)[0;0m ERROR 12-05 16:08:04 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4065529)[0;0m ERROR 12-05 16:08:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4065529)[0;0m ERROR 12-05 16:08:04 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4065529)[0;0m ERROR 12-05 16:08:04 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065529)[0;0m ERROR 12-05 16:08:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4065529)[0;0m ERROR 12-05 16:08:04 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4065529)[0;0m ERROR 12-05 16:08:04 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065529)[0;0m ERROR 12-05 16:08:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4065529)[0;0m ERROR 12-05 16:08:04 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4065529)[0;0m ERROR 12-05 16:08:04 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065529)[0;0m ERROR 12-05 16:08:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4065529)[0;0m ERROR 12-05 16:08:04 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4065529)[0;0m ERROR 12-05 16:08:04 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065529)[0;0m ERROR 12-05 16:08:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4065529)[0;0m ERROR 12-05 16:08:04 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4065529)[0;0m ERROR 12-05 16:08:04 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065529)[0;0m ERROR 12-05 16:08:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4065529)[0;0m ERROR 12-05 16:08:04 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4065529)[0;0m ERROR 12-05 16:08:04 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065529)[0;0m ERROR 12-05 16:08:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4065529)[0;0m ERROR 12-05 16:08:04 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4065529)[0;0m ERROR 12-05 16:08:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4065529)[0;0m ERROR 12-05 16:08:04 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4065529)[0;0m ERROR 12-05 16:08:04 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065529)[0;0m ERROR 12-05 16:08:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4065529)[0;0m ERROR 12-05 16:08:04 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4065529)[0;0m ERROR 12-05 16:08:04 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065529)[0;0m ERROR 12-05 16:08:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4065529)[0;0m ERROR 12-05 16:08:04 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4065529)[0;0m ERROR 12-05 16:08:04 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065529)[0;0m ERROR 12-05 16:08:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4065529)[0;0m ERROR 12-05 16:08:04 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4065529)[0;0m ERROR 12-05 16:08:04 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065529)[0;0m ERROR 12-05 16:08:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4065529)[0;0m ERROR 12-05 16:08:04 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4065529)[0;0m ERROR 12-05 16:08:04 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065529)[0;0m ERROR 12-05 16:08:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4065529)[0;0m ERROR 12-05 16:08:04 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4065529)[0;0m ERROR 12-05 16:08:04 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065529)[0;0m ERROR 12-05 16:08:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4065529)[0;0m ERROR 12-05 16:08:04 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4065529)[0;0m ERROR 12-05 16:08:04 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065529)[0;0m ERROR 12-05 16:08:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4065529)[0;0m ERROR 12-05 16:08:04 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4065529)[0;0m ERROR 12-05 16:08:04 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065529)[0;0m ERROR 12-05 16:08:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4065529)[0;0m ERROR 12-05 16:08:04 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4065529)[0;0m ERROR 12-05 16:08:04 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065529)[0;0m ERROR 12-05 16:08:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4065529)[0;0m ERROR 12-05 16:08:04 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4065529)[0;0m ERROR 12-05 16:08:04 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065529)[0;0m ERROR 12-05 16:08:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4065529)[0;0m ERROR 12-05 16:08:04 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4065529)[0;0m ERROR 12-05 16:08:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4065529)[0;0m ERROR 12-05 16:08:04 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4065529)[0;0m ERROR 12-05 16:08:04 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065529)[0;0m ERROR 12-05 16:08:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4065529)[0;0m ERROR 12-05 16:08:04 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4065529)[0;0m ERROR 12-05 16:08:04 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065529)[0;0m ERROR 12-05 16:08:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4065529)[0;0m ERROR 12-05 16:08:04 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4065529)[0;0m ERROR 12-05 16:08:04 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065529)[0;0m ERROR 12-05 16:08:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4065529)[0;0m ERROR 12-05 16:08:04 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4065529)[0;0m ERROR 12-05 16:08:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4065529)[0;0m ERROR 12-05 16:08:04 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4065529)[0;0m ERROR 12-05 16:08:04 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065529)[0;0m ERROR 12-05 16:08:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4065529)[0;0m ERROR 12-05 16:08:04 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4065529)[0;0m ERROR 12-05 16:08:04 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065529)[0;0m ERROR 12-05 16:08:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4065529)[0;0m ERROR 12-05 16:08:04 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4065529)[0;0m ERROR 12-05 16:08:04 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065529)[0;0m ERROR 12-05 16:08:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4065529)[0;0m ERROR 12-05 16:08:04 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4065529)[0;0m ERROR 12-05 16:08:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4065529)[0;0m ERROR 12-05 16:08:04 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4065529)[0;0m ERROR 12-05 16:08:04 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065529)[0;0m ERROR 12-05 16:08:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4065529)[0;0m ERROR 12-05 16:08:04 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4065529)[0;0m ERROR 12-05 16:08:04 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065529)[0;0m ERROR 12-05 16:08:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4065529)[0;0m ERROR 12-05 16:08:04 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4065529)[0;0m ERROR 12-05 16:08:04 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065529)[0;0m ERROR 12-05 16:08:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4065529)[0;0m ERROR 12-05 16:08:04 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4065529)[0;0m ERROR 12-05 16:08:04 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065529)[0;0m ERROR 12-05 16:08:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4065529)[0;0m ERROR 12-05 16:08:04 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4065529)[0;0m ERROR 12-05 16:08:04 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7f98d01b92b0>' raised:
[1;36m(EngineCore_DP0 pid=4065529)[0;0m ERROR 12-05 16:08:04 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4065529)[0;0m ERROR 12-05 16:08:04 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4065529)[0;0m ERROR 12-05 16:08:04 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4065529)[0;0m ERROR 12-05 16:08:04 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 16:08:25 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 16:08:25 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 16:08:25 [model.py:1745] Using max model len 131072
INFO 12-05 16:08:25 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4065605)[0;0m INFO 12-05 16:08:33 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4065605)[0;0m INFO 12-05 16:08:34 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:36497 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4065605)[0;0m INFO 12-05 16:08:34 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4065605)[0;0m INFO 12-05 16:08:34 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4065605)[0;0m INFO 12-05 16:08:35 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4065605)[0;0m INFO 12-05 16:08:35 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4065605)[0;0m INFO 12-05 16:08:36 [default_loader.py:314] Loading weights took 0.87 seconds
[1;36m(EngineCore_DP0 pid=4065605)[0;0m INFO 12-05 16:08:36 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.417947 seconds
[1;36m(EngineCore_DP0 pid=4065605)[0;0m INFO 12-05 16:08:40 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4065605)[0;0m INFO 12-05 16:08:40 [backends.py:647] Dynamo bytecode transform time: 3.66 s
[1;36m(EngineCore_DP0 pid=4065605)[0;0m ERROR 12-05 16:08:40 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4065605)[0;0m ERROR 12-05 16:08:40 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4065605)[0;0m ERROR 12-05 16:08:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4065605)[0;0m ERROR 12-05 16:08:40 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4065605)[0;0m ERROR 12-05 16:08:40 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065605)[0;0m ERROR 12-05 16:08:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4065605)[0;0m ERROR 12-05 16:08:40 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4065605)[0;0m ERROR 12-05 16:08:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4065605)[0;0m ERROR 12-05 16:08:40 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4065605)[0;0m ERROR 12-05 16:08:40 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065605)[0;0m ERROR 12-05 16:08:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4065605)[0;0m ERROR 12-05 16:08:40 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4065605)[0;0m ERROR 12-05 16:08:40 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065605)[0;0m ERROR 12-05 16:08:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4065605)[0;0m ERROR 12-05 16:08:40 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4065605)[0;0m ERROR 12-05 16:08:40 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065605)[0;0m ERROR 12-05 16:08:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4065605)[0;0m ERROR 12-05 16:08:40 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4065605)[0;0m ERROR 12-05 16:08:40 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065605)[0;0m ERROR 12-05 16:08:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4065605)[0;0m ERROR 12-05 16:08:40 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4065605)[0;0m ERROR 12-05 16:08:40 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065605)[0;0m ERROR 12-05 16:08:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4065605)[0;0m ERROR 12-05 16:08:40 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4065605)[0;0m ERROR 12-05 16:08:40 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065605)[0;0m ERROR 12-05 16:08:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4065605)[0;0m ERROR 12-05 16:08:40 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4065605)[0;0m ERROR 12-05 16:08:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4065605)[0;0m ERROR 12-05 16:08:40 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4065605)[0;0m ERROR 12-05 16:08:40 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065605)[0;0m ERROR 12-05 16:08:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4065605)[0;0m ERROR 12-05 16:08:40 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4065605)[0;0m ERROR 12-05 16:08:40 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065605)[0;0m ERROR 12-05 16:08:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4065605)[0;0m ERROR 12-05 16:08:40 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4065605)[0;0m ERROR 12-05 16:08:40 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065605)[0;0m ERROR 12-05 16:08:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4065605)[0;0m ERROR 12-05 16:08:40 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4065605)[0;0m ERROR 12-05 16:08:40 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065605)[0;0m ERROR 12-05 16:08:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4065605)[0;0m ERROR 12-05 16:08:40 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4065605)[0;0m ERROR 12-05 16:08:40 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065605)[0;0m ERROR 12-05 16:08:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4065605)[0;0m ERROR 12-05 16:08:40 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4065605)[0;0m ERROR 12-05 16:08:40 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065605)[0;0m ERROR 12-05 16:08:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4065605)[0;0m ERROR 12-05 16:08:40 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4065605)[0;0m ERROR 12-05 16:08:40 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065605)[0;0m ERROR 12-05 16:08:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4065605)[0;0m ERROR 12-05 16:08:40 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4065605)[0;0m ERROR 12-05 16:08:40 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065605)[0;0m ERROR 12-05 16:08:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4065605)[0;0m ERROR 12-05 16:08:40 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4065605)[0;0m ERROR 12-05 16:08:40 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065605)[0;0m ERROR 12-05 16:08:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4065605)[0;0m ERROR 12-05 16:08:40 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4065605)[0;0m ERROR 12-05 16:08:40 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065605)[0;0m ERROR 12-05 16:08:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4065605)[0;0m ERROR 12-05 16:08:40 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4065605)[0;0m ERROR 12-05 16:08:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4065605)[0;0m ERROR 12-05 16:08:40 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4065605)[0;0m ERROR 12-05 16:08:40 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065605)[0;0m ERROR 12-05 16:08:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4065605)[0;0m ERROR 12-05 16:08:40 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4065605)[0;0m ERROR 12-05 16:08:40 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065605)[0;0m ERROR 12-05 16:08:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4065605)[0;0m ERROR 12-05 16:08:40 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4065605)[0;0m ERROR 12-05 16:08:40 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065605)[0;0m ERROR 12-05 16:08:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4065605)[0;0m ERROR 12-05 16:08:40 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4065605)[0;0m ERROR 12-05 16:08:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4065605)[0;0m ERROR 12-05 16:08:40 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4065605)[0;0m ERROR 12-05 16:08:40 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065605)[0;0m ERROR 12-05 16:08:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4065605)[0;0m ERROR 12-05 16:08:40 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4065605)[0;0m ERROR 12-05 16:08:40 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065605)[0;0m ERROR 12-05 16:08:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4065605)[0;0m ERROR 12-05 16:08:40 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4065605)[0;0m ERROR 12-05 16:08:40 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065605)[0;0m ERROR 12-05 16:08:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4065605)[0;0m ERROR 12-05 16:08:40 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4065605)[0;0m ERROR 12-05 16:08:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4065605)[0;0m ERROR 12-05 16:08:40 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4065605)[0;0m ERROR 12-05 16:08:40 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065605)[0;0m ERROR 12-05 16:08:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4065605)[0;0m ERROR 12-05 16:08:40 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4065605)[0;0m ERROR 12-05 16:08:40 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065605)[0;0m ERROR 12-05 16:08:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4065605)[0;0m ERROR 12-05 16:08:40 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4065605)[0;0m ERROR 12-05 16:08:40 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065605)[0;0m ERROR 12-05 16:08:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4065605)[0;0m ERROR 12-05 16:08:40 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4065605)[0;0m ERROR 12-05 16:08:40 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065605)[0;0m ERROR 12-05 16:08:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4065605)[0;0m ERROR 12-05 16:08:40 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4065605)[0;0m ERROR 12-05 16:08:40 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7f1804327ef0>' raised:
[1;36m(EngineCore_DP0 pid=4065605)[0;0m ERROR 12-05 16:08:40 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4065605)[0;0m ERROR 12-05 16:08:40 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4065605)[0;0m ERROR 12-05 16:08:40 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4065605)[0;0m ERROR 12-05 16:08:40 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 16:09:02 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 16:09:02 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 16:09:02 [model.py:1745] Using max model len 131072
INFO 12-05 16:09:02 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4065696)[0;0m INFO 12-05 16:09:09 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4065696)[0;0m INFO 12-05 16:09:10 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:38917 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4065696)[0;0m INFO 12-05 16:09:10 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4065696)[0;0m INFO 12-05 16:09:10 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4065696)[0;0m INFO 12-05 16:09:11 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4065696)[0;0m INFO 12-05 16:09:11 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4065696)[0;0m INFO 12-05 16:09:12 [default_loader.py:314] Loading weights took 0.87 seconds
[1;36m(EngineCore_DP0 pid=4065696)[0;0m INFO 12-05 16:09:12 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.372577 seconds
[1;36m(EngineCore_DP0 pid=4065696)[0;0m INFO 12-05 16:09:16 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4065696)[0;0m INFO 12-05 16:09:16 [backends.py:647] Dynamo bytecode transform time: 3.66 s
[1;36m(EngineCore_DP0 pid=4065696)[0;0m ERROR 12-05 16:09:16 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4065696)[0;0m ERROR 12-05 16:09:16 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4065696)[0;0m ERROR 12-05 16:09:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4065696)[0;0m ERROR 12-05 16:09:16 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4065696)[0;0m ERROR 12-05 16:09:16 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065696)[0;0m ERROR 12-05 16:09:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4065696)[0;0m ERROR 12-05 16:09:16 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4065696)[0;0m ERROR 12-05 16:09:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4065696)[0;0m ERROR 12-05 16:09:16 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4065696)[0;0m ERROR 12-05 16:09:16 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065696)[0;0m ERROR 12-05 16:09:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4065696)[0;0m ERROR 12-05 16:09:16 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4065696)[0;0m ERROR 12-05 16:09:16 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065696)[0;0m ERROR 12-05 16:09:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4065696)[0;0m ERROR 12-05 16:09:16 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4065696)[0;0m ERROR 12-05 16:09:16 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065696)[0;0m ERROR 12-05 16:09:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4065696)[0;0m ERROR 12-05 16:09:16 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4065696)[0;0m ERROR 12-05 16:09:16 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065696)[0;0m ERROR 12-05 16:09:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4065696)[0;0m ERROR 12-05 16:09:16 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4065696)[0;0m ERROR 12-05 16:09:16 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065696)[0;0m ERROR 12-05 16:09:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4065696)[0;0m ERROR 12-05 16:09:16 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4065696)[0;0m ERROR 12-05 16:09:16 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065696)[0;0m ERROR 12-05 16:09:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4065696)[0;0m ERROR 12-05 16:09:16 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4065696)[0;0m ERROR 12-05 16:09:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4065696)[0;0m ERROR 12-05 16:09:16 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4065696)[0;0m ERROR 12-05 16:09:16 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065696)[0;0m ERROR 12-05 16:09:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4065696)[0;0m ERROR 12-05 16:09:16 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4065696)[0;0m ERROR 12-05 16:09:16 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065696)[0;0m ERROR 12-05 16:09:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4065696)[0;0m ERROR 12-05 16:09:16 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4065696)[0;0m ERROR 12-05 16:09:16 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065696)[0;0m ERROR 12-05 16:09:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4065696)[0;0m ERROR 12-05 16:09:16 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4065696)[0;0m ERROR 12-05 16:09:16 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065696)[0;0m ERROR 12-05 16:09:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4065696)[0;0m ERROR 12-05 16:09:16 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4065696)[0;0m ERROR 12-05 16:09:16 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065696)[0;0m ERROR 12-05 16:09:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4065696)[0;0m ERROR 12-05 16:09:16 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4065696)[0;0m ERROR 12-05 16:09:16 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065696)[0;0m ERROR 12-05 16:09:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4065696)[0;0m ERROR 12-05 16:09:16 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4065696)[0;0m ERROR 12-05 16:09:16 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065696)[0;0m ERROR 12-05 16:09:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4065696)[0;0m ERROR 12-05 16:09:16 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4065696)[0;0m ERROR 12-05 16:09:16 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065696)[0;0m ERROR 12-05 16:09:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4065696)[0;0m ERROR 12-05 16:09:16 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4065696)[0;0m ERROR 12-05 16:09:16 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065696)[0;0m ERROR 12-05 16:09:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4065696)[0;0m ERROR 12-05 16:09:16 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4065696)[0;0m ERROR 12-05 16:09:16 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065696)[0;0m ERROR 12-05 16:09:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4065696)[0;0m ERROR 12-05 16:09:16 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4065696)[0;0m ERROR 12-05 16:09:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4065696)[0;0m ERROR 12-05 16:09:16 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4065696)[0;0m ERROR 12-05 16:09:16 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065696)[0;0m ERROR 12-05 16:09:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4065696)[0;0m ERROR 12-05 16:09:16 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4065696)[0;0m ERROR 12-05 16:09:16 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065696)[0;0m ERROR 12-05 16:09:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4065696)[0;0m ERROR 12-05 16:09:16 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4065696)[0;0m ERROR 12-05 16:09:16 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065696)[0;0m ERROR 12-05 16:09:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4065696)[0;0m ERROR 12-05 16:09:16 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4065696)[0;0m ERROR 12-05 16:09:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4065696)[0;0m ERROR 12-05 16:09:16 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4065696)[0;0m ERROR 12-05 16:09:16 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065696)[0;0m ERROR 12-05 16:09:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4065696)[0;0m ERROR 12-05 16:09:16 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4065696)[0;0m ERROR 12-05 16:09:16 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065696)[0;0m ERROR 12-05 16:09:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4065696)[0;0m ERROR 12-05 16:09:16 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4065696)[0;0m ERROR 12-05 16:09:16 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065696)[0;0m ERROR 12-05 16:09:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4065696)[0;0m ERROR 12-05 16:09:16 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4065696)[0;0m ERROR 12-05 16:09:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4065696)[0;0m ERROR 12-05 16:09:16 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4065696)[0;0m ERROR 12-05 16:09:16 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065696)[0;0m ERROR 12-05 16:09:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4065696)[0;0m ERROR 12-05 16:09:16 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4065696)[0;0m ERROR 12-05 16:09:16 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065696)[0;0m ERROR 12-05 16:09:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4065696)[0;0m ERROR 12-05 16:09:16 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4065696)[0;0m ERROR 12-05 16:09:16 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065696)[0;0m ERROR 12-05 16:09:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4065696)[0;0m ERROR 12-05 16:09:16 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4065696)[0;0m ERROR 12-05 16:09:16 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065696)[0;0m ERROR 12-05 16:09:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4065696)[0;0m ERROR 12-05 16:09:16 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4065696)[0;0m ERROR 12-05 16:09:16 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7f4828228c20>' raised:
[1;36m(EngineCore_DP0 pid=4065696)[0;0m ERROR 12-05 16:09:16 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4065696)[0;0m ERROR 12-05 16:09:16 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4065696)[0;0m ERROR 12-05 16:09:16 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4065696)[0;0m ERROR 12-05 16:09:16 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 16:09:38 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 16:09:38 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 16:09:38 [model.py:1745] Using max model len 131072
INFO 12-05 16:09:38 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4065778)[0;0m INFO 12-05 16:09:46 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4065778)[0;0m INFO 12-05 16:09:47 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:51131 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4065778)[0;0m INFO 12-05 16:09:47 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4065778)[0;0m INFO 12-05 16:09:47 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4065778)[0;0m INFO 12-05 16:09:48 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4065778)[0;0m INFO 12-05 16:09:48 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4065778)[0;0m INFO 12-05 16:09:49 [default_loader.py:314] Loading weights took 0.88 seconds
[1;36m(EngineCore_DP0 pid=4065778)[0;0m INFO 12-05 16:09:49 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.445539 seconds
[1;36m(EngineCore_DP0 pid=4065778)[0;0m INFO 12-05 16:09:53 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4065778)[0;0m INFO 12-05 16:09:53 [backends.py:647] Dynamo bytecode transform time: 3.67 s
[1;36m(EngineCore_DP0 pid=4065778)[0;0m ERROR 12-05 16:09:53 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4065778)[0;0m ERROR 12-05 16:09:53 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4065778)[0;0m ERROR 12-05 16:09:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4065778)[0;0m ERROR 12-05 16:09:53 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4065778)[0;0m ERROR 12-05 16:09:53 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065778)[0;0m ERROR 12-05 16:09:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4065778)[0;0m ERROR 12-05 16:09:53 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4065778)[0;0m ERROR 12-05 16:09:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4065778)[0;0m ERROR 12-05 16:09:53 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4065778)[0;0m ERROR 12-05 16:09:53 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065778)[0;0m ERROR 12-05 16:09:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4065778)[0;0m ERROR 12-05 16:09:53 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4065778)[0;0m ERROR 12-05 16:09:53 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065778)[0;0m ERROR 12-05 16:09:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4065778)[0;0m ERROR 12-05 16:09:53 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4065778)[0;0m ERROR 12-05 16:09:53 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065778)[0;0m ERROR 12-05 16:09:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4065778)[0;0m ERROR 12-05 16:09:53 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4065778)[0;0m ERROR 12-05 16:09:53 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065778)[0;0m ERROR 12-05 16:09:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4065778)[0;0m ERROR 12-05 16:09:53 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4065778)[0;0m ERROR 12-05 16:09:53 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065778)[0;0m ERROR 12-05 16:09:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4065778)[0;0m ERROR 12-05 16:09:53 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4065778)[0;0m ERROR 12-05 16:09:53 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065778)[0;0m ERROR 12-05 16:09:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4065778)[0;0m ERROR 12-05 16:09:53 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4065778)[0;0m ERROR 12-05 16:09:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4065778)[0;0m ERROR 12-05 16:09:53 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4065778)[0;0m ERROR 12-05 16:09:53 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065778)[0;0m ERROR 12-05 16:09:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4065778)[0;0m ERROR 12-05 16:09:53 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4065778)[0;0m ERROR 12-05 16:09:53 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065778)[0;0m ERROR 12-05 16:09:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4065778)[0;0m ERROR 12-05 16:09:53 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4065778)[0;0m ERROR 12-05 16:09:53 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065778)[0;0m ERROR 12-05 16:09:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4065778)[0;0m ERROR 12-05 16:09:53 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4065778)[0;0m ERROR 12-05 16:09:53 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065778)[0;0m ERROR 12-05 16:09:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4065778)[0;0m ERROR 12-05 16:09:53 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4065778)[0;0m ERROR 12-05 16:09:53 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065778)[0;0m ERROR 12-05 16:09:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4065778)[0;0m ERROR 12-05 16:09:53 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4065778)[0;0m ERROR 12-05 16:09:53 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065778)[0;0m ERROR 12-05 16:09:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4065778)[0;0m ERROR 12-05 16:09:53 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4065778)[0;0m ERROR 12-05 16:09:53 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065778)[0;0m ERROR 12-05 16:09:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4065778)[0;0m ERROR 12-05 16:09:53 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4065778)[0;0m ERROR 12-05 16:09:53 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065778)[0;0m ERROR 12-05 16:09:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4065778)[0;0m ERROR 12-05 16:09:53 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4065778)[0;0m ERROR 12-05 16:09:53 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065778)[0;0m ERROR 12-05 16:09:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4065778)[0;0m ERROR 12-05 16:09:53 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4065778)[0;0m ERROR 12-05 16:09:53 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065778)[0;0m ERROR 12-05 16:09:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4065778)[0;0m ERROR 12-05 16:09:53 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4065778)[0;0m ERROR 12-05 16:09:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4065778)[0;0m ERROR 12-05 16:09:53 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4065778)[0;0m ERROR 12-05 16:09:53 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065778)[0;0m ERROR 12-05 16:09:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4065778)[0;0m ERROR 12-05 16:09:53 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4065778)[0;0m ERROR 12-05 16:09:53 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065778)[0;0m ERROR 12-05 16:09:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4065778)[0;0m ERROR 12-05 16:09:53 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4065778)[0;0m ERROR 12-05 16:09:53 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065778)[0;0m ERROR 12-05 16:09:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4065778)[0;0m ERROR 12-05 16:09:53 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4065778)[0;0m ERROR 12-05 16:09:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4065778)[0;0m ERROR 12-05 16:09:53 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4065778)[0;0m ERROR 12-05 16:09:53 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065778)[0;0m ERROR 12-05 16:09:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4065778)[0;0m ERROR 12-05 16:09:53 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4065778)[0;0m ERROR 12-05 16:09:53 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065778)[0;0m ERROR 12-05 16:09:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4065778)[0;0m ERROR 12-05 16:09:53 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4065778)[0;0m ERROR 12-05 16:09:53 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065778)[0;0m ERROR 12-05 16:09:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4065778)[0;0m ERROR 12-05 16:09:53 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4065778)[0;0m ERROR 12-05 16:09:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4065778)[0;0m ERROR 12-05 16:09:53 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4065778)[0;0m ERROR 12-05 16:09:53 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065778)[0;0m ERROR 12-05 16:09:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4065778)[0;0m ERROR 12-05 16:09:53 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4065778)[0;0m ERROR 12-05 16:09:53 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065778)[0;0m ERROR 12-05 16:09:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4065778)[0;0m ERROR 12-05 16:09:53 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4065778)[0;0m ERROR 12-05 16:09:53 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065778)[0;0m ERROR 12-05 16:09:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4065778)[0;0m ERROR 12-05 16:09:53 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4065778)[0;0m ERROR 12-05 16:09:53 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065778)[0;0m ERROR 12-05 16:09:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4065778)[0;0m ERROR 12-05 16:09:53 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4065778)[0;0m ERROR 12-05 16:09:53 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7fb3a839ff20>' raised:
[1;36m(EngineCore_DP0 pid=4065778)[0;0m ERROR 12-05 16:09:53 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4065778)[0;0m ERROR 12-05 16:09:53 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4065778)[0;0m ERROR 12-05 16:09:53 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4065778)[0;0m ERROR 12-05 16:09:53 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 16:10:15 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 16:10:15 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 16:10:15 [model.py:1745] Using max model len 131072
INFO 12-05 16:10:15 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4065866)[0;0m INFO 12-05 16:10:22 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4065866)[0;0m INFO 12-05 16:10:23 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:44887 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4065866)[0;0m INFO 12-05 16:10:23 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4065866)[0;0m INFO 12-05 16:10:23 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4065866)[0;0m INFO 12-05 16:10:23 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4065866)[0;0m INFO 12-05 16:10:23 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4065866)[0;0m INFO 12-05 16:10:25 [default_loader.py:314] Loading weights took 0.87 seconds
[1;36m(EngineCore_DP0 pid=4065866)[0;0m INFO 12-05 16:10:25 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.392558 seconds
[1;36m(EngineCore_DP0 pid=4065866)[0;0m INFO 12-05 16:10:29 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4065866)[0;0m INFO 12-05 16:10:29 [backends.py:647] Dynamo bytecode transform time: 3.63 s
[1;36m(EngineCore_DP0 pid=4065866)[0;0m ERROR 12-05 16:10:29 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4065866)[0;0m ERROR 12-05 16:10:29 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4065866)[0;0m ERROR 12-05 16:10:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4065866)[0;0m ERROR 12-05 16:10:29 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4065866)[0;0m ERROR 12-05 16:10:29 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065866)[0;0m ERROR 12-05 16:10:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4065866)[0;0m ERROR 12-05 16:10:29 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4065866)[0;0m ERROR 12-05 16:10:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4065866)[0;0m ERROR 12-05 16:10:29 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4065866)[0;0m ERROR 12-05 16:10:29 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065866)[0;0m ERROR 12-05 16:10:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4065866)[0;0m ERROR 12-05 16:10:29 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4065866)[0;0m ERROR 12-05 16:10:29 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065866)[0;0m ERROR 12-05 16:10:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4065866)[0;0m ERROR 12-05 16:10:29 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4065866)[0;0m ERROR 12-05 16:10:29 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065866)[0;0m ERROR 12-05 16:10:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4065866)[0;0m ERROR 12-05 16:10:29 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4065866)[0;0m ERROR 12-05 16:10:29 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065866)[0;0m ERROR 12-05 16:10:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4065866)[0;0m ERROR 12-05 16:10:29 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4065866)[0;0m ERROR 12-05 16:10:29 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065866)[0;0m ERROR 12-05 16:10:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4065866)[0;0m ERROR 12-05 16:10:29 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4065866)[0;0m ERROR 12-05 16:10:29 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065866)[0;0m ERROR 12-05 16:10:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4065866)[0;0m ERROR 12-05 16:10:29 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4065866)[0;0m ERROR 12-05 16:10:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4065866)[0;0m ERROR 12-05 16:10:29 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4065866)[0;0m ERROR 12-05 16:10:29 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065866)[0;0m ERROR 12-05 16:10:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4065866)[0;0m ERROR 12-05 16:10:29 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4065866)[0;0m ERROR 12-05 16:10:29 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065866)[0;0m ERROR 12-05 16:10:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4065866)[0;0m ERROR 12-05 16:10:29 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4065866)[0;0m ERROR 12-05 16:10:29 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065866)[0;0m ERROR 12-05 16:10:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4065866)[0;0m ERROR 12-05 16:10:29 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4065866)[0;0m ERROR 12-05 16:10:29 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065866)[0;0m ERROR 12-05 16:10:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4065866)[0;0m ERROR 12-05 16:10:29 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4065866)[0;0m ERROR 12-05 16:10:29 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065866)[0;0m ERROR 12-05 16:10:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4065866)[0;0m ERROR 12-05 16:10:29 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4065866)[0;0m ERROR 12-05 16:10:29 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065866)[0;0m ERROR 12-05 16:10:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4065866)[0;0m ERROR 12-05 16:10:29 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4065866)[0;0m ERROR 12-05 16:10:29 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065866)[0;0m ERROR 12-05 16:10:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4065866)[0;0m ERROR 12-05 16:10:29 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4065866)[0;0m ERROR 12-05 16:10:29 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065866)[0;0m ERROR 12-05 16:10:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4065866)[0;0m ERROR 12-05 16:10:29 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4065866)[0;0m ERROR 12-05 16:10:29 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065866)[0;0m ERROR 12-05 16:10:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4065866)[0;0m ERROR 12-05 16:10:29 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4065866)[0;0m ERROR 12-05 16:10:29 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065866)[0;0m ERROR 12-05 16:10:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4065866)[0;0m ERROR 12-05 16:10:29 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4065866)[0;0m ERROR 12-05 16:10:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4065866)[0;0m ERROR 12-05 16:10:29 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4065866)[0;0m ERROR 12-05 16:10:29 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065866)[0;0m ERROR 12-05 16:10:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4065866)[0;0m ERROR 12-05 16:10:29 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4065866)[0;0m ERROR 12-05 16:10:29 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065866)[0;0m ERROR 12-05 16:10:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4065866)[0;0m ERROR 12-05 16:10:29 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4065866)[0;0m ERROR 12-05 16:10:29 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065866)[0;0m ERROR 12-05 16:10:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4065866)[0;0m ERROR 12-05 16:10:29 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4065866)[0;0m ERROR 12-05 16:10:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4065866)[0;0m ERROR 12-05 16:10:29 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4065866)[0;0m ERROR 12-05 16:10:29 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065866)[0;0m ERROR 12-05 16:10:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4065866)[0;0m ERROR 12-05 16:10:29 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4065866)[0;0m ERROR 12-05 16:10:29 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065866)[0;0m ERROR 12-05 16:10:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4065866)[0;0m ERROR 12-05 16:10:29 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4065866)[0;0m ERROR 12-05 16:10:29 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065866)[0;0m ERROR 12-05 16:10:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4065866)[0;0m ERROR 12-05 16:10:29 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4065866)[0;0m ERROR 12-05 16:10:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4065866)[0;0m ERROR 12-05 16:10:29 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4065866)[0;0m ERROR 12-05 16:10:29 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065866)[0;0m ERROR 12-05 16:10:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4065866)[0;0m ERROR 12-05 16:10:29 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4065866)[0;0m ERROR 12-05 16:10:29 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065866)[0;0m ERROR 12-05 16:10:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4065866)[0;0m ERROR 12-05 16:10:29 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4065866)[0;0m ERROR 12-05 16:10:29 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065866)[0;0m ERROR 12-05 16:10:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4065866)[0;0m ERROR 12-05 16:10:29 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4065866)[0;0m ERROR 12-05 16:10:29 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065866)[0;0m ERROR 12-05 16:10:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4065866)[0;0m ERROR 12-05 16:10:29 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4065866)[0;0m ERROR 12-05 16:10:29 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7f4f540da480>' raised:
[1;36m(EngineCore_DP0 pid=4065866)[0;0m ERROR 12-05 16:10:29 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4065866)[0;0m ERROR 12-05 16:10:29 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4065866)[0;0m ERROR 12-05 16:10:29 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4065866)[0;0m ERROR 12-05 16:10:29 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 16:10:50 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 16:10:51 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 16:10:51 [model.py:1745] Using max model len 131072
INFO 12-05 16:10:51 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4065954)[0;0m INFO 12-05 16:10:59 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4065954)[0;0m INFO 12-05 16:10:59 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:36611 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4065954)[0;0m INFO 12-05 16:10:59 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4065954)[0;0m INFO 12-05 16:11:00 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4065954)[0;0m INFO 12-05 16:11:00 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4065954)[0;0m INFO 12-05 16:11:00 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4065954)[0;0m INFO 12-05 16:11:01 [default_loader.py:314] Loading weights took 0.87 seconds
[1;36m(EngineCore_DP0 pid=4065954)[0;0m INFO 12-05 16:11:02 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.433702 seconds
[1;36m(EngineCore_DP0 pid=4065954)[0;0m INFO 12-05 16:11:06 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4065954)[0;0m INFO 12-05 16:11:06 [backends.py:647] Dynamo bytecode transform time: 3.67 s
[1;36m(EngineCore_DP0 pid=4065954)[0;0m ERROR 12-05 16:11:06 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4065954)[0;0m ERROR 12-05 16:11:06 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4065954)[0;0m ERROR 12-05 16:11:06 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4065954)[0;0m ERROR 12-05 16:11:06 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4065954)[0;0m ERROR 12-05 16:11:06 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065954)[0;0m ERROR 12-05 16:11:06 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4065954)[0;0m ERROR 12-05 16:11:06 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4065954)[0;0m ERROR 12-05 16:11:06 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4065954)[0;0m ERROR 12-05 16:11:06 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4065954)[0;0m ERROR 12-05 16:11:06 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065954)[0;0m ERROR 12-05 16:11:06 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4065954)[0;0m ERROR 12-05 16:11:06 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4065954)[0;0m ERROR 12-05 16:11:06 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065954)[0;0m ERROR 12-05 16:11:06 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4065954)[0;0m ERROR 12-05 16:11:06 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4065954)[0;0m ERROR 12-05 16:11:06 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065954)[0;0m ERROR 12-05 16:11:06 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4065954)[0;0m ERROR 12-05 16:11:06 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4065954)[0;0m ERROR 12-05 16:11:06 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065954)[0;0m ERROR 12-05 16:11:06 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4065954)[0;0m ERROR 12-05 16:11:06 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4065954)[0;0m ERROR 12-05 16:11:06 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065954)[0;0m ERROR 12-05 16:11:06 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4065954)[0;0m ERROR 12-05 16:11:06 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4065954)[0;0m ERROR 12-05 16:11:06 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065954)[0;0m ERROR 12-05 16:11:06 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4065954)[0;0m ERROR 12-05 16:11:06 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4065954)[0;0m ERROR 12-05 16:11:06 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4065954)[0;0m ERROR 12-05 16:11:06 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4065954)[0;0m ERROR 12-05 16:11:06 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065954)[0;0m ERROR 12-05 16:11:06 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4065954)[0;0m ERROR 12-05 16:11:06 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4065954)[0;0m ERROR 12-05 16:11:06 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065954)[0;0m ERROR 12-05 16:11:06 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4065954)[0;0m ERROR 12-05 16:11:06 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4065954)[0;0m ERROR 12-05 16:11:06 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065954)[0;0m ERROR 12-05 16:11:06 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4065954)[0;0m ERROR 12-05 16:11:06 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4065954)[0;0m ERROR 12-05 16:11:06 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065954)[0;0m ERROR 12-05 16:11:06 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4065954)[0;0m ERROR 12-05 16:11:06 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4065954)[0;0m ERROR 12-05 16:11:06 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065954)[0;0m ERROR 12-05 16:11:06 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4065954)[0;0m ERROR 12-05 16:11:06 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4065954)[0;0m ERROR 12-05 16:11:06 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065954)[0;0m ERROR 12-05 16:11:06 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4065954)[0;0m ERROR 12-05 16:11:06 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4065954)[0;0m ERROR 12-05 16:11:06 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065954)[0;0m ERROR 12-05 16:11:06 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4065954)[0;0m ERROR 12-05 16:11:06 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4065954)[0;0m ERROR 12-05 16:11:06 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065954)[0;0m ERROR 12-05 16:11:06 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4065954)[0;0m ERROR 12-05 16:11:06 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4065954)[0;0m ERROR 12-05 16:11:06 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065954)[0;0m ERROR 12-05 16:11:06 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4065954)[0;0m ERROR 12-05 16:11:06 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4065954)[0;0m ERROR 12-05 16:11:06 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065954)[0;0m ERROR 12-05 16:11:06 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4065954)[0;0m ERROR 12-05 16:11:06 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4065954)[0;0m ERROR 12-05 16:11:06 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4065954)[0;0m ERROR 12-05 16:11:06 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4065954)[0;0m ERROR 12-05 16:11:06 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065954)[0;0m ERROR 12-05 16:11:06 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4065954)[0;0m ERROR 12-05 16:11:06 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4065954)[0;0m ERROR 12-05 16:11:06 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065954)[0;0m ERROR 12-05 16:11:06 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4065954)[0;0m ERROR 12-05 16:11:06 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4065954)[0;0m ERROR 12-05 16:11:06 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065954)[0;0m ERROR 12-05 16:11:06 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4065954)[0;0m ERROR 12-05 16:11:06 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4065954)[0;0m ERROR 12-05 16:11:06 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4065954)[0;0m ERROR 12-05 16:11:06 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4065954)[0;0m ERROR 12-05 16:11:06 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065954)[0;0m ERROR 12-05 16:11:06 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4065954)[0;0m ERROR 12-05 16:11:06 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4065954)[0;0m ERROR 12-05 16:11:06 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065954)[0;0m ERROR 12-05 16:11:06 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4065954)[0;0m ERROR 12-05 16:11:06 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4065954)[0;0m ERROR 12-05 16:11:06 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065954)[0;0m ERROR 12-05 16:11:06 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4065954)[0;0m ERROR 12-05 16:11:06 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4065954)[0;0m ERROR 12-05 16:11:06 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4065954)[0;0m ERROR 12-05 16:11:06 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4065954)[0;0m ERROR 12-05 16:11:06 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065954)[0;0m ERROR 12-05 16:11:06 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4065954)[0;0m ERROR 12-05 16:11:06 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4065954)[0;0m ERROR 12-05 16:11:06 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065954)[0;0m ERROR 12-05 16:11:06 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4065954)[0;0m ERROR 12-05 16:11:06 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4065954)[0;0m ERROR 12-05 16:11:06 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065954)[0;0m ERROR 12-05 16:11:06 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4065954)[0;0m ERROR 12-05 16:11:06 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4065954)[0;0m ERROR 12-05 16:11:06 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4065954)[0;0m ERROR 12-05 16:11:06 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4065954)[0;0m ERROR 12-05 16:11:06 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4065954)[0;0m ERROR 12-05 16:11:06 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7f65c8044530>' raised:
[1;36m(EngineCore_DP0 pid=4065954)[0;0m ERROR 12-05 16:11:06 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4065954)[0;0m ERROR 12-05 16:11:06 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4065954)[0;0m ERROR 12-05 16:11:06 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4065954)[0;0m ERROR 12-05 16:11:06 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 16:11:27 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 16:11:28 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 16:11:28 [model.py:1745] Using max model len 131072
INFO 12-05 16:11:28 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4066128)[0;0m INFO 12-05 16:11:35 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4066128)[0;0m INFO 12-05 16:11:35 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:33171 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4066128)[0;0m INFO 12-05 16:11:35 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4066128)[0;0m INFO 12-05 16:11:36 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4066128)[0;0m INFO 12-05 16:11:36 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4066128)[0;0m INFO 12-05 16:11:36 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4066128)[0;0m INFO 12-05 16:11:37 [default_loader.py:314] Loading weights took 0.87 seconds
[1;36m(EngineCore_DP0 pid=4066128)[0;0m INFO 12-05 16:11:38 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.403331 seconds
[1;36m(EngineCore_DP0 pid=4066128)[0;0m INFO 12-05 16:11:42 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4066128)[0;0m INFO 12-05 16:11:42 [backends.py:647] Dynamo bytecode transform time: 3.69 s
[1;36m(EngineCore_DP0 pid=4066128)[0;0m ERROR 12-05 16:11:42 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4066128)[0;0m ERROR 12-05 16:11:42 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4066128)[0;0m ERROR 12-05 16:11:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4066128)[0;0m ERROR 12-05 16:11:42 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4066128)[0;0m ERROR 12-05 16:11:42 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066128)[0;0m ERROR 12-05 16:11:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4066128)[0;0m ERROR 12-05 16:11:42 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4066128)[0;0m ERROR 12-05 16:11:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4066128)[0;0m ERROR 12-05 16:11:42 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4066128)[0;0m ERROR 12-05 16:11:42 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066128)[0;0m ERROR 12-05 16:11:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4066128)[0;0m ERROR 12-05 16:11:42 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4066128)[0;0m ERROR 12-05 16:11:42 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066128)[0;0m ERROR 12-05 16:11:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4066128)[0;0m ERROR 12-05 16:11:42 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4066128)[0;0m ERROR 12-05 16:11:42 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066128)[0;0m ERROR 12-05 16:11:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4066128)[0;0m ERROR 12-05 16:11:42 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4066128)[0;0m ERROR 12-05 16:11:42 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066128)[0;0m ERROR 12-05 16:11:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4066128)[0;0m ERROR 12-05 16:11:42 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4066128)[0;0m ERROR 12-05 16:11:42 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066128)[0;0m ERROR 12-05 16:11:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4066128)[0;0m ERROR 12-05 16:11:42 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4066128)[0;0m ERROR 12-05 16:11:42 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066128)[0;0m ERROR 12-05 16:11:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4066128)[0;0m ERROR 12-05 16:11:42 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4066128)[0;0m ERROR 12-05 16:11:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4066128)[0;0m ERROR 12-05 16:11:42 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4066128)[0;0m ERROR 12-05 16:11:42 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066128)[0;0m ERROR 12-05 16:11:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4066128)[0;0m ERROR 12-05 16:11:42 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4066128)[0;0m ERROR 12-05 16:11:42 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066128)[0;0m ERROR 12-05 16:11:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4066128)[0;0m ERROR 12-05 16:11:42 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4066128)[0;0m ERROR 12-05 16:11:42 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066128)[0;0m ERROR 12-05 16:11:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4066128)[0;0m ERROR 12-05 16:11:42 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4066128)[0;0m ERROR 12-05 16:11:42 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066128)[0;0m ERROR 12-05 16:11:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4066128)[0;0m ERROR 12-05 16:11:42 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4066128)[0;0m ERROR 12-05 16:11:42 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066128)[0;0m ERROR 12-05 16:11:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4066128)[0;0m ERROR 12-05 16:11:42 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4066128)[0;0m ERROR 12-05 16:11:42 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066128)[0;0m ERROR 12-05 16:11:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4066128)[0;0m ERROR 12-05 16:11:42 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4066128)[0;0m ERROR 12-05 16:11:42 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066128)[0;0m ERROR 12-05 16:11:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4066128)[0;0m ERROR 12-05 16:11:42 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4066128)[0;0m ERROR 12-05 16:11:42 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066128)[0;0m ERROR 12-05 16:11:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4066128)[0;0m ERROR 12-05 16:11:42 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4066128)[0;0m ERROR 12-05 16:11:42 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066128)[0;0m ERROR 12-05 16:11:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4066128)[0;0m ERROR 12-05 16:11:42 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4066128)[0;0m ERROR 12-05 16:11:42 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066128)[0;0m ERROR 12-05 16:11:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4066128)[0;0m ERROR 12-05 16:11:42 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4066128)[0;0m ERROR 12-05 16:11:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4066128)[0;0m ERROR 12-05 16:11:42 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4066128)[0;0m ERROR 12-05 16:11:42 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066128)[0;0m ERROR 12-05 16:11:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4066128)[0;0m ERROR 12-05 16:11:42 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4066128)[0;0m ERROR 12-05 16:11:42 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066128)[0;0m ERROR 12-05 16:11:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4066128)[0;0m ERROR 12-05 16:11:42 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4066128)[0;0m ERROR 12-05 16:11:42 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066128)[0;0m ERROR 12-05 16:11:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4066128)[0;0m ERROR 12-05 16:11:42 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4066128)[0;0m ERROR 12-05 16:11:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4066128)[0;0m ERROR 12-05 16:11:42 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4066128)[0;0m ERROR 12-05 16:11:42 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066128)[0;0m ERROR 12-05 16:11:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4066128)[0;0m ERROR 12-05 16:11:42 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4066128)[0;0m ERROR 12-05 16:11:42 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066128)[0;0m ERROR 12-05 16:11:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4066128)[0;0m ERROR 12-05 16:11:42 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4066128)[0;0m ERROR 12-05 16:11:42 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066128)[0;0m ERROR 12-05 16:11:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4066128)[0;0m ERROR 12-05 16:11:42 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4066128)[0;0m ERROR 12-05 16:11:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4066128)[0;0m ERROR 12-05 16:11:42 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4066128)[0;0m ERROR 12-05 16:11:42 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066128)[0;0m ERROR 12-05 16:11:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4066128)[0;0m ERROR 12-05 16:11:42 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4066128)[0;0m ERROR 12-05 16:11:42 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066128)[0;0m ERROR 12-05 16:11:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4066128)[0;0m ERROR 12-05 16:11:42 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4066128)[0;0m ERROR 12-05 16:11:42 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066128)[0;0m ERROR 12-05 16:11:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4066128)[0;0m ERROR 12-05 16:11:42 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4066128)[0;0m ERROR 12-05 16:11:42 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066128)[0;0m ERROR 12-05 16:11:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4066128)[0;0m ERROR 12-05 16:11:42 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4066128)[0;0m ERROR 12-05 16:11:42 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7f6198072de0>' raised:
[1;36m(EngineCore_DP0 pid=4066128)[0;0m ERROR 12-05 16:11:42 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4066128)[0;0m ERROR 12-05 16:11:42 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4066128)[0;0m ERROR 12-05 16:11:42 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4066128)[0;0m ERROR 12-05 16:11:42 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 16:12:03 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 16:12:04 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 16:12:04 [model.py:1745] Using max model len 131072
INFO 12-05 16:12:04 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4066214)[0;0m INFO 12-05 16:12:12 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4066214)[0;0m INFO 12-05 16:12:13 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:37753 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4066214)[0;0m INFO 12-05 16:12:13 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4066214)[0;0m INFO 12-05 16:12:13 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4066214)[0;0m INFO 12-05 16:12:13 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4066214)[0;0m INFO 12-05 16:12:13 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4066214)[0;0m INFO 12-05 16:12:15 [default_loader.py:314] Loading weights took 0.87 seconds
[1;36m(EngineCore_DP0 pid=4066214)[0;0m INFO 12-05 16:12:15 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.429359 seconds
[1;36m(EngineCore_DP0 pid=4066214)[0;0m INFO 12-05 16:12:19 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4066214)[0;0m INFO 12-05 16:12:19 [backends.py:647] Dynamo bytecode transform time: 3.66 s
[1;36m(EngineCore_DP0 pid=4066214)[0;0m ERROR 12-05 16:12:19 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4066214)[0;0m ERROR 12-05 16:12:19 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4066214)[0;0m ERROR 12-05 16:12:19 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4066214)[0;0m ERROR 12-05 16:12:19 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4066214)[0;0m ERROR 12-05 16:12:19 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066214)[0;0m ERROR 12-05 16:12:19 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4066214)[0;0m ERROR 12-05 16:12:19 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4066214)[0;0m ERROR 12-05 16:12:19 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4066214)[0;0m ERROR 12-05 16:12:19 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4066214)[0;0m ERROR 12-05 16:12:19 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066214)[0;0m ERROR 12-05 16:12:19 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4066214)[0;0m ERROR 12-05 16:12:19 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4066214)[0;0m ERROR 12-05 16:12:19 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066214)[0;0m ERROR 12-05 16:12:19 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4066214)[0;0m ERROR 12-05 16:12:19 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4066214)[0;0m ERROR 12-05 16:12:19 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066214)[0;0m ERROR 12-05 16:12:19 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4066214)[0;0m ERROR 12-05 16:12:19 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4066214)[0;0m ERROR 12-05 16:12:19 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066214)[0;0m ERROR 12-05 16:12:19 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4066214)[0;0m ERROR 12-05 16:12:19 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4066214)[0;0m ERROR 12-05 16:12:19 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066214)[0;0m ERROR 12-05 16:12:19 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4066214)[0;0m ERROR 12-05 16:12:19 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4066214)[0;0m ERROR 12-05 16:12:19 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066214)[0;0m ERROR 12-05 16:12:19 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4066214)[0;0m ERROR 12-05 16:12:19 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4066214)[0;0m ERROR 12-05 16:12:19 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4066214)[0;0m ERROR 12-05 16:12:19 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4066214)[0;0m ERROR 12-05 16:12:19 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066214)[0;0m ERROR 12-05 16:12:19 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4066214)[0;0m ERROR 12-05 16:12:19 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4066214)[0;0m ERROR 12-05 16:12:19 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066214)[0;0m ERROR 12-05 16:12:19 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4066214)[0;0m ERROR 12-05 16:12:19 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4066214)[0;0m ERROR 12-05 16:12:19 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066214)[0;0m ERROR 12-05 16:12:19 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4066214)[0;0m ERROR 12-05 16:12:19 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4066214)[0;0m ERROR 12-05 16:12:19 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066214)[0;0m ERROR 12-05 16:12:19 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4066214)[0;0m ERROR 12-05 16:12:19 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4066214)[0;0m ERROR 12-05 16:12:19 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066214)[0;0m ERROR 12-05 16:12:19 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4066214)[0;0m ERROR 12-05 16:12:19 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4066214)[0;0m ERROR 12-05 16:12:19 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066214)[0;0m ERROR 12-05 16:12:19 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4066214)[0;0m ERROR 12-05 16:12:19 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4066214)[0;0m ERROR 12-05 16:12:19 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066214)[0;0m ERROR 12-05 16:12:19 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4066214)[0;0m ERROR 12-05 16:12:19 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4066214)[0;0m ERROR 12-05 16:12:19 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066214)[0;0m ERROR 12-05 16:12:19 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4066214)[0;0m ERROR 12-05 16:12:19 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4066214)[0;0m ERROR 12-05 16:12:19 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066214)[0;0m ERROR 12-05 16:12:19 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4066214)[0;0m ERROR 12-05 16:12:19 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4066214)[0;0m ERROR 12-05 16:12:19 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066214)[0;0m ERROR 12-05 16:12:19 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4066214)[0;0m ERROR 12-05 16:12:19 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4066214)[0;0m ERROR 12-05 16:12:19 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4066214)[0;0m ERROR 12-05 16:12:19 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4066214)[0;0m ERROR 12-05 16:12:19 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066214)[0;0m ERROR 12-05 16:12:19 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4066214)[0;0m ERROR 12-05 16:12:19 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4066214)[0;0m ERROR 12-05 16:12:19 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066214)[0;0m ERROR 12-05 16:12:19 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4066214)[0;0m ERROR 12-05 16:12:19 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4066214)[0;0m ERROR 12-05 16:12:19 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066214)[0;0m ERROR 12-05 16:12:19 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4066214)[0;0m ERROR 12-05 16:12:19 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4066214)[0;0m ERROR 12-05 16:12:19 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4066214)[0;0m ERROR 12-05 16:12:19 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4066214)[0;0m ERROR 12-05 16:12:19 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066214)[0;0m ERROR 12-05 16:12:19 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4066214)[0;0m ERROR 12-05 16:12:19 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4066214)[0;0m ERROR 12-05 16:12:19 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066214)[0;0m ERROR 12-05 16:12:19 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4066214)[0;0m ERROR 12-05 16:12:19 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4066214)[0;0m ERROR 12-05 16:12:19 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066214)[0;0m ERROR 12-05 16:12:19 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4066214)[0;0m ERROR 12-05 16:12:19 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4066214)[0;0m ERROR 12-05 16:12:19 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4066214)[0;0m ERROR 12-05 16:12:19 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4066214)[0;0m ERROR 12-05 16:12:19 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066214)[0;0m ERROR 12-05 16:12:19 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4066214)[0;0m ERROR 12-05 16:12:19 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4066214)[0;0m ERROR 12-05 16:12:19 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066214)[0;0m ERROR 12-05 16:12:19 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4066214)[0;0m ERROR 12-05 16:12:19 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4066214)[0;0m ERROR 12-05 16:12:19 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066214)[0;0m ERROR 12-05 16:12:19 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4066214)[0;0m ERROR 12-05 16:12:19 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4066214)[0;0m ERROR 12-05 16:12:19 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066214)[0;0m ERROR 12-05 16:12:19 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4066214)[0;0m ERROR 12-05 16:12:19 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4066214)[0;0m ERROR 12-05 16:12:19 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7f05d976ed20>' raised:
[1;36m(EngineCore_DP0 pid=4066214)[0;0m ERROR 12-05 16:12:19 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4066214)[0;0m ERROR 12-05 16:12:19 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4066214)[0;0m ERROR 12-05 16:12:19 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4066214)[0;0m ERROR 12-05 16:12:19 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 16:12:40 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 16:12:41 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 16:12:41 [model.py:1745] Using max model len 131072
INFO 12-05 16:12:41 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4066295)[0;0m INFO 12-05 16:12:48 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4066295)[0;0m INFO 12-05 16:12:49 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:33395 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4066295)[0;0m INFO 12-05 16:12:49 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4066295)[0;0m INFO 12-05 16:12:49 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4066295)[0;0m INFO 12-05 16:12:50 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4066295)[0;0m INFO 12-05 16:12:50 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4066295)[0;0m INFO 12-05 16:12:51 [default_loader.py:314] Loading weights took 0.88 seconds
[1;36m(EngineCore_DP0 pid=4066295)[0;0m INFO 12-05 16:12:51 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.395985 seconds
[1;36m(EngineCore_DP0 pid=4066295)[0;0m INFO 12-05 16:12:56 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4066295)[0;0m INFO 12-05 16:12:56 [backends.py:647] Dynamo bytecode transform time: 4.60 s
[1;36m(EngineCore_DP0 pid=4066295)[0;0m ERROR 12-05 16:12:56 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4066295)[0;0m ERROR 12-05 16:12:56 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4066295)[0;0m ERROR 12-05 16:12:56 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4066295)[0;0m ERROR 12-05 16:12:56 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4066295)[0;0m ERROR 12-05 16:12:56 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066295)[0;0m ERROR 12-05 16:12:56 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4066295)[0;0m ERROR 12-05 16:12:56 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4066295)[0;0m ERROR 12-05 16:12:56 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4066295)[0;0m ERROR 12-05 16:12:56 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4066295)[0;0m ERROR 12-05 16:12:56 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066295)[0;0m ERROR 12-05 16:12:56 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4066295)[0;0m ERROR 12-05 16:12:56 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4066295)[0;0m ERROR 12-05 16:12:56 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066295)[0;0m ERROR 12-05 16:12:56 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4066295)[0;0m ERROR 12-05 16:12:56 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4066295)[0;0m ERROR 12-05 16:12:56 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066295)[0;0m ERROR 12-05 16:12:56 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4066295)[0;0m ERROR 12-05 16:12:56 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4066295)[0;0m ERROR 12-05 16:12:56 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066295)[0;0m ERROR 12-05 16:12:56 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4066295)[0;0m ERROR 12-05 16:12:56 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4066295)[0;0m ERROR 12-05 16:12:56 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066295)[0;0m ERROR 12-05 16:12:56 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4066295)[0;0m ERROR 12-05 16:12:56 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4066295)[0;0m ERROR 12-05 16:12:56 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066295)[0;0m ERROR 12-05 16:12:56 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4066295)[0;0m ERROR 12-05 16:12:56 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4066295)[0;0m ERROR 12-05 16:12:56 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4066295)[0;0m ERROR 12-05 16:12:56 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4066295)[0;0m ERROR 12-05 16:12:56 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066295)[0;0m ERROR 12-05 16:12:56 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4066295)[0;0m ERROR 12-05 16:12:56 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4066295)[0;0m ERROR 12-05 16:12:56 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066295)[0;0m ERROR 12-05 16:12:56 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4066295)[0;0m ERROR 12-05 16:12:56 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4066295)[0;0m ERROR 12-05 16:12:56 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066295)[0;0m ERROR 12-05 16:12:56 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4066295)[0;0m ERROR 12-05 16:12:56 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4066295)[0;0m ERROR 12-05 16:12:56 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066295)[0;0m ERROR 12-05 16:12:56 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4066295)[0;0m ERROR 12-05 16:12:56 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4066295)[0;0m ERROR 12-05 16:12:56 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066295)[0;0m ERROR 12-05 16:12:56 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4066295)[0;0m ERROR 12-05 16:12:56 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4066295)[0;0m ERROR 12-05 16:12:56 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066295)[0;0m ERROR 12-05 16:12:56 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4066295)[0;0m ERROR 12-05 16:12:56 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4066295)[0;0m ERROR 12-05 16:12:56 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066295)[0;0m ERROR 12-05 16:12:56 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4066295)[0;0m ERROR 12-05 16:12:56 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4066295)[0;0m ERROR 12-05 16:12:56 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066295)[0;0m ERROR 12-05 16:12:56 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4066295)[0;0m ERROR 12-05 16:12:56 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4066295)[0;0m ERROR 12-05 16:12:56 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066295)[0;0m ERROR 12-05 16:12:56 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4066295)[0;0m ERROR 12-05 16:12:56 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4066295)[0;0m ERROR 12-05 16:12:56 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066295)[0;0m ERROR 12-05 16:12:56 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4066295)[0;0m ERROR 12-05 16:12:56 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4066295)[0;0m ERROR 12-05 16:12:56 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4066295)[0;0m ERROR 12-05 16:12:56 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4066295)[0;0m ERROR 12-05 16:12:56 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066295)[0;0m ERROR 12-05 16:12:56 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4066295)[0;0m ERROR 12-05 16:12:56 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4066295)[0;0m ERROR 12-05 16:12:56 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066295)[0;0m ERROR 12-05 16:12:56 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4066295)[0;0m ERROR 12-05 16:12:56 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4066295)[0;0m ERROR 12-05 16:12:56 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066295)[0;0m ERROR 12-05 16:12:56 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4066295)[0;0m ERROR 12-05 16:12:56 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4066295)[0;0m ERROR 12-05 16:12:56 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4066295)[0;0m ERROR 12-05 16:12:56 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4066295)[0;0m ERROR 12-05 16:12:56 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066295)[0;0m ERROR 12-05 16:12:56 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4066295)[0;0m ERROR 12-05 16:12:56 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4066295)[0;0m ERROR 12-05 16:12:56 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066295)[0;0m ERROR 12-05 16:12:56 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4066295)[0;0m ERROR 12-05 16:12:56 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4066295)[0;0m ERROR 12-05 16:12:56 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066295)[0;0m ERROR 12-05 16:12:56 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4066295)[0;0m ERROR 12-05 16:12:56 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4066295)[0;0m ERROR 12-05 16:12:56 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4066295)[0;0m ERROR 12-05 16:12:56 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4066295)[0;0m ERROR 12-05 16:12:56 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066295)[0;0m ERROR 12-05 16:12:56 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4066295)[0;0m ERROR 12-05 16:12:56 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4066295)[0;0m ERROR 12-05 16:12:56 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066295)[0;0m ERROR 12-05 16:12:56 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4066295)[0;0m ERROR 12-05 16:12:56 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4066295)[0;0m ERROR 12-05 16:12:56 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066295)[0;0m ERROR 12-05 16:12:56 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4066295)[0;0m ERROR 12-05 16:12:56 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4066295)[0;0m ERROR 12-05 16:12:56 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066295)[0;0m ERROR 12-05 16:12:56 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4066295)[0;0m ERROR 12-05 16:12:56 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4066295)[0;0m ERROR 12-05 16:12:56 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7f1141d34920>' raised:
[1;36m(EngineCore_DP0 pid=4066295)[0;0m ERROR 12-05 16:12:56 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4066295)[0;0m ERROR 12-05 16:12:56 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4066295)[0;0m ERROR 12-05 16:12:56 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4066295)[0;0m ERROR 12-05 16:12:56 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 16:13:18 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 16:13:18 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 16:13:18 [model.py:1745] Using max model len 131072
INFO 12-05 16:13:18 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4066380)[0;0m INFO 12-05 16:13:26 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4066380)[0;0m INFO 12-05 16:13:27 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:53183 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4066380)[0;0m INFO 12-05 16:13:27 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4066380)[0;0m INFO 12-05 16:13:27 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4066380)[0;0m INFO 12-05 16:13:27 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4066380)[0;0m INFO 12-05 16:13:27 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4066380)[0;0m INFO 12-05 16:13:29 [default_loader.py:314] Loading weights took 0.88 seconds
[1;36m(EngineCore_DP0 pid=4066380)[0;0m INFO 12-05 16:13:29 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.431904 seconds
[1;36m(EngineCore_DP0 pid=4066380)[0;0m INFO 12-05 16:13:33 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4066380)[0;0m INFO 12-05 16:13:33 [backends.py:647] Dynamo bytecode transform time: 3.65 s
[1;36m(EngineCore_DP0 pid=4066380)[0;0m ERROR 12-05 16:13:33 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4066380)[0;0m ERROR 12-05 16:13:33 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4066380)[0;0m ERROR 12-05 16:13:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4066380)[0;0m ERROR 12-05 16:13:33 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4066380)[0;0m ERROR 12-05 16:13:33 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066380)[0;0m ERROR 12-05 16:13:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4066380)[0;0m ERROR 12-05 16:13:33 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4066380)[0;0m ERROR 12-05 16:13:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4066380)[0;0m ERROR 12-05 16:13:33 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4066380)[0;0m ERROR 12-05 16:13:33 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066380)[0;0m ERROR 12-05 16:13:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4066380)[0;0m ERROR 12-05 16:13:33 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4066380)[0;0m ERROR 12-05 16:13:33 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066380)[0;0m ERROR 12-05 16:13:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4066380)[0;0m ERROR 12-05 16:13:33 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4066380)[0;0m ERROR 12-05 16:13:33 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066380)[0;0m ERROR 12-05 16:13:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4066380)[0;0m ERROR 12-05 16:13:33 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4066380)[0;0m ERROR 12-05 16:13:33 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066380)[0;0m ERROR 12-05 16:13:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4066380)[0;0m ERROR 12-05 16:13:33 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4066380)[0;0m ERROR 12-05 16:13:33 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066380)[0;0m ERROR 12-05 16:13:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4066380)[0;0m ERROR 12-05 16:13:33 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4066380)[0;0m ERROR 12-05 16:13:33 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066380)[0;0m ERROR 12-05 16:13:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4066380)[0;0m ERROR 12-05 16:13:33 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4066380)[0;0m ERROR 12-05 16:13:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4066380)[0;0m ERROR 12-05 16:13:33 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4066380)[0;0m ERROR 12-05 16:13:33 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066380)[0;0m ERROR 12-05 16:13:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4066380)[0;0m ERROR 12-05 16:13:33 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4066380)[0;0m ERROR 12-05 16:13:33 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066380)[0;0m ERROR 12-05 16:13:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4066380)[0;0m ERROR 12-05 16:13:33 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4066380)[0;0m ERROR 12-05 16:13:33 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066380)[0;0m ERROR 12-05 16:13:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4066380)[0;0m ERROR 12-05 16:13:33 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4066380)[0;0m ERROR 12-05 16:13:33 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066380)[0;0m ERROR 12-05 16:13:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4066380)[0;0m ERROR 12-05 16:13:33 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4066380)[0;0m ERROR 12-05 16:13:33 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066380)[0;0m ERROR 12-05 16:13:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4066380)[0;0m ERROR 12-05 16:13:33 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4066380)[0;0m ERROR 12-05 16:13:33 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066380)[0;0m ERROR 12-05 16:13:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4066380)[0;0m ERROR 12-05 16:13:33 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4066380)[0;0m ERROR 12-05 16:13:33 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066380)[0;0m ERROR 12-05 16:13:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4066380)[0;0m ERROR 12-05 16:13:33 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4066380)[0;0m ERROR 12-05 16:13:33 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066380)[0;0m ERROR 12-05 16:13:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4066380)[0;0m ERROR 12-05 16:13:33 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4066380)[0;0m ERROR 12-05 16:13:33 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066380)[0;0m ERROR 12-05 16:13:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4066380)[0;0m ERROR 12-05 16:13:33 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4066380)[0;0m ERROR 12-05 16:13:33 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066380)[0;0m ERROR 12-05 16:13:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4066380)[0;0m ERROR 12-05 16:13:33 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4066380)[0;0m ERROR 12-05 16:13:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4066380)[0;0m ERROR 12-05 16:13:33 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4066380)[0;0m ERROR 12-05 16:13:33 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066380)[0;0m ERROR 12-05 16:13:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4066380)[0;0m ERROR 12-05 16:13:33 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4066380)[0;0m ERROR 12-05 16:13:33 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066380)[0;0m ERROR 12-05 16:13:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4066380)[0;0m ERROR 12-05 16:13:33 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4066380)[0;0m ERROR 12-05 16:13:33 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066380)[0;0m ERROR 12-05 16:13:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4066380)[0;0m ERROR 12-05 16:13:33 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4066380)[0;0m ERROR 12-05 16:13:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4066380)[0;0m ERROR 12-05 16:13:33 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4066380)[0;0m ERROR 12-05 16:13:33 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066380)[0;0m ERROR 12-05 16:13:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4066380)[0;0m ERROR 12-05 16:13:33 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4066380)[0;0m ERROR 12-05 16:13:33 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066380)[0;0m ERROR 12-05 16:13:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4066380)[0;0m ERROR 12-05 16:13:33 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4066380)[0;0m ERROR 12-05 16:13:33 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066380)[0;0m ERROR 12-05 16:13:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4066380)[0;0m ERROR 12-05 16:13:33 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4066380)[0;0m ERROR 12-05 16:13:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4066380)[0;0m ERROR 12-05 16:13:33 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4066380)[0;0m ERROR 12-05 16:13:33 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066380)[0;0m ERROR 12-05 16:13:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4066380)[0;0m ERROR 12-05 16:13:33 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4066380)[0;0m ERROR 12-05 16:13:33 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066380)[0;0m ERROR 12-05 16:13:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4066380)[0;0m ERROR 12-05 16:13:33 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4066380)[0;0m ERROR 12-05 16:13:33 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066380)[0;0m ERROR 12-05 16:13:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4066380)[0;0m ERROR 12-05 16:13:33 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4066380)[0;0m ERROR 12-05 16:13:33 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066380)[0;0m ERROR 12-05 16:13:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4066380)[0;0m ERROR 12-05 16:13:33 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4066380)[0;0m ERROR 12-05 16:13:33 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7f87e0164c80>' raised:
[1;36m(EngineCore_DP0 pid=4066380)[0;0m ERROR 12-05 16:13:33 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4066380)[0;0m ERROR 12-05 16:13:33 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4066380)[0;0m ERROR 12-05 16:13:33 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4066380)[0;0m ERROR 12-05 16:13:33 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 16:13:55 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 16:13:55 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 16:13:55 [model.py:1745] Using max model len 131072
INFO 12-05 16:13:55 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4066468)[0;0m INFO 12-05 16:14:02 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4066468)[0;0m INFO 12-05 16:14:02 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:57285 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4066468)[0;0m INFO 12-05 16:14:02 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4066468)[0;0m INFO 12-05 16:14:03 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4066468)[0;0m INFO 12-05 16:14:03 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4066468)[0;0m INFO 12-05 16:14:03 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4066468)[0;0m INFO 12-05 16:14:04 [default_loader.py:314] Loading weights took 0.87 seconds
[1;36m(EngineCore_DP0 pid=4066468)[0;0m INFO 12-05 16:14:05 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.376810 seconds
[1;36m(EngineCore_DP0 pid=4066468)[0;0m INFO 12-05 16:14:09 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4066468)[0;0m INFO 12-05 16:14:09 [backends.py:647] Dynamo bytecode transform time: 3.64 s
[1;36m(EngineCore_DP0 pid=4066468)[0;0m ERROR 12-05 16:14:09 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4066468)[0;0m ERROR 12-05 16:14:09 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4066468)[0;0m ERROR 12-05 16:14:09 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4066468)[0;0m ERROR 12-05 16:14:09 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4066468)[0;0m ERROR 12-05 16:14:09 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066468)[0;0m ERROR 12-05 16:14:09 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4066468)[0;0m ERROR 12-05 16:14:09 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4066468)[0;0m ERROR 12-05 16:14:09 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4066468)[0;0m ERROR 12-05 16:14:09 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4066468)[0;0m ERROR 12-05 16:14:09 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066468)[0;0m ERROR 12-05 16:14:09 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4066468)[0;0m ERROR 12-05 16:14:09 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4066468)[0;0m ERROR 12-05 16:14:09 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066468)[0;0m ERROR 12-05 16:14:09 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4066468)[0;0m ERROR 12-05 16:14:09 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4066468)[0;0m ERROR 12-05 16:14:09 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066468)[0;0m ERROR 12-05 16:14:09 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4066468)[0;0m ERROR 12-05 16:14:09 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4066468)[0;0m ERROR 12-05 16:14:09 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066468)[0;0m ERROR 12-05 16:14:09 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4066468)[0;0m ERROR 12-05 16:14:09 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4066468)[0;0m ERROR 12-05 16:14:09 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066468)[0;0m ERROR 12-05 16:14:09 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4066468)[0;0m ERROR 12-05 16:14:09 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4066468)[0;0m ERROR 12-05 16:14:09 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066468)[0;0m ERROR 12-05 16:14:09 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4066468)[0;0m ERROR 12-05 16:14:09 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4066468)[0;0m ERROR 12-05 16:14:09 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4066468)[0;0m ERROR 12-05 16:14:09 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4066468)[0;0m ERROR 12-05 16:14:09 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066468)[0;0m ERROR 12-05 16:14:09 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4066468)[0;0m ERROR 12-05 16:14:09 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4066468)[0;0m ERROR 12-05 16:14:09 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066468)[0;0m ERROR 12-05 16:14:09 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4066468)[0;0m ERROR 12-05 16:14:09 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4066468)[0;0m ERROR 12-05 16:14:09 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066468)[0;0m ERROR 12-05 16:14:09 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4066468)[0;0m ERROR 12-05 16:14:09 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4066468)[0;0m ERROR 12-05 16:14:09 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066468)[0;0m ERROR 12-05 16:14:09 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4066468)[0;0m ERROR 12-05 16:14:09 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4066468)[0;0m ERROR 12-05 16:14:09 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066468)[0;0m ERROR 12-05 16:14:09 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4066468)[0;0m ERROR 12-05 16:14:09 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4066468)[0;0m ERROR 12-05 16:14:09 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066468)[0;0m ERROR 12-05 16:14:09 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4066468)[0;0m ERROR 12-05 16:14:09 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4066468)[0;0m ERROR 12-05 16:14:09 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066468)[0;0m ERROR 12-05 16:14:09 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4066468)[0;0m ERROR 12-05 16:14:09 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4066468)[0;0m ERROR 12-05 16:14:09 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066468)[0;0m ERROR 12-05 16:14:09 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4066468)[0;0m ERROR 12-05 16:14:09 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4066468)[0;0m ERROR 12-05 16:14:09 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066468)[0;0m ERROR 12-05 16:14:09 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4066468)[0;0m ERROR 12-05 16:14:09 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4066468)[0;0m ERROR 12-05 16:14:09 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066468)[0;0m ERROR 12-05 16:14:09 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4066468)[0;0m ERROR 12-05 16:14:09 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4066468)[0;0m ERROR 12-05 16:14:09 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4066468)[0;0m ERROR 12-05 16:14:09 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4066468)[0;0m ERROR 12-05 16:14:09 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066468)[0;0m ERROR 12-05 16:14:09 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4066468)[0;0m ERROR 12-05 16:14:09 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4066468)[0;0m ERROR 12-05 16:14:09 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066468)[0;0m ERROR 12-05 16:14:09 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4066468)[0;0m ERROR 12-05 16:14:09 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4066468)[0;0m ERROR 12-05 16:14:09 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066468)[0;0m ERROR 12-05 16:14:09 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4066468)[0;0m ERROR 12-05 16:14:09 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4066468)[0;0m ERROR 12-05 16:14:09 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4066468)[0;0m ERROR 12-05 16:14:09 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4066468)[0;0m ERROR 12-05 16:14:09 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066468)[0;0m ERROR 12-05 16:14:09 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4066468)[0;0m ERROR 12-05 16:14:09 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4066468)[0;0m ERROR 12-05 16:14:09 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066468)[0;0m ERROR 12-05 16:14:09 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4066468)[0;0m ERROR 12-05 16:14:09 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4066468)[0;0m ERROR 12-05 16:14:09 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066468)[0;0m ERROR 12-05 16:14:09 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4066468)[0;0m ERROR 12-05 16:14:09 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4066468)[0;0m ERROR 12-05 16:14:09 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4066468)[0;0m ERROR 12-05 16:14:09 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4066468)[0;0m ERROR 12-05 16:14:09 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066468)[0;0m ERROR 12-05 16:14:09 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4066468)[0;0m ERROR 12-05 16:14:09 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4066468)[0;0m ERROR 12-05 16:14:09 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066468)[0;0m ERROR 12-05 16:14:09 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4066468)[0;0m ERROR 12-05 16:14:09 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4066468)[0;0m ERROR 12-05 16:14:09 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066468)[0;0m ERROR 12-05 16:14:09 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4066468)[0;0m ERROR 12-05 16:14:09 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4066468)[0;0m ERROR 12-05 16:14:09 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066468)[0;0m ERROR 12-05 16:14:09 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4066468)[0;0m ERROR 12-05 16:14:09 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4066468)[0;0m ERROR 12-05 16:14:09 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7fd23c3404a0>' raised:
[1;36m(EngineCore_DP0 pid=4066468)[0;0m ERROR 12-05 16:14:09 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4066468)[0;0m ERROR 12-05 16:14:09 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4066468)[0;0m ERROR 12-05 16:14:09 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4066468)[0;0m ERROR 12-05 16:14:09 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 16:14:30 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 16:14:30 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 16:14:30 [model.py:1745] Using max model len 131072
INFO 12-05 16:14:30 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4066548)[0;0m INFO 12-05 16:14:39 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4066548)[0;0m INFO 12-05 16:14:40 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:36369 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4066548)[0;0m INFO 12-05 16:14:40 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4066548)[0;0m INFO 12-05 16:14:40 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4066548)[0;0m INFO 12-05 16:14:41 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4066548)[0;0m INFO 12-05 16:14:41 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4066548)[0;0m INFO 12-05 16:14:42 [default_loader.py:314] Loading weights took 0.87 seconds
[1;36m(EngineCore_DP0 pid=4066548)[0;0m INFO 12-05 16:14:42 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.421857 seconds
[1;36m(EngineCore_DP0 pid=4066548)[0;0m INFO 12-05 16:14:46 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4066548)[0;0m INFO 12-05 16:14:46 [backends.py:647] Dynamo bytecode transform time: 3.82 s
[1;36m(EngineCore_DP0 pid=4066548)[0;0m ERROR 12-05 16:14:46 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4066548)[0;0m ERROR 12-05 16:14:46 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4066548)[0;0m ERROR 12-05 16:14:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4066548)[0;0m ERROR 12-05 16:14:46 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4066548)[0;0m ERROR 12-05 16:14:46 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066548)[0;0m ERROR 12-05 16:14:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4066548)[0;0m ERROR 12-05 16:14:46 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4066548)[0;0m ERROR 12-05 16:14:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4066548)[0;0m ERROR 12-05 16:14:46 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4066548)[0;0m ERROR 12-05 16:14:46 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066548)[0;0m ERROR 12-05 16:14:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4066548)[0;0m ERROR 12-05 16:14:46 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4066548)[0;0m ERROR 12-05 16:14:46 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066548)[0;0m ERROR 12-05 16:14:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4066548)[0;0m ERROR 12-05 16:14:46 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4066548)[0;0m ERROR 12-05 16:14:46 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066548)[0;0m ERROR 12-05 16:14:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4066548)[0;0m ERROR 12-05 16:14:46 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4066548)[0;0m ERROR 12-05 16:14:46 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066548)[0;0m ERROR 12-05 16:14:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4066548)[0;0m ERROR 12-05 16:14:46 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4066548)[0;0m ERROR 12-05 16:14:46 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066548)[0;0m ERROR 12-05 16:14:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4066548)[0;0m ERROR 12-05 16:14:46 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4066548)[0;0m ERROR 12-05 16:14:46 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066548)[0;0m ERROR 12-05 16:14:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4066548)[0;0m ERROR 12-05 16:14:46 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4066548)[0;0m ERROR 12-05 16:14:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4066548)[0;0m ERROR 12-05 16:14:46 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4066548)[0;0m ERROR 12-05 16:14:46 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066548)[0;0m ERROR 12-05 16:14:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4066548)[0;0m ERROR 12-05 16:14:46 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4066548)[0;0m ERROR 12-05 16:14:46 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066548)[0;0m ERROR 12-05 16:14:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4066548)[0;0m ERROR 12-05 16:14:46 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4066548)[0;0m ERROR 12-05 16:14:46 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066548)[0;0m ERROR 12-05 16:14:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4066548)[0;0m ERROR 12-05 16:14:46 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4066548)[0;0m ERROR 12-05 16:14:46 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066548)[0;0m ERROR 12-05 16:14:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4066548)[0;0m ERROR 12-05 16:14:46 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4066548)[0;0m ERROR 12-05 16:14:46 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066548)[0;0m ERROR 12-05 16:14:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4066548)[0;0m ERROR 12-05 16:14:46 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4066548)[0;0m ERROR 12-05 16:14:46 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066548)[0;0m ERROR 12-05 16:14:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4066548)[0;0m ERROR 12-05 16:14:46 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4066548)[0;0m ERROR 12-05 16:14:46 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066548)[0;0m ERROR 12-05 16:14:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4066548)[0;0m ERROR 12-05 16:14:46 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4066548)[0;0m ERROR 12-05 16:14:46 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066548)[0;0m ERROR 12-05 16:14:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4066548)[0;0m ERROR 12-05 16:14:46 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4066548)[0;0m ERROR 12-05 16:14:46 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066548)[0;0m ERROR 12-05 16:14:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4066548)[0;0m ERROR 12-05 16:14:46 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4066548)[0;0m ERROR 12-05 16:14:46 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066548)[0;0m ERROR 12-05 16:14:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4066548)[0;0m ERROR 12-05 16:14:46 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4066548)[0;0m ERROR 12-05 16:14:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4066548)[0;0m ERROR 12-05 16:14:46 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4066548)[0;0m ERROR 12-05 16:14:46 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066548)[0;0m ERROR 12-05 16:14:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4066548)[0;0m ERROR 12-05 16:14:46 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4066548)[0;0m ERROR 12-05 16:14:46 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066548)[0;0m ERROR 12-05 16:14:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4066548)[0;0m ERROR 12-05 16:14:46 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4066548)[0;0m ERROR 12-05 16:14:46 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066548)[0;0m ERROR 12-05 16:14:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4066548)[0;0m ERROR 12-05 16:14:46 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4066548)[0;0m ERROR 12-05 16:14:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4066548)[0;0m ERROR 12-05 16:14:46 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4066548)[0;0m ERROR 12-05 16:14:46 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066548)[0;0m ERROR 12-05 16:14:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4066548)[0;0m ERROR 12-05 16:14:46 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4066548)[0;0m ERROR 12-05 16:14:46 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066548)[0;0m ERROR 12-05 16:14:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4066548)[0;0m ERROR 12-05 16:14:46 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4066548)[0;0m ERROR 12-05 16:14:46 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066548)[0;0m ERROR 12-05 16:14:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4066548)[0;0m ERROR 12-05 16:14:46 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4066548)[0;0m ERROR 12-05 16:14:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4066548)[0;0m ERROR 12-05 16:14:46 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4066548)[0;0m ERROR 12-05 16:14:46 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066548)[0;0m ERROR 12-05 16:14:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4066548)[0;0m ERROR 12-05 16:14:46 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4066548)[0;0m ERROR 12-05 16:14:46 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066548)[0;0m ERROR 12-05 16:14:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4066548)[0;0m ERROR 12-05 16:14:46 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4066548)[0;0m ERROR 12-05 16:14:46 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066548)[0;0m ERROR 12-05 16:14:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4066548)[0;0m ERROR 12-05 16:14:46 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4066548)[0;0m ERROR 12-05 16:14:46 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066548)[0;0m ERROR 12-05 16:14:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4066548)[0;0m ERROR 12-05 16:14:46 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4066548)[0;0m ERROR 12-05 16:14:46 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7fcc2016c3e0>' raised:
[1;36m(EngineCore_DP0 pid=4066548)[0;0m ERROR 12-05 16:14:46 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4066548)[0;0m ERROR 12-05 16:14:46 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4066548)[0;0m ERROR 12-05 16:14:46 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4066548)[0;0m ERROR 12-05 16:14:46 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 16:15:08 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 16:15:08 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 16:15:08 [model.py:1745] Using max model len 131072
INFO 12-05 16:15:08 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4066635)[0;0m INFO 12-05 16:15:15 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4066635)[0;0m INFO 12-05 16:15:16 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:40871 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4066635)[0;0m INFO 12-05 16:15:16 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4066635)[0;0m INFO 12-05 16:15:16 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4066635)[0;0m INFO 12-05 16:15:16 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4066635)[0;0m INFO 12-05 16:15:16 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4066635)[0;0m INFO 12-05 16:15:18 [default_loader.py:314] Loading weights took 0.87 seconds
[1;36m(EngineCore_DP0 pid=4066635)[0;0m INFO 12-05 16:15:18 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.424733 seconds
[1;36m(EngineCore_DP0 pid=4066635)[0;0m INFO 12-05 16:15:22 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4066635)[0;0m INFO 12-05 16:15:22 [backends.py:647] Dynamo bytecode transform time: 3.68 s
[1;36m(EngineCore_DP0 pid=4066635)[0;0m ERROR 12-05 16:15:22 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4066635)[0;0m ERROR 12-05 16:15:22 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4066635)[0;0m ERROR 12-05 16:15:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4066635)[0;0m ERROR 12-05 16:15:22 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4066635)[0;0m ERROR 12-05 16:15:22 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066635)[0;0m ERROR 12-05 16:15:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4066635)[0;0m ERROR 12-05 16:15:22 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4066635)[0;0m ERROR 12-05 16:15:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4066635)[0;0m ERROR 12-05 16:15:22 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4066635)[0;0m ERROR 12-05 16:15:22 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066635)[0;0m ERROR 12-05 16:15:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4066635)[0;0m ERROR 12-05 16:15:22 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4066635)[0;0m ERROR 12-05 16:15:22 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066635)[0;0m ERROR 12-05 16:15:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4066635)[0;0m ERROR 12-05 16:15:22 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4066635)[0;0m ERROR 12-05 16:15:22 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066635)[0;0m ERROR 12-05 16:15:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4066635)[0;0m ERROR 12-05 16:15:22 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4066635)[0;0m ERROR 12-05 16:15:22 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066635)[0;0m ERROR 12-05 16:15:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4066635)[0;0m ERROR 12-05 16:15:22 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4066635)[0;0m ERROR 12-05 16:15:22 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066635)[0;0m ERROR 12-05 16:15:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4066635)[0;0m ERROR 12-05 16:15:22 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4066635)[0;0m ERROR 12-05 16:15:22 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066635)[0;0m ERROR 12-05 16:15:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4066635)[0;0m ERROR 12-05 16:15:22 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4066635)[0;0m ERROR 12-05 16:15:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4066635)[0;0m ERROR 12-05 16:15:22 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4066635)[0;0m ERROR 12-05 16:15:22 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066635)[0;0m ERROR 12-05 16:15:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4066635)[0;0m ERROR 12-05 16:15:22 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4066635)[0;0m ERROR 12-05 16:15:22 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066635)[0;0m ERROR 12-05 16:15:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4066635)[0;0m ERROR 12-05 16:15:22 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4066635)[0;0m ERROR 12-05 16:15:22 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066635)[0;0m ERROR 12-05 16:15:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4066635)[0;0m ERROR 12-05 16:15:22 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4066635)[0;0m ERROR 12-05 16:15:22 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066635)[0;0m ERROR 12-05 16:15:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4066635)[0;0m ERROR 12-05 16:15:22 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4066635)[0;0m ERROR 12-05 16:15:22 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066635)[0;0m ERROR 12-05 16:15:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4066635)[0;0m ERROR 12-05 16:15:22 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4066635)[0;0m ERROR 12-05 16:15:22 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066635)[0;0m ERROR 12-05 16:15:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4066635)[0;0m ERROR 12-05 16:15:22 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4066635)[0;0m ERROR 12-05 16:15:22 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066635)[0;0m ERROR 12-05 16:15:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4066635)[0;0m ERROR 12-05 16:15:22 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4066635)[0;0m ERROR 12-05 16:15:22 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066635)[0;0m ERROR 12-05 16:15:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4066635)[0;0m ERROR 12-05 16:15:22 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4066635)[0;0m ERROR 12-05 16:15:22 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066635)[0;0m ERROR 12-05 16:15:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4066635)[0;0m ERROR 12-05 16:15:22 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4066635)[0;0m ERROR 12-05 16:15:22 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066635)[0;0m ERROR 12-05 16:15:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4066635)[0;0m ERROR 12-05 16:15:22 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4066635)[0;0m ERROR 12-05 16:15:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4066635)[0;0m ERROR 12-05 16:15:22 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4066635)[0;0m ERROR 12-05 16:15:22 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066635)[0;0m ERROR 12-05 16:15:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4066635)[0;0m ERROR 12-05 16:15:22 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4066635)[0;0m ERROR 12-05 16:15:22 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066635)[0;0m ERROR 12-05 16:15:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4066635)[0;0m ERROR 12-05 16:15:22 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4066635)[0;0m ERROR 12-05 16:15:22 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066635)[0;0m ERROR 12-05 16:15:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4066635)[0;0m ERROR 12-05 16:15:22 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4066635)[0;0m ERROR 12-05 16:15:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4066635)[0;0m ERROR 12-05 16:15:22 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4066635)[0;0m ERROR 12-05 16:15:22 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066635)[0;0m ERROR 12-05 16:15:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4066635)[0;0m ERROR 12-05 16:15:22 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4066635)[0;0m ERROR 12-05 16:15:22 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066635)[0;0m ERROR 12-05 16:15:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4066635)[0;0m ERROR 12-05 16:15:22 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4066635)[0;0m ERROR 12-05 16:15:22 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066635)[0;0m ERROR 12-05 16:15:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4066635)[0;0m ERROR 12-05 16:15:22 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4066635)[0;0m ERROR 12-05 16:15:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4066635)[0;0m ERROR 12-05 16:15:22 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4066635)[0;0m ERROR 12-05 16:15:22 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066635)[0;0m ERROR 12-05 16:15:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4066635)[0;0m ERROR 12-05 16:15:22 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4066635)[0;0m ERROR 12-05 16:15:22 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066635)[0;0m ERROR 12-05 16:15:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4066635)[0;0m ERROR 12-05 16:15:22 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4066635)[0;0m ERROR 12-05 16:15:22 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066635)[0;0m ERROR 12-05 16:15:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4066635)[0;0m ERROR 12-05 16:15:22 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4066635)[0;0m ERROR 12-05 16:15:22 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066635)[0;0m ERROR 12-05 16:15:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4066635)[0;0m ERROR 12-05 16:15:22 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4066635)[0;0m ERROR 12-05 16:15:22 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7f0250cbe690>' raised:
[1;36m(EngineCore_DP0 pid=4066635)[0;0m ERROR 12-05 16:15:22 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4066635)[0;0m ERROR 12-05 16:15:22 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4066635)[0;0m ERROR 12-05 16:15:22 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4066635)[0;0m ERROR 12-05 16:15:22 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 16:15:44 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 16:15:44 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 16:15:44 [model.py:1745] Using max model len 131072
INFO 12-05 16:15:44 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4066715)[0;0m INFO 12-05 16:15:52 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4066715)[0;0m INFO 12-05 16:15:53 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:59775 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4066715)[0;0m INFO 12-05 16:15:53 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4066715)[0;0m INFO 12-05 16:15:53 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4066715)[0;0m INFO 12-05 16:15:53 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4066715)[0;0m INFO 12-05 16:15:53 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4066715)[0;0m INFO 12-05 16:15:55 [default_loader.py:314] Loading weights took 0.87 seconds
[1;36m(EngineCore_DP0 pid=4066715)[0;0m INFO 12-05 16:15:55 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.434197 seconds
[1;36m(EngineCore_DP0 pid=4066715)[0;0m INFO 12-05 16:15:59 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4066715)[0;0m INFO 12-05 16:15:59 [backends.py:647] Dynamo bytecode transform time: 3.68 s
[1;36m(EngineCore_DP0 pid=4066715)[0;0m ERROR 12-05 16:15:59 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4066715)[0;0m ERROR 12-05 16:15:59 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4066715)[0;0m ERROR 12-05 16:15:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4066715)[0;0m ERROR 12-05 16:15:59 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4066715)[0;0m ERROR 12-05 16:15:59 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066715)[0;0m ERROR 12-05 16:15:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4066715)[0;0m ERROR 12-05 16:15:59 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4066715)[0;0m ERROR 12-05 16:15:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4066715)[0;0m ERROR 12-05 16:15:59 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4066715)[0;0m ERROR 12-05 16:15:59 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066715)[0;0m ERROR 12-05 16:15:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4066715)[0;0m ERROR 12-05 16:15:59 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4066715)[0;0m ERROR 12-05 16:15:59 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066715)[0;0m ERROR 12-05 16:15:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4066715)[0;0m ERROR 12-05 16:15:59 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4066715)[0;0m ERROR 12-05 16:15:59 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066715)[0;0m ERROR 12-05 16:15:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4066715)[0;0m ERROR 12-05 16:15:59 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4066715)[0;0m ERROR 12-05 16:15:59 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066715)[0;0m ERROR 12-05 16:15:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4066715)[0;0m ERROR 12-05 16:15:59 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4066715)[0;0m ERROR 12-05 16:15:59 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066715)[0;0m ERROR 12-05 16:15:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4066715)[0;0m ERROR 12-05 16:15:59 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4066715)[0;0m ERROR 12-05 16:15:59 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066715)[0;0m ERROR 12-05 16:15:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4066715)[0;0m ERROR 12-05 16:15:59 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4066715)[0;0m ERROR 12-05 16:15:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4066715)[0;0m ERROR 12-05 16:15:59 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4066715)[0;0m ERROR 12-05 16:15:59 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066715)[0;0m ERROR 12-05 16:15:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4066715)[0;0m ERROR 12-05 16:15:59 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4066715)[0;0m ERROR 12-05 16:15:59 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066715)[0;0m ERROR 12-05 16:15:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4066715)[0;0m ERROR 12-05 16:15:59 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4066715)[0;0m ERROR 12-05 16:15:59 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066715)[0;0m ERROR 12-05 16:15:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4066715)[0;0m ERROR 12-05 16:15:59 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4066715)[0;0m ERROR 12-05 16:15:59 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066715)[0;0m ERROR 12-05 16:15:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4066715)[0;0m ERROR 12-05 16:15:59 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4066715)[0;0m ERROR 12-05 16:15:59 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066715)[0;0m ERROR 12-05 16:15:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4066715)[0;0m ERROR 12-05 16:15:59 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4066715)[0;0m ERROR 12-05 16:15:59 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066715)[0;0m ERROR 12-05 16:15:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4066715)[0;0m ERROR 12-05 16:15:59 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4066715)[0;0m ERROR 12-05 16:15:59 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066715)[0;0m ERROR 12-05 16:15:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4066715)[0;0m ERROR 12-05 16:15:59 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4066715)[0;0m ERROR 12-05 16:15:59 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066715)[0;0m ERROR 12-05 16:15:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4066715)[0;0m ERROR 12-05 16:15:59 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4066715)[0;0m ERROR 12-05 16:15:59 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066715)[0;0m ERROR 12-05 16:15:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4066715)[0;0m ERROR 12-05 16:15:59 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4066715)[0;0m ERROR 12-05 16:15:59 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066715)[0;0m ERROR 12-05 16:15:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4066715)[0;0m ERROR 12-05 16:15:59 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4066715)[0;0m ERROR 12-05 16:15:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4066715)[0;0m ERROR 12-05 16:15:59 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4066715)[0;0m ERROR 12-05 16:15:59 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066715)[0;0m ERROR 12-05 16:15:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4066715)[0;0m ERROR 12-05 16:15:59 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4066715)[0;0m ERROR 12-05 16:15:59 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066715)[0;0m ERROR 12-05 16:15:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4066715)[0;0m ERROR 12-05 16:15:59 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4066715)[0;0m ERROR 12-05 16:15:59 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066715)[0;0m ERROR 12-05 16:15:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4066715)[0;0m ERROR 12-05 16:15:59 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4066715)[0;0m ERROR 12-05 16:15:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4066715)[0;0m ERROR 12-05 16:15:59 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4066715)[0;0m ERROR 12-05 16:15:59 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066715)[0;0m ERROR 12-05 16:15:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4066715)[0;0m ERROR 12-05 16:15:59 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4066715)[0;0m ERROR 12-05 16:15:59 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066715)[0;0m ERROR 12-05 16:15:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4066715)[0;0m ERROR 12-05 16:15:59 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4066715)[0;0m ERROR 12-05 16:15:59 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066715)[0;0m ERROR 12-05 16:15:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4066715)[0;0m ERROR 12-05 16:15:59 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4066715)[0;0m ERROR 12-05 16:15:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4066715)[0;0m ERROR 12-05 16:15:59 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4066715)[0;0m ERROR 12-05 16:15:59 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066715)[0;0m ERROR 12-05 16:15:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4066715)[0;0m ERROR 12-05 16:15:59 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4066715)[0;0m ERROR 12-05 16:15:59 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066715)[0;0m ERROR 12-05 16:15:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4066715)[0;0m ERROR 12-05 16:15:59 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4066715)[0;0m ERROR 12-05 16:15:59 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066715)[0;0m ERROR 12-05 16:15:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4066715)[0;0m ERROR 12-05 16:15:59 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4066715)[0;0m ERROR 12-05 16:15:59 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066715)[0;0m ERROR 12-05 16:15:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4066715)[0;0m ERROR 12-05 16:15:59 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4066715)[0;0m ERROR 12-05 16:15:59 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7fe4b0187860>' raised:
[1;36m(EngineCore_DP0 pid=4066715)[0;0m ERROR 12-05 16:15:59 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4066715)[0;0m ERROR 12-05 16:15:59 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4066715)[0;0m ERROR 12-05 16:15:59 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4066715)[0;0m ERROR 12-05 16:15:59 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 16:16:20 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 16:16:21 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 16:16:21 [model.py:1745] Using max model len 131072
INFO 12-05 16:16:21 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4066900)[0;0m INFO 12-05 16:16:28 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4066900)[0;0m INFO 12-05 16:16:28 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:39267 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4066900)[0;0m INFO 12-05 16:16:28 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4066900)[0;0m INFO 12-05 16:16:29 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4066900)[0;0m INFO 12-05 16:16:29 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4066900)[0;0m INFO 12-05 16:16:29 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4066900)[0;0m INFO 12-05 16:16:30 [default_loader.py:314] Loading weights took 0.88 seconds
[1;36m(EngineCore_DP0 pid=4066900)[0;0m INFO 12-05 16:16:31 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.366072 seconds
[1;36m(EngineCore_DP0 pid=4066900)[0;0m INFO 12-05 16:16:35 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4066900)[0;0m INFO 12-05 16:16:35 [backends.py:647] Dynamo bytecode transform time: 3.69 s
[1;36m(EngineCore_DP0 pid=4066900)[0;0m ERROR 12-05 16:16:35 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4066900)[0;0m ERROR 12-05 16:16:35 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4066900)[0;0m ERROR 12-05 16:16:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4066900)[0;0m ERROR 12-05 16:16:35 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4066900)[0;0m ERROR 12-05 16:16:35 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066900)[0;0m ERROR 12-05 16:16:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4066900)[0;0m ERROR 12-05 16:16:35 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4066900)[0;0m ERROR 12-05 16:16:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4066900)[0;0m ERROR 12-05 16:16:35 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4066900)[0;0m ERROR 12-05 16:16:35 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066900)[0;0m ERROR 12-05 16:16:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4066900)[0;0m ERROR 12-05 16:16:35 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4066900)[0;0m ERROR 12-05 16:16:35 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066900)[0;0m ERROR 12-05 16:16:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4066900)[0;0m ERROR 12-05 16:16:35 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4066900)[0;0m ERROR 12-05 16:16:35 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066900)[0;0m ERROR 12-05 16:16:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4066900)[0;0m ERROR 12-05 16:16:35 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4066900)[0;0m ERROR 12-05 16:16:35 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066900)[0;0m ERROR 12-05 16:16:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4066900)[0;0m ERROR 12-05 16:16:35 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4066900)[0;0m ERROR 12-05 16:16:35 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066900)[0;0m ERROR 12-05 16:16:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4066900)[0;0m ERROR 12-05 16:16:35 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4066900)[0;0m ERROR 12-05 16:16:35 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066900)[0;0m ERROR 12-05 16:16:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4066900)[0;0m ERROR 12-05 16:16:35 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4066900)[0;0m ERROR 12-05 16:16:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4066900)[0;0m ERROR 12-05 16:16:35 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4066900)[0;0m ERROR 12-05 16:16:35 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066900)[0;0m ERROR 12-05 16:16:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4066900)[0;0m ERROR 12-05 16:16:35 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4066900)[0;0m ERROR 12-05 16:16:35 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066900)[0;0m ERROR 12-05 16:16:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4066900)[0;0m ERROR 12-05 16:16:35 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4066900)[0;0m ERROR 12-05 16:16:35 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066900)[0;0m ERROR 12-05 16:16:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4066900)[0;0m ERROR 12-05 16:16:35 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4066900)[0;0m ERROR 12-05 16:16:35 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066900)[0;0m ERROR 12-05 16:16:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4066900)[0;0m ERROR 12-05 16:16:35 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4066900)[0;0m ERROR 12-05 16:16:35 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066900)[0;0m ERROR 12-05 16:16:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4066900)[0;0m ERROR 12-05 16:16:35 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4066900)[0;0m ERROR 12-05 16:16:35 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066900)[0;0m ERROR 12-05 16:16:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4066900)[0;0m ERROR 12-05 16:16:35 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4066900)[0;0m ERROR 12-05 16:16:35 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066900)[0;0m ERROR 12-05 16:16:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4066900)[0;0m ERROR 12-05 16:16:35 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4066900)[0;0m ERROR 12-05 16:16:35 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066900)[0;0m ERROR 12-05 16:16:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4066900)[0;0m ERROR 12-05 16:16:35 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4066900)[0;0m ERROR 12-05 16:16:35 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066900)[0;0m ERROR 12-05 16:16:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4066900)[0;0m ERROR 12-05 16:16:35 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4066900)[0;0m ERROR 12-05 16:16:35 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066900)[0;0m ERROR 12-05 16:16:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4066900)[0;0m ERROR 12-05 16:16:35 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4066900)[0;0m ERROR 12-05 16:16:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4066900)[0;0m ERROR 12-05 16:16:35 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4066900)[0;0m ERROR 12-05 16:16:35 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066900)[0;0m ERROR 12-05 16:16:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4066900)[0;0m ERROR 12-05 16:16:35 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4066900)[0;0m ERROR 12-05 16:16:35 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066900)[0;0m ERROR 12-05 16:16:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4066900)[0;0m ERROR 12-05 16:16:35 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4066900)[0;0m ERROR 12-05 16:16:35 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066900)[0;0m ERROR 12-05 16:16:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4066900)[0;0m ERROR 12-05 16:16:35 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4066900)[0;0m ERROR 12-05 16:16:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4066900)[0;0m ERROR 12-05 16:16:35 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4066900)[0;0m ERROR 12-05 16:16:35 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066900)[0;0m ERROR 12-05 16:16:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4066900)[0;0m ERROR 12-05 16:16:35 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4066900)[0;0m ERROR 12-05 16:16:35 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066900)[0;0m ERROR 12-05 16:16:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4066900)[0;0m ERROR 12-05 16:16:35 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4066900)[0;0m ERROR 12-05 16:16:35 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066900)[0;0m ERROR 12-05 16:16:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4066900)[0;0m ERROR 12-05 16:16:35 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4066900)[0;0m ERROR 12-05 16:16:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4066900)[0;0m ERROR 12-05 16:16:35 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4066900)[0;0m ERROR 12-05 16:16:35 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066900)[0;0m ERROR 12-05 16:16:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4066900)[0;0m ERROR 12-05 16:16:35 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4066900)[0;0m ERROR 12-05 16:16:35 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066900)[0;0m ERROR 12-05 16:16:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4066900)[0;0m ERROR 12-05 16:16:35 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4066900)[0;0m ERROR 12-05 16:16:35 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066900)[0;0m ERROR 12-05 16:16:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4066900)[0;0m ERROR 12-05 16:16:35 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4066900)[0;0m ERROR 12-05 16:16:35 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066900)[0;0m ERROR 12-05 16:16:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4066900)[0;0m ERROR 12-05 16:16:35 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4066900)[0;0m ERROR 12-05 16:16:35 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7f39d423ea80>' raised:
[1;36m(EngineCore_DP0 pid=4066900)[0;0m ERROR 12-05 16:16:35 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4066900)[0;0m ERROR 12-05 16:16:35 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4066900)[0;0m ERROR 12-05 16:16:35 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4066900)[0;0m ERROR 12-05 16:16:35 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 16:16:56 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 16:16:56 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 16:16:56 [model.py:1745] Using max model len 131072
INFO 12-05 16:16:56 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4066987)[0;0m INFO 12-05 16:17:05 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4066987)[0;0m INFO 12-05 16:17:06 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:57695 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4066987)[0;0m INFO 12-05 16:17:06 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4066987)[0;0m INFO 12-05 16:17:06 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4066987)[0;0m INFO 12-05 16:17:07 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4066987)[0;0m INFO 12-05 16:17:07 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4066987)[0;0m INFO 12-05 16:17:08 [default_loader.py:314] Loading weights took 0.87 seconds
[1;36m(EngineCore_DP0 pid=4066987)[0;0m INFO 12-05 16:17:08 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.424077 seconds
[1;36m(EngineCore_DP0 pid=4066987)[0;0m INFO 12-05 16:17:12 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4066987)[0;0m INFO 12-05 16:17:12 [backends.py:647] Dynamo bytecode transform time: 3.68 s
[1;36m(EngineCore_DP0 pid=4066987)[0;0m ERROR 12-05 16:17:12 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4066987)[0;0m ERROR 12-05 16:17:12 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4066987)[0;0m ERROR 12-05 16:17:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4066987)[0;0m ERROR 12-05 16:17:12 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4066987)[0;0m ERROR 12-05 16:17:12 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066987)[0;0m ERROR 12-05 16:17:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4066987)[0;0m ERROR 12-05 16:17:12 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4066987)[0;0m ERROR 12-05 16:17:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4066987)[0;0m ERROR 12-05 16:17:12 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4066987)[0;0m ERROR 12-05 16:17:12 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066987)[0;0m ERROR 12-05 16:17:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4066987)[0;0m ERROR 12-05 16:17:12 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4066987)[0;0m ERROR 12-05 16:17:12 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066987)[0;0m ERROR 12-05 16:17:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4066987)[0;0m ERROR 12-05 16:17:12 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4066987)[0;0m ERROR 12-05 16:17:12 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066987)[0;0m ERROR 12-05 16:17:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4066987)[0;0m ERROR 12-05 16:17:12 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4066987)[0;0m ERROR 12-05 16:17:12 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066987)[0;0m ERROR 12-05 16:17:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4066987)[0;0m ERROR 12-05 16:17:12 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4066987)[0;0m ERROR 12-05 16:17:12 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066987)[0;0m ERROR 12-05 16:17:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4066987)[0;0m ERROR 12-05 16:17:12 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4066987)[0;0m ERROR 12-05 16:17:12 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066987)[0;0m ERROR 12-05 16:17:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4066987)[0;0m ERROR 12-05 16:17:12 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4066987)[0;0m ERROR 12-05 16:17:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4066987)[0;0m ERROR 12-05 16:17:12 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4066987)[0;0m ERROR 12-05 16:17:12 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066987)[0;0m ERROR 12-05 16:17:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4066987)[0;0m ERROR 12-05 16:17:12 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4066987)[0;0m ERROR 12-05 16:17:12 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066987)[0;0m ERROR 12-05 16:17:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4066987)[0;0m ERROR 12-05 16:17:12 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4066987)[0;0m ERROR 12-05 16:17:12 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066987)[0;0m ERROR 12-05 16:17:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4066987)[0;0m ERROR 12-05 16:17:12 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4066987)[0;0m ERROR 12-05 16:17:12 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066987)[0;0m ERROR 12-05 16:17:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4066987)[0;0m ERROR 12-05 16:17:12 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4066987)[0;0m ERROR 12-05 16:17:12 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066987)[0;0m ERROR 12-05 16:17:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4066987)[0;0m ERROR 12-05 16:17:12 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4066987)[0;0m ERROR 12-05 16:17:12 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066987)[0;0m ERROR 12-05 16:17:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4066987)[0;0m ERROR 12-05 16:17:12 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4066987)[0;0m ERROR 12-05 16:17:12 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066987)[0;0m ERROR 12-05 16:17:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4066987)[0;0m ERROR 12-05 16:17:12 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4066987)[0;0m ERROR 12-05 16:17:12 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066987)[0;0m ERROR 12-05 16:17:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4066987)[0;0m ERROR 12-05 16:17:12 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4066987)[0;0m ERROR 12-05 16:17:12 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066987)[0;0m ERROR 12-05 16:17:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4066987)[0;0m ERROR 12-05 16:17:12 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4066987)[0;0m ERROR 12-05 16:17:12 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066987)[0;0m ERROR 12-05 16:17:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4066987)[0;0m ERROR 12-05 16:17:12 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4066987)[0;0m ERROR 12-05 16:17:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4066987)[0;0m ERROR 12-05 16:17:12 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4066987)[0;0m ERROR 12-05 16:17:12 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066987)[0;0m ERROR 12-05 16:17:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4066987)[0;0m ERROR 12-05 16:17:12 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4066987)[0;0m ERROR 12-05 16:17:12 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066987)[0;0m ERROR 12-05 16:17:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4066987)[0;0m ERROR 12-05 16:17:12 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4066987)[0;0m ERROR 12-05 16:17:12 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066987)[0;0m ERROR 12-05 16:17:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4066987)[0;0m ERROR 12-05 16:17:12 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4066987)[0;0m ERROR 12-05 16:17:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4066987)[0;0m ERROR 12-05 16:17:12 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4066987)[0;0m ERROR 12-05 16:17:12 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066987)[0;0m ERROR 12-05 16:17:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4066987)[0;0m ERROR 12-05 16:17:12 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4066987)[0;0m ERROR 12-05 16:17:12 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066987)[0;0m ERROR 12-05 16:17:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4066987)[0;0m ERROR 12-05 16:17:12 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4066987)[0;0m ERROR 12-05 16:17:12 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066987)[0;0m ERROR 12-05 16:17:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4066987)[0;0m ERROR 12-05 16:17:12 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4066987)[0;0m ERROR 12-05 16:17:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4066987)[0;0m ERROR 12-05 16:17:12 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4066987)[0;0m ERROR 12-05 16:17:12 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066987)[0;0m ERROR 12-05 16:17:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4066987)[0;0m ERROR 12-05 16:17:12 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4066987)[0;0m ERROR 12-05 16:17:12 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066987)[0;0m ERROR 12-05 16:17:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4066987)[0;0m ERROR 12-05 16:17:12 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4066987)[0;0m ERROR 12-05 16:17:12 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066987)[0;0m ERROR 12-05 16:17:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4066987)[0;0m ERROR 12-05 16:17:12 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4066987)[0;0m ERROR 12-05 16:17:12 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4066987)[0;0m ERROR 12-05 16:17:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4066987)[0;0m ERROR 12-05 16:17:12 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4066987)[0;0m ERROR 12-05 16:17:12 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7faeac244680>' raised:
[1;36m(EngineCore_DP0 pid=4066987)[0;0m ERROR 12-05 16:17:12 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4066987)[0;0m ERROR 12-05 16:17:12 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4066987)[0;0m ERROR 12-05 16:17:12 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4066987)[0;0m ERROR 12-05 16:17:12 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 16:17:34 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 16:17:34 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 16:17:34 [model.py:1745] Using max model len 131072
INFO 12-05 16:17:34 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4067068)[0;0m INFO 12-05 16:17:41 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4067068)[0;0m INFO 12-05 16:17:42 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:50383 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4067068)[0;0m INFO 12-05 16:17:42 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4067068)[0;0m INFO 12-05 16:17:42 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4067068)[0;0m INFO 12-05 16:17:43 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4067068)[0;0m INFO 12-05 16:17:43 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4067068)[0;0m INFO 12-05 16:17:44 [default_loader.py:314] Loading weights took 0.88 seconds
[1;36m(EngineCore_DP0 pid=4067068)[0;0m INFO 12-05 16:17:44 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.397966 seconds
[1;36m(EngineCore_DP0 pid=4067068)[0;0m INFO 12-05 16:17:48 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4067068)[0;0m INFO 12-05 16:17:48 [backends.py:647] Dynamo bytecode transform time: 3.74 s
[1;36m(EngineCore_DP0 pid=4067068)[0;0m ERROR 12-05 16:17:48 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4067068)[0;0m ERROR 12-05 16:17:48 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4067068)[0;0m ERROR 12-05 16:17:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4067068)[0;0m ERROR 12-05 16:17:48 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4067068)[0;0m ERROR 12-05 16:17:48 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067068)[0;0m ERROR 12-05 16:17:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4067068)[0;0m ERROR 12-05 16:17:48 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4067068)[0;0m ERROR 12-05 16:17:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4067068)[0;0m ERROR 12-05 16:17:48 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4067068)[0;0m ERROR 12-05 16:17:48 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067068)[0;0m ERROR 12-05 16:17:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4067068)[0;0m ERROR 12-05 16:17:48 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4067068)[0;0m ERROR 12-05 16:17:48 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067068)[0;0m ERROR 12-05 16:17:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4067068)[0;0m ERROR 12-05 16:17:48 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4067068)[0;0m ERROR 12-05 16:17:48 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067068)[0;0m ERROR 12-05 16:17:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4067068)[0;0m ERROR 12-05 16:17:48 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4067068)[0;0m ERROR 12-05 16:17:48 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067068)[0;0m ERROR 12-05 16:17:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4067068)[0;0m ERROR 12-05 16:17:48 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4067068)[0;0m ERROR 12-05 16:17:48 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067068)[0;0m ERROR 12-05 16:17:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4067068)[0;0m ERROR 12-05 16:17:48 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4067068)[0;0m ERROR 12-05 16:17:48 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067068)[0;0m ERROR 12-05 16:17:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4067068)[0;0m ERROR 12-05 16:17:48 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4067068)[0;0m ERROR 12-05 16:17:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4067068)[0;0m ERROR 12-05 16:17:48 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4067068)[0;0m ERROR 12-05 16:17:48 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067068)[0;0m ERROR 12-05 16:17:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4067068)[0;0m ERROR 12-05 16:17:48 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4067068)[0;0m ERROR 12-05 16:17:48 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067068)[0;0m ERROR 12-05 16:17:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4067068)[0;0m ERROR 12-05 16:17:48 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4067068)[0;0m ERROR 12-05 16:17:48 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067068)[0;0m ERROR 12-05 16:17:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4067068)[0;0m ERROR 12-05 16:17:48 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4067068)[0;0m ERROR 12-05 16:17:48 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067068)[0;0m ERROR 12-05 16:17:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4067068)[0;0m ERROR 12-05 16:17:48 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4067068)[0;0m ERROR 12-05 16:17:48 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067068)[0;0m ERROR 12-05 16:17:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4067068)[0;0m ERROR 12-05 16:17:48 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4067068)[0;0m ERROR 12-05 16:17:48 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067068)[0;0m ERROR 12-05 16:17:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4067068)[0;0m ERROR 12-05 16:17:48 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4067068)[0;0m ERROR 12-05 16:17:48 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067068)[0;0m ERROR 12-05 16:17:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4067068)[0;0m ERROR 12-05 16:17:48 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4067068)[0;0m ERROR 12-05 16:17:48 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067068)[0;0m ERROR 12-05 16:17:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4067068)[0;0m ERROR 12-05 16:17:48 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4067068)[0;0m ERROR 12-05 16:17:48 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067068)[0;0m ERROR 12-05 16:17:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4067068)[0;0m ERROR 12-05 16:17:48 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4067068)[0;0m ERROR 12-05 16:17:48 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067068)[0;0m ERROR 12-05 16:17:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4067068)[0;0m ERROR 12-05 16:17:48 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4067068)[0;0m ERROR 12-05 16:17:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4067068)[0;0m ERROR 12-05 16:17:48 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4067068)[0;0m ERROR 12-05 16:17:48 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067068)[0;0m ERROR 12-05 16:17:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4067068)[0;0m ERROR 12-05 16:17:48 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4067068)[0;0m ERROR 12-05 16:17:48 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067068)[0;0m ERROR 12-05 16:17:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4067068)[0;0m ERROR 12-05 16:17:48 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4067068)[0;0m ERROR 12-05 16:17:48 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067068)[0;0m ERROR 12-05 16:17:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4067068)[0;0m ERROR 12-05 16:17:48 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4067068)[0;0m ERROR 12-05 16:17:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4067068)[0;0m ERROR 12-05 16:17:48 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4067068)[0;0m ERROR 12-05 16:17:48 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067068)[0;0m ERROR 12-05 16:17:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4067068)[0;0m ERROR 12-05 16:17:48 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4067068)[0;0m ERROR 12-05 16:17:48 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067068)[0;0m ERROR 12-05 16:17:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4067068)[0;0m ERROR 12-05 16:17:48 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4067068)[0;0m ERROR 12-05 16:17:48 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067068)[0;0m ERROR 12-05 16:17:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4067068)[0;0m ERROR 12-05 16:17:48 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4067068)[0;0m ERROR 12-05 16:17:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4067068)[0;0m ERROR 12-05 16:17:48 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4067068)[0;0m ERROR 12-05 16:17:48 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067068)[0;0m ERROR 12-05 16:17:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4067068)[0;0m ERROR 12-05 16:17:48 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4067068)[0;0m ERROR 12-05 16:17:48 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067068)[0;0m ERROR 12-05 16:17:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4067068)[0;0m ERROR 12-05 16:17:48 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4067068)[0;0m ERROR 12-05 16:17:48 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067068)[0;0m ERROR 12-05 16:17:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4067068)[0;0m ERROR 12-05 16:17:48 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4067068)[0;0m ERROR 12-05 16:17:48 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067068)[0;0m ERROR 12-05 16:17:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4067068)[0;0m ERROR 12-05 16:17:48 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4067068)[0;0m ERROR 12-05 16:17:48 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7fc7d822b710>' raised:
[1;36m(EngineCore_DP0 pid=4067068)[0;0m ERROR 12-05 16:17:48 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4067068)[0;0m ERROR 12-05 16:17:48 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4067068)[0;0m ERROR 12-05 16:17:48 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4067068)[0;0m ERROR 12-05 16:17:48 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 16:18:10 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 16:18:10 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 16:18:10 [model.py:1745] Using max model len 131072
INFO 12-05 16:18:10 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4067152)[0;0m INFO 12-05 16:18:18 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4067152)[0;0m INFO 12-05 16:18:19 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:36731 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4067152)[0;0m INFO 12-05 16:18:19 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4067152)[0;0m INFO 12-05 16:18:19 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4067152)[0;0m INFO 12-05 16:18:19 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4067152)[0;0m INFO 12-05 16:18:19 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4067152)[0;0m INFO 12-05 16:18:21 [default_loader.py:314] Loading weights took 0.87 seconds
[1;36m(EngineCore_DP0 pid=4067152)[0;0m INFO 12-05 16:18:21 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.414740 seconds
[1;36m(EngineCore_DP0 pid=4067152)[0;0m INFO 12-05 16:18:25 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4067152)[0;0m INFO 12-05 16:18:25 [backends.py:647] Dynamo bytecode transform time: 3.70 s
[1;36m(EngineCore_DP0 pid=4067152)[0;0m ERROR 12-05 16:18:25 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4067152)[0;0m ERROR 12-05 16:18:25 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4067152)[0;0m ERROR 12-05 16:18:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4067152)[0;0m ERROR 12-05 16:18:25 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4067152)[0;0m ERROR 12-05 16:18:25 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067152)[0;0m ERROR 12-05 16:18:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4067152)[0;0m ERROR 12-05 16:18:25 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4067152)[0;0m ERROR 12-05 16:18:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4067152)[0;0m ERROR 12-05 16:18:25 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4067152)[0;0m ERROR 12-05 16:18:25 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067152)[0;0m ERROR 12-05 16:18:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4067152)[0;0m ERROR 12-05 16:18:25 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4067152)[0;0m ERROR 12-05 16:18:25 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067152)[0;0m ERROR 12-05 16:18:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4067152)[0;0m ERROR 12-05 16:18:25 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4067152)[0;0m ERROR 12-05 16:18:25 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067152)[0;0m ERROR 12-05 16:18:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4067152)[0;0m ERROR 12-05 16:18:25 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4067152)[0;0m ERROR 12-05 16:18:25 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067152)[0;0m ERROR 12-05 16:18:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4067152)[0;0m ERROR 12-05 16:18:25 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4067152)[0;0m ERROR 12-05 16:18:25 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067152)[0;0m ERROR 12-05 16:18:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4067152)[0;0m ERROR 12-05 16:18:25 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4067152)[0;0m ERROR 12-05 16:18:25 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067152)[0;0m ERROR 12-05 16:18:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4067152)[0;0m ERROR 12-05 16:18:25 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4067152)[0;0m ERROR 12-05 16:18:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4067152)[0;0m ERROR 12-05 16:18:25 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4067152)[0;0m ERROR 12-05 16:18:25 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067152)[0;0m ERROR 12-05 16:18:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4067152)[0;0m ERROR 12-05 16:18:25 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4067152)[0;0m ERROR 12-05 16:18:25 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067152)[0;0m ERROR 12-05 16:18:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4067152)[0;0m ERROR 12-05 16:18:25 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4067152)[0;0m ERROR 12-05 16:18:25 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067152)[0;0m ERROR 12-05 16:18:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4067152)[0;0m ERROR 12-05 16:18:25 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4067152)[0;0m ERROR 12-05 16:18:25 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067152)[0;0m ERROR 12-05 16:18:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4067152)[0;0m ERROR 12-05 16:18:25 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4067152)[0;0m ERROR 12-05 16:18:25 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067152)[0;0m ERROR 12-05 16:18:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4067152)[0;0m ERROR 12-05 16:18:25 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4067152)[0;0m ERROR 12-05 16:18:25 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067152)[0;0m ERROR 12-05 16:18:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4067152)[0;0m ERROR 12-05 16:18:25 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4067152)[0;0m ERROR 12-05 16:18:25 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067152)[0;0m ERROR 12-05 16:18:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4067152)[0;0m ERROR 12-05 16:18:25 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4067152)[0;0m ERROR 12-05 16:18:25 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067152)[0;0m ERROR 12-05 16:18:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4067152)[0;0m ERROR 12-05 16:18:25 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4067152)[0;0m ERROR 12-05 16:18:25 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067152)[0;0m ERROR 12-05 16:18:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4067152)[0;0m ERROR 12-05 16:18:25 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4067152)[0;0m ERROR 12-05 16:18:25 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067152)[0;0m ERROR 12-05 16:18:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4067152)[0;0m ERROR 12-05 16:18:25 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4067152)[0;0m ERROR 12-05 16:18:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4067152)[0;0m ERROR 12-05 16:18:25 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4067152)[0;0m ERROR 12-05 16:18:25 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067152)[0;0m ERROR 12-05 16:18:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4067152)[0;0m ERROR 12-05 16:18:25 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4067152)[0;0m ERROR 12-05 16:18:25 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067152)[0;0m ERROR 12-05 16:18:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4067152)[0;0m ERROR 12-05 16:18:25 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4067152)[0;0m ERROR 12-05 16:18:25 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067152)[0;0m ERROR 12-05 16:18:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4067152)[0;0m ERROR 12-05 16:18:25 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4067152)[0;0m ERROR 12-05 16:18:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4067152)[0;0m ERROR 12-05 16:18:25 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4067152)[0;0m ERROR 12-05 16:18:25 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067152)[0;0m ERROR 12-05 16:18:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4067152)[0;0m ERROR 12-05 16:18:25 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4067152)[0;0m ERROR 12-05 16:18:25 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067152)[0;0m ERROR 12-05 16:18:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4067152)[0;0m ERROR 12-05 16:18:25 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4067152)[0;0m ERROR 12-05 16:18:25 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067152)[0;0m ERROR 12-05 16:18:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4067152)[0;0m ERROR 12-05 16:18:25 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4067152)[0;0m ERROR 12-05 16:18:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4067152)[0;0m ERROR 12-05 16:18:25 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4067152)[0;0m ERROR 12-05 16:18:25 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067152)[0;0m ERROR 12-05 16:18:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4067152)[0;0m ERROR 12-05 16:18:25 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4067152)[0;0m ERROR 12-05 16:18:25 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067152)[0;0m ERROR 12-05 16:18:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4067152)[0;0m ERROR 12-05 16:18:25 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4067152)[0;0m ERROR 12-05 16:18:25 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067152)[0;0m ERROR 12-05 16:18:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4067152)[0;0m ERROR 12-05 16:18:25 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4067152)[0;0m ERROR 12-05 16:18:25 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067152)[0;0m ERROR 12-05 16:18:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4067152)[0;0m ERROR 12-05 16:18:25 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4067152)[0;0m ERROR 12-05 16:18:25 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7fb8e81d6cf0>' raised:
[1;36m(EngineCore_DP0 pid=4067152)[0;0m ERROR 12-05 16:18:25 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4067152)[0;0m ERROR 12-05 16:18:25 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4067152)[0;0m ERROR 12-05 16:18:25 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4067152)[0;0m ERROR 12-05 16:18:25 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 16:18:47 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 16:18:47 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 16:18:47 [model.py:1745] Using max model len 131072
INFO 12-05 16:18:47 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4067241)[0;0m INFO 12-05 16:18:54 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4067241)[0;0m INFO 12-05 16:18:55 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:57587 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4067241)[0;0m INFO 12-05 16:18:55 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4067241)[0;0m INFO 12-05 16:18:55 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4067241)[0;0m INFO 12-05 16:18:55 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4067241)[0;0m INFO 12-05 16:18:55 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4067241)[0;0m INFO 12-05 16:18:57 [default_loader.py:314] Loading weights took 0.87 seconds
[1;36m(EngineCore_DP0 pid=4067241)[0;0m INFO 12-05 16:18:57 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.407282 seconds
[1;36m(EngineCore_DP0 pid=4067241)[0;0m INFO 12-05 16:19:01 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4067241)[0;0m INFO 12-05 16:19:01 [backends.py:647] Dynamo bytecode transform time: 4.31 s
[1;36m(EngineCore_DP0 pid=4067241)[0;0m ERROR 12-05 16:19:02 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4067241)[0;0m ERROR 12-05 16:19:02 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4067241)[0;0m ERROR 12-05 16:19:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4067241)[0;0m ERROR 12-05 16:19:02 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4067241)[0;0m ERROR 12-05 16:19:02 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067241)[0;0m ERROR 12-05 16:19:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4067241)[0;0m ERROR 12-05 16:19:02 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4067241)[0;0m ERROR 12-05 16:19:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4067241)[0;0m ERROR 12-05 16:19:02 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4067241)[0;0m ERROR 12-05 16:19:02 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067241)[0;0m ERROR 12-05 16:19:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4067241)[0;0m ERROR 12-05 16:19:02 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4067241)[0;0m ERROR 12-05 16:19:02 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067241)[0;0m ERROR 12-05 16:19:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4067241)[0;0m ERROR 12-05 16:19:02 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4067241)[0;0m ERROR 12-05 16:19:02 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067241)[0;0m ERROR 12-05 16:19:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4067241)[0;0m ERROR 12-05 16:19:02 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4067241)[0;0m ERROR 12-05 16:19:02 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067241)[0;0m ERROR 12-05 16:19:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4067241)[0;0m ERROR 12-05 16:19:02 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4067241)[0;0m ERROR 12-05 16:19:02 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067241)[0;0m ERROR 12-05 16:19:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4067241)[0;0m ERROR 12-05 16:19:02 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4067241)[0;0m ERROR 12-05 16:19:02 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067241)[0;0m ERROR 12-05 16:19:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4067241)[0;0m ERROR 12-05 16:19:02 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4067241)[0;0m ERROR 12-05 16:19:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4067241)[0;0m ERROR 12-05 16:19:02 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4067241)[0;0m ERROR 12-05 16:19:02 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067241)[0;0m ERROR 12-05 16:19:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4067241)[0;0m ERROR 12-05 16:19:02 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4067241)[0;0m ERROR 12-05 16:19:02 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067241)[0;0m ERROR 12-05 16:19:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4067241)[0;0m ERROR 12-05 16:19:02 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4067241)[0;0m ERROR 12-05 16:19:02 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067241)[0;0m ERROR 12-05 16:19:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4067241)[0;0m ERROR 12-05 16:19:02 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4067241)[0;0m ERROR 12-05 16:19:02 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067241)[0;0m ERROR 12-05 16:19:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4067241)[0;0m ERROR 12-05 16:19:02 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4067241)[0;0m ERROR 12-05 16:19:02 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067241)[0;0m ERROR 12-05 16:19:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4067241)[0;0m ERROR 12-05 16:19:02 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4067241)[0;0m ERROR 12-05 16:19:02 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067241)[0;0m ERROR 12-05 16:19:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4067241)[0;0m ERROR 12-05 16:19:02 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4067241)[0;0m ERROR 12-05 16:19:02 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067241)[0;0m ERROR 12-05 16:19:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4067241)[0;0m ERROR 12-05 16:19:02 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4067241)[0;0m ERROR 12-05 16:19:02 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067241)[0;0m ERROR 12-05 16:19:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4067241)[0;0m ERROR 12-05 16:19:02 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4067241)[0;0m ERROR 12-05 16:19:02 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067241)[0;0m ERROR 12-05 16:19:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4067241)[0;0m ERROR 12-05 16:19:02 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4067241)[0;0m ERROR 12-05 16:19:02 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067241)[0;0m ERROR 12-05 16:19:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4067241)[0;0m ERROR 12-05 16:19:02 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4067241)[0;0m ERROR 12-05 16:19:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4067241)[0;0m ERROR 12-05 16:19:02 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4067241)[0;0m ERROR 12-05 16:19:02 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067241)[0;0m ERROR 12-05 16:19:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4067241)[0;0m ERROR 12-05 16:19:02 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4067241)[0;0m ERROR 12-05 16:19:02 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067241)[0;0m ERROR 12-05 16:19:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4067241)[0;0m ERROR 12-05 16:19:02 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4067241)[0;0m ERROR 12-05 16:19:02 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067241)[0;0m ERROR 12-05 16:19:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4067241)[0;0m ERROR 12-05 16:19:02 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4067241)[0;0m ERROR 12-05 16:19:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4067241)[0;0m ERROR 12-05 16:19:02 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4067241)[0;0m ERROR 12-05 16:19:02 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067241)[0;0m ERROR 12-05 16:19:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4067241)[0;0m ERROR 12-05 16:19:02 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4067241)[0;0m ERROR 12-05 16:19:02 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067241)[0;0m ERROR 12-05 16:19:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4067241)[0;0m ERROR 12-05 16:19:02 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4067241)[0;0m ERROR 12-05 16:19:02 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067241)[0;0m ERROR 12-05 16:19:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4067241)[0;0m ERROR 12-05 16:19:02 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4067241)[0;0m ERROR 12-05 16:19:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4067241)[0;0m ERROR 12-05 16:19:02 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4067241)[0;0m ERROR 12-05 16:19:02 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067241)[0;0m ERROR 12-05 16:19:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4067241)[0;0m ERROR 12-05 16:19:02 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4067241)[0;0m ERROR 12-05 16:19:02 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067241)[0;0m ERROR 12-05 16:19:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4067241)[0;0m ERROR 12-05 16:19:02 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4067241)[0;0m ERROR 12-05 16:19:02 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067241)[0;0m ERROR 12-05 16:19:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4067241)[0;0m ERROR 12-05 16:19:02 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4067241)[0;0m ERROR 12-05 16:19:02 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067241)[0;0m ERROR 12-05 16:19:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4067241)[0;0m ERROR 12-05 16:19:02 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4067241)[0;0m ERROR 12-05 16:19:02 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7ff5bd273290>' raised:
[1;36m(EngineCore_DP0 pid=4067241)[0;0m ERROR 12-05 16:19:02 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4067241)[0;0m ERROR 12-05 16:19:02 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4067241)[0;0m ERROR 12-05 16:19:02 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4067241)[0;0m ERROR 12-05 16:19:02 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 16:19:23 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 16:19:23 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 16:19:23 [model.py:1745] Using max model len 131072
INFO 12-05 16:19:23 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4067319)[0;0m INFO 12-05 16:19:32 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4067319)[0;0m INFO 12-05 16:19:33 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:35815 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4067319)[0;0m INFO 12-05 16:19:33 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4067319)[0;0m INFO 12-05 16:19:33 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4067319)[0;0m INFO 12-05 16:19:34 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4067319)[0;0m INFO 12-05 16:19:34 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4067319)[0;0m INFO 12-05 16:19:35 [default_loader.py:314] Loading weights took 0.88 seconds
[1;36m(EngineCore_DP0 pid=4067319)[0;0m INFO 12-05 16:19:35 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.449205 seconds
[1;36m(EngineCore_DP0 pid=4067319)[0;0m INFO 12-05 16:19:39 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4067319)[0;0m INFO 12-05 16:19:39 [backends.py:647] Dynamo bytecode transform time: 4.00 s
[1;36m(EngineCore_DP0 pid=4067319)[0;0m ERROR 12-05 16:19:40 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4067319)[0;0m ERROR 12-05 16:19:40 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4067319)[0;0m ERROR 12-05 16:19:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4067319)[0;0m ERROR 12-05 16:19:40 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4067319)[0;0m ERROR 12-05 16:19:40 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067319)[0;0m ERROR 12-05 16:19:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4067319)[0;0m ERROR 12-05 16:19:40 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4067319)[0;0m ERROR 12-05 16:19:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4067319)[0;0m ERROR 12-05 16:19:40 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4067319)[0;0m ERROR 12-05 16:19:40 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067319)[0;0m ERROR 12-05 16:19:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4067319)[0;0m ERROR 12-05 16:19:40 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4067319)[0;0m ERROR 12-05 16:19:40 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067319)[0;0m ERROR 12-05 16:19:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4067319)[0;0m ERROR 12-05 16:19:40 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4067319)[0;0m ERROR 12-05 16:19:40 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067319)[0;0m ERROR 12-05 16:19:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4067319)[0;0m ERROR 12-05 16:19:40 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4067319)[0;0m ERROR 12-05 16:19:40 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067319)[0;0m ERROR 12-05 16:19:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4067319)[0;0m ERROR 12-05 16:19:40 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4067319)[0;0m ERROR 12-05 16:19:40 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067319)[0;0m ERROR 12-05 16:19:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4067319)[0;0m ERROR 12-05 16:19:40 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4067319)[0;0m ERROR 12-05 16:19:40 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067319)[0;0m ERROR 12-05 16:19:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4067319)[0;0m ERROR 12-05 16:19:40 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4067319)[0;0m ERROR 12-05 16:19:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4067319)[0;0m ERROR 12-05 16:19:40 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4067319)[0;0m ERROR 12-05 16:19:40 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067319)[0;0m ERROR 12-05 16:19:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4067319)[0;0m ERROR 12-05 16:19:40 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4067319)[0;0m ERROR 12-05 16:19:40 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067319)[0;0m ERROR 12-05 16:19:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4067319)[0;0m ERROR 12-05 16:19:40 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4067319)[0;0m ERROR 12-05 16:19:40 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067319)[0;0m ERROR 12-05 16:19:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4067319)[0;0m ERROR 12-05 16:19:40 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4067319)[0;0m ERROR 12-05 16:19:40 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067319)[0;0m ERROR 12-05 16:19:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4067319)[0;0m ERROR 12-05 16:19:40 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4067319)[0;0m ERROR 12-05 16:19:40 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067319)[0;0m ERROR 12-05 16:19:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4067319)[0;0m ERROR 12-05 16:19:40 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4067319)[0;0m ERROR 12-05 16:19:40 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067319)[0;0m ERROR 12-05 16:19:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4067319)[0;0m ERROR 12-05 16:19:40 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4067319)[0;0m ERROR 12-05 16:19:40 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067319)[0;0m ERROR 12-05 16:19:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4067319)[0;0m ERROR 12-05 16:19:40 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4067319)[0;0m ERROR 12-05 16:19:40 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067319)[0;0m ERROR 12-05 16:19:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4067319)[0;0m ERROR 12-05 16:19:40 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4067319)[0;0m ERROR 12-05 16:19:40 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067319)[0;0m ERROR 12-05 16:19:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4067319)[0;0m ERROR 12-05 16:19:40 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4067319)[0;0m ERROR 12-05 16:19:40 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067319)[0;0m ERROR 12-05 16:19:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4067319)[0;0m ERROR 12-05 16:19:40 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4067319)[0;0m ERROR 12-05 16:19:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4067319)[0;0m ERROR 12-05 16:19:40 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4067319)[0;0m ERROR 12-05 16:19:40 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067319)[0;0m ERROR 12-05 16:19:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4067319)[0;0m ERROR 12-05 16:19:40 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4067319)[0;0m ERROR 12-05 16:19:40 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067319)[0;0m ERROR 12-05 16:19:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4067319)[0;0m ERROR 12-05 16:19:40 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4067319)[0;0m ERROR 12-05 16:19:40 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067319)[0;0m ERROR 12-05 16:19:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4067319)[0;0m ERROR 12-05 16:19:40 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4067319)[0;0m ERROR 12-05 16:19:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4067319)[0;0m ERROR 12-05 16:19:40 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4067319)[0;0m ERROR 12-05 16:19:40 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067319)[0;0m ERROR 12-05 16:19:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4067319)[0;0m ERROR 12-05 16:19:40 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4067319)[0;0m ERROR 12-05 16:19:40 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067319)[0;0m ERROR 12-05 16:19:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4067319)[0;0m ERROR 12-05 16:19:40 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4067319)[0;0m ERROR 12-05 16:19:40 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067319)[0;0m ERROR 12-05 16:19:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4067319)[0;0m ERROR 12-05 16:19:40 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4067319)[0;0m ERROR 12-05 16:19:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4067319)[0;0m ERROR 12-05 16:19:40 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4067319)[0;0m ERROR 12-05 16:19:40 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067319)[0;0m ERROR 12-05 16:19:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4067319)[0;0m ERROR 12-05 16:19:40 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4067319)[0;0m ERROR 12-05 16:19:40 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067319)[0;0m ERROR 12-05 16:19:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4067319)[0;0m ERROR 12-05 16:19:40 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4067319)[0;0m ERROR 12-05 16:19:40 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067319)[0;0m ERROR 12-05 16:19:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4067319)[0;0m ERROR 12-05 16:19:40 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4067319)[0;0m ERROR 12-05 16:19:40 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067319)[0;0m ERROR 12-05 16:19:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4067319)[0;0m ERROR 12-05 16:19:40 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4067319)[0;0m ERROR 12-05 16:19:40 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7f51141c2e70>' raised:
[1;36m(EngineCore_DP0 pid=4067319)[0;0m ERROR 12-05 16:19:40 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4067319)[0;0m ERROR 12-05 16:19:40 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4067319)[0;0m ERROR 12-05 16:19:40 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4067319)[0;0m ERROR 12-05 16:19:40 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 16:20:01 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 16:20:01 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 16:20:01 [model.py:1745] Using max model len 131072
INFO 12-05 16:20:01 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4067411)[0;0m INFO 12-05 16:20:09 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4067411)[0;0m INFO 12-05 16:20:10 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:36121 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4067411)[0;0m INFO 12-05 16:20:10 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4067411)[0;0m INFO 12-05 16:20:10 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4067411)[0;0m INFO 12-05 16:20:11 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4067411)[0;0m INFO 12-05 16:20:11 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4067411)[0;0m INFO 12-05 16:20:12 [default_loader.py:314] Loading weights took 0.87 seconds
[1;36m(EngineCore_DP0 pid=4067411)[0;0m INFO 12-05 16:20:12 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.411502 seconds
[1;36m(EngineCore_DP0 pid=4067411)[0;0m INFO 12-05 16:20:17 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4067411)[0;0m INFO 12-05 16:20:17 [backends.py:647] Dynamo bytecode transform time: 4.39 s
[1;36m(EngineCore_DP0 pid=4067411)[0;0m ERROR 12-05 16:20:17 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4067411)[0;0m ERROR 12-05 16:20:17 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4067411)[0;0m ERROR 12-05 16:20:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4067411)[0;0m ERROR 12-05 16:20:17 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4067411)[0;0m ERROR 12-05 16:20:17 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067411)[0;0m ERROR 12-05 16:20:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4067411)[0;0m ERROR 12-05 16:20:17 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4067411)[0;0m ERROR 12-05 16:20:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4067411)[0;0m ERROR 12-05 16:20:17 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4067411)[0;0m ERROR 12-05 16:20:17 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067411)[0;0m ERROR 12-05 16:20:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4067411)[0;0m ERROR 12-05 16:20:17 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4067411)[0;0m ERROR 12-05 16:20:17 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067411)[0;0m ERROR 12-05 16:20:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4067411)[0;0m ERROR 12-05 16:20:17 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4067411)[0;0m ERROR 12-05 16:20:17 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067411)[0;0m ERROR 12-05 16:20:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4067411)[0;0m ERROR 12-05 16:20:17 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4067411)[0;0m ERROR 12-05 16:20:17 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067411)[0;0m ERROR 12-05 16:20:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4067411)[0;0m ERROR 12-05 16:20:17 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4067411)[0;0m ERROR 12-05 16:20:17 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067411)[0;0m ERROR 12-05 16:20:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4067411)[0;0m ERROR 12-05 16:20:17 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4067411)[0;0m ERROR 12-05 16:20:17 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067411)[0;0m ERROR 12-05 16:20:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4067411)[0;0m ERROR 12-05 16:20:17 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4067411)[0;0m ERROR 12-05 16:20:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4067411)[0;0m ERROR 12-05 16:20:17 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4067411)[0;0m ERROR 12-05 16:20:17 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067411)[0;0m ERROR 12-05 16:20:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4067411)[0;0m ERROR 12-05 16:20:17 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4067411)[0;0m ERROR 12-05 16:20:17 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067411)[0;0m ERROR 12-05 16:20:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4067411)[0;0m ERROR 12-05 16:20:17 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4067411)[0;0m ERROR 12-05 16:20:17 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067411)[0;0m ERROR 12-05 16:20:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4067411)[0;0m ERROR 12-05 16:20:17 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4067411)[0;0m ERROR 12-05 16:20:17 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067411)[0;0m ERROR 12-05 16:20:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4067411)[0;0m ERROR 12-05 16:20:17 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4067411)[0;0m ERROR 12-05 16:20:17 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067411)[0;0m ERROR 12-05 16:20:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4067411)[0;0m ERROR 12-05 16:20:17 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4067411)[0;0m ERROR 12-05 16:20:17 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067411)[0;0m ERROR 12-05 16:20:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4067411)[0;0m ERROR 12-05 16:20:17 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4067411)[0;0m ERROR 12-05 16:20:17 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067411)[0;0m ERROR 12-05 16:20:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4067411)[0;0m ERROR 12-05 16:20:17 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4067411)[0;0m ERROR 12-05 16:20:17 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067411)[0;0m ERROR 12-05 16:20:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4067411)[0;0m ERROR 12-05 16:20:17 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4067411)[0;0m ERROR 12-05 16:20:17 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067411)[0;0m ERROR 12-05 16:20:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4067411)[0;0m ERROR 12-05 16:20:17 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4067411)[0;0m ERROR 12-05 16:20:17 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067411)[0;0m ERROR 12-05 16:20:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4067411)[0;0m ERROR 12-05 16:20:17 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4067411)[0;0m ERROR 12-05 16:20:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4067411)[0;0m ERROR 12-05 16:20:17 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4067411)[0;0m ERROR 12-05 16:20:17 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067411)[0;0m ERROR 12-05 16:20:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4067411)[0;0m ERROR 12-05 16:20:17 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4067411)[0;0m ERROR 12-05 16:20:17 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067411)[0;0m ERROR 12-05 16:20:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4067411)[0;0m ERROR 12-05 16:20:17 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4067411)[0;0m ERROR 12-05 16:20:17 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067411)[0;0m ERROR 12-05 16:20:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4067411)[0;0m ERROR 12-05 16:20:17 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4067411)[0;0m ERROR 12-05 16:20:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4067411)[0;0m ERROR 12-05 16:20:17 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4067411)[0;0m ERROR 12-05 16:20:17 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067411)[0;0m ERROR 12-05 16:20:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4067411)[0;0m ERROR 12-05 16:20:17 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4067411)[0;0m ERROR 12-05 16:20:17 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067411)[0;0m ERROR 12-05 16:20:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4067411)[0;0m ERROR 12-05 16:20:17 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4067411)[0;0m ERROR 12-05 16:20:17 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067411)[0;0m ERROR 12-05 16:20:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4067411)[0;0m ERROR 12-05 16:20:17 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4067411)[0;0m ERROR 12-05 16:20:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4067411)[0;0m ERROR 12-05 16:20:17 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4067411)[0;0m ERROR 12-05 16:20:17 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067411)[0;0m ERROR 12-05 16:20:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4067411)[0;0m ERROR 12-05 16:20:17 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4067411)[0;0m ERROR 12-05 16:20:17 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067411)[0;0m ERROR 12-05 16:20:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4067411)[0;0m ERROR 12-05 16:20:17 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4067411)[0;0m ERROR 12-05 16:20:17 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067411)[0;0m ERROR 12-05 16:20:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4067411)[0;0m ERROR 12-05 16:20:17 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4067411)[0;0m ERROR 12-05 16:20:17 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067411)[0;0m ERROR 12-05 16:20:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4067411)[0;0m ERROR 12-05 16:20:17 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4067411)[0;0m ERROR 12-05 16:20:17 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7f87dee7efc0>' raised:
[1;36m(EngineCore_DP0 pid=4067411)[0;0m ERROR 12-05 16:20:17 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4067411)[0;0m ERROR 12-05 16:20:17 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4067411)[0;0m ERROR 12-05 16:20:17 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4067411)[0;0m ERROR 12-05 16:20:17 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 16:20:39 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 16:20:39 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 16:20:39 [model.py:1745] Using max model len 131072
INFO 12-05 16:20:39 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4067492)[0;0m INFO 12-05 16:20:47 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4067492)[0;0m INFO 12-05 16:20:48 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:56387 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4067492)[0;0m INFO 12-05 16:20:48 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4067492)[0;0m INFO 12-05 16:20:48 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4067492)[0;0m INFO 12-05 16:20:48 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4067492)[0;0m INFO 12-05 16:20:48 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4067492)[0;0m INFO 12-05 16:20:50 [default_loader.py:314] Loading weights took 0.88 seconds
[1;36m(EngineCore_DP0 pid=4067492)[0;0m INFO 12-05 16:20:50 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.438467 seconds
[1;36m(EngineCore_DP0 pid=4067492)[0;0m INFO 12-05 16:20:54 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4067492)[0;0m INFO 12-05 16:20:54 [backends.py:647] Dynamo bytecode transform time: 3.62 s
[1;36m(EngineCore_DP0 pid=4067492)[0;0m ERROR 12-05 16:20:54 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4067492)[0;0m ERROR 12-05 16:20:54 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4067492)[0;0m ERROR 12-05 16:20:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4067492)[0;0m ERROR 12-05 16:20:54 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4067492)[0;0m ERROR 12-05 16:20:54 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067492)[0;0m ERROR 12-05 16:20:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4067492)[0;0m ERROR 12-05 16:20:54 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4067492)[0;0m ERROR 12-05 16:20:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4067492)[0;0m ERROR 12-05 16:20:54 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4067492)[0;0m ERROR 12-05 16:20:54 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067492)[0;0m ERROR 12-05 16:20:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4067492)[0;0m ERROR 12-05 16:20:54 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4067492)[0;0m ERROR 12-05 16:20:54 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067492)[0;0m ERROR 12-05 16:20:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4067492)[0;0m ERROR 12-05 16:20:54 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4067492)[0;0m ERROR 12-05 16:20:54 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067492)[0;0m ERROR 12-05 16:20:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4067492)[0;0m ERROR 12-05 16:20:54 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4067492)[0;0m ERROR 12-05 16:20:54 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067492)[0;0m ERROR 12-05 16:20:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4067492)[0;0m ERROR 12-05 16:20:54 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4067492)[0;0m ERROR 12-05 16:20:54 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067492)[0;0m ERROR 12-05 16:20:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4067492)[0;0m ERROR 12-05 16:20:54 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4067492)[0;0m ERROR 12-05 16:20:54 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067492)[0;0m ERROR 12-05 16:20:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4067492)[0;0m ERROR 12-05 16:20:54 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4067492)[0;0m ERROR 12-05 16:20:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4067492)[0;0m ERROR 12-05 16:20:54 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4067492)[0;0m ERROR 12-05 16:20:54 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067492)[0;0m ERROR 12-05 16:20:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4067492)[0;0m ERROR 12-05 16:20:54 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4067492)[0;0m ERROR 12-05 16:20:54 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067492)[0;0m ERROR 12-05 16:20:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4067492)[0;0m ERROR 12-05 16:20:54 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4067492)[0;0m ERROR 12-05 16:20:54 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067492)[0;0m ERROR 12-05 16:20:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4067492)[0;0m ERROR 12-05 16:20:54 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4067492)[0;0m ERROR 12-05 16:20:54 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067492)[0;0m ERROR 12-05 16:20:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4067492)[0;0m ERROR 12-05 16:20:54 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4067492)[0;0m ERROR 12-05 16:20:54 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067492)[0;0m ERROR 12-05 16:20:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4067492)[0;0m ERROR 12-05 16:20:54 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4067492)[0;0m ERROR 12-05 16:20:54 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067492)[0;0m ERROR 12-05 16:20:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4067492)[0;0m ERROR 12-05 16:20:54 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4067492)[0;0m ERROR 12-05 16:20:54 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067492)[0;0m ERROR 12-05 16:20:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4067492)[0;0m ERROR 12-05 16:20:54 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4067492)[0;0m ERROR 12-05 16:20:54 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067492)[0;0m ERROR 12-05 16:20:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4067492)[0;0m ERROR 12-05 16:20:54 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4067492)[0;0m ERROR 12-05 16:20:54 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067492)[0;0m ERROR 12-05 16:20:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4067492)[0;0m ERROR 12-05 16:20:54 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4067492)[0;0m ERROR 12-05 16:20:54 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067492)[0;0m ERROR 12-05 16:20:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4067492)[0;0m ERROR 12-05 16:20:54 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4067492)[0;0m ERROR 12-05 16:20:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4067492)[0;0m ERROR 12-05 16:20:54 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4067492)[0;0m ERROR 12-05 16:20:54 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067492)[0;0m ERROR 12-05 16:20:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4067492)[0;0m ERROR 12-05 16:20:54 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4067492)[0;0m ERROR 12-05 16:20:54 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067492)[0;0m ERROR 12-05 16:20:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4067492)[0;0m ERROR 12-05 16:20:54 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4067492)[0;0m ERROR 12-05 16:20:54 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067492)[0;0m ERROR 12-05 16:20:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4067492)[0;0m ERROR 12-05 16:20:54 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4067492)[0;0m ERROR 12-05 16:20:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4067492)[0;0m ERROR 12-05 16:20:54 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4067492)[0;0m ERROR 12-05 16:20:54 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067492)[0;0m ERROR 12-05 16:20:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4067492)[0;0m ERROR 12-05 16:20:54 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4067492)[0;0m ERROR 12-05 16:20:54 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067492)[0;0m ERROR 12-05 16:20:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4067492)[0;0m ERROR 12-05 16:20:54 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4067492)[0;0m ERROR 12-05 16:20:54 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067492)[0;0m ERROR 12-05 16:20:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4067492)[0;0m ERROR 12-05 16:20:54 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4067492)[0;0m ERROR 12-05 16:20:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4067492)[0;0m ERROR 12-05 16:20:54 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4067492)[0;0m ERROR 12-05 16:20:54 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067492)[0;0m ERROR 12-05 16:20:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4067492)[0;0m ERROR 12-05 16:20:54 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4067492)[0;0m ERROR 12-05 16:20:54 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067492)[0;0m ERROR 12-05 16:20:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4067492)[0;0m ERROR 12-05 16:20:54 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4067492)[0;0m ERROR 12-05 16:20:54 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067492)[0;0m ERROR 12-05 16:20:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4067492)[0;0m ERROR 12-05 16:20:54 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4067492)[0;0m ERROR 12-05 16:20:54 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067492)[0;0m ERROR 12-05 16:20:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4067492)[0;0m ERROR 12-05 16:20:54 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4067492)[0;0m ERROR 12-05 16:20:54 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7f22f41205f0>' raised:
[1;36m(EngineCore_DP0 pid=4067492)[0;0m ERROR 12-05 16:20:54 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4067492)[0;0m ERROR 12-05 16:20:54 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4067492)[0;0m ERROR 12-05 16:20:54 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4067492)[0;0m ERROR 12-05 16:20:54 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 16:21:15 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 16:21:16 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 16:21:16 [model.py:1745] Using max model len 131072
INFO 12-05 16:21:16 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4067673)[0;0m INFO 12-05 16:21:22 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4067673)[0;0m INFO 12-05 16:21:23 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:56171 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4067673)[0;0m INFO 12-05 16:21:23 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4067673)[0;0m INFO 12-05 16:21:23 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4067673)[0;0m INFO 12-05 16:21:24 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4067673)[0;0m INFO 12-05 16:21:24 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4067673)[0;0m INFO 12-05 16:21:25 [default_loader.py:314] Loading weights took 0.87 seconds
[1;36m(EngineCore_DP0 pid=4067673)[0;0m INFO 12-05 16:21:25 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.343154 seconds
[1;36m(EngineCore_DP0 pid=4067673)[0;0m INFO 12-05 16:21:29 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4067673)[0;0m INFO 12-05 16:21:29 [backends.py:647] Dynamo bytecode transform time: 3.58 s
[1;36m(EngineCore_DP0 pid=4067673)[0;0m ERROR 12-05 16:21:29 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4067673)[0;0m ERROR 12-05 16:21:29 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4067673)[0;0m ERROR 12-05 16:21:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4067673)[0;0m ERROR 12-05 16:21:29 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4067673)[0;0m ERROR 12-05 16:21:29 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067673)[0;0m ERROR 12-05 16:21:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4067673)[0;0m ERROR 12-05 16:21:29 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4067673)[0;0m ERROR 12-05 16:21:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4067673)[0;0m ERROR 12-05 16:21:29 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4067673)[0;0m ERROR 12-05 16:21:29 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067673)[0;0m ERROR 12-05 16:21:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4067673)[0;0m ERROR 12-05 16:21:29 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4067673)[0;0m ERROR 12-05 16:21:29 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067673)[0;0m ERROR 12-05 16:21:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4067673)[0;0m ERROR 12-05 16:21:29 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4067673)[0;0m ERROR 12-05 16:21:29 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067673)[0;0m ERROR 12-05 16:21:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4067673)[0;0m ERROR 12-05 16:21:29 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4067673)[0;0m ERROR 12-05 16:21:29 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067673)[0;0m ERROR 12-05 16:21:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4067673)[0;0m ERROR 12-05 16:21:29 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4067673)[0;0m ERROR 12-05 16:21:29 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067673)[0;0m ERROR 12-05 16:21:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4067673)[0;0m ERROR 12-05 16:21:29 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4067673)[0;0m ERROR 12-05 16:21:29 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067673)[0;0m ERROR 12-05 16:21:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4067673)[0;0m ERROR 12-05 16:21:29 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4067673)[0;0m ERROR 12-05 16:21:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4067673)[0;0m ERROR 12-05 16:21:29 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4067673)[0;0m ERROR 12-05 16:21:29 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067673)[0;0m ERROR 12-05 16:21:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4067673)[0;0m ERROR 12-05 16:21:29 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4067673)[0;0m ERROR 12-05 16:21:29 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067673)[0;0m ERROR 12-05 16:21:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4067673)[0;0m ERROR 12-05 16:21:29 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4067673)[0;0m ERROR 12-05 16:21:29 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067673)[0;0m ERROR 12-05 16:21:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4067673)[0;0m ERROR 12-05 16:21:29 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4067673)[0;0m ERROR 12-05 16:21:29 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067673)[0;0m ERROR 12-05 16:21:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4067673)[0;0m ERROR 12-05 16:21:29 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4067673)[0;0m ERROR 12-05 16:21:29 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067673)[0;0m ERROR 12-05 16:21:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4067673)[0;0m ERROR 12-05 16:21:29 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4067673)[0;0m ERROR 12-05 16:21:29 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067673)[0;0m ERROR 12-05 16:21:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4067673)[0;0m ERROR 12-05 16:21:29 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4067673)[0;0m ERROR 12-05 16:21:29 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067673)[0;0m ERROR 12-05 16:21:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4067673)[0;0m ERROR 12-05 16:21:29 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4067673)[0;0m ERROR 12-05 16:21:29 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067673)[0;0m ERROR 12-05 16:21:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4067673)[0;0m ERROR 12-05 16:21:29 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4067673)[0;0m ERROR 12-05 16:21:29 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067673)[0;0m ERROR 12-05 16:21:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4067673)[0;0m ERROR 12-05 16:21:29 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4067673)[0;0m ERROR 12-05 16:21:29 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067673)[0;0m ERROR 12-05 16:21:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4067673)[0;0m ERROR 12-05 16:21:29 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4067673)[0;0m ERROR 12-05 16:21:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4067673)[0;0m ERROR 12-05 16:21:29 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4067673)[0;0m ERROR 12-05 16:21:29 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067673)[0;0m ERROR 12-05 16:21:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4067673)[0;0m ERROR 12-05 16:21:29 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4067673)[0;0m ERROR 12-05 16:21:29 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067673)[0;0m ERROR 12-05 16:21:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4067673)[0;0m ERROR 12-05 16:21:29 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4067673)[0;0m ERROR 12-05 16:21:29 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067673)[0;0m ERROR 12-05 16:21:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4067673)[0;0m ERROR 12-05 16:21:29 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4067673)[0;0m ERROR 12-05 16:21:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4067673)[0;0m ERROR 12-05 16:21:29 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4067673)[0;0m ERROR 12-05 16:21:29 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067673)[0;0m ERROR 12-05 16:21:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4067673)[0;0m ERROR 12-05 16:21:29 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4067673)[0;0m ERROR 12-05 16:21:29 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067673)[0;0m ERROR 12-05 16:21:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4067673)[0;0m ERROR 12-05 16:21:29 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4067673)[0;0m ERROR 12-05 16:21:29 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067673)[0;0m ERROR 12-05 16:21:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4067673)[0;0m ERROR 12-05 16:21:29 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4067673)[0;0m ERROR 12-05 16:21:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4067673)[0;0m ERROR 12-05 16:21:29 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4067673)[0;0m ERROR 12-05 16:21:29 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067673)[0;0m ERROR 12-05 16:21:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4067673)[0;0m ERROR 12-05 16:21:29 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4067673)[0;0m ERROR 12-05 16:21:29 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067673)[0;0m ERROR 12-05 16:21:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4067673)[0;0m ERROR 12-05 16:21:29 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4067673)[0;0m ERROR 12-05 16:21:29 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067673)[0;0m ERROR 12-05 16:21:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4067673)[0;0m ERROR 12-05 16:21:29 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4067673)[0;0m ERROR 12-05 16:21:29 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067673)[0;0m ERROR 12-05 16:21:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4067673)[0;0m ERROR 12-05 16:21:29 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4067673)[0;0m ERROR 12-05 16:21:29 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7f94343c9250>' raised:
[1;36m(EngineCore_DP0 pid=4067673)[0;0m ERROR 12-05 16:21:29 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4067673)[0;0m ERROR 12-05 16:21:29 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4067673)[0;0m ERROR 12-05 16:21:29 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4067673)[0;0m ERROR 12-05 16:21:29 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 16:21:51 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 16:21:51 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 16:21:51 [model.py:1745] Using max model len 131072
INFO 12-05 16:21:51 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4067764)[0;0m INFO 12-05 16:22:00 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4067764)[0;0m INFO 12-05 16:22:01 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:48835 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4067764)[0;0m INFO 12-05 16:22:01 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4067764)[0;0m INFO 12-05 16:22:01 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4067764)[0;0m INFO 12-05 16:22:02 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4067764)[0;0m INFO 12-05 16:22:02 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4067764)[0;0m INFO 12-05 16:22:03 [default_loader.py:314] Loading weights took 0.88 seconds
[1;36m(EngineCore_DP0 pid=4067764)[0;0m INFO 12-05 16:22:03 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.515454 seconds
[1;36m(EngineCore_DP0 pid=4067764)[0;0m INFO 12-05 16:22:08 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4067764)[0;0m INFO 12-05 16:22:08 [backends.py:647] Dynamo bytecode transform time: 4.42 s
[1;36m(EngineCore_DP0 pid=4067764)[0;0m ERROR 12-05 16:22:08 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4067764)[0;0m ERROR 12-05 16:22:08 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4067764)[0;0m ERROR 12-05 16:22:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4067764)[0;0m ERROR 12-05 16:22:08 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4067764)[0;0m ERROR 12-05 16:22:08 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067764)[0;0m ERROR 12-05 16:22:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4067764)[0;0m ERROR 12-05 16:22:08 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4067764)[0;0m ERROR 12-05 16:22:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4067764)[0;0m ERROR 12-05 16:22:08 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4067764)[0;0m ERROR 12-05 16:22:08 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067764)[0;0m ERROR 12-05 16:22:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4067764)[0;0m ERROR 12-05 16:22:08 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4067764)[0;0m ERROR 12-05 16:22:08 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067764)[0;0m ERROR 12-05 16:22:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4067764)[0;0m ERROR 12-05 16:22:08 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4067764)[0;0m ERROR 12-05 16:22:08 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067764)[0;0m ERROR 12-05 16:22:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4067764)[0;0m ERROR 12-05 16:22:08 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4067764)[0;0m ERROR 12-05 16:22:08 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067764)[0;0m ERROR 12-05 16:22:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4067764)[0;0m ERROR 12-05 16:22:08 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4067764)[0;0m ERROR 12-05 16:22:08 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067764)[0;0m ERROR 12-05 16:22:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4067764)[0;0m ERROR 12-05 16:22:08 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4067764)[0;0m ERROR 12-05 16:22:08 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067764)[0;0m ERROR 12-05 16:22:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4067764)[0;0m ERROR 12-05 16:22:08 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4067764)[0;0m ERROR 12-05 16:22:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4067764)[0;0m ERROR 12-05 16:22:08 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4067764)[0;0m ERROR 12-05 16:22:08 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067764)[0;0m ERROR 12-05 16:22:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4067764)[0;0m ERROR 12-05 16:22:08 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4067764)[0;0m ERROR 12-05 16:22:08 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067764)[0;0m ERROR 12-05 16:22:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4067764)[0;0m ERROR 12-05 16:22:08 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4067764)[0;0m ERROR 12-05 16:22:08 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067764)[0;0m ERROR 12-05 16:22:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4067764)[0;0m ERROR 12-05 16:22:08 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4067764)[0;0m ERROR 12-05 16:22:08 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067764)[0;0m ERROR 12-05 16:22:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4067764)[0;0m ERROR 12-05 16:22:08 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4067764)[0;0m ERROR 12-05 16:22:08 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067764)[0;0m ERROR 12-05 16:22:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4067764)[0;0m ERROR 12-05 16:22:08 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4067764)[0;0m ERROR 12-05 16:22:08 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067764)[0;0m ERROR 12-05 16:22:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4067764)[0;0m ERROR 12-05 16:22:08 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4067764)[0;0m ERROR 12-05 16:22:08 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067764)[0;0m ERROR 12-05 16:22:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4067764)[0;0m ERROR 12-05 16:22:08 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4067764)[0;0m ERROR 12-05 16:22:08 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067764)[0;0m ERROR 12-05 16:22:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4067764)[0;0m ERROR 12-05 16:22:08 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4067764)[0;0m ERROR 12-05 16:22:08 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067764)[0;0m ERROR 12-05 16:22:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4067764)[0;0m ERROR 12-05 16:22:08 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4067764)[0;0m ERROR 12-05 16:22:08 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067764)[0;0m ERROR 12-05 16:22:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4067764)[0;0m ERROR 12-05 16:22:08 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4067764)[0;0m ERROR 12-05 16:22:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4067764)[0;0m ERROR 12-05 16:22:08 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4067764)[0;0m ERROR 12-05 16:22:08 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067764)[0;0m ERROR 12-05 16:22:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4067764)[0;0m ERROR 12-05 16:22:08 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4067764)[0;0m ERROR 12-05 16:22:08 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067764)[0;0m ERROR 12-05 16:22:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4067764)[0;0m ERROR 12-05 16:22:08 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4067764)[0;0m ERROR 12-05 16:22:08 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067764)[0;0m ERROR 12-05 16:22:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4067764)[0;0m ERROR 12-05 16:22:08 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4067764)[0;0m ERROR 12-05 16:22:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4067764)[0;0m ERROR 12-05 16:22:08 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4067764)[0;0m ERROR 12-05 16:22:08 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067764)[0;0m ERROR 12-05 16:22:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4067764)[0;0m ERROR 12-05 16:22:08 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4067764)[0;0m ERROR 12-05 16:22:08 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067764)[0;0m ERROR 12-05 16:22:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4067764)[0;0m ERROR 12-05 16:22:08 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4067764)[0;0m ERROR 12-05 16:22:08 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067764)[0;0m ERROR 12-05 16:22:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4067764)[0;0m ERROR 12-05 16:22:08 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4067764)[0;0m ERROR 12-05 16:22:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4067764)[0;0m ERROR 12-05 16:22:08 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4067764)[0;0m ERROR 12-05 16:22:08 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067764)[0;0m ERROR 12-05 16:22:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4067764)[0;0m ERROR 12-05 16:22:08 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4067764)[0;0m ERROR 12-05 16:22:08 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067764)[0;0m ERROR 12-05 16:22:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4067764)[0;0m ERROR 12-05 16:22:08 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4067764)[0;0m ERROR 12-05 16:22:08 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067764)[0;0m ERROR 12-05 16:22:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4067764)[0;0m ERROR 12-05 16:22:08 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4067764)[0;0m ERROR 12-05 16:22:08 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067764)[0;0m ERROR 12-05 16:22:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4067764)[0;0m ERROR 12-05 16:22:08 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4067764)[0;0m ERROR 12-05 16:22:08 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7fe436143140>' raised:
[1;36m(EngineCore_DP0 pid=4067764)[0;0m ERROR 12-05 16:22:08 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4067764)[0;0m ERROR 12-05 16:22:08 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4067764)[0;0m ERROR 12-05 16:22:08 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4067764)[0;0m ERROR 12-05 16:22:08 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 16:22:30 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 16:22:30 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 16:22:30 [model.py:1745] Using max model len 131072
INFO 12-05 16:22:30 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4067844)[0;0m INFO 12-05 16:22:38 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4067844)[0;0m INFO 12-05 16:22:39 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:33555 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4067844)[0;0m INFO 12-05 16:22:39 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4067844)[0;0m INFO 12-05 16:22:39 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4067844)[0;0m INFO 12-05 16:22:40 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4067844)[0;0m INFO 12-05 16:22:40 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4067844)[0;0m INFO 12-05 16:22:41 [default_loader.py:314] Loading weights took 0.87 seconds
[1;36m(EngineCore_DP0 pid=4067844)[0;0m INFO 12-05 16:22:41 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.406434 seconds
[1;36m(EngineCore_DP0 pid=4067844)[0;0m INFO 12-05 16:22:46 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4067844)[0;0m INFO 12-05 16:22:46 [backends.py:647] Dynamo bytecode transform time: 4.01 s
[1;36m(EngineCore_DP0 pid=4067844)[0;0m ERROR 12-05 16:22:46 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4067844)[0;0m ERROR 12-05 16:22:46 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4067844)[0;0m ERROR 12-05 16:22:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4067844)[0;0m ERROR 12-05 16:22:46 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4067844)[0;0m ERROR 12-05 16:22:46 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067844)[0;0m ERROR 12-05 16:22:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4067844)[0;0m ERROR 12-05 16:22:46 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4067844)[0;0m ERROR 12-05 16:22:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4067844)[0;0m ERROR 12-05 16:22:46 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4067844)[0;0m ERROR 12-05 16:22:46 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067844)[0;0m ERROR 12-05 16:22:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4067844)[0;0m ERROR 12-05 16:22:46 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4067844)[0;0m ERROR 12-05 16:22:46 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067844)[0;0m ERROR 12-05 16:22:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4067844)[0;0m ERROR 12-05 16:22:46 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4067844)[0;0m ERROR 12-05 16:22:46 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067844)[0;0m ERROR 12-05 16:22:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4067844)[0;0m ERROR 12-05 16:22:46 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4067844)[0;0m ERROR 12-05 16:22:46 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067844)[0;0m ERROR 12-05 16:22:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4067844)[0;0m ERROR 12-05 16:22:46 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4067844)[0;0m ERROR 12-05 16:22:46 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067844)[0;0m ERROR 12-05 16:22:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4067844)[0;0m ERROR 12-05 16:22:46 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4067844)[0;0m ERROR 12-05 16:22:46 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067844)[0;0m ERROR 12-05 16:22:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4067844)[0;0m ERROR 12-05 16:22:46 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4067844)[0;0m ERROR 12-05 16:22:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4067844)[0;0m ERROR 12-05 16:22:46 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4067844)[0;0m ERROR 12-05 16:22:46 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067844)[0;0m ERROR 12-05 16:22:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4067844)[0;0m ERROR 12-05 16:22:46 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4067844)[0;0m ERROR 12-05 16:22:46 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067844)[0;0m ERROR 12-05 16:22:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4067844)[0;0m ERROR 12-05 16:22:46 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4067844)[0;0m ERROR 12-05 16:22:46 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067844)[0;0m ERROR 12-05 16:22:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4067844)[0;0m ERROR 12-05 16:22:46 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4067844)[0;0m ERROR 12-05 16:22:46 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067844)[0;0m ERROR 12-05 16:22:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4067844)[0;0m ERROR 12-05 16:22:46 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4067844)[0;0m ERROR 12-05 16:22:46 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067844)[0;0m ERROR 12-05 16:22:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4067844)[0;0m ERROR 12-05 16:22:46 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4067844)[0;0m ERROR 12-05 16:22:46 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067844)[0;0m ERROR 12-05 16:22:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4067844)[0;0m ERROR 12-05 16:22:46 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4067844)[0;0m ERROR 12-05 16:22:46 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067844)[0;0m ERROR 12-05 16:22:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4067844)[0;0m ERROR 12-05 16:22:46 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4067844)[0;0m ERROR 12-05 16:22:46 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067844)[0;0m ERROR 12-05 16:22:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4067844)[0;0m ERROR 12-05 16:22:46 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4067844)[0;0m ERROR 12-05 16:22:46 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067844)[0;0m ERROR 12-05 16:22:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4067844)[0;0m ERROR 12-05 16:22:46 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4067844)[0;0m ERROR 12-05 16:22:46 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067844)[0;0m ERROR 12-05 16:22:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4067844)[0;0m ERROR 12-05 16:22:46 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4067844)[0;0m ERROR 12-05 16:22:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4067844)[0;0m ERROR 12-05 16:22:46 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4067844)[0;0m ERROR 12-05 16:22:46 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067844)[0;0m ERROR 12-05 16:22:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4067844)[0;0m ERROR 12-05 16:22:46 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4067844)[0;0m ERROR 12-05 16:22:46 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067844)[0;0m ERROR 12-05 16:22:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4067844)[0;0m ERROR 12-05 16:22:46 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4067844)[0;0m ERROR 12-05 16:22:46 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067844)[0;0m ERROR 12-05 16:22:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4067844)[0;0m ERROR 12-05 16:22:46 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4067844)[0;0m ERROR 12-05 16:22:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4067844)[0;0m ERROR 12-05 16:22:46 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4067844)[0;0m ERROR 12-05 16:22:46 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067844)[0;0m ERROR 12-05 16:22:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4067844)[0;0m ERROR 12-05 16:22:46 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4067844)[0;0m ERROR 12-05 16:22:46 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067844)[0;0m ERROR 12-05 16:22:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4067844)[0;0m ERROR 12-05 16:22:46 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4067844)[0;0m ERROR 12-05 16:22:46 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067844)[0;0m ERROR 12-05 16:22:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4067844)[0;0m ERROR 12-05 16:22:46 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4067844)[0;0m ERROR 12-05 16:22:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4067844)[0;0m ERROR 12-05 16:22:46 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4067844)[0;0m ERROR 12-05 16:22:46 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067844)[0;0m ERROR 12-05 16:22:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4067844)[0;0m ERROR 12-05 16:22:46 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4067844)[0;0m ERROR 12-05 16:22:46 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067844)[0;0m ERROR 12-05 16:22:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4067844)[0;0m ERROR 12-05 16:22:46 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4067844)[0;0m ERROR 12-05 16:22:46 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067844)[0;0m ERROR 12-05 16:22:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4067844)[0;0m ERROR 12-05 16:22:46 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4067844)[0;0m ERROR 12-05 16:22:46 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067844)[0;0m ERROR 12-05 16:22:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4067844)[0;0m ERROR 12-05 16:22:46 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4067844)[0;0m ERROR 12-05 16:22:46 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7f24ab4c7020>' raised:
[1;36m(EngineCore_DP0 pid=4067844)[0;0m ERROR 12-05 16:22:46 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4067844)[0;0m ERROR 12-05 16:22:46 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4067844)[0;0m ERROR 12-05 16:22:46 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4067844)[0;0m ERROR 12-05 16:22:46 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 16:23:07 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 16:23:07 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 16:23:07 [model.py:1745] Using max model len 131072
INFO 12-05 16:23:07 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4067929)[0;0m INFO 12-05 16:23:18 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4067929)[0;0m INFO 12-05 16:23:19 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:45621 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4067929)[0;0m INFO 12-05 16:23:19 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4067929)[0;0m INFO 12-05 16:23:19 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4067929)[0;0m INFO 12-05 16:23:20 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4067929)[0;0m INFO 12-05 16:23:20 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4067929)[0;0m INFO 12-05 16:23:21 [default_loader.py:314] Loading weights took 0.88 seconds
[1;36m(EngineCore_DP0 pid=4067929)[0;0m INFO 12-05 16:23:21 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.489662 seconds
[1;36m(EngineCore_DP0 pid=4067929)[0;0m INFO 12-05 16:23:26 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4067929)[0;0m INFO 12-05 16:23:26 [backends.py:647] Dynamo bytecode transform time: 4.36 s
[1;36m(EngineCore_DP0 pid=4067929)[0;0m ERROR 12-05 16:23:26 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4067929)[0;0m ERROR 12-05 16:23:26 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4067929)[0;0m ERROR 12-05 16:23:26 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4067929)[0;0m ERROR 12-05 16:23:26 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4067929)[0;0m ERROR 12-05 16:23:26 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067929)[0;0m ERROR 12-05 16:23:26 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4067929)[0;0m ERROR 12-05 16:23:26 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4067929)[0;0m ERROR 12-05 16:23:26 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4067929)[0;0m ERROR 12-05 16:23:26 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4067929)[0;0m ERROR 12-05 16:23:26 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067929)[0;0m ERROR 12-05 16:23:26 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4067929)[0;0m ERROR 12-05 16:23:26 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4067929)[0;0m ERROR 12-05 16:23:26 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067929)[0;0m ERROR 12-05 16:23:26 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4067929)[0;0m ERROR 12-05 16:23:26 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4067929)[0;0m ERROR 12-05 16:23:26 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067929)[0;0m ERROR 12-05 16:23:26 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4067929)[0;0m ERROR 12-05 16:23:26 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4067929)[0;0m ERROR 12-05 16:23:26 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067929)[0;0m ERROR 12-05 16:23:26 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4067929)[0;0m ERROR 12-05 16:23:26 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4067929)[0;0m ERROR 12-05 16:23:26 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067929)[0;0m ERROR 12-05 16:23:26 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4067929)[0;0m ERROR 12-05 16:23:26 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4067929)[0;0m ERROR 12-05 16:23:26 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067929)[0;0m ERROR 12-05 16:23:26 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4067929)[0;0m ERROR 12-05 16:23:26 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4067929)[0;0m ERROR 12-05 16:23:26 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4067929)[0;0m ERROR 12-05 16:23:26 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4067929)[0;0m ERROR 12-05 16:23:26 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067929)[0;0m ERROR 12-05 16:23:26 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4067929)[0;0m ERROR 12-05 16:23:26 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4067929)[0;0m ERROR 12-05 16:23:26 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067929)[0;0m ERROR 12-05 16:23:26 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4067929)[0;0m ERROR 12-05 16:23:26 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4067929)[0;0m ERROR 12-05 16:23:26 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067929)[0;0m ERROR 12-05 16:23:26 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4067929)[0;0m ERROR 12-05 16:23:26 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4067929)[0;0m ERROR 12-05 16:23:26 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067929)[0;0m ERROR 12-05 16:23:26 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4067929)[0;0m ERROR 12-05 16:23:26 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4067929)[0;0m ERROR 12-05 16:23:26 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067929)[0;0m ERROR 12-05 16:23:26 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4067929)[0;0m ERROR 12-05 16:23:26 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4067929)[0;0m ERROR 12-05 16:23:26 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067929)[0;0m ERROR 12-05 16:23:26 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4067929)[0;0m ERROR 12-05 16:23:26 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4067929)[0;0m ERROR 12-05 16:23:26 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067929)[0;0m ERROR 12-05 16:23:26 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4067929)[0;0m ERROR 12-05 16:23:26 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4067929)[0;0m ERROR 12-05 16:23:26 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067929)[0;0m ERROR 12-05 16:23:26 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4067929)[0;0m ERROR 12-05 16:23:26 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4067929)[0;0m ERROR 12-05 16:23:26 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067929)[0;0m ERROR 12-05 16:23:26 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4067929)[0;0m ERROR 12-05 16:23:26 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4067929)[0;0m ERROR 12-05 16:23:26 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067929)[0;0m ERROR 12-05 16:23:26 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4067929)[0;0m ERROR 12-05 16:23:26 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4067929)[0;0m ERROR 12-05 16:23:26 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4067929)[0;0m ERROR 12-05 16:23:26 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4067929)[0;0m ERROR 12-05 16:23:26 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067929)[0;0m ERROR 12-05 16:23:26 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4067929)[0;0m ERROR 12-05 16:23:26 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4067929)[0;0m ERROR 12-05 16:23:26 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067929)[0;0m ERROR 12-05 16:23:26 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4067929)[0;0m ERROR 12-05 16:23:26 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4067929)[0;0m ERROR 12-05 16:23:26 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067929)[0;0m ERROR 12-05 16:23:26 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4067929)[0;0m ERROR 12-05 16:23:26 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4067929)[0;0m ERROR 12-05 16:23:26 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4067929)[0;0m ERROR 12-05 16:23:26 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4067929)[0;0m ERROR 12-05 16:23:26 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067929)[0;0m ERROR 12-05 16:23:26 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4067929)[0;0m ERROR 12-05 16:23:26 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4067929)[0;0m ERROR 12-05 16:23:26 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067929)[0;0m ERROR 12-05 16:23:26 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4067929)[0;0m ERROR 12-05 16:23:26 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4067929)[0;0m ERROR 12-05 16:23:26 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067929)[0;0m ERROR 12-05 16:23:26 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4067929)[0;0m ERROR 12-05 16:23:26 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4067929)[0;0m ERROR 12-05 16:23:26 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4067929)[0;0m ERROR 12-05 16:23:26 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4067929)[0;0m ERROR 12-05 16:23:26 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067929)[0;0m ERROR 12-05 16:23:26 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4067929)[0;0m ERROR 12-05 16:23:26 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4067929)[0;0m ERROR 12-05 16:23:26 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067929)[0;0m ERROR 12-05 16:23:26 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4067929)[0;0m ERROR 12-05 16:23:26 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4067929)[0;0m ERROR 12-05 16:23:26 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067929)[0;0m ERROR 12-05 16:23:26 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4067929)[0;0m ERROR 12-05 16:23:26 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4067929)[0;0m ERROR 12-05 16:23:26 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4067929)[0;0m ERROR 12-05 16:23:26 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4067929)[0;0m ERROR 12-05 16:23:26 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4067929)[0;0m ERROR 12-05 16:23:26 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7f82042ad7f0>' raised:
[1;36m(EngineCore_DP0 pid=4067929)[0;0m ERROR 12-05 16:23:26 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4067929)[0;0m ERROR 12-05 16:23:26 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4067929)[0;0m ERROR 12-05 16:23:26 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4067929)[0;0m ERROR 12-05 16:23:26 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 16:23:47 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 16:23:48 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 16:23:48 [model.py:1745] Using max model len 131072
INFO 12-05 16:23:48 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4068018)[0;0m INFO 12-05 16:23:56 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4068018)[0;0m INFO 12-05 16:23:57 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:40429 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4068018)[0;0m INFO 12-05 16:23:57 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4068018)[0;0m INFO 12-05 16:23:57 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4068018)[0;0m INFO 12-05 16:23:58 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4068018)[0;0m INFO 12-05 16:23:58 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4068018)[0;0m INFO 12-05 16:23:59 [default_loader.py:314] Loading weights took 0.88 seconds
[1;36m(EngineCore_DP0 pid=4068018)[0;0m INFO 12-05 16:23:59 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.422628 seconds
[1;36m(EngineCore_DP0 pid=4068018)[0;0m INFO 12-05 16:24:03 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4068018)[0;0m INFO 12-05 16:24:03 [backends.py:647] Dynamo bytecode transform time: 4.01 s
[1;36m(EngineCore_DP0 pid=4068018)[0;0m ERROR 12-05 16:24:04 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4068018)[0;0m ERROR 12-05 16:24:04 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4068018)[0;0m ERROR 12-05 16:24:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4068018)[0;0m ERROR 12-05 16:24:04 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4068018)[0;0m ERROR 12-05 16:24:04 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068018)[0;0m ERROR 12-05 16:24:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4068018)[0;0m ERROR 12-05 16:24:04 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4068018)[0;0m ERROR 12-05 16:24:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4068018)[0;0m ERROR 12-05 16:24:04 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4068018)[0;0m ERROR 12-05 16:24:04 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068018)[0;0m ERROR 12-05 16:24:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4068018)[0;0m ERROR 12-05 16:24:04 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4068018)[0;0m ERROR 12-05 16:24:04 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068018)[0;0m ERROR 12-05 16:24:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4068018)[0;0m ERROR 12-05 16:24:04 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4068018)[0;0m ERROR 12-05 16:24:04 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068018)[0;0m ERROR 12-05 16:24:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4068018)[0;0m ERROR 12-05 16:24:04 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4068018)[0;0m ERROR 12-05 16:24:04 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068018)[0;0m ERROR 12-05 16:24:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4068018)[0;0m ERROR 12-05 16:24:04 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4068018)[0;0m ERROR 12-05 16:24:04 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068018)[0;0m ERROR 12-05 16:24:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4068018)[0;0m ERROR 12-05 16:24:04 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4068018)[0;0m ERROR 12-05 16:24:04 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068018)[0;0m ERROR 12-05 16:24:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4068018)[0;0m ERROR 12-05 16:24:04 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4068018)[0;0m ERROR 12-05 16:24:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4068018)[0;0m ERROR 12-05 16:24:04 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4068018)[0;0m ERROR 12-05 16:24:04 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068018)[0;0m ERROR 12-05 16:24:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4068018)[0;0m ERROR 12-05 16:24:04 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4068018)[0;0m ERROR 12-05 16:24:04 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068018)[0;0m ERROR 12-05 16:24:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4068018)[0;0m ERROR 12-05 16:24:04 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4068018)[0;0m ERROR 12-05 16:24:04 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068018)[0;0m ERROR 12-05 16:24:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4068018)[0;0m ERROR 12-05 16:24:04 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4068018)[0;0m ERROR 12-05 16:24:04 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068018)[0;0m ERROR 12-05 16:24:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4068018)[0;0m ERROR 12-05 16:24:04 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4068018)[0;0m ERROR 12-05 16:24:04 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068018)[0;0m ERROR 12-05 16:24:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4068018)[0;0m ERROR 12-05 16:24:04 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4068018)[0;0m ERROR 12-05 16:24:04 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068018)[0;0m ERROR 12-05 16:24:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4068018)[0;0m ERROR 12-05 16:24:04 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4068018)[0;0m ERROR 12-05 16:24:04 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068018)[0;0m ERROR 12-05 16:24:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4068018)[0;0m ERROR 12-05 16:24:04 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4068018)[0;0m ERROR 12-05 16:24:04 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068018)[0;0m ERROR 12-05 16:24:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4068018)[0;0m ERROR 12-05 16:24:04 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4068018)[0;0m ERROR 12-05 16:24:04 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068018)[0;0m ERROR 12-05 16:24:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4068018)[0;0m ERROR 12-05 16:24:04 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4068018)[0;0m ERROR 12-05 16:24:04 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068018)[0;0m ERROR 12-05 16:24:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4068018)[0;0m ERROR 12-05 16:24:04 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4068018)[0;0m ERROR 12-05 16:24:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4068018)[0;0m ERROR 12-05 16:24:04 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4068018)[0;0m ERROR 12-05 16:24:04 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068018)[0;0m ERROR 12-05 16:24:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4068018)[0;0m ERROR 12-05 16:24:04 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4068018)[0;0m ERROR 12-05 16:24:04 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068018)[0;0m ERROR 12-05 16:24:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4068018)[0;0m ERROR 12-05 16:24:04 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4068018)[0;0m ERROR 12-05 16:24:04 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068018)[0;0m ERROR 12-05 16:24:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4068018)[0;0m ERROR 12-05 16:24:04 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4068018)[0;0m ERROR 12-05 16:24:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4068018)[0;0m ERROR 12-05 16:24:04 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4068018)[0;0m ERROR 12-05 16:24:04 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068018)[0;0m ERROR 12-05 16:24:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4068018)[0;0m ERROR 12-05 16:24:04 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4068018)[0;0m ERROR 12-05 16:24:04 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068018)[0;0m ERROR 12-05 16:24:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4068018)[0;0m ERROR 12-05 16:24:04 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4068018)[0;0m ERROR 12-05 16:24:04 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068018)[0;0m ERROR 12-05 16:24:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4068018)[0;0m ERROR 12-05 16:24:04 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4068018)[0;0m ERROR 12-05 16:24:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4068018)[0;0m ERROR 12-05 16:24:04 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4068018)[0;0m ERROR 12-05 16:24:04 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068018)[0;0m ERROR 12-05 16:24:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4068018)[0;0m ERROR 12-05 16:24:04 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4068018)[0;0m ERROR 12-05 16:24:04 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068018)[0;0m ERROR 12-05 16:24:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4068018)[0;0m ERROR 12-05 16:24:04 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4068018)[0;0m ERROR 12-05 16:24:04 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068018)[0;0m ERROR 12-05 16:24:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4068018)[0;0m ERROR 12-05 16:24:04 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4068018)[0;0m ERROR 12-05 16:24:04 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068018)[0;0m ERROR 12-05 16:24:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4068018)[0;0m ERROR 12-05 16:24:04 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4068018)[0;0m ERROR 12-05 16:24:04 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7f4502f66fc0>' raised:
[1;36m(EngineCore_DP0 pid=4068018)[0;0m ERROR 12-05 16:24:04 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4068018)[0;0m ERROR 12-05 16:24:04 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4068018)[0;0m ERROR 12-05 16:24:04 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4068018)[0;0m ERROR 12-05 16:24:04 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 16:24:25 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 16:24:25 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 16:24:25 [model.py:1745] Using max model len 131072
INFO 12-05 16:24:25 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4068095)[0;0m INFO 12-05 16:24:34 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4068095)[0;0m INFO 12-05 16:24:36 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:42591 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4068095)[0;0m INFO 12-05 16:24:36 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4068095)[0;0m INFO 12-05 16:24:36 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4068095)[0;0m INFO 12-05 16:24:36 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4068095)[0;0m INFO 12-05 16:24:36 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4068095)[0;0m INFO 12-05 16:24:38 [default_loader.py:314] Loading weights took 0.87 seconds
[1;36m(EngineCore_DP0 pid=4068095)[0;0m INFO 12-05 16:24:38 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.480199 seconds
[1;36m(EngineCore_DP0 pid=4068095)[0;0m INFO 12-05 16:24:42 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4068095)[0;0m INFO 12-05 16:24:42 [backends.py:647] Dynamo bytecode transform time: 4.09 s
[1;36m(EngineCore_DP0 pid=4068095)[0;0m ERROR 12-05 16:24:42 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4068095)[0;0m ERROR 12-05 16:24:42 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4068095)[0;0m ERROR 12-05 16:24:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4068095)[0;0m ERROR 12-05 16:24:42 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4068095)[0;0m ERROR 12-05 16:24:42 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068095)[0;0m ERROR 12-05 16:24:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4068095)[0;0m ERROR 12-05 16:24:42 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4068095)[0;0m ERROR 12-05 16:24:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4068095)[0;0m ERROR 12-05 16:24:42 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4068095)[0;0m ERROR 12-05 16:24:42 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068095)[0;0m ERROR 12-05 16:24:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4068095)[0;0m ERROR 12-05 16:24:42 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4068095)[0;0m ERROR 12-05 16:24:42 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068095)[0;0m ERROR 12-05 16:24:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4068095)[0;0m ERROR 12-05 16:24:42 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4068095)[0;0m ERROR 12-05 16:24:42 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068095)[0;0m ERROR 12-05 16:24:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4068095)[0;0m ERROR 12-05 16:24:42 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4068095)[0;0m ERROR 12-05 16:24:42 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068095)[0;0m ERROR 12-05 16:24:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4068095)[0;0m ERROR 12-05 16:24:42 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4068095)[0;0m ERROR 12-05 16:24:42 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068095)[0;0m ERROR 12-05 16:24:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4068095)[0;0m ERROR 12-05 16:24:42 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4068095)[0;0m ERROR 12-05 16:24:42 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068095)[0;0m ERROR 12-05 16:24:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4068095)[0;0m ERROR 12-05 16:24:42 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4068095)[0;0m ERROR 12-05 16:24:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4068095)[0;0m ERROR 12-05 16:24:42 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4068095)[0;0m ERROR 12-05 16:24:42 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068095)[0;0m ERROR 12-05 16:24:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4068095)[0;0m ERROR 12-05 16:24:42 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4068095)[0;0m ERROR 12-05 16:24:42 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068095)[0;0m ERROR 12-05 16:24:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4068095)[0;0m ERROR 12-05 16:24:42 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4068095)[0;0m ERROR 12-05 16:24:42 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068095)[0;0m ERROR 12-05 16:24:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4068095)[0;0m ERROR 12-05 16:24:42 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4068095)[0;0m ERROR 12-05 16:24:42 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068095)[0;0m ERROR 12-05 16:24:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4068095)[0;0m ERROR 12-05 16:24:42 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4068095)[0;0m ERROR 12-05 16:24:42 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068095)[0;0m ERROR 12-05 16:24:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4068095)[0;0m ERROR 12-05 16:24:42 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4068095)[0;0m ERROR 12-05 16:24:42 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068095)[0;0m ERROR 12-05 16:24:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4068095)[0;0m ERROR 12-05 16:24:42 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4068095)[0;0m ERROR 12-05 16:24:42 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068095)[0;0m ERROR 12-05 16:24:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4068095)[0;0m ERROR 12-05 16:24:42 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4068095)[0;0m ERROR 12-05 16:24:42 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068095)[0;0m ERROR 12-05 16:24:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4068095)[0;0m ERROR 12-05 16:24:42 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4068095)[0;0m ERROR 12-05 16:24:42 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068095)[0;0m ERROR 12-05 16:24:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4068095)[0;0m ERROR 12-05 16:24:42 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4068095)[0;0m ERROR 12-05 16:24:42 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068095)[0;0m ERROR 12-05 16:24:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4068095)[0;0m ERROR 12-05 16:24:42 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4068095)[0;0m ERROR 12-05 16:24:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4068095)[0;0m ERROR 12-05 16:24:42 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4068095)[0;0m ERROR 12-05 16:24:42 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068095)[0;0m ERROR 12-05 16:24:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4068095)[0;0m ERROR 12-05 16:24:42 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4068095)[0;0m ERROR 12-05 16:24:42 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068095)[0;0m ERROR 12-05 16:24:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4068095)[0;0m ERROR 12-05 16:24:42 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4068095)[0;0m ERROR 12-05 16:24:42 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068095)[0;0m ERROR 12-05 16:24:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4068095)[0;0m ERROR 12-05 16:24:42 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4068095)[0;0m ERROR 12-05 16:24:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4068095)[0;0m ERROR 12-05 16:24:42 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4068095)[0;0m ERROR 12-05 16:24:42 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068095)[0;0m ERROR 12-05 16:24:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4068095)[0;0m ERROR 12-05 16:24:42 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4068095)[0;0m ERROR 12-05 16:24:42 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068095)[0;0m ERROR 12-05 16:24:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4068095)[0;0m ERROR 12-05 16:24:42 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4068095)[0;0m ERROR 12-05 16:24:42 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068095)[0;0m ERROR 12-05 16:24:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4068095)[0;0m ERROR 12-05 16:24:42 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4068095)[0;0m ERROR 12-05 16:24:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4068095)[0;0m ERROR 12-05 16:24:42 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4068095)[0;0m ERROR 12-05 16:24:42 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068095)[0;0m ERROR 12-05 16:24:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4068095)[0;0m ERROR 12-05 16:24:42 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4068095)[0;0m ERROR 12-05 16:24:42 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068095)[0;0m ERROR 12-05 16:24:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4068095)[0;0m ERROR 12-05 16:24:42 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4068095)[0;0m ERROR 12-05 16:24:42 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068095)[0;0m ERROR 12-05 16:24:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4068095)[0;0m ERROR 12-05 16:24:42 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4068095)[0;0m ERROR 12-05 16:24:42 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068095)[0;0m ERROR 12-05 16:24:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4068095)[0;0m ERROR 12-05 16:24:42 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4068095)[0;0m ERROR 12-05 16:24:42 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7fbffc25e2d0>' raised:
[1;36m(EngineCore_DP0 pid=4068095)[0;0m ERROR 12-05 16:24:42 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4068095)[0;0m ERROR 12-05 16:24:42 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4068095)[0;0m ERROR 12-05 16:24:42 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4068095)[0;0m ERROR 12-05 16:24:42 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 16:25:04 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 16:25:04 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 16:25:04 [model.py:1745] Using max model len 131072
INFO 12-05 16:25:04 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4068186)[0;0m INFO 12-05 16:25:14 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4068186)[0;0m INFO 12-05 16:25:15 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:54145 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4068186)[0;0m INFO 12-05 16:25:15 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4068186)[0;0m INFO 12-05 16:25:15 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4068186)[0;0m INFO 12-05 16:25:15 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4068186)[0;0m INFO 12-05 16:25:15 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4068186)[0;0m INFO 12-05 16:25:17 [default_loader.py:314] Loading weights took 0.87 seconds
[1;36m(EngineCore_DP0 pid=4068186)[0;0m INFO 12-05 16:25:17 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.435698 seconds
[1;36m(EngineCore_DP0 pid=4068186)[0;0m INFO 12-05 16:25:21 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4068186)[0;0m INFO 12-05 16:25:21 [backends.py:647] Dynamo bytecode transform time: 4.19 s
[1;36m(EngineCore_DP0 pid=4068186)[0;0m ERROR 12-05 16:25:22 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4068186)[0;0m ERROR 12-05 16:25:22 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4068186)[0;0m ERROR 12-05 16:25:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4068186)[0;0m ERROR 12-05 16:25:22 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4068186)[0;0m ERROR 12-05 16:25:22 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068186)[0;0m ERROR 12-05 16:25:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4068186)[0;0m ERROR 12-05 16:25:22 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4068186)[0;0m ERROR 12-05 16:25:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4068186)[0;0m ERROR 12-05 16:25:22 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4068186)[0;0m ERROR 12-05 16:25:22 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068186)[0;0m ERROR 12-05 16:25:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4068186)[0;0m ERROR 12-05 16:25:22 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4068186)[0;0m ERROR 12-05 16:25:22 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068186)[0;0m ERROR 12-05 16:25:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4068186)[0;0m ERROR 12-05 16:25:22 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4068186)[0;0m ERROR 12-05 16:25:22 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068186)[0;0m ERROR 12-05 16:25:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4068186)[0;0m ERROR 12-05 16:25:22 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4068186)[0;0m ERROR 12-05 16:25:22 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068186)[0;0m ERROR 12-05 16:25:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4068186)[0;0m ERROR 12-05 16:25:22 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4068186)[0;0m ERROR 12-05 16:25:22 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068186)[0;0m ERROR 12-05 16:25:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4068186)[0;0m ERROR 12-05 16:25:22 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4068186)[0;0m ERROR 12-05 16:25:22 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068186)[0;0m ERROR 12-05 16:25:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4068186)[0;0m ERROR 12-05 16:25:22 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4068186)[0;0m ERROR 12-05 16:25:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4068186)[0;0m ERROR 12-05 16:25:22 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4068186)[0;0m ERROR 12-05 16:25:22 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068186)[0;0m ERROR 12-05 16:25:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4068186)[0;0m ERROR 12-05 16:25:22 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4068186)[0;0m ERROR 12-05 16:25:22 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068186)[0;0m ERROR 12-05 16:25:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4068186)[0;0m ERROR 12-05 16:25:22 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4068186)[0;0m ERROR 12-05 16:25:22 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068186)[0;0m ERROR 12-05 16:25:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4068186)[0;0m ERROR 12-05 16:25:22 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4068186)[0;0m ERROR 12-05 16:25:22 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068186)[0;0m ERROR 12-05 16:25:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4068186)[0;0m ERROR 12-05 16:25:22 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4068186)[0;0m ERROR 12-05 16:25:22 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068186)[0;0m ERROR 12-05 16:25:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4068186)[0;0m ERROR 12-05 16:25:22 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4068186)[0;0m ERROR 12-05 16:25:22 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068186)[0;0m ERROR 12-05 16:25:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4068186)[0;0m ERROR 12-05 16:25:22 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4068186)[0;0m ERROR 12-05 16:25:22 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068186)[0;0m ERROR 12-05 16:25:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4068186)[0;0m ERROR 12-05 16:25:22 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4068186)[0;0m ERROR 12-05 16:25:22 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068186)[0;0m ERROR 12-05 16:25:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4068186)[0;0m ERROR 12-05 16:25:22 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4068186)[0;0m ERROR 12-05 16:25:22 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068186)[0;0m ERROR 12-05 16:25:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4068186)[0;0m ERROR 12-05 16:25:22 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4068186)[0;0m ERROR 12-05 16:25:22 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068186)[0;0m ERROR 12-05 16:25:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4068186)[0;0m ERROR 12-05 16:25:22 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4068186)[0;0m ERROR 12-05 16:25:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4068186)[0;0m ERROR 12-05 16:25:22 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4068186)[0;0m ERROR 12-05 16:25:22 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068186)[0;0m ERROR 12-05 16:25:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4068186)[0;0m ERROR 12-05 16:25:22 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4068186)[0;0m ERROR 12-05 16:25:22 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068186)[0;0m ERROR 12-05 16:25:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4068186)[0;0m ERROR 12-05 16:25:22 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4068186)[0;0m ERROR 12-05 16:25:22 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068186)[0;0m ERROR 12-05 16:25:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4068186)[0;0m ERROR 12-05 16:25:22 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4068186)[0;0m ERROR 12-05 16:25:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4068186)[0;0m ERROR 12-05 16:25:22 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4068186)[0;0m ERROR 12-05 16:25:22 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068186)[0;0m ERROR 12-05 16:25:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4068186)[0;0m ERROR 12-05 16:25:22 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4068186)[0;0m ERROR 12-05 16:25:22 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068186)[0;0m ERROR 12-05 16:25:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4068186)[0;0m ERROR 12-05 16:25:22 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4068186)[0;0m ERROR 12-05 16:25:22 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068186)[0;0m ERROR 12-05 16:25:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4068186)[0;0m ERROR 12-05 16:25:22 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4068186)[0;0m ERROR 12-05 16:25:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4068186)[0;0m ERROR 12-05 16:25:22 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4068186)[0;0m ERROR 12-05 16:25:22 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068186)[0;0m ERROR 12-05 16:25:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4068186)[0;0m ERROR 12-05 16:25:22 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4068186)[0;0m ERROR 12-05 16:25:22 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068186)[0;0m ERROR 12-05 16:25:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4068186)[0;0m ERROR 12-05 16:25:22 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4068186)[0;0m ERROR 12-05 16:25:22 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068186)[0;0m ERROR 12-05 16:25:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4068186)[0;0m ERROR 12-05 16:25:22 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4068186)[0;0m ERROR 12-05 16:25:22 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068186)[0;0m ERROR 12-05 16:25:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4068186)[0;0m ERROR 12-05 16:25:22 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4068186)[0;0m ERROR 12-05 16:25:22 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7f93f416fe30>' raised:
[1;36m(EngineCore_DP0 pid=4068186)[0;0m ERROR 12-05 16:25:22 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4068186)[0;0m ERROR 12-05 16:25:22 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4068186)[0;0m ERROR 12-05 16:25:22 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4068186)[0;0m ERROR 12-05 16:25:22 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 16:25:43 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 16:25:43 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 16:25:43 [model.py:1745] Using max model len 131072
INFO 12-05 16:25:43 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4068266)[0;0m INFO 12-05 16:25:52 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4068266)[0;0m INFO 12-05 16:25:53 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:33489 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4068266)[0;0m INFO 12-05 16:25:53 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4068266)[0;0m INFO 12-05 16:25:54 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4068266)[0;0m INFO 12-05 16:25:54 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4068266)[0;0m INFO 12-05 16:25:54 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4068266)[0;0m INFO 12-05 16:25:55 [default_loader.py:314] Loading weights took 0.88 seconds
[1;36m(EngineCore_DP0 pid=4068266)[0;0m INFO 12-05 16:25:56 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.475256 seconds
[1;36m(EngineCore_DP0 pid=4068266)[0;0m INFO 12-05 16:26:00 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4068266)[0;0m INFO 12-05 16:26:00 [backends.py:647] Dynamo bytecode transform time: 4.24 s
[1;36m(EngineCore_DP0 pid=4068266)[0;0m ERROR 12-05 16:26:00 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4068266)[0;0m ERROR 12-05 16:26:00 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4068266)[0;0m ERROR 12-05 16:26:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4068266)[0;0m ERROR 12-05 16:26:00 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4068266)[0;0m ERROR 12-05 16:26:00 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068266)[0;0m ERROR 12-05 16:26:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4068266)[0;0m ERROR 12-05 16:26:00 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4068266)[0;0m ERROR 12-05 16:26:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4068266)[0;0m ERROR 12-05 16:26:00 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4068266)[0;0m ERROR 12-05 16:26:00 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068266)[0;0m ERROR 12-05 16:26:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4068266)[0;0m ERROR 12-05 16:26:00 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4068266)[0;0m ERROR 12-05 16:26:00 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068266)[0;0m ERROR 12-05 16:26:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4068266)[0;0m ERROR 12-05 16:26:00 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4068266)[0;0m ERROR 12-05 16:26:00 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068266)[0;0m ERROR 12-05 16:26:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4068266)[0;0m ERROR 12-05 16:26:00 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4068266)[0;0m ERROR 12-05 16:26:00 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068266)[0;0m ERROR 12-05 16:26:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4068266)[0;0m ERROR 12-05 16:26:00 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4068266)[0;0m ERROR 12-05 16:26:00 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068266)[0;0m ERROR 12-05 16:26:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4068266)[0;0m ERROR 12-05 16:26:00 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4068266)[0;0m ERROR 12-05 16:26:00 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068266)[0;0m ERROR 12-05 16:26:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4068266)[0;0m ERROR 12-05 16:26:00 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4068266)[0;0m ERROR 12-05 16:26:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4068266)[0;0m ERROR 12-05 16:26:00 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4068266)[0;0m ERROR 12-05 16:26:00 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068266)[0;0m ERROR 12-05 16:26:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4068266)[0;0m ERROR 12-05 16:26:00 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4068266)[0;0m ERROR 12-05 16:26:00 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068266)[0;0m ERROR 12-05 16:26:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4068266)[0;0m ERROR 12-05 16:26:00 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4068266)[0;0m ERROR 12-05 16:26:00 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068266)[0;0m ERROR 12-05 16:26:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4068266)[0;0m ERROR 12-05 16:26:00 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4068266)[0;0m ERROR 12-05 16:26:00 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068266)[0;0m ERROR 12-05 16:26:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4068266)[0;0m ERROR 12-05 16:26:00 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4068266)[0;0m ERROR 12-05 16:26:00 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068266)[0;0m ERROR 12-05 16:26:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4068266)[0;0m ERROR 12-05 16:26:00 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4068266)[0;0m ERROR 12-05 16:26:00 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068266)[0;0m ERROR 12-05 16:26:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4068266)[0;0m ERROR 12-05 16:26:00 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4068266)[0;0m ERROR 12-05 16:26:00 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068266)[0;0m ERROR 12-05 16:26:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4068266)[0;0m ERROR 12-05 16:26:00 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4068266)[0;0m ERROR 12-05 16:26:00 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068266)[0;0m ERROR 12-05 16:26:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4068266)[0;0m ERROR 12-05 16:26:00 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4068266)[0;0m ERROR 12-05 16:26:00 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068266)[0;0m ERROR 12-05 16:26:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4068266)[0;0m ERROR 12-05 16:26:00 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4068266)[0;0m ERROR 12-05 16:26:00 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068266)[0;0m ERROR 12-05 16:26:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4068266)[0;0m ERROR 12-05 16:26:00 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4068266)[0;0m ERROR 12-05 16:26:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4068266)[0;0m ERROR 12-05 16:26:00 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4068266)[0;0m ERROR 12-05 16:26:00 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068266)[0;0m ERROR 12-05 16:26:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4068266)[0;0m ERROR 12-05 16:26:00 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4068266)[0;0m ERROR 12-05 16:26:00 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068266)[0;0m ERROR 12-05 16:26:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4068266)[0;0m ERROR 12-05 16:26:00 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4068266)[0;0m ERROR 12-05 16:26:00 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068266)[0;0m ERROR 12-05 16:26:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4068266)[0;0m ERROR 12-05 16:26:00 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4068266)[0;0m ERROR 12-05 16:26:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4068266)[0;0m ERROR 12-05 16:26:00 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4068266)[0;0m ERROR 12-05 16:26:00 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068266)[0;0m ERROR 12-05 16:26:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4068266)[0;0m ERROR 12-05 16:26:00 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4068266)[0;0m ERROR 12-05 16:26:00 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068266)[0;0m ERROR 12-05 16:26:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4068266)[0;0m ERROR 12-05 16:26:00 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4068266)[0;0m ERROR 12-05 16:26:00 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068266)[0;0m ERROR 12-05 16:26:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4068266)[0;0m ERROR 12-05 16:26:00 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4068266)[0;0m ERROR 12-05 16:26:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4068266)[0;0m ERROR 12-05 16:26:00 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4068266)[0;0m ERROR 12-05 16:26:00 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068266)[0;0m ERROR 12-05 16:26:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4068266)[0;0m ERROR 12-05 16:26:00 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4068266)[0;0m ERROR 12-05 16:26:00 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068266)[0;0m ERROR 12-05 16:26:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4068266)[0;0m ERROR 12-05 16:26:00 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4068266)[0;0m ERROR 12-05 16:26:00 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068266)[0;0m ERROR 12-05 16:26:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4068266)[0;0m ERROR 12-05 16:26:00 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4068266)[0;0m ERROR 12-05 16:26:00 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068266)[0;0m ERROR 12-05 16:26:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4068266)[0;0m ERROR 12-05 16:26:00 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4068266)[0;0m ERROR 12-05 16:26:00 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7f6a3809a6f0>' raised:
[1;36m(EngineCore_DP0 pid=4068266)[0;0m ERROR 12-05 16:26:00 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4068266)[0;0m ERROR 12-05 16:26:00 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4068266)[0;0m ERROR 12-05 16:26:00 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4068266)[0;0m ERROR 12-05 16:26:00 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 16:26:22 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 16:26:22 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 16:26:22 [model.py:1745] Using max model len 131072
INFO 12-05 16:26:22 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4068447)[0;0m INFO 12-05 16:26:30 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4068447)[0;0m INFO 12-05 16:26:31 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:41993 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4068447)[0;0m INFO 12-05 16:26:31 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4068447)[0;0m INFO 12-05 16:26:31 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4068447)[0;0m INFO 12-05 16:26:32 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4068447)[0;0m INFO 12-05 16:26:32 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4068447)[0;0m INFO 12-05 16:26:33 [default_loader.py:314] Loading weights took 0.88 seconds
[1;36m(EngineCore_DP0 pid=4068447)[0;0m INFO 12-05 16:26:33 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.386602 seconds
[1;36m(EngineCore_DP0 pid=4068447)[0;0m INFO 12-05 16:26:37 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4068447)[0;0m INFO 12-05 16:26:37 [backends.py:647] Dynamo bytecode transform time: 3.79 s
[1;36m(EngineCore_DP0 pid=4068447)[0;0m ERROR 12-05 16:26:38 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4068447)[0;0m ERROR 12-05 16:26:38 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4068447)[0;0m ERROR 12-05 16:26:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4068447)[0;0m ERROR 12-05 16:26:38 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4068447)[0;0m ERROR 12-05 16:26:38 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068447)[0;0m ERROR 12-05 16:26:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4068447)[0;0m ERROR 12-05 16:26:38 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4068447)[0;0m ERROR 12-05 16:26:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4068447)[0;0m ERROR 12-05 16:26:38 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4068447)[0;0m ERROR 12-05 16:26:38 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068447)[0;0m ERROR 12-05 16:26:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4068447)[0;0m ERROR 12-05 16:26:38 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4068447)[0;0m ERROR 12-05 16:26:38 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068447)[0;0m ERROR 12-05 16:26:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4068447)[0;0m ERROR 12-05 16:26:38 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4068447)[0;0m ERROR 12-05 16:26:38 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068447)[0;0m ERROR 12-05 16:26:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4068447)[0;0m ERROR 12-05 16:26:38 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4068447)[0;0m ERROR 12-05 16:26:38 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068447)[0;0m ERROR 12-05 16:26:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4068447)[0;0m ERROR 12-05 16:26:38 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4068447)[0;0m ERROR 12-05 16:26:38 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068447)[0;0m ERROR 12-05 16:26:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4068447)[0;0m ERROR 12-05 16:26:38 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4068447)[0;0m ERROR 12-05 16:26:38 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068447)[0;0m ERROR 12-05 16:26:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4068447)[0;0m ERROR 12-05 16:26:38 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4068447)[0;0m ERROR 12-05 16:26:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4068447)[0;0m ERROR 12-05 16:26:38 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4068447)[0;0m ERROR 12-05 16:26:38 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068447)[0;0m ERROR 12-05 16:26:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4068447)[0;0m ERROR 12-05 16:26:38 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4068447)[0;0m ERROR 12-05 16:26:38 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068447)[0;0m ERROR 12-05 16:26:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4068447)[0;0m ERROR 12-05 16:26:38 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4068447)[0;0m ERROR 12-05 16:26:38 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068447)[0;0m ERROR 12-05 16:26:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4068447)[0;0m ERROR 12-05 16:26:38 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4068447)[0;0m ERROR 12-05 16:26:38 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068447)[0;0m ERROR 12-05 16:26:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4068447)[0;0m ERROR 12-05 16:26:38 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4068447)[0;0m ERROR 12-05 16:26:38 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068447)[0;0m ERROR 12-05 16:26:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4068447)[0;0m ERROR 12-05 16:26:38 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4068447)[0;0m ERROR 12-05 16:26:38 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068447)[0;0m ERROR 12-05 16:26:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4068447)[0;0m ERROR 12-05 16:26:38 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4068447)[0;0m ERROR 12-05 16:26:38 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068447)[0;0m ERROR 12-05 16:26:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4068447)[0;0m ERROR 12-05 16:26:38 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4068447)[0;0m ERROR 12-05 16:26:38 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068447)[0;0m ERROR 12-05 16:26:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4068447)[0;0m ERROR 12-05 16:26:38 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4068447)[0;0m ERROR 12-05 16:26:38 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068447)[0;0m ERROR 12-05 16:26:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4068447)[0;0m ERROR 12-05 16:26:38 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4068447)[0;0m ERROR 12-05 16:26:38 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068447)[0;0m ERROR 12-05 16:26:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4068447)[0;0m ERROR 12-05 16:26:38 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4068447)[0;0m ERROR 12-05 16:26:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4068447)[0;0m ERROR 12-05 16:26:38 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4068447)[0;0m ERROR 12-05 16:26:38 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068447)[0;0m ERROR 12-05 16:26:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4068447)[0;0m ERROR 12-05 16:26:38 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4068447)[0;0m ERROR 12-05 16:26:38 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068447)[0;0m ERROR 12-05 16:26:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4068447)[0;0m ERROR 12-05 16:26:38 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4068447)[0;0m ERROR 12-05 16:26:38 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068447)[0;0m ERROR 12-05 16:26:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4068447)[0;0m ERROR 12-05 16:26:38 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4068447)[0;0m ERROR 12-05 16:26:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4068447)[0;0m ERROR 12-05 16:26:38 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4068447)[0;0m ERROR 12-05 16:26:38 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068447)[0;0m ERROR 12-05 16:26:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4068447)[0;0m ERROR 12-05 16:26:38 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4068447)[0;0m ERROR 12-05 16:26:38 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068447)[0;0m ERROR 12-05 16:26:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4068447)[0;0m ERROR 12-05 16:26:38 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4068447)[0;0m ERROR 12-05 16:26:38 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068447)[0;0m ERROR 12-05 16:26:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4068447)[0;0m ERROR 12-05 16:26:38 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4068447)[0;0m ERROR 12-05 16:26:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4068447)[0;0m ERROR 12-05 16:26:38 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4068447)[0;0m ERROR 12-05 16:26:38 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068447)[0;0m ERROR 12-05 16:26:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4068447)[0;0m ERROR 12-05 16:26:38 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4068447)[0;0m ERROR 12-05 16:26:38 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068447)[0;0m ERROR 12-05 16:26:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4068447)[0;0m ERROR 12-05 16:26:38 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4068447)[0;0m ERROR 12-05 16:26:38 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068447)[0;0m ERROR 12-05 16:26:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4068447)[0;0m ERROR 12-05 16:26:38 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4068447)[0;0m ERROR 12-05 16:26:38 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068447)[0;0m ERROR 12-05 16:26:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4068447)[0;0m ERROR 12-05 16:26:38 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4068447)[0;0m ERROR 12-05 16:26:38 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7fa58c283200>' raised:
[1;36m(EngineCore_DP0 pid=4068447)[0;0m ERROR 12-05 16:26:38 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4068447)[0;0m ERROR 12-05 16:26:38 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4068447)[0;0m ERROR 12-05 16:26:38 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4068447)[0;0m ERROR 12-05 16:26:38 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 16:26:59 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 16:26:59 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 16:26:59 [model.py:1745] Using max model len 131072
INFO 12-05 16:26:59 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4068537)[0;0m INFO 12-05 16:27:08 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4068537)[0;0m INFO 12-05 16:27:08 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:47073 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4068537)[0;0m INFO 12-05 16:27:09 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4068537)[0;0m INFO 12-05 16:27:09 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4068537)[0;0m INFO 12-05 16:27:09 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4068537)[0;0m INFO 12-05 16:27:09 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4068537)[0;0m INFO 12-05 16:27:10 [default_loader.py:314] Loading weights took 0.88 seconds
[1;36m(EngineCore_DP0 pid=4068537)[0;0m INFO 12-05 16:27:11 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.425114 seconds
[1;36m(EngineCore_DP0 pid=4068537)[0;0m INFO 12-05 16:27:15 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4068537)[0;0m INFO 12-05 16:27:15 [backends.py:647] Dynamo bytecode transform time: 4.08 s
[1;36m(EngineCore_DP0 pid=4068537)[0;0m ERROR 12-05 16:27:15 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4068537)[0;0m ERROR 12-05 16:27:15 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4068537)[0;0m ERROR 12-05 16:27:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4068537)[0;0m ERROR 12-05 16:27:15 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4068537)[0;0m ERROR 12-05 16:27:15 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068537)[0;0m ERROR 12-05 16:27:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4068537)[0;0m ERROR 12-05 16:27:15 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4068537)[0;0m ERROR 12-05 16:27:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4068537)[0;0m ERROR 12-05 16:27:15 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4068537)[0;0m ERROR 12-05 16:27:15 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068537)[0;0m ERROR 12-05 16:27:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4068537)[0;0m ERROR 12-05 16:27:15 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4068537)[0;0m ERROR 12-05 16:27:15 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068537)[0;0m ERROR 12-05 16:27:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4068537)[0;0m ERROR 12-05 16:27:15 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4068537)[0;0m ERROR 12-05 16:27:15 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068537)[0;0m ERROR 12-05 16:27:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4068537)[0;0m ERROR 12-05 16:27:15 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4068537)[0;0m ERROR 12-05 16:27:15 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068537)[0;0m ERROR 12-05 16:27:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4068537)[0;0m ERROR 12-05 16:27:15 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4068537)[0;0m ERROR 12-05 16:27:15 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068537)[0;0m ERROR 12-05 16:27:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4068537)[0;0m ERROR 12-05 16:27:15 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4068537)[0;0m ERROR 12-05 16:27:15 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068537)[0;0m ERROR 12-05 16:27:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4068537)[0;0m ERROR 12-05 16:27:15 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4068537)[0;0m ERROR 12-05 16:27:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4068537)[0;0m ERROR 12-05 16:27:15 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4068537)[0;0m ERROR 12-05 16:27:15 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068537)[0;0m ERROR 12-05 16:27:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4068537)[0;0m ERROR 12-05 16:27:15 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4068537)[0;0m ERROR 12-05 16:27:15 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068537)[0;0m ERROR 12-05 16:27:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4068537)[0;0m ERROR 12-05 16:27:15 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4068537)[0;0m ERROR 12-05 16:27:15 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068537)[0;0m ERROR 12-05 16:27:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4068537)[0;0m ERROR 12-05 16:27:15 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4068537)[0;0m ERROR 12-05 16:27:15 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068537)[0;0m ERROR 12-05 16:27:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4068537)[0;0m ERROR 12-05 16:27:15 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4068537)[0;0m ERROR 12-05 16:27:15 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068537)[0;0m ERROR 12-05 16:27:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4068537)[0;0m ERROR 12-05 16:27:15 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4068537)[0;0m ERROR 12-05 16:27:15 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068537)[0;0m ERROR 12-05 16:27:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4068537)[0;0m ERROR 12-05 16:27:15 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4068537)[0;0m ERROR 12-05 16:27:15 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068537)[0;0m ERROR 12-05 16:27:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4068537)[0;0m ERROR 12-05 16:27:15 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4068537)[0;0m ERROR 12-05 16:27:15 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068537)[0;0m ERROR 12-05 16:27:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4068537)[0;0m ERROR 12-05 16:27:15 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4068537)[0;0m ERROR 12-05 16:27:15 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068537)[0;0m ERROR 12-05 16:27:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4068537)[0;0m ERROR 12-05 16:27:15 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4068537)[0;0m ERROR 12-05 16:27:15 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068537)[0;0m ERROR 12-05 16:27:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4068537)[0;0m ERROR 12-05 16:27:15 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4068537)[0;0m ERROR 12-05 16:27:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4068537)[0;0m ERROR 12-05 16:27:15 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4068537)[0;0m ERROR 12-05 16:27:15 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068537)[0;0m ERROR 12-05 16:27:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4068537)[0;0m ERROR 12-05 16:27:15 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4068537)[0;0m ERROR 12-05 16:27:15 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068537)[0;0m ERROR 12-05 16:27:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4068537)[0;0m ERROR 12-05 16:27:15 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4068537)[0;0m ERROR 12-05 16:27:15 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068537)[0;0m ERROR 12-05 16:27:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4068537)[0;0m ERROR 12-05 16:27:15 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4068537)[0;0m ERROR 12-05 16:27:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4068537)[0;0m ERROR 12-05 16:27:15 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4068537)[0;0m ERROR 12-05 16:27:15 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068537)[0;0m ERROR 12-05 16:27:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4068537)[0;0m ERROR 12-05 16:27:15 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4068537)[0;0m ERROR 12-05 16:27:15 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068537)[0;0m ERROR 12-05 16:27:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4068537)[0;0m ERROR 12-05 16:27:15 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4068537)[0;0m ERROR 12-05 16:27:15 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068537)[0;0m ERROR 12-05 16:27:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4068537)[0;0m ERROR 12-05 16:27:15 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4068537)[0;0m ERROR 12-05 16:27:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4068537)[0;0m ERROR 12-05 16:27:15 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4068537)[0;0m ERROR 12-05 16:27:15 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068537)[0;0m ERROR 12-05 16:27:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4068537)[0;0m ERROR 12-05 16:27:15 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4068537)[0;0m ERROR 12-05 16:27:15 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068537)[0;0m ERROR 12-05 16:27:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4068537)[0;0m ERROR 12-05 16:27:15 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4068537)[0;0m ERROR 12-05 16:27:15 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068537)[0;0m ERROR 12-05 16:27:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4068537)[0;0m ERROR 12-05 16:27:15 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4068537)[0;0m ERROR 12-05 16:27:15 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068537)[0;0m ERROR 12-05 16:27:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4068537)[0;0m ERROR 12-05 16:27:15 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4068537)[0;0m ERROR 12-05 16:27:15 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7fae034ab170>' raised:
[1;36m(EngineCore_DP0 pid=4068537)[0;0m ERROR 12-05 16:27:15 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4068537)[0;0m ERROR 12-05 16:27:15 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4068537)[0;0m ERROR 12-05 16:27:15 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4068537)[0;0m ERROR 12-05 16:27:15 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 16:27:37 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 16:27:37 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 16:27:37 [model.py:1745] Using max model len 131072
INFO 12-05 16:27:37 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4068618)[0;0m INFO 12-05 16:27:44 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4068618)[0;0m INFO 12-05 16:27:45 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:43259 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4068618)[0;0m INFO 12-05 16:27:45 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4068618)[0;0m INFO 12-05 16:27:45 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4068618)[0;0m INFO 12-05 16:27:46 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4068618)[0;0m INFO 12-05 16:27:46 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4068618)[0;0m INFO 12-05 16:27:47 [default_loader.py:314] Loading weights took 0.87 seconds
[1;36m(EngineCore_DP0 pid=4068618)[0;0m INFO 12-05 16:27:47 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.384349 seconds
[1;36m(EngineCore_DP0 pid=4068618)[0;0m INFO 12-05 16:27:51 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4068618)[0;0m INFO 12-05 16:27:51 [backends.py:647] Dynamo bytecode transform time: 3.80 s
[1;36m(EngineCore_DP0 pid=4068618)[0;0m ERROR 12-05 16:27:52 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4068618)[0;0m ERROR 12-05 16:27:52 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4068618)[0;0m ERROR 12-05 16:27:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4068618)[0;0m ERROR 12-05 16:27:52 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4068618)[0;0m ERROR 12-05 16:27:52 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068618)[0;0m ERROR 12-05 16:27:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4068618)[0;0m ERROR 12-05 16:27:52 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4068618)[0;0m ERROR 12-05 16:27:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4068618)[0;0m ERROR 12-05 16:27:52 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4068618)[0;0m ERROR 12-05 16:27:52 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068618)[0;0m ERROR 12-05 16:27:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4068618)[0;0m ERROR 12-05 16:27:52 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4068618)[0;0m ERROR 12-05 16:27:52 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068618)[0;0m ERROR 12-05 16:27:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4068618)[0;0m ERROR 12-05 16:27:52 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4068618)[0;0m ERROR 12-05 16:27:52 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068618)[0;0m ERROR 12-05 16:27:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4068618)[0;0m ERROR 12-05 16:27:52 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4068618)[0;0m ERROR 12-05 16:27:52 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068618)[0;0m ERROR 12-05 16:27:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4068618)[0;0m ERROR 12-05 16:27:52 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4068618)[0;0m ERROR 12-05 16:27:52 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068618)[0;0m ERROR 12-05 16:27:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4068618)[0;0m ERROR 12-05 16:27:52 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4068618)[0;0m ERROR 12-05 16:27:52 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068618)[0;0m ERROR 12-05 16:27:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4068618)[0;0m ERROR 12-05 16:27:52 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4068618)[0;0m ERROR 12-05 16:27:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4068618)[0;0m ERROR 12-05 16:27:52 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4068618)[0;0m ERROR 12-05 16:27:52 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068618)[0;0m ERROR 12-05 16:27:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4068618)[0;0m ERROR 12-05 16:27:52 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4068618)[0;0m ERROR 12-05 16:27:52 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068618)[0;0m ERROR 12-05 16:27:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4068618)[0;0m ERROR 12-05 16:27:52 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4068618)[0;0m ERROR 12-05 16:27:52 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068618)[0;0m ERROR 12-05 16:27:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4068618)[0;0m ERROR 12-05 16:27:52 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4068618)[0;0m ERROR 12-05 16:27:52 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068618)[0;0m ERROR 12-05 16:27:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4068618)[0;0m ERROR 12-05 16:27:52 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4068618)[0;0m ERROR 12-05 16:27:52 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068618)[0;0m ERROR 12-05 16:27:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4068618)[0;0m ERROR 12-05 16:27:52 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4068618)[0;0m ERROR 12-05 16:27:52 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068618)[0;0m ERROR 12-05 16:27:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4068618)[0;0m ERROR 12-05 16:27:52 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4068618)[0;0m ERROR 12-05 16:27:52 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068618)[0;0m ERROR 12-05 16:27:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4068618)[0;0m ERROR 12-05 16:27:52 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4068618)[0;0m ERROR 12-05 16:27:52 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068618)[0;0m ERROR 12-05 16:27:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4068618)[0;0m ERROR 12-05 16:27:52 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4068618)[0;0m ERROR 12-05 16:27:52 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068618)[0;0m ERROR 12-05 16:27:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4068618)[0;0m ERROR 12-05 16:27:52 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4068618)[0;0m ERROR 12-05 16:27:52 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068618)[0;0m ERROR 12-05 16:27:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4068618)[0;0m ERROR 12-05 16:27:52 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4068618)[0;0m ERROR 12-05 16:27:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4068618)[0;0m ERROR 12-05 16:27:52 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4068618)[0;0m ERROR 12-05 16:27:52 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068618)[0;0m ERROR 12-05 16:27:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4068618)[0;0m ERROR 12-05 16:27:52 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4068618)[0;0m ERROR 12-05 16:27:52 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068618)[0;0m ERROR 12-05 16:27:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4068618)[0;0m ERROR 12-05 16:27:52 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4068618)[0;0m ERROR 12-05 16:27:52 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068618)[0;0m ERROR 12-05 16:27:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4068618)[0;0m ERROR 12-05 16:27:52 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4068618)[0;0m ERROR 12-05 16:27:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4068618)[0;0m ERROR 12-05 16:27:52 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4068618)[0;0m ERROR 12-05 16:27:52 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068618)[0;0m ERROR 12-05 16:27:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4068618)[0;0m ERROR 12-05 16:27:52 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4068618)[0;0m ERROR 12-05 16:27:52 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068618)[0;0m ERROR 12-05 16:27:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4068618)[0;0m ERROR 12-05 16:27:52 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4068618)[0;0m ERROR 12-05 16:27:52 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068618)[0;0m ERROR 12-05 16:27:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4068618)[0;0m ERROR 12-05 16:27:52 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4068618)[0;0m ERROR 12-05 16:27:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4068618)[0;0m ERROR 12-05 16:27:52 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4068618)[0;0m ERROR 12-05 16:27:52 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068618)[0;0m ERROR 12-05 16:27:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4068618)[0;0m ERROR 12-05 16:27:52 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4068618)[0;0m ERROR 12-05 16:27:52 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068618)[0;0m ERROR 12-05 16:27:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4068618)[0;0m ERROR 12-05 16:27:52 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4068618)[0;0m ERROR 12-05 16:27:52 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068618)[0;0m ERROR 12-05 16:27:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4068618)[0;0m ERROR 12-05 16:27:52 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4068618)[0;0m ERROR 12-05 16:27:52 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068618)[0;0m ERROR 12-05 16:27:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4068618)[0;0m ERROR 12-05 16:27:52 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4068618)[0;0m ERROR 12-05 16:27:52 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7f5e182a09b0>' raised:
[1;36m(EngineCore_DP0 pid=4068618)[0;0m ERROR 12-05 16:27:52 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4068618)[0;0m ERROR 12-05 16:27:52 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4068618)[0;0m ERROR 12-05 16:27:52 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4068618)[0;0m ERROR 12-05 16:27:52 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 16:28:13 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 16:28:13 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 16:28:13 [model.py:1745] Using max model len 131072
INFO 12-05 16:28:13 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4068704)[0;0m INFO 12-05 16:28:22 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4068704)[0;0m INFO 12-05 16:28:23 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:43503 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4068704)[0;0m INFO 12-05 16:28:23 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4068704)[0;0m INFO 12-05 16:28:24 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4068704)[0;0m INFO 12-05 16:28:24 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4068704)[0;0m INFO 12-05 16:28:24 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4068704)[0;0m INFO 12-05 16:28:26 [default_loader.py:314] Loading weights took 0.87 seconds
[1;36m(EngineCore_DP0 pid=4068704)[0;0m INFO 12-05 16:28:26 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.976035 seconds
[1;36m(EngineCore_DP0 pid=4068704)[0;0m INFO 12-05 16:28:30 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4068704)[0;0m INFO 12-05 16:28:30 [backends.py:647] Dynamo bytecode transform time: 3.89 s
[1;36m(EngineCore_DP0 pid=4068704)[0;0m ERROR 12-05 16:28:31 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4068704)[0;0m ERROR 12-05 16:28:31 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4068704)[0;0m ERROR 12-05 16:28:31 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4068704)[0;0m ERROR 12-05 16:28:31 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4068704)[0;0m ERROR 12-05 16:28:31 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068704)[0;0m ERROR 12-05 16:28:31 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4068704)[0;0m ERROR 12-05 16:28:31 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4068704)[0;0m ERROR 12-05 16:28:31 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4068704)[0;0m ERROR 12-05 16:28:31 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4068704)[0;0m ERROR 12-05 16:28:31 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068704)[0;0m ERROR 12-05 16:28:31 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4068704)[0;0m ERROR 12-05 16:28:31 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4068704)[0;0m ERROR 12-05 16:28:31 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068704)[0;0m ERROR 12-05 16:28:31 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4068704)[0;0m ERROR 12-05 16:28:31 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4068704)[0;0m ERROR 12-05 16:28:31 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068704)[0;0m ERROR 12-05 16:28:31 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4068704)[0;0m ERROR 12-05 16:28:31 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4068704)[0;0m ERROR 12-05 16:28:31 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068704)[0;0m ERROR 12-05 16:28:31 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4068704)[0;0m ERROR 12-05 16:28:31 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4068704)[0;0m ERROR 12-05 16:28:31 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068704)[0;0m ERROR 12-05 16:28:31 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4068704)[0;0m ERROR 12-05 16:28:31 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4068704)[0;0m ERROR 12-05 16:28:31 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068704)[0;0m ERROR 12-05 16:28:31 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4068704)[0;0m ERROR 12-05 16:28:31 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4068704)[0;0m ERROR 12-05 16:28:31 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4068704)[0;0m ERROR 12-05 16:28:31 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4068704)[0;0m ERROR 12-05 16:28:31 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068704)[0;0m ERROR 12-05 16:28:31 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4068704)[0;0m ERROR 12-05 16:28:31 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4068704)[0;0m ERROR 12-05 16:28:31 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068704)[0;0m ERROR 12-05 16:28:31 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4068704)[0;0m ERROR 12-05 16:28:31 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4068704)[0;0m ERROR 12-05 16:28:31 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068704)[0;0m ERROR 12-05 16:28:31 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4068704)[0;0m ERROR 12-05 16:28:31 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4068704)[0;0m ERROR 12-05 16:28:31 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068704)[0;0m ERROR 12-05 16:28:31 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4068704)[0;0m ERROR 12-05 16:28:31 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4068704)[0;0m ERROR 12-05 16:28:31 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068704)[0;0m ERROR 12-05 16:28:31 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4068704)[0;0m ERROR 12-05 16:28:31 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4068704)[0;0m ERROR 12-05 16:28:31 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068704)[0;0m ERROR 12-05 16:28:31 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4068704)[0;0m ERROR 12-05 16:28:31 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4068704)[0;0m ERROR 12-05 16:28:31 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068704)[0;0m ERROR 12-05 16:28:31 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4068704)[0;0m ERROR 12-05 16:28:31 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4068704)[0;0m ERROR 12-05 16:28:31 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068704)[0;0m ERROR 12-05 16:28:31 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4068704)[0;0m ERROR 12-05 16:28:31 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4068704)[0;0m ERROR 12-05 16:28:31 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068704)[0;0m ERROR 12-05 16:28:31 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4068704)[0;0m ERROR 12-05 16:28:31 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4068704)[0;0m ERROR 12-05 16:28:31 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068704)[0;0m ERROR 12-05 16:28:31 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4068704)[0;0m ERROR 12-05 16:28:31 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4068704)[0;0m ERROR 12-05 16:28:31 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4068704)[0;0m ERROR 12-05 16:28:31 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4068704)[0;0m ERROR 12-05 16:28:31 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068704)[0;0m ERROR 12-05 16:28:31 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4068704)[0;0m ERROR 12-05 16:28:31 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4068704)[0;0m ERROR 12-05 16:28:31 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068704)[0;0m ERROR 12-05 16:28:31 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4068704)[0;0m ERROR 12-05 16:28:31 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4068704)[0;0m ERROR 12-05 16:28:31 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068704)[0;0m ERROR 12-05 16:28:31 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4068704)[0;0m ERROR 12-05 16:28:31 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4068704)[0;0m ERROR 12-05 16:28:31 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4068704)[0;0m ERROR 12-05 16:28:31 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4068704)[0;0m ERROR 12-05 16:28:31 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068704)[0;0m ERROR 12-05 16:28:31 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4068704)[0;0m ERROR 12-05 16:28:31 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4068704)[0;0m ERROR 12-05 16:28:31 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068704)[0;0m ERROR 12-05 16:28:31 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4068704)[0;0m ERROR 12-05 16:28:31 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4068704)[0;0m ERROR 12-05 16:28:31 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068704)[0;0m ERROR 12-05 16:28:31 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4068704)[0;0m ERROR 12-05 16:28:31 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4068704)[0;0m ERROR 12-05 16:28:31 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4068704)[0;0m ERROR 12-05 16:28:31 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4068704)[0;0m ERROR 12-05 16:28:31 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068704)[0;0m ERROR 12-05 16:28:31 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4068704)[0;0m ERROR 12-05 16:28:31 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4068704)[0;0m ERROR 12-05 16:28:31 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068704)[0;0m ERROR 12-05 16:28:31 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4068704)[0;0m ERROR 12-05 16:28:31 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4068704)[0;0m ERROR 12-05 16:28:31 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068704)[0;0m ERROR 12-05 16:28:31 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4068704)[0;0m ERROR 12-05 16:28:31 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4068704)[0;0m ERROR 12-05 16:28:31 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068704)[0;0m ERROR 12-05 16:28:31 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4068704)[0;0m ERROR 12-05 16:28:31 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4068704)[0;0m ERROR 12-05 16:28:31 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7f5148108f50>' raised:
[1;36m(EngineCore_DP0 pid=4068704)[0;0m ERROR 12-05 16:28:31 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4068704)[0;0m ERROR 12-05 16:28:31 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4068704)[0;0m ERROR 12-05 16:28:31 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4068704)[0;0m ERROR 12-05 16:28:31 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 16:28:52 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 16:28:52 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 16:28:52 [model.py:1745] Using max model len 131072
INFO 12-05 16:28:52 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4068792)[0;0m INFO 12-05 16:29:00 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4068792)[0;0m INFO 12-05 16:29:00 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:37563 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4068792)[0;0m INFO 12-05 16:29:00 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4068792)[0;0m INFO 12-05 16:29:01 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4068792)[0;0m INFO 12-05 16:29:01 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4068792)[0;0m INFO 12-05 16:29:01 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4068792)[0;0m INFO 12-05 16:29:02 [default_loader.py:314] Loading weights took 0.87 seconds
[1;36m(EngineCore_DP0 pid=4068792)[0;0m INFO 12-05 16:29:03 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.418674 seconds
[1;36m(EngineCore_DP0 pid=4068792)[0;0m INFO 12-05 16:29:07 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4068792)[0;0m INFO 12-05 16:29:07 [backends.py:647] Dynamo bytecode transform time: 3.80 s
[1;36m(EngineCore_DP0 pid=4068792)[0;0m ERROR 12-05 16:29:07 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4068792)[0;0m ERROR 12-05 16:29:07 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4068792)[0;0m ERROR 12-05 16:29:07 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4068792)[0;0m ERROR 12-05 16:29:07 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4068792)[0;0m ERROR 12-05 16:29:07 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068792)[0;0m ERROR 12-05 16:29:07 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4068792)[0;0m ERROR 12-05 16:29:07 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4068792)[0;0m ERROR 12-05 16:29:07 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4068792)[0;0m ERROR 12-05 16:29:07 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4068792)[0;0m ERROR 12-05 16:29:07 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068792)[0;0m ERROR 12-05 16:29:07 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4068792)[0;0m ERROR 12-05 16:29:07 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4068792)[0;0m ERROR 12-05 16:29:07 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068792)[0;0m ERROR 12-05 16:29:07 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4068792)[0;0m ERROR 12-05 16:29:07 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4068792)[0;0m ERROR 12-05 16:29:07 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068792)[0;0m ERROR 12-05 16:29:07 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4068792)[0;0m ERROR 12-05 16:29:07 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4068792)[0;0m ERROR 12-05 16:29:07 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068792)[0;0m ERROR 12-05 16:29:07 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4068792)[0;0m ERROR 12-05 16:29:07 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4068792)[0;0m ERROR 12-05 16:29:07 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068792)[0;0m ERROR 12-05 16:29:07 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4068792)[0;0m ERROR 12-05 16:29:07 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4068792)[0;0m ERROR 12-05 16:29:07 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068792)[0;0m ERROR 12-05 16:29:07 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4068792)[0;0m ERROR 12-05 16:29:07 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4068792)[0;0m ERROR 12-05 16:29:07 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4068792)[0;0m ERROR 12-05 16:29:07 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4068792)[0;0m ERROR 12-05 16:29:07 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068792)[0;0m ERROR 12-05 16:29:07 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4068792)[0;0m ERROR 12-05 16:29:07 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4068792)[0;0m ERROR 12-05 16:29:07 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068792)[0;0m ERROR 12-05 16:29:07 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4068792)[0;0m ERROR 12-05 16:29:07 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4068792)[0;0m ERROR 12-05 16:29:07 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068792)[0;0m ERROR 12-05 16:29:07 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4068792)[0;0m ERROR 12-05 16:29:07 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4068792)[0;0m ERROR 12-05 16:29:07 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068792)[0;0m ERROR 12-05 16:29:07 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4068792)[0;0m ERROR 12-05 16:29:07 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4068792)[0;0m ERROR 12-05 16:29:07 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068792)[0;0m ERROR 12-05 16:29:07 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4068792)[0;0m ERROR 12-05 16:29:07 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4068792)[0;0m ERROR 12-05 16:29:07 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068792)[0;0m ERROR 12-05 16:29:07 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4068792)[0;0m ERROR 12-05 16:29:07 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4068792)[0;0m ERROR 12-05 16:29:07 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068792)[0;0m ERROR 12-05 16:29:07 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4068792)[0;0m ERROR 12-05 16:29:07 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4068792)[0;0m ERROR 12-05 16:29:07 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068792)[0;0m ERROR 12-05 16:29:07 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4068792)[0;0m ERROR 12-05 16:29:07 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4068792)[0;0m ERROR 12-05 16:29:07 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068792)[0;0m ERROR 12-05 16:29:07 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4068792)[0;0m ERROR 12-05 16:29:07 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4068792)[0;0m ERROR 12-05 16:29:07 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068792)[0;0m ERROR 12-05 16:29:07 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4068792)[0;0m ERROR 12-05 16:29:07 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4068792)[0;0m ERROR 12-05 16:29:07 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4068792)[0;0m ERROR 12-05 16:29:07 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4068792)[0;0m ERROR 12-05 16:29:07 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068792)[0;0m ERROR 12-05 16:29:07 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4068792)[0;0m ERROR 12-05 16:29:07 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4068792)[0;0m ERROR 12-05 16:29:07 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068792)[0;0m ERROR 12-05 16:29:07 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4068792)[0;0m ERROR 12-05 16:29:07 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4068792)[0;0m ERROR 12-05 16:29:07 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068792)[0;0m ERROR 12-05 16:29:07 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4068792)[0;0m ERROR 12-05 16:29:07 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4068792)[0;0m ERROR 12-05 16:29:07 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4068792)[0;0m ERROR 12-05 16:29:07 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4068792)[0;0m ERROR 12-05 16:29:07 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068792)[0;0m ERROR 12-05 16:29:07 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4068792)[0;0m ERROR 12-05 16:29:07 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4068792)[0;0m ERROR 12-05 16:29:07 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068792)[0;0m ERROR 12-05 16:29:07 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4068792)[0;0m ERROR 12-05 16:29:07 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4068792)[0;0m ERROR 12-05 16:29:07 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068792)[0;0m ERROR 12-05 16:29:07 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4068792)[0;0m ERROR 12-05 16:29:07 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4068792)[0;0m ERROR 12-05 16:29:07 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4068792)[0;0m ERROR 12-05 16:29:07 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4068792)[0;0m ERROR 12-05 16:29:07 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068792)[0;0m ERROR 12-05 16:29:07 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4068792)[0;0m ERROR 12-05 16:29:07 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4068792)[0;0m ERROR 12-05 16:29:07 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068792)[0;0m ERROR 12-05 16:29:07 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4068792)[0;0m ERROR 12-05 16:29:07 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4068792)[0;0m ERROR 12-05 16:29:07 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068792)[0;0m ERROR 12-05 16:29:07 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4068792)[0;0m ERROR 12-05 16:29:07 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4068792)[0;0m ERROR 12-05 16:29:07 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068792)[0;0m ERROR 12-05 16:29:07 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4068792)[0;0m ERROR 12-05 16:29:07 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4068792)[0;0m ERROR 12-05 16:29:07 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7fad0c177770>' raised:
[1;36m(EngineCore_DP0 pid=4068792)[0;0m ERROR 12-05 16:29:07 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4068792)[0;0m ERROR 12-05 16:29:07 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4068792)[0;0m ERROR 12-05 16:29:07 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4068792)[0;0m ERROR 12-05 16:29:07 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 16:29:28 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 16:29:29 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 16:29:29 [model.py:1745] Using max model len 131072
INFO 12-05 16:29:29 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4068871)[0;0m INFO 12-05 16:29:38 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4068871)[0;0m INFO 12-05 16:29:39 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:43311 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4068871)[0;0m INFO 12-05 16:29:39 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4068871)[0;0m INFO 12-05 16:29:39 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4068871)[0;0m INFO 12-05 16:29:40 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4068871)[0;0m INFO 12-05 16:29:40 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4068871)[0;0m INFO 12-05 16:29:41 [default_loader.py:314] Loading weights took 0.88 seconds
[1;36m(EngineCore_DP0 pid=4068871)[0;0m INFO 12-05 16:29:41 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.489654 seconds
[1;36m(EngineCore_DP0 pid=4068871)[0;0m INFO 12-05 16:29:45 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4068871)[0;0m INFO 12-05 16:29:45 [backends.py:647] Dynamo bytecode transform time: 3.84 s
[1;36m(EngineCore_DP0 pid=4068871)[0;0m ERROR 12-05 16:29:46 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4068871)[0;0m ERROR 12-05 16:29:46 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4068871)[0;0m ERROR 12-05 16:29:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4068871)[0;0m ERROR 12-05 16:29:46 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4068871)[0;0m ERROR 12-05 16:29:46 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068871)[0;0m ERROR 12-05 16:29:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4068871)[0;0m ERROR 12-05 16:29:46 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4068871)[0;0m ERROR 12-05 16:29:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4068871)[0;0m ERROR 12-05 16:29:46 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4068871)[0;0m ERROR 12-05 16:29:46 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068871)[0;0m ERROR 12-05 16:29:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4068871)[0;0m ERROR 12-05 16:29:46 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4068871)[0;0m ERROR 12-05 16:29:46 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068871)[0;0m ERROR 12-05 16:29:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4068871)[0;0m ERROR 12-05 16:29:46 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4068871)[0;0m ERROR 12-05 16:29:46 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068871)[0;0m ERROR 12-05 16:29:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4068871)[0;0m ERROR 12-05 16:29:46 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4068871)[0;0m ERROR 12-05 16:29:46 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068871)[0;0m ERROR 12-05 16:29:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4068871)[0;0m ERROR 12-05 16:29:46 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4068871)[0;0m ERROR 12-05 16:29:46 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068871)[0;0m ERROR 12-05 16:29:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4068871)[0;0m ERROR 12-05 16:29:46 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4068871)[0;0m ERROR 12-05 16:29:46 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068871)[0;0m ERROR 12-05 16:29:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4068871)[0;0m ERROR 12-05 16:29:46 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4068871)[0;0m ERROR 12-05 16:29:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4068871)[0;0m ERROR 12-05 16:29:46 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4068871)[0;0m ERROR 12-05 16:29:46 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068871)[0;0m ERROR 12-05 16:29:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4068871)[0;0m ERROR 12-05 16:29:46 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4068871)[0;0m ERROR 12-05 16:29:46 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068871)[0;0m ERROR 12-05 16:29:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4068871)[0;0m ERROR 12-05 16:29:46 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4068871)[0;0m ERROR 12-05 16:29:46 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068871)[0;0m ERROR 12-05 16:29:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4068871)[0;0m ERROR 12-05 16:29:46 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4068871)[0;0m ERROR 12-05 16:29:46 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068871)[0;0m ERROR 12-05 16:29:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4068871)[0;0m ERROR 12-05 16:29:46 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4068871)[0;0m ERROR 12-05 16:29:46 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068871)[0;0m ERROR 12-05 16:29:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4068871)[0;0m ERROR 12-05 16:29:46 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4068871)[0;0m ERROR 12-05 16:29:46 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068871)[0;0m ERROR 12-05 16:29:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4068871)[0;0m ERROR 12-05 16:29:46 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4068871)[0;0m ERROR 12-05 16:29:46 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068871)[0;0m ERROR 12-05 16:29:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4068871)[0;0m ERROR 12-05 16:29:46 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4068871)[0;0m ERROR 12-05 16:29:46 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068871)[0;0m ERROR 12-05 16:29:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4068871)[0;0m ERROR 12-05 16:29:46 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4068871)[0;0m ERROR 12-05 16:29:46 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068871)[0;0m ERROR 12-05 16:29:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4068871)[0;0m ERROR 12-05 16:29:46 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4068871)[0;0m ERROR 12-05 16:29:46 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068871)[0;0m ERROR 12-05 16:29:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4068871)[0;0m ERROR 12-05 16:29:46 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4068871)[0;0m ERROR 12-05 16:29:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4068871)[0;0m ERROR 12-05 16:29:46 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4068871)[0;0m ERROR 12-05 16:29:46 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068871)[0;0m ERROR 12-05 16:29:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4068871)[0;0m ERROR 12-05 16:29:46 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4068871)[0;0m ERROR 12-05 16:29:46 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068871)[0;0m ERROR 12-05 16:29:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4068871)[0;0m ERROR 12-05 16:29:46 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4068871)[0;0m ERROR 12-05 16:29:46 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068871)[0;0m ERROR 12-05 16:29:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4068871)[0;0m ERROR 12-05 16:29:46 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4068871)[0;0m ERROR 12-05 16:29:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4068871)[0;0m ERROR 12-05 16:29:46 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4068871)[0;0m ERROR 12-05 16:29:46 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068871)[0;0m ERROR 12-05 16:29:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4068871)[0;0m ERROR 12-05 16:29:46 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4068871)[0;0m ERROR 12-05 16:29:46 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068871)[0;0m ERROR 12-05 16:29:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4068871)[0;0m ERROR 12-05 16:29:46 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4068871)[0;0m ERROR 12-05 16:29:46 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068871)[0;0m ERROR 12-05 16:29:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4068871)[0;0m ERROR 12-05 16:29:46 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4068871)[0;0m ERROR 12-05 16:29:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4068871)[0;0m ERROR 12-05 16:29:46 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4068871)[0;0m ERROR 12-05 16:29:46 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068871)[0;0m ERROR 12-05 16:29:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4068871)[0;0m ERROR 12-05 16:29:46 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4068871)[0;0m ERROR 12-05 16:29:46 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068871)[0;0m ERROR 12-05 16:29:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4068871)[0;0m ERROR 12-05 16:29:46 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4068871)[0;0m ERROR 12-05 16:29:46 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068871)[0;0m ERROR 12-05 16:29:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4068871)[0;0m ERROR 12-05 16:29:46 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4068871)[0;0m ERROR 12-05 16:29:46 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068871)[0;0m ERROR 12-05 16:29:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4068871)[0;0m ERROR 12-05 16:29:46 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4068871)[0;0m ERROR 12-05 16:29:46 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7f5d281efd10>' raised:
[1;36m(EngineCore_DP0 pid=4068871)[0;0m ERROR 12-05 16:29:46 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4068871)[0;0m ERROR 12-05 16:29:46 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4068871)[0;0m ERROR 12-05 16:29:46 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4068871)[0;0m ERROR 12-05 16:29:46 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 16:30:07 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 16:30:07 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 16:30:07 [model.py:1745] Using max model len 131072
INFO 12-05 16:30:07 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4068958)[0;0m INFO 12-05 16:30:16 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4068958)[0;0m INFO 12-05 16:30:17 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:53709 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4068958)[0;0m INFO 12-05 16:30:17 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4068958)[0;0m INFO 12-05 16:30:18 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4068958)[0;0m INFO 12-05 16:30:18 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4068958)[0;0m INFO 12-05 16:30:18 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4068958)[0;0m INFO 12-05 16:30:19 [default_loader.py:314] Loading weights took 0.88 seconds
[1;36m(EngineCore_DP0 pid=4068958)[0;0m INFO 12-05 16:30:20 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.426027 seconds
[1;36m(EngineCore_DP0 pid=4068958)[0;0m INFO 12-05 16:30:24 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4068958)[0;0m INFO 12-05 16:30:24 [backends.py:647] Dynamo bytecode transform time: 3.77 s
[1;36m(EngineCore_DP0 pid=4068958)[0;0m ERROR 12-05 16:30:24 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4068958)[0;0m ERROR 12-05 16:30:24 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4068958)[0;0m ERROR 12-05 16:30:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4068958)[0;0m ERROR 12-05 16:30:24 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4068958)[0;0m ERROR 12-05 16:30:24 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068958)[0;0m ERROR 12-05 16:30:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4068958)[0;0m ERROR 12-05 16:30:24 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4068958)[0;0m ERROR 12-05 16:30:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4068958)[0;0m ERROR 12-05 16:30:24 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4068958)[0;0m ERROR 12-05 16:30:24 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068958)[0;0m ERROR 12-05 16:30:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4068958)[0;0m ERROR 12-05 16:30:24 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4068958)[0;0m ERROR 12-05 16:30:24 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068958)[0;0m ERROR 12-05 16:30:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4068958)[0;0m ERROR 12-05 16:30:24 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4068958)[0;0m ERROR 12-05 16:30:24 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068958)[0;0m ERROR 12-05 16:30:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4068958)[0;0m ERROR 12-05 16:30:24 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4068958)[0;0m ERROR 12-05 16:30:24 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068958)[0;0m ERROR 12-05 16:30:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4068958)[0;0m ERROR 12-05 16:30:24 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4068958)[0;0m ERROR 12-05 16:30:24 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068958)[0;0m ERROR 12-05 16:30:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4068958)[0;0m ERROR 12-05 16:30:24 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4068958)[0;0m ERROR 12-05 16:30:24 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068958)[0;0m ERROR 12-05 16:30:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4068958)[0;0m ERROR 12-05 16:30:24 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4068958)[0;0m ERROR 12-05 16:30:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4068958)[0;0m ERROR 12-05 16:30:24 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4068958)[0;0m ERROR 12-05 16:30:24 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068958)[0;0m ERROR 12-05 16:30:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4068958)[0;0m ERROR 12-05 16:30:24 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4068958)[0;0m ERROR 12-05 16:30:24 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068958)[0;0m ERROR 12-05 16:30:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4068958)[0;0m ERROR 12-05 16:30:24 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4068958)[0;0m ERROR 12-05 16:30:24 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068958)[0;0m ERROR 12-05 16:30:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4068958)[0;0m ERROR 12-05 16:30:24 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4068958)[0;0m ERROR 12-05 16:30:24 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068958)[0;0m ERROR 12-05 16:30:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4068958)[0;0m ERROR 12-05 16:30:24 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4068958)[0;0m ERROR 12-05 16:30:24 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068958)[0;0m ERROR 12-05 16:30:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4068958)[0;0m ERROR 12-05 16:30:24 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4068958)[0;0m ERROR 12-05 16:30:24 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068958)[0;0m ERROR 12-05 16:30:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4068958)[0;0m ERROR 12-05 16:30:24 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4068958)[0;0m ERROR 12-05 16:30:24 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068958)[0;0m ERROR 12-05 16:30:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4068958)[0;0m ERROR 12-05 16:30:24 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4068958)[0;0m ERROR 12-05 16:30:24 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068958)[0;0m ERROR 12-05 16:30:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4068958)[0;0m ERROR 12-05 16:30:24 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4068958)[0;0m ERROR 12-05 16:30:24 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068958)[0;0m ERROR 12-05 16:30:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4068958)[0;0m ERROR 12-05 16:30:24 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4068958)[0;0m ERROR 12-05 16:30:24 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068958)[0;0m ERROR 12-05 16:30:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4068958)[0;0m ERROR 12-05 16:30:24 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4068958)[0;0m ERROR 12-05 16:30:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4068958)[0;0m ERROR 12-05 16:30:24 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4068958)[0;0m ERROR 12-05 16:30:24 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068958)[0;0m ERROR 12-05 16:30:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4068958)[0;0m ERROR 12-05 16:30:24 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4068958)[0;0m ERROR 12-05 16:30:24 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068958)[0;0m ERROR 12-05 16:30:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4068958)[0;0m ERROR 12-05 16:30:24 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4068958)[0;0m ERROR 12-05 16:30:24 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068958)[0;0m ERROR 12-05 16:30:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4068958)[0;0m ERROR 12-05 16:30:24 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4068958)[0;0m ERROR 12-05 16:30:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4068958)[0;0m ERROR 12-05 16:30:24 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4068958)[0;0m ERROR 12-05 16:30:24 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068958)[0;0m ERROR 12-05 16:30:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4068958)[0;0m ERROR 12-05 16:30:24 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4068958)[0;0m ERROR 12-05 16:30:24 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068958)[0;0m ERROR 12-05 16:30:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4068958)[0;0m ERROR 12-05 16:30:24 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4068958)[0;0m ERROR 12-05 16:30:24 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068958)[0;0m ERROR 12-05 16:30:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4068958)[0;0m ERROR 12-05 16:30:24 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4068958)[0;0m ERROR 12-05 16:30:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4068958)[0;0m ERROR 12-05 16:30:24 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4068958)[0;0m ERROR 12-05 16:30:24 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068958)[0;0m ERROR 12-05 16:30:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4068958)[0;0m ERROR 12-05 16:30:24 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4068958)[0;0m ERROR 12-05 16:30:24 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068958)[0;0m ERROR 12-05 16:30:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4068958)[0;0m ERROR 12-05 16:30:24 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4068958)[0;0m ERROR 12-05 16:30:24 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068958)[0;0m ERROR 12-05 16:30:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4068958)[0;0m ERROR 12-05 16:30:24 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4068958)[0;0m ERROR 12-05 16:30:24 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4068958)[0;0m ERROR 12-05 16:30:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4068958)[0;0m ERROR 12-05 16:30:24 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4068958)[0;0m ERROR 12-05 16:30:24 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7f9948086120>' raised:
[1;36m(EngineCore_DP0 pid=4068958)[0;0m ERROR 12-05 16:30:24 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4068958)[0;0m ERROR 12-05 16:30:24 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4068958)[0;0m ERROR 12-05 16:30:24 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4068958)[0;0m ERROR 12-05 16:30:24 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 16:30:45 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 16:30:46 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 16:30:46 [model.py:1745] Using max model len 131072
INFO 12-05 16:30:46 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4069038)[0;0m INFO 12-05 16:30:54 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4069038)[0;0m INFO 12-05 16:30:55 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:48949 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4069038)[0;0m INFO 12-05 16:30:55 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4069038)[0;0m INFO 12-05 16:30:55 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4069038)[0;0m INFO 12-05 16:30:56 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4069038)[0;0m INFO 12-05 16:30:56 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4069038)[0;0m INFO 12-05 16:30:57 [default_loader.py:314] Loading weights took 0.87 seconds
[1;36m(EngineCore_DP0 pid=4069038)[0;0m INFO 12-05 16:30:58 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.562649 seconds
[1;36m(EngineCore_DP0 pid=4069038)[0;0m INFO 12-05 16:31:02 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4069038)[0;0m INFO 12-05 16:31:02 [backends.py:647] Dynamo bytecode transform time: 4.13 s
[1;36m(EngineCore_DP0 pid=4069038)[0;0m ERROR 12-05 16:31:02 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4069038)[0;0m ERROR 12-05 16:31:02 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4069038)[0;0m ERROR 12-05 16:31:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4069038)[0;0m ERROR 12-05 16:31:02 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4069038)[0;0m ERROR 12-05 16:31:02 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069038)[0;0m ERROR 12-05 16:31:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4069038)[0;0m ERROR 12-05 16:31:02 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4069038)[0;0m ERROR 12-05 16:31:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4069038)[0;0m ERROR 12-05 16:31:02 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4069038)[0;0m ERROR 12-05 16:31:02 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069038)[0;0m ERROR 12-05 16:31:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4069038)[0;0m ERROR 12-05 16:31:02 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4069038)[0;0m ERROR 12-05 16:31:02 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069038)[0;0m ERROR 12-05 16:31:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4069038)[0;0m ERROR 12-05 16:31:02 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4069038)[0;0m ERROR 12-05 16:31:02 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069038)[0;0m ERROR 12-05 16:31:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4069038)[0;0m ERROR 12-05 16:31:02 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4069038)[0;0m ERROR 12-05 16:31:02 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069038)[0;0m ERROR 12-05 16:31:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4069038)[0;0m ERROR 12-05 16:31:02 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4069038)[0;0m ERROR 12-05 16:31:02 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069038)[0;0m ERROR 12-05 16:31:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4069038)[0;0m ERROR 12-05 16:31:02 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4069038)[0;0m ERROR 12-05 16:31:02 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069038)[0;0m ERROR 12-05 16:31:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4069038)[0;0m ERROR 12-05 16:31:02 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4069038)[0;0m ERROR 12-05 16:31:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4069038)[0;0m ERROR 12-05 16:31:02 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4069038)[0;0m ERROR 12-05 16:31:02 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069038)[0;0m ERROR 12-05 16:31:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4069038)[0;0m ERROR 12-05 16:31:02 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4069038)[0;0m ERROR 12-05 16:31:02 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069038)[0;0m ERROR 12-05 16:31:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4069038)[0;0m ERROR 12-05 16:31:02 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4069038)[0;0m ERROR 12-05 16:31:02 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069038)[0;0m ERROR 12-05 16:31:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4069038)[0;0m ERROR 12-05 16:31:02 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4069038)[0;0m ERROR 12-05 16:31:02 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069038)[0;0m ERROR 12-05 16:31:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4069038)[0;0m ERROR 12-05 16:31:02 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4069038)[0;0m ERROR 12-05 16:31:02 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069038)[0;0m ERROR 12-05 16:31:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4069038)[0;0m ERROR 12-05 16:31:02 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4069038)[0;0m ERROR 12-05 16:31:02 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069038)[0;0m ERROR 12-05 16:31:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4069038)[0;0m ERROR 12-05 16:31:02 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4069038)[0;0m ERROR 12-05 16:31:02 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069038)[0;0m ERROR 12-05 16:31:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4069038)[0;0m ERROR 12-05 16:31:02 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4069038)[0;0m ERROR 12-05 16:31:02 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069038)[0;0m ERROR 12-05 16:31:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4069038)[0;0m ERROR 12-05 16:31:02 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4069038)[0;0m ERROR 12-05 16:31:02 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069038)[0;0m ERROR 12-05 16:31:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4069038)[0;0m ERROR 12-05 16:31:02 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4069038)[0;0m ERROR 12-05 16:31:02 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069038)[0;0m ERROR 12-05 16:31:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4069038)[0;0m ERROR 12-05 16:31:02 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4069038)[0;0m ERROR 12-05 16:31:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4069038)[0;0m ERROR 12-05 16:31:02 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4069038)[0;0m ERROR 12-05 16:31:02 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069038)[0;0m ERROR 12-05 16:31:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4069038)[0;0m ERROR 12-05 16:31:02 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4069038)[0;0m ERROR 12-05 16:31:02 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069038)[0;0m ERROR 12-05 16:31:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4069038)[0;0m ERROR 12-05 16:31:02 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4069038)[0;0m ERROR 12-05 16:31:02 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069038)[0;0m ERROR 12-05 16:31:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4069038)[0;0m ERROR 12-05 16:31:02 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4069038)[0;0m ERROR 12-05 16:31:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4069038)[0;0m ERROR 12-05 16:31:02 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4069038)[0;0m ERROR 12-05 16:31:02 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069038)[0;0m ERROR 12-05 16:31:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4069038)[0;0m ERROR 12-05 16:31:02 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4069038)[0;0m ERROR 12-05 16:31:02 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069038)[0;0m ERROR 12-05 16:31:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4069038)[0;0m ERROR 12-05 16:31:02 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4069038)[0;0m ERROR 12-05 16:31:02 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069038)[0;0m ERROR 12-05 16:31:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4069038)[0;0m ERROR 12-05 16:31:02 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4069038)[0;0m ERROR 12-05 16:31:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4069038)[0;0m ERROR 12-05 16:31:02 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4069038)[0;0m ERROR 12-05 16:31:02 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069038)[0;0m ERROR 12-05 16:31:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4069038)[0;0m ERROR 12-05 16:31:02 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4069038)[0;0m ERROR 12-05 16:31:02 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069038)[0;0m ERROR 12-05 16:31:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4069038)[0;0m ERROR 12-05 16:31:02 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4069038)[0;0m ERROR 12-05 16:31:02 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069038)[0;0m ERROR 12-05 16:31:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4069038)[0;0m ERROR 12-05 16:31:02 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4069038)[0;0m ERROR 12-05 16:31:02 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069038)[0;0m ERROR 12-05 16:31:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4069038)[0;0m ERROR 12-05 16:31:02 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4069038)[0;0m ERROR 12-05 16:31:02 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7f4088166180>' raised:
[1;36m(EngineCore_DP0 pid=4069038)[0;0m ERROR 12-05 16:31:02 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4069038)[0;0m ERROR 12-05 16:31:02 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4069038)[0;0m ERROR 12-05 16:31:02 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4069038)[0;0m ERROR 12-05 16:31:02 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 16:31:24 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 16:31:24 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 16:31:24 [model.py:1745] Using max model len 131072
INFO 12-05 16:31:24 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4069218)[0;0m INFO 12-05 16:31:33 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4069218)[0;0m INFO 12-05 16:31:34 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:45709 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4069218)[0;0m INFO 12-05 16:31:34 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4069218)[0;0m INFO 12-05 16:31:34 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4069218)[0;0m INFO 12-05 16:31:35 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4069218)[0;0m INFO 12-05 16:31:35 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4069218)[0;0m INFO 12-05 16:31:36 [default_loader.py:314] Loading weights took 0.88 seconds
[1;36m(EngineCore_DP0 pid=4069218)[0;0m INFO 12-05 16:31:36 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.455681 seconds
[1;36m(EngineCore_DP0 pid=4069218)[0;0m INFO 12-05 16:31:41 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4069218)[0;0m INFO 12-05 16:31:41 [backends.py:647] Dynamo bytecode transform time: 4.16 s
[1;36m(EngineCore_DP0 pid=4069218)[0;0m ERROR 12-05 16:31:41 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4069218)[0;0m ERROR 12-05 16:31:41 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4069218)[0;0m ERROR 12-05 16:31:41 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4069218)[0;0m ERROR 12-05 16:31:41 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4069218)[0;0m ERROR 12-05 16:31:41 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069218)[0;0m ERROR 12-05 16:31:41 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4069218)[0;0m ERROR 12-05 16:31:41 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4069218)[0;0m ERROR 12-05 16:31:41 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4069218)[0;0m ERROR 12-05 16:31:41 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4069218)[0;0m ERROR 12-05 16:31:41 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069218)[0;0m ERROR 12-05 16:31:41 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4069218)[0;0m ERROR 12-05 16:31:41 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4069218)[0;0m ERROR 12-05 16:31:41 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069218)[0;0m ERROR 12-05 16:31:41 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4069218)[0;0m ERROR 12-05 16:31:41 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4069218)[0;0m ERROR 12-05 16:31:41 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069218)[0;0m ERROR 12-05 16:31:41 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4069218)[0;0m ERROR 12-05 16:31:41 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4069218)[0;0m ERROR 12-05 16:31:41 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069218)[0;0m ERROR 12-05 16:31:41 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4069218)[0;0m ERROR 12-05 16:31:41 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4069218)[0;0m ERROR 12-05 16:31:41 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069218)[0;0m ERROR 12-05 16:31:41 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4069218)[0;0m ERROR 12-05 16:31:41 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4069218)[0;0m ERROR 12-05 16:31:41 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069218)[0;0m ERROR 12-05 16:31:41 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4069218)[0;0m ERROR 12-05 16:31:41 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4069218)[0;0m ERROR 12-05 16:31:41 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4069218)[0;0m ERROR 12-05 16:31:41 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4069218)[0;0m ERROR 12-05 16:31:41 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069218)[0;0m ERROR 12-05 16:31:41 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4069218)[0;0m ERROR 12-05 16:31:41 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4069218)[0;0m ERROR 12-05 16:31:41 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069218)[0;0m ERROR 12-05 16:31:41 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4069218)[0;0m ERROR 12-05 16:31:41 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4069218)[0;0m ERROR 12-05 16:31:41 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069218)[0;0m ERROR 12-05 16:31:41 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4069218)[0;0m ERROR 12-05 16:31:41 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4069218)[0;0m ERROR 12-05 16:31:41 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069218)[0;0m ERROR 12-05 16:31:41 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4069218)[0;0m ERROR 12-05 16:31:41 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4069218)[0;0m ERROR 12-05 16:31:41 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069218)[0;0m ERROR 12-05 16:31:41 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4069218)[0;0m ERROR 12-05 16:31:41 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4069218)[0;0m ERROR 12-05 16:31:41 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069218)[0;0m ERROR 12-05 16:31:41 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4069218)[0;0m ERROR 12-05 16:31:41 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4069218)[0;0m ERROR 12-05 16:31:41 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069218)[0;0m ERROR 12-05 16:31:41 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4069218)[0;0m ERROR 12-05 16:31:41 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4069218)[0;0m ERROR 12-05 16:31:41 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069218)[0;0m ERROR 12-05 16:31:41 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4069218)[0;0m ERROR 12-05 16:31:41 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4069218)[0;0m ERROR 12-05 16:31:41 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069218)[0;0m ERROR 12-05 16:31:41 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4069218)[0;0m ERROR 12-05 16:31:41 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4069218)[0;0m ERROR 12-05 16:31:41 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069218)[0;0m ERROR 12-05 16:31:41 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4069218)[0;0m ERROR 12-05 16:31:41 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4069218)[0;0m ERROR 12-05 16:31:41 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4069218)[0;0m ERROR 12-05 16:31:41 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4069218)[0;0m ERROR 12-05 16:31:41 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069218)[0;0m ERROR 12-05 16:31:41 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4069218)[0;0m ERROR 12-05 16:31:41 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4069218)[0;0m ERROR 12-05 16:31:41 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069218)[0;0m ERROR 12-05 16:31:41 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4069218)[0;0m ERROR 12-05 16:31:41 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4069218)[0;0m ERROR 12-05 16:31:41 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069218)[0;0m ERROR 12-05 16:31:41 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4069218)[0;0m ERROR 12-05 16:31:41 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4069218)[0;0m ERROR 12-05 16:31:41 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4069218)[0;0m ERROR 12-05 16:31:41 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4069218)[0;0m ERROR 12-05 16:31:41 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069218)[0;0m ERROR 12-05 16:31:41 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4069218)[0;0m ERROR 12-05 16:31:41 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4069218)[0;0m ERROR 12-05 16:31:41 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069218)[0;0m ERROR 12-05 16:31:41 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4069218)[0;0m ERROR 12-05 16:31:41 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4069218)[0;0m ERROR 12-05 16:31:41 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069218)[0;0m ERROR 12-05 16:31:41 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4069218)[0;0m ERROR 12-05 16:31:41 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4069218)[0;0m ERROR 12-05 16:31:41 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4069218)[0;0m ERROR 12-05 16:31:41 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4069218)[0;0m ERROR 12-05 16:31:41 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069218)[0;0m ERROR 12-05 16:31:41 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4069218)[0;0m ERROR 12-05 16:31:41 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4069218)[0;0m ERROR 12-05 16:31:41 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069218)[0;0m ERROR 12-05 16:31:41 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4069218)[0;0m ERROR 12-05 16:31:41 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4069218)[0;0m ERROR 12-05 16:31:41 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069218)[0;0m ERROR 12-05 16:31:41 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4069218)[0;0m ERROR 12-05 16:31:41 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4069218)[0;0m ERROR 12-05 16:31:41 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069218)[0;0m ERROR 12-05 16:31:41 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4069218)[0;0m ERROR 12-05 16:31:41 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4069218)[0;0m ERROR 12-05 16:31:41 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7fc05027c560>' raised:
[1;36m(EngineCore_DP0 pid=4069218)[0;0m ERROR 12-05 16:31:41 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4069218)[0;0m ERROR 12-05 16:31:41 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4069218)[0;0m ERROR 12-05 16:31:41 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4069218)[0;0m ERROR 12-05 16:31:41 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 16:32:02 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 16:32:02 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 16:32:02 [model.py:1745] Using max model len 131072
INFO 12-05 16:32:02 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4069306)[0;0m INFO 12-05 16:32:12 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4069306)[0;0m INFO 12-05 16:32:13 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:41469 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4069306)[0;0m INFO 12-05 16:32:13 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4069306)[0;0m INFO 12-05 16:32:13 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4069306)[0;0m INFO 12-05 16:32:14 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4069306)[0;0m INFO 12-05 16:32:14 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4069306)[0;0m INFO 12-05 16:32:15 [default_loader.py:314] Loading weights took 0.87 seconds
[1;36m(EngineCore_DP0 pid=4069306)[0;0m INFO 12-05 16:32:15 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.466446 seconds
[1;36m(EngineCore_DP0 pid=4069306)[0;0m INFO 12-05 16:32:20 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4069306)[0;0m INFO 12-05 16:32:20 [backends.py:647] Dynamo bytecode transform time: 4.06 s
[1;36m(EngineCore_DP0 pid=4069306)[0;0m ERROR 12-05 16:32:20 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4069306)[0;0m ERROR 12-05 16:32:20 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4069306)[0;0m ERROR 12-05 16:32:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4069306)[0;0m ERROR 12-05 16:32:20 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4069306)[0;0m ERROR 12-05 16:32:20 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069306)[0;0m ERROR 12-05 16:32:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4069306)[0;0m ERROR 12-05 16:32:20 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4069306)[0;0m ERROR 12-05 16:32:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4069306)[0;0m ERROR 12-05 16:32:20 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4069306)[0;0m ERROR 12-05 16:32:20 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069306)[0;0m ERROR 12-05 16:32:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4069306)[0;0m ERROR 12-05 16:32:20 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4069306)[0;0m ERROR 12-05 16:32:20 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069306)[0;0m ERROR 12-05 16:32:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4069306)[0;0m ERROR 12-05 16:32:20 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4069306)[0;0m ERROR 12-05 16:32:20 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069306)[0;0m ERROR 12-05 16:32:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4069306)[0;0m ERROR 12-05 16:32:20 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4069306)[0;0m ERROR 12-05 16:32:20 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069306)[0;0m ERROR 12-05 16:32:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4069306)[0;0m ERROR 12-05 16:32:20 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4069306)[0;0m ERROR 12-05 16:32:20 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069306)[0;0m ERROR 12-05 16:32:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4069306)[0;0m ERROR 12-05 16:32:20 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4069306)[0;0m ERROR 12-05 16:32:20 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069306)[0;0m ERROR 12-05 16:32:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4069306)[0;0m ERROR 12-05 16:32:20 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4069306)[0;0m ERROR 12-05 16:32:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4069306)[0;0m ERROR 12-05 16:32:20 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4069306)[0;0m ERROR 12-05 16:32:20 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069306)[0;0m ERROR 12-05 16:32:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4069306)[0;0m ERROR 12-05 16:32:20 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4069306)[0;0m ERROR 12-05 16:32:20 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069306)[0;0m ERROR 12-05 16:32:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4069306)[0;0m ERROR 12-05 16:32:20 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4069306)[0;0m ERROR 12-05 16:32:20 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069306)[0;0m ERROR 12-05 16:32:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4069306)[0;0m ERROR 12-05 16:32:20 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4069306)[0;0m ERROR 12-05 16:32:20 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069306)[0;0m ERROR 12-05 16:32:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4069306)[0;0m ERROR 12-05 16:32:20 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4069306)[0;0m ERROR 12-05 16:32:20 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069306)[0;0m ERROR 12-05 16:32:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4069306)[0;0m ERROR 12-05 16:32:20 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4069306)[0;0m ERROR 12-05 16:32:20 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069306)[0;0m ERROR 12-05 16:32:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4069306)[0;0m ERROR 12-05 16:32:20 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4069306)[0;0m ERROR 12-05 16:32:20 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069306)[0;0m ERROR 12-05 16:32:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4069306)[0;0m ERROR 12-05 16:32:20 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4069306)[0;0m ERROR 12-05 16:32:20 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069306)[0;0m ERROR 12-05 16:32:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4069306)[0;0m ERROR 12-05 16:32:20 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4069306)[0;0m ERROR 12-05 16:32:20 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069306)[0;0m ERROR 12-05 16:32:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4069306)[0;0m ERROR 12-05 16:32:20 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4069306)[0;0m ERROR 12-05 16:32:20 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069306)[0;0m ERROR 12-05 16:32:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4069306)[0;0m ERROR 12-05 16:32:20 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4069306)[0;0m ERROR 12-05 16:32:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4069306)[0;0m ERROR 12-05 16:32:20 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4069306)[0;0m ERROR 12-05 16:32:20 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069306)[0;0m ERROR 12-05 16:32:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4069306)[0;0m ERROR 12-05 16:32:20 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4069306)[0;0m ERROR 12-05 16:32:20 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069306)[0;0m ERROR 12-05 16:32:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4069306)[0;0m ERROR 12-05 16:32:20 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4069306)[0;0m ERROR 12-05 16:32:20 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069306)[0;0m ERROR 12-05 16:32:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4069306)[0;0m ERROR 12-05 16:32:20 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4069306)[0;0m ERROR 12-05 16:32:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4069306)[0;0m ERROR 12-05 16:32:20 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4069306)[0;0m ERROR 12-05 16:32:20 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069306)[0;0m ERROR 12-05 16:32:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4069306)[0;0m ERROR 12-05 16:32:20 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4069306)[0;0m ERROR 12-05 16:32:20 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069306)[0;0m ERROR 12-05 16:32:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4069306)[0;0m ERROR 12-05 16:32:20 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4069306)[0;0m ERROR 12-05 16:32:20 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069306)[0;0m ERROR 12-05 16:32:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4069306)[0;0m ERROR 12-05 16:32:20 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4069306)[0;0m ERROR 12-05 16:32:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4069306)[0;0m ERROR 12-05 16:32:20 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4069306)[0;0m ERROR 12-05 16:32:20 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069306)[0;0m ERROR 12-05 16:32:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4069306)[0;0m ERROR 12-05 16:32:20 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4069306)[0;0m ERROR 12-05 16:32:20 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069306)[0;0m ERROR 12-05 16:32:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4069306)[0;0m ERROR 12-05 16:32:20 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4069306)[0;0m ERROR 12-05 16:32:20 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069306)[0;0m ERROR 12-05 16:32:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4069306)[0;0m ERROR 12-05 16:32:20 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4069306)[0;0m ERROR 12-05 16:32:20 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069306)[0;0m ERROR 12-05 16:32:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4069306)[0;0m ERROR 12-05 16:32:20 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4069306)[0;0m ERROR 12-05 16:32:20 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7fbd3486a840>' raised:
[1;36m(EngineCore_DP0 pid=4069306)[0;0m ERROR 12-05 16:32:20 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4069306)[0;0m ERROR 12-05 16:32:20 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4069306)[0;0m ERROR 12-05 16:32:20 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4069306)[0;0m ERROR 12-05 16:32:20 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 16:32:41 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 16:32:41 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 16:32:41 [model.py:1745] Using max model len 131072
INFO 12-05 16:32:41 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4069387)[0;0m INFO 12-05 16:32:50 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4069387)[0;0m INFO 12-05 16:32:50 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:46293 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4069387)[0;0m INFO 12-05 16:32:50 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4069387)[0;0m INFO 12-05 16:32:51 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4069387)[0;0m INFO 12-05 16:32:51 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4069387)[0;0m INFO 12-05 16:32:51 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4069387)[0;0m INFO 12-05 16:32:53 [default_loader.py:314] Loading weights took 0.88 seconds
[1;36m(EngineCore_DP0 pid=4069387)[0;0m INFO 12-05 16:32:53 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.516653 seconds
[1;36m(EngineCore_DP0 pid=4069387)[0;0m INFO 12-05 16:32:57 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4069387)[0;0m INFO 12-05 16:32:57 [backends.py:647] Dynamo bytecode transform time: 3.93 s
[1;36m(EngineCore_DP0 pid=4069387)[0;0m ERROR 12-05 16:32:57 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4069387)[0;0m ERROR 12-05 16:32:57 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4069387)[0;0m ERROR 12-05 16:32:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4069387)[0;0m ERROR 12-05 16:32:57 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4069387)[0;0m ERROR 12-05 16:32:57 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069387)[0;0m ERROR 12-05 16:32:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4069387)[0;0m ERROR 12-05 16:32:57 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4069387)[0;0m ERROR 12-05 16:32:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4069387)[0;0m ERROR 12-05 16:32:57 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4069387)[0;0m ERROR 12-05 16:32:57 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069387)[0;0m ERROR 12-05 16:32:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4069387)[0;0m ERROR 12-05 16:32:57 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4069387)[0;0m ERROR 12-05 16:32:57 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069387)[0;0m ERROR 12-05 16:32:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4069387)[0;0m ERROR 12-05 16:32:57 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4069387)[0;0m ERROR 12-05 16:32:57 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069387)[0;0m ERROR 12-05 16:32:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4069387)[0;0m ERROR 12-05 16:32:57 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4069387)[0;0m ERROR 12-05 16:32:57 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069387)[0;0m ERROR 12-05 16:32:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4069387)[0;0m ERROR 12-05 16:32:57 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4069387)[0;0m ERROR 12-05 16:32:57 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069387)[0;0m ERROR 12-05 16:32:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4069387)[0;0m ERROR 12-05 16:32:57 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4069387)[0;0m ERROR 12-05 16:32:57 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069387)[0;0m ERROR 12-05 16:32:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4069387)[0;0m ERROR 12-05 16:32:57 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4069387)[0;0m ERROR 12-05 16:32:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4069387)[0;0m ERROR 12-05 16:32:57 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4069387)[0;0m ERROR 12-05 16:32:57 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069387)[0;0m ERROR 12-05 16:32:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4069387)[0;0m ERROR 12-05 16:32:57 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4069387)[0;0m ERROR 12-05 16:32:57 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069387)[0;0m ERROR 12-05 16:32:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4069387)[0;0m ERROR 12-05 16:32:57 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4069387)[0;0m ERROR 12-05 16:32:57 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069387)[0;0m ERROR 12-05 16:32:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4069387)[0;0m ERROR 12-05 16:32:57 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4069387)[0;0m ERROR 12-05 16:32:57 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069387)[0;0m ERROR 12-05 16:32:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4069387)[0;0m ERROR 12-05 16:32:57 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4069387)[0;0m ERROR 12-05 16:32:57 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069387)[0;0m ERROR 12-05 16:32:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4069387)[0;0m ERROR 12-05 16:32:57 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4069387)[0;0m ERROR 12-05 16:32:57 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069387)[0;0m ERROR 12-05 16:32:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4069387)[0;0m ERROR 12-05 16:32:57 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4069387)[0;0m ERROR 12-05 16:32:57 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069387)[0;0m ERROR 12-05 16:32:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4069387)[0;0m ERROR 12-05 16:32:57 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4069387)[0;0m ERROR 12-05 16:32:57 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069387)[0;0m ERROR 12-05 16:32:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4069387)[0;0m ERROR 12-05 16:32:57 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4069387)[0;0m ERROR 12-05 16:32:57 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069387)[0;0m ERROR 12-05 16:32:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4069387)[0;0m ERROR 12-05 16:32:57 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4069387)[0;0m ERROR 12-05 16:32:57 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069387)[0;0m ERROR 12-05 16:32:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4069387)[0;0m ERROR 12-05 16:32:57 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4069387)[0;0m ERROR 12-05 16:32:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4069387)[0;0m ERROR 12-05 16:32:57 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4069387)[0;0m ERROR 12-05 16:32:57 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069387)[0;0m ERROR 12-05 16:32:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4069387)[0;0m ERROR 12-05 16:32:57 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4069387)[0;0m ERROR 12-05 16:32:57 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069387)[0;0m ERROR 12-05 16:32:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4069387)[0;0m ERROR 12-05 16:32:57 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4069387)[0;0m ERROR 12-05 16:32:57 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069387)[0;0m ERROR 12-05 16:32:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4069387)[0;0m ERROR 12-05 16:32:57 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4069387)[0;0m ERROR 12-05 16:32:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4069387)[0;0m ERROR 12-05 16:32:57 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4069387)[0;0m ERROR 12-05 16:32:57 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069387)[0;0m ERROR 12-05 16:32:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4069387)[0;0m ERROR 12-05 16:32:57 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4069387)[0;0m ERROR 12-05 16:32:57 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069387)[0;0m ERROR 12-05 16:32:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4069387)[0;0m ERROR 12-05 16:32:57 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4069387)[0;0m ERROR 12-05 16:32:57 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069387)[0;0m ERROR 12-05 16:32:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4069387)[0;0m ERROR 12-05 16:32:57 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4069387)[0;0m ERROR 12-05 16:32:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4069387)[0;0m ERROR 12-05 16:32:57 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4069387)[0;0m ERROR 12-05 16:32:57 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069387)[0;0m ERROR 12-05 16:32:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4069387)[0;0m ERROR 12-05 16:32:57 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4069387)[0;0m ERROR 12-05 16:32:57 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069387)[0;0m ERROR 12-05 16:32:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4069387)[0;0m ERROR 12-05 16:32:57 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4069387)[0;0m ERROR 12-05 16:32:57 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069387)[0;0m ERROR 12-05 16:32:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4069387)[0;0m ERROR 12-05 16:32:57 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4069387)[0;0m ERROR 12-05 16:32:57 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069387)[0;0m ERROR 12-05 16:32:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4069387)[0;0m ERROR 12-05 16:32:57 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4069387)[0;0m ERROR 12-05 16:32:57 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7fb4cc1f07a0>' raised:
[1;36m(EngineCore_DP0 pid=4069387)[0;0m ERROR 12-05 16:32:57 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4069387)[0;0m ERROR 12-05 16:32:57 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4069387)[0;0m ERROR 12-05 16:32:57 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4069387)[0;0m ERROR 12-05 16:32:57 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 16:33:19 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 16:33:19 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 16:33:19 [model.py:1745] Using max model len 131072
INFO 12-05 16:33:19 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4069474)[0;0m INFO 12-05 16:33:28 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4069474)[0;0m INFO 12-05 16:33:29 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:56257 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4069474)[0;0m INFO 12-05 16:33:29 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4069474)[0;0m INFO 12-05 16:33:29 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4069474)[0;0m INFO 12-05 16:33:30 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4069474)[0;0m INFO 12-05 16:33:30 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4069474)[0;0m INFO 12-05 16:33:31 [default_loader.py:314] Loading weights took 0.88 seconds
[1;36m(EngineCore_DP0 pid=4069474)[0;0m INFO 12-05 16:33:31 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.482849 seconds
[1;36m(EngineCore_DP0 pid=4069474)[0;0m INFO 12-05 16:33:35 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4069474)[0;0m INFO 12-05 16:33:35 [backends.py:647] Dynamo bytecode transform time: 3.99 s
[1;36m(EngineCore_DP0 pid=4069474)[0;0m ERROR 12-05 16:33:36 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4069474)[0;0m ERROR 12-05 16:33:36 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4069474)[0;0m ERROR 12-05 16:33:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4069474)[0;0m ERROR 12-05 16:33:36 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4069474)[0;0m ERROR 12-05 16:33:36 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069474)[0;0m ERROR 12-05 16:33:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4069474)[0;0m ERROR 12-05 16:33:36 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4069474)[0;0m ERROR 12-05 16:33:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4069474)[0;0m ERROR 12-05 16:33:36 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4069474)[0;0m ERROR 12-05 16:33:36 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069474)[0;0m ERROR 12-05 16:33:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4069474)[0;0m ERROR 12-05 16:33:36 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4069474)[0;0m ERROR 12-05 16:33:36 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069474)[0;0m ERROR 12-05 16:33:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4069474)[0;0m ERROR 12-05 16:33:36 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4069474)[0;0m ERROR 12-05 16:33:36 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069474)[0;0m ERROR 12-05 16:33:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4069474)[0;0m ERROR 12-05 16:33:36 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4069474)[0;0m ERROR 12-05 16:33:36 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069474)[0;0m ERROR 12-05 16:33:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4069474)[0;0m ERROR 12-05 16:33:36 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4069474)[0;0m ERROR 12-05 16:33:36 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069474)[0;0m ERROR 12-05 16:33:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4069474)[0;0m ERROR 12-05 16:33:36 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4069474)[0;0m ERROR 12-05 16:33:36 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069474)[0;0m ERROR 12-05 16:33:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4069474)[0;0m ERROR 12-05 16:33:36 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4069474)[0;0m ERROR 12-05 16:33:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4069474)[0;0m ERROR 12-05 16:33:36 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4069474)[0;0m ERROR 12-05 16:33:36 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069474)[0;0m ERROR 12-05 16:33:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4069474)[0;0m ERROR 12-05 16:33:36 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4069474)[0;0m ERROR 12-05 16:33:36 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069474)[0;0m ERROR 12-05 16:33:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4069474)[0;0m ERROR 12-05 16:33:36 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4069474)[0;0m ERROR 12-05 16:33:36 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069474)[0;0m ERROR 12-05 16:33:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4069474)[0;0m ERROR 12-05 16:33:36 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4069474)[0;0m ERROR 12-05 16:33:36 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069474)[0;0m ERROR 12-05 16:33:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4069474)[0;0m ERROR 12-05 16:33:36 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4069474)[0;0m ERROR 12-05 16:33:36 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069474)[0;0m ERROR 12-05 16:33:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4069474)[0;0m ERROR 12-05 16:33:36 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4069474)[0;0m ERROR 12-05 16:33:36 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069474)[0;0m ERROR 12-05 16:33:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4069474)[0;0m ERROR 12-05 16:33:36 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4069474)[0;0m ERROR 12-05 16:33:36 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069474)[0;0m ERROR 12-05 16:33:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4069474)[0;0m ERROR 12-05 16:33:36 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4069474)[0;0m ERROR 12-05 16:33:36 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069474)[0;0m ERROR 12-05 16:33:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4069474)[0;0m ERROR 12-05 16:33:36 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4069474)[0;0m ERROR 12-05 16:33:36 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069474)[0;0m ERROR 12-05 16:33:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4069474)[0;0m ERROR 12-05 16:33:36 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4069474)[0;0m ERROR 12-05 16:33:36 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069474)[0;0m ERROR 12-05 16:33:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4069474)[0;0m ERROR 12-05 16:33:36 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4069474)[0;0m ERROR 12-05 16:33:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4069474)[0;0m ERROR 12-05 16:33:36 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4069474)[0;0m ERROR 12-05 16:33:36 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069474)[0;0m ERROR 12-05 16:33:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4069474)[0;0m ERROR 12-05 16:33:36 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4069474)[0;0m ERROR 12-05 16:33:36 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069474)[0;0m ERROR 12-05 16:33:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4069474)[0;0m ERROR 12-05 16:33:36 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4069474)[0;0m ERROR 12-05 16:33:36 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069474)[0;0m ERROR 12-05 16:33:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4069474)[0;0m ERROR 12-05 16:33:36 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4069474)[0;0m ERROR 12-05 16:33:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4069474)[0;0m ERROR 12-05 16:33:36 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4069474)[0;0m ERROR 12-05 16:33:36 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069474)[0;0m ERROR 12-05 16:33:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4069474)[0;0m ERROR 12-05 16:33:36 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4069474)[0;0m ERROR 12-05 16:33:36 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069474)[0;0m ERROR 12-05 16:33:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4069474)[0;0m ERROR 12-05 16:33:36 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4069474)[0;0m ERROR 12-05 16:33:36 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069474)[0;0m ERROR 12-05 16:33:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4069474)[0;0m ERROR 12-05 16:33:36 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4069474)[0;0m ERROR 12-05 16:33:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4069474)[0;0m ERROR 12-05 16:33:36 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4069474)[0;0m ERROR 12-05 16:33:36 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069474)[0;0m ERROR 12-05 16:33:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4069474)[0;0m ERROR 12-05 16:33:36 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4069474)[0;0m ERROR 12-05 16:33:36 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069474)[0;0m ERROR 12-05 16:33:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4069474)[0;0m ERROR 12-05 16:33:36 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4069474)[0;0m ERROR 12-05 16:33:36 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069474)[0;0m ERROR 12-05 16:33:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4069474)[0;0m ERROR 12-05 16:33:36 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4069474)[0;0m ERROR 12-05 16:33:36 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069474)[0;0m ERROR 12-05 16:33:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4069474)[0;0m ERROR 12-05 16:33:36 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4069474)[0;0m ERROR 12-05 16:33:36 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7ff9243c3560>' raised:
[1;36m(EngineCore_DP0 pid=4069474)[0;0m ERROR 12-05 16:33:36 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4069474)[0;0m ERROR 12-05 16:33:36 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4069474)[0;0m ERROR 12-05 16:33:36 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4069474)[0;0m ERROR 12-05 16:33:36 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 16:33:57 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 16:33:57 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 16:33:57 [model.py:1745] Using max model len 131072
INFO 12-05 16:33:57 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4069562)[0;0m INFO 12-05 16:34:05 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4069562)[0;0m INFO 12-05 16:34:05 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:43057 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4069562)[0;0m INFO 12-05 16:34:06 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4069562)[0;0m INFO 12-05 16:34:06 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4069562)[0;0m INFO 12-05 16:34:06 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4069562)[0;0m INFO 12-05 16:34:06 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4069562)[0;0m INFO 12-05 16:34:07 [default_loader.py:314] Loading weights took 0.88 seconds
[1;36m(EngineCore_DP0 pid=4069562)[0;0m INFO 12-05 16:34:08 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.410999 seconds
[1;36m(EngineCore_DP0 pid=4069562)[0;0m INFO 12-05 16:34:12 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4069562)[0;0m INFO 12-05 16:34:12 [backends.py:647] Dynamo bytecode transform time: 3.74 s
[1;36m(EngineCore_DP0 pid=4069562)[0;0m ERROR 12-05 16:34:12 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4069562)[0;0m ERROR 12-05 16:34:12 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4069562)[0;0m ERROR 12-05 16:34:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4069562)[0;0m ERROR 12-05 16:34:12 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4069562)[0;0m ERROR 12-05 16:34:12 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069562)[0;0m ERROR 12-05 16:34:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4069562)[0;0m ERROR 12-05 16:34:12 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4069562)[0;0m ERROR 12-05 16:34:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4069562)[0;0m ERROR 12-05 16:34:12 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4069562)[0;0m ERROR 12-05 16:34:12 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069562)[0;0m ERROR 12-05 16:34:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4069562)[0;0m ERROR 12-05 16:34:12 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4069562)[0;0m ERROR 12-05 16:34:12 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069562)[0;0m ERROR 12-05 16:34:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4069562)[0;0m ERROR 12-05 16:34:12 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4069562)[0;0m ERROR 12-05 16:34:12 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069562)[0;0m ERROR 12-05 16:34:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4069562)[0;0m ERROR 12-05 16:34:12 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4069562)[0;0m ERROR 12-05 16:34:12 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069562)[0;0m ERROR 12-05 16:34:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4069562)[0;0m ERROR 12-05 16:34:12 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4069562)[0;0m ERROR 12-05 16:34:12 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069562)[0;0m ERROR 12-05 16:34:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4069562)[0;0m ERROR 12-05 16:34:12 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4069562)[0;0m ERROR 12-05 16:34:12 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069562)[0;0m ERROR 12-05 16:34:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4069562)[0;0m ERROR 12-05 16:34:12 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4069562)[0;0m ERROR 12-05 16:34:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4069562)[0;0m ERROR 12-05 16:34:12 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4069562)[0;0m ERROR 12-05 16:34:12 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069562)[0;0m ERROR 12-05 16:34:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4069562)[0;0m ERROR 12-05 16:34:12 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4069562)[0;0m ERROR 12-05 16:34:12 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069562)[0;0m ERROR 12-05 16:34:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4069562)[0;0m ERROR 12-05 16:34:12 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4069562)[0;0m ERROR 12-05 16:34:12 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069562)[0;0m ERROR 12-05 16:34:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4069562)[0;0m ERROR 12-05 16:34:12 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4069562)[0;0m ERROR 12-05 16:34:12 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069562)[0;0m ERROR 12-05 16:34:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4069562)[0;0m ERROR 12-05 16:34:12 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4069562)[0;0m ERROR 12-05 16:34:12 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069562)[0;0m ERROR 12-05 16:34:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4069562)[0;0m ERROR 12-05 16:34:12 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4069562)[0;0m ERROR 12-05 16:34:12 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069562)[0;0m ERROR 12-05 16:34:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4069562)[0;0m ERROR 12-05 16:34:12 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4069562)[0;0m ERROR 12-05 16:34:12 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069562)[0;0m ERROR 12-05 16:34:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4069562)[0;0m ERROR 12-05 16:34:12 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4069562)[0;0m ERROR 12-05 16:34:12 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069562)[0;0m ERROR 12-05 16:34:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4069562)[0;0m ERROR 12-05 16:34:12 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4069562)[0;0m ERROR 12-05 16:34:12 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069562)[0;0m ERROR 12-05 16:34:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4069562)[0;0m ERROR 12-05 16:34:12 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4069562)[0;0m ERROR 12-05 16:34:12 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069562)[0;0m ERROR 12-05 16:34:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4069562)[0;0m ERROR 12-05 16:34:12 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4069562)[0;0m ERROR 12-05 16:34:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4069562)[0;0m ERROR 12-05 16:34:12 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4069562)[0;0m ERROR 12-05 16:34:12 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069562)[0;0m ERROR 12-05 16:34:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4069562)[0;0m ERROR 12-05 16:34:12 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4069562)[0;0m ERROR 12-05 16:34:12 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069562)[0;0m ERROR 12-05 16:34:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4069562)[0;0m ERROR 12-05 16:34:12 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4069562)[0;0m ERROR 12-05 16:34:12 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069562)[0;0m ERROR 12-05 16:34:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4069562)[0;0m ERROR 12-05 16:34:12 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4069562)[0;0m ERROR 12-05 16:34:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4069562)[0;0m ERROR 12-05 16:34:12 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4069562)[0;0m ERROR 12-05 16:34:12 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069562)[0;0m ERROR 12-05 16:34:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4069562)[0;0m ERROR 12-05 16:34:12 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4069562)[0;0m ERROR 12-05 16:34:12 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069562)[0;0m ERROR 12-05 16:34:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4069562)[0;0m ERROR 12-05 16:34:12 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4069562)[0;0m ERROR 12-05 16:34:12 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069562)[0;0m ERROR 12-05 16:34:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4069562)[0;0m ERROR 12-05 16:34:12 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4069562)[0;0m ERROR 12-05 16:34:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4069562)[0;0m ERROR 12-05 16:34:12 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4069562)[0;0m ERROR 12-05 16:34:12 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069562)[0;0m ERROR 12-05 16:34:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4069562)[0;0m ERROR 12-05 16:34:12 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4069562)[0;0m ERROR 12-05 16:34:12 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069562)[0;0m ERROR 12-05 16:34:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4069562)[0;0m ERROR 12-05 16:34:12 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4069562)[0;0m ERROR 12-05 16:34:12 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069562)[0;0m ERROR 12-05 16:34:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4069562)[0;0m ERROR 12-05 16:34:12 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4069562)[0;0m ERROR 12-05 16:34:12 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069562)[0;0m ERROR 12-05 16:34:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4069562)[0;0m ERROR 12-05 16:34:12 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4069562)[0;0m ERROR 12-05 16:34:12 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7ff5e425c620>' raised:
[1;36m(EngineCore_DP0 pid=4069562)[0;0m ERROR 12-05 16:34:12 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4069562)[0;0m ERROR 12-05 16:34:12 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4069562)[0;0m ERROR 12-05 16:34:12 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4069562)[0;0m ERROR 12-05 16:34:12 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 16:34:33 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 16:34:34 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 16:34:34 [model.py:1745] Using max model len 131072
INFO 12-05 16:34:34 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4069644)[0;0m INFO 12-05 16:34:42 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4069644)[0;0m INFO 12-05 16:34:43 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:47983 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4069644)[0;0m INFO 12-05 16:34:43 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4069644)[0;0m INFO 12-05 16:34:43 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4069644)[0;0m INFO 12-05 16:34:44 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4069644)[0;0m INFO 12-05 16:34:44 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4069644)[0;0m INFO 12-05 16:34:45 [default_loader.py:314] Loading weights took 0.87 seconds
[1;36m(EngineCore_DP0 pid=4069644)[0;0m INFO 12-05 16:34:46 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.477293 seconds
[1;36m(EngineCore_DP0 pid=4069644)[0;0m INFO 12-05 16:34:50 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4069644)[0;0m INFO 12-05 16:34:50 [backends.py:647] Dynamo bytecode transform time: 4.06 s
[1;36m(EngineCore_DP0 pid=4069644)[0;0m ERROR 12-05 16:34:50 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4069644)[0;0m ERROR 12-05 16:34:50 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4069644)[0;0m ERROR 12-05 16:34:50 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4069644)[0;0m ERROR 12-05 16:34:50 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4069644)[0;0m ERROR 12-05 16:34:50 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069644)[0;0m ERROR 12-05 16:34:50 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4069644)[0;0m ERROR 12-05 16:34:50 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4069644)[0;0m ERROR 12-05 16:34:50 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4069644)[0;0m ERROR 12-05 16:34:50 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4069644)[0;0m ERROR 12-05 16:34:50 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069644)[0;0m ERROR 12-05 16:34:50 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4069644)[0;0m ERROR 12-05 16:34:50 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4069644)[0;0m ERROR 12-05 16:34:50 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069644)[0;0m ERROR 12-05 16:34:50 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4069644)[0;0m ERROR 12-05 16:34:50 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4069644)[0;0m ERROR 12-05 16:34:50 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069644)[0;0m ERROR 12-05 16:34:50 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4069644)[0;0m ERROR 12-05 16:34:50 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4069644)[0;0m ERROR 12-05 16:34:50 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069644)[0;0m ERROR 12-05 16:34:50 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4069644)[0;0m ERROR 12-05 16:34:50 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4069644)[0;0m ERROR 12-05 16:34:50 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069644)[0;0m ERROR 12-05 16:34:50 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4069644)[0;0m ERROR 12-05 16:34:50 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4069644)[0;0m ERROR 12-05 16:34:50 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069644)[0;0m ERROR 12-05 16:34:50 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4069644)[0;0m ERROR 12-05 16:34:50 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4069644)[0;0m ERROR 12-05 16:34:50 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4069644)[0;0m ERROR 12-05 16:34:50 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4069644)[0;0m ERROR 12-05 16:34:50 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069644)[0;0m ERROR 12-05 16:34:50 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4069644)[0;0m ERROR 12-05 16:34:50 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4069644)[0;0m ERROR 12-05 16:34:50 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069644)[0;0m ERROR 12-05 16:34:50 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4069644)[0;0m ERROR 12-05 16:34:50 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4069644)[0;0m ERROR 12-05 16:34:50 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069644)[0;0m ERROR 12-05 16:34:50 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4069644)[0;0m ERROR 12-05 16:34:50 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4069644)[0;0m ERROR 12-05 16:34:50 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069644)[0;0m ERROR 12-05 16:34:50 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4069644)[0;0m ERROR 12-05 16:34:50 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4069644)[0;0m ERROR 12-05 16:34:50 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069644)[0;0m ERROR 12-05 16:34:50 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4069644)[0;0m ERROR 12-05 16:34:50 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4069644)[0;0m ERROR 12-05 16:34:50 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069644)[0;0m ERROR 12-05 16:34:50 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4069644)[0;0m ERROR 12-05 16:34:50 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4069644)[0;0m ERROR 12-05 16:34:50 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069644)[0;0m ERROR 12-05 16:34:50 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4069644)[0;0m ERROR 12-05 16:34:50 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4069644)[0;0m ERROR 12-05 16:34:50 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069644)[0;0m ERROR 12-05 16:34:50 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4069644)[0;0m ERROR 12-05 16:34:50 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4069644)[0;0m ERROR 12-05 16:34:50 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069644)[0;0m ERROR 12-05 16:34:50 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4069644)[0;0m ERROR 12-05 16:34:50 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4069644)[0;0m ERROR 12-05 16:34:50 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069644)[0;0m ERROR 12-05 16:34:50 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4069644)[0;0m ERROR 12-05 16:34:50 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4069644)[0;0m ERROR 12-05 16:34:50 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4069644)[0;0m ERROR 12-05 16:34:50 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4069644)[0;0m ERROR 12-05 16:34:50 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069644)[0;0m ERROR 12-05 16:34:50 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4069644)[0;0m ERROR 12-05 16:34:50 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4069644)[0;0m ERROR 12-05 16:34:50 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069644)[0;0m ERROR 12-05 16:34:50 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4069644)[0;0m ERROR 12-05 16:34:50 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4069644)[0;0m ERROR 12-05 16:34:50 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069644)[0;0m ERROR 12-05 16:34:50 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4069644)[0;0m ERROR 12-05 16:34:50 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4069644)[0;0m ERROR 12-05 16:34:50 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4069644)[0;0m ERROR 12-05 16:34:50 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4069644)[0;0m ERROR 12-05 16:34:50 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069644)[0;0m ERROR 12-05 16:34:50 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4069644)[0;0m ERROR 12-05 16:34:50 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4069644)[0;0m ERROR 12-05 16:34:50 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069644)[0;0m ERROR 12-05 16:34:50 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4069644)[0;0m ERROR 12-05 16:34:50 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4069644)[0;0m ERROR 12-05 16:34:50 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069644)[0;0m ERROR 12-05 16:34:50 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4069644)[0;0m ERROR 12-05 16:34:50 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4069644)[0;0m ERROR 12-05 16:34:50 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4069644)[0;0m ERROR 12-05 16:34:50 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4069644)[0;0m ERROR 12-05 16:34:50 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069644)[0;0m ERROR 12-05 16:34:50 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4069644)[0;0m ERROR 12-05 16:34:50 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4069644)[0;0m ERROR 12-05 16:34:50 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069644)[0;0m ERROR 12-05 16:34:50 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4069644)[0;0m ERROR 12-05 16:34:50 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4069644)[0;0m ERROR 12-05 16:34:50 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069644)[0;0m ERROR 12-05 16:34:50 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4069644)[0;0m ERROR 12-05 16:34:50 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4069644)[0;0m ERROR 12-05 16:34:50 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069644)[0;0m ERROR 12-05 16:34:50 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4069644)[0;0m ERROR 12-05 16:34:50 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4069644)[0;0m ERROR 12-05 16:34:50 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7f23f2d8f470>' raised:
[1;36m(EngineCore_DP0 pid=4069644)[0;0m ERROR 12-05 16:34:50 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4069644)[0;0m ERROR 12-05 16:34:50 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4069644)[0;0m ERROR 12-05 16:34:50 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4069644)[0;0m ERROR 12-05 16:34:50 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 16:35:11 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 16:35:12 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 16:35:12 [model.py:1745] Using max model len 131072
INFO 12-05 16:35:12 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4069733)[0;0m INFO 12-05 16:35:20 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4069733)[0;0m INFO 12-05 16:35:21 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:42065 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4069733)[0;0m INFO 12-05 16:35:21 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4069733)[0;0m INFO 12-05 16:35:21 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4069733)[0;0m INFO 12-05 16:35:21 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4069733)[0;0m INFO 12-05 16:35:21 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4069733)[0;0m INFO 12-05 16:35:22 [default_loader.py:314] Loading weights took 0.87 seconds
[1;36m(EngineCore_DP0 pid=4069733)[0;0m INFO 12-05 16:35:23 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.420779 seconds
[1;36m(EngineCore_DP0 pid=4069733)[0;0m INFO 12-05 16:35:27 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4069733)[0;0m INFO 12-05 16:35:27 [backends.py:647] Dynamo bytecode transform time: 3.92 s
[1;36m(EngineCore_DP0 pid=4069733)[0;0m ERROR 12-05 16:35:27 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4069733)[0;0m ERROR 12-05 16:35:27 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4069733)[0;0m ERROR 12-05 16:35:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4069733)[0;0m ERROR 12-05 16:35:27 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4069733)[0;0m ERROR 12-05 16:35:27 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069733)[0;0m ERROR 12-05 16:35:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4069733)[0;0m ERROR 12-05 16:35:27 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4069733)[0;0m ERROR 12-05 16:35:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4069733)[0;0m ERROR 12-05 16:35:27 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4069733)[0;0m ERROR 12-05 16:35:27 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069733)[0;0m ERROR 12-05 16:35:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4069733)[0;0m ERROR 12-05 16:35:27 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4069733)[0;0m ERROR 12-05 16:35:27 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069733)[0;0m ERROR 12-05 16:35:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4069733)[0;0m ERROR 12-05 16:35:27 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4069733)[0;0m ERROR 12-05 16:35:27 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069733)[0;0m ERROR 12-05 16:35:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4069733)[0;0m ERROR 12-05 16:35:27 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4069733)[0;0m ERROR 12-05 16:35:27 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069733)[0;0m ERROR 12-05 16:35:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4069733)[0;0m ERROR 12-05 16:35:27 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4069733)[0;0m ERROR 12-05 16:35:27 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069733)[0;0m ERROR 12-05 16:35:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4069733)[0;0m ERROR 12-05 16:35:27 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4069733)[0;0m ERROR 12-05 16:35:27 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069733)[0;0m ERROR 12-05 16:35:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4069733)[0;0m ERROR 12-05 16:35:27 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4069733)[0;0m ERROR 12-05 16:35:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4069733)[0;0m ERROR 12-05 16:35:27 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4069733)[0;0m ERROR 12-05 16:35:27 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069733)[0;0m ERROR 12-05 16:35:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4069733)[0;0m ERROR 12-05 16:35:27 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4069733)[0;0m ERROR 12-05 16:35:27 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069733)[0;0m ERROR 12-05 16:35:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4069733)[0;0m ERROR 12-05 16:35:27 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4069733)[0;0m ERROR 12-05 16:35:27 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069733)[0;0m ERROR 12-05 16:35:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4069733)[0;0m ERROR 12-05 16:35:27 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4069733)[0;0m ERROR 12-05 16:35:27 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069733)[0;0m ERROR 12-05 16:35:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4069733)[0;0m ERROR 12-05 16:35:27 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4069733)[0;0m ERROR 12-05 16:35:27 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069733)[0;0m ERROR 12-05 16:35:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4069733)[0;0m ERROR 12-05 16:35:27 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4069733)[0;0m ERROR 12-05 16:35:27 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069733)[0;0m ERROR 12-05 16:35:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4069733)[0;0m ERROR 12-05 16:35:27 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4069733)[0;0m ERROR 12-05 16:35:27 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069733)[0;0m ERROR 12-05 16:35:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4069733)[0;0m ERROR 12-05 16:35:27 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4069733)[0;0m ERROR 12-05 16:35:27 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069733)[0;0m ERROR 12-05 16:35:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4069733)[0;0m ERROR 12-05 16:35:27 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4069733)[0;0m ERROR 12-05 16:35:27 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069733)[0;0m ERROR 12-05 16:35:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4069733)[0;0m ERROR 12-05 16:35:27 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4069733)[0;0m ERROR 12-05 16:35:27 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069733)[0;0m ERROR 12-05 16:35:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4069733)[0;0m ERROR 12-05 16:35:27 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4069733)[0;0m ERROR 12-05 16:35:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4069733)[0;0m ERROR 12-05 16:35:27 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4069733)[0;0m ERROR 12-05 16:35:27 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069733)[0;0m ERROR 12-05 16:35:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4069733)[0;0m ERROR 12-05 16:35:27 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4069733)[0;0m ERROR 12-05 16:35:27 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069733)[0;0m ERROR 12-05 16:35:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4069733)[0;0m ERROR 12-05 16:35:27 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4069733)[0;0m ERROR 12-05 16:35:27 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069733)[0;0m ERROR 12-05 16:35:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4069733)[0;0m ERROR 12-05 16:35:27 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4069733)[0;0m ERROR 12-05 16:35:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4069733)[0;0m ERROR 12-05 16:35:27 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4069733)[0;0m ERROR 12-05 16:35:27 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069733)[0;0m ERROR 12-05 16:35:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4069733)[0;0m ERROR 12-05 16:35:27 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4069733)[0;0m ERROR 12-05 16:35:27 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069733)[0;0m ERROR 12-05 16:35:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4069733)[0;0m ERROR 12-05 16:35:27 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4069733)[0;0m ERROR 12-05 16:35:27 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069733)[0;0m ERROR 12-05 16:35:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4069733)[0;0m ERROR 12-05 16:35:27 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4069733)[0;0m ERROR 12-05 16:35:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4069733)[0;0m ERROR 12-05 16:35:27 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4069733)[0;0m ERROR 12-05 16:35:27 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069733)[0;0m ERROR 12-05 16:35:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4069733)[0;0m ERROR 12-05 16:35:27 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4069733)[0;0m ERROR 12-05 16:35:27 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069733)[0;0m ERROR 12-05 16:35:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4069733)[0;0m ERROR 12-05 16:35:27 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4069733)[0;0m ERROR 12-05 16:35:27 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069733)[0;0m ERROR 12-05 16:35:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4069733)[0;0m ERROR 12-05 16:35:27 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4069733)[0;0m ERROR 12-05 16:35:27 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069733)[0;0m ERROR 12-05 16:35:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4069733)[0;0m ERROR 12-05 16:35:27 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4069733)[0;0m ERROR 12-05 16:35:27 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7f317c379730>' raised:
[1;36m(EngineCore_DP0 pid=4069733)[0;0m ERROR 12-05 16:35:27 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4069733)[0;0m ERROR 12-05 16:35:27 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4069733)[0;0m ERROR 12-05 16:35:27 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4069733)[0;0m ERROR 12-05 16:35:27 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 16:35:49 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 16:35:49 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 16:35:49 [model.py:1745] Using max model len 131072
INFO 12-05 16:35:49 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4069820)[0;0m INFO 12-05 16:35:58 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4069820)[0;0m INFO 12-05 16:35:59 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:57935 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4069820)[0;0m INFO 12-05 16:35:59 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4069820)[0;0m INFO 12-05 16:35:59 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4069820)[0;0m INFO 12-05 16:36:00 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4069820)[0;0m INFO 12-05 16:36:00 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4069820)[0;0m INFO 12-05 16:36:01 [default_loader.py:314] Loading weights took 0.88 seconds
[1;36m(EngineCore_DP0 pid=4069820)[0;0m INFO 12-05 16:36:01 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.427028 seconds
[1;36m(EngineCore_DP0 pid=4069820)[0;0m INFO 12-05 16:36:05 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4069820)[0;0m INFO 12-05 16:36:05 [backends.py:647] Dynamo bytecode transform time: 3.76 s
[1;36m(EngineCore_DP0 pid=4069820)[0;0m ERROR 12-05 16:36:05 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4069820)[0;0m ERROR 12-05 16:36:05 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4069820)[0;0m ERROR 12-05 16:36:05 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4069820)[0;0m ERROR 12-05 16:36:05 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4069820)[0;0m ERROR 12-05 16:36:05 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069820)[0;0m ERROR 12-05 16:36:05 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4069820)[0;0m ERROR 12-05 16:36:05 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4069820)[0;0m ERROR 12-05 16:36:05 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4069820)[0;0m ERROR 12-05 16:36:05 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4069820)[0;0m ERROR 12-05 16:36:05 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069820)[0;0m ERROR 12-05 16:36:05 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4069820)[0;0m ERROR 12-05 16:36:05 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4069820)[0;0m ERROR 12-05 16:36:05 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069820)[0;0m ERROR 12-05 16:36:05 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4069820)[0;0m ERROR 12-05 16:36:05 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4069820)[0;0m ERROR 12-05 16:36:05 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069820)[0;0m ERROR 12-05 16:36:05 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4069820)[0;0m ERROR 12-05 16:36:05 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4069820)[0;0m ERROR 12-05 16:36:05 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069820)[0;0m ERROR 12-05 16:36:05 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4069820)[0;0m ERROR 12-05 16:36:05 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4069820)[0;0m ERROR 12-05 16:36:05 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069820)[0;0m ERROR 12-05 16:36:05 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4069820)[0;0m ERROR 12-05 16:36:05 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4069820)[0;0m ERROR 12-05 16:36:05 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069820)[0;0m ERROR 12-05 16:36:05 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4069820)[0;0m ERROR 12-05 16:36:05 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4069820)[0;0m ERROR 12-05 16:36:05 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4069820)[0;0m ERROR 12-05 16:36:05 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4069820)[0;0m ERROR 12-05 16:36:05 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069820)[0;0m ERROR 12-05 16:36:05 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4069820)[0;0m ERROR 12-05 16:36:05 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4069820)[0;0m ERROR 12-05 16:36:05 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069820)[0;0m ERROR 12-05 16:36:05 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4069820)[0;0m ERROR 12-05 16:36:05 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4069820)[0;0m ERROR 12-05 16:36:05 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069820)[0;0m ERROR 12-05 16:36:05 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4069820)[0;0m ERROR 12-05 16:36:05 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4069820)[0;0m ERROR 12-05 16:36:05 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069820)[0;0m ERROR 12-05 16:36:05 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4069820)[0;0m ERROR 12-05 16:36:05 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4069820)[0;0m ERROR 12-05 16:36:05 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069820)[0;0m ERROR 12-05 16:36:05 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4069820)[0;0m ERROR 12-05 16:36:05 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4069820)[0;0m ERROR 12-05 16:36:05 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069820)[0;0m ERROR 12-05 16:36:05 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4069820)[0;0m ERROR 12-05 16:36:05 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4069820)[0;0m ERROR 12-05 16:36:05 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069820)[0;0m ERROR 12-05 16:36:05 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4069820)[0;0m ERROR 12-05 16:36:05 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4069820)[0;0m ERROR 12-05 16:36:05 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069820)[0;0m ERROR 12-05 16:36:05 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4069820)[0;0m ERROR 12-05 16:36:05 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4069820)[0;0m ERROR 12-05 16:36:05 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069820)[0;0m ERROR 12-05 16:36:05 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4069820)[0;0m ERROR 12-05 16:36:05 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4069820)[0;0m ERROR 12-05 16:36:05 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069820)[0;0m ERROR 12-05 16:36:05 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4069820)[0;0m ERROR 12-05 16:36:05 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4069820)[0;0m ERROR 12-05 16:36:05 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4069820)[0;0m ERROR 12-05 16:36:05 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4069820)[0;0m ERROR 12-05 16:36:05 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069820)[0;0m ERROR 12-05 16:36:05 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4069820)[0;0m ERROR 12-05 16:36:05 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4069820)[0;0m ERROR 12-05 16:36:05 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069820)[0;0m ERROR 12-05 16:36:05 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4069820)[0;0m ERROR 12-05 16:36:05 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4069820)[0;0m ERROR 12-05 16:36:05 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069820)[0;0m ERROR 12-05 16:36:05 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4069820)[0;0m ERROR 12-05 16:36:05 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4069820)[0;0m ERROR 12-05 16:36:05 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4069820)[0;0m ERROR 12-05 16:36:05 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4069820)[0;0m ERROR 12-05 16:36:05 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069820)[0;0m ERROR 12-05 16:36:05 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4069820)[0;0m ERROR 12-05 16:36:05 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4069820)[0;0m ERROR 12-05 16:36:05 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069820)[0;0m ERROR 12-05 16:36:05 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4069820)[0;0m ERROR 12-05 16:36:05 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4069820)[0;0m ERROR 12-05 16:36:05 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069820)[0;0m ERROR 12-05 16:36:05 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4069820)[0;0m ERROR 12-05 16:36:05 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4069820)[0;0m ERROR 12-05 16:36:05 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4069820)[0;0m ERROR 12-05 16:36:05 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4069820)[0;0m ERROR 12-05 16:36:05 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069820)[0;0m ERROR 12-05 16:36:05 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4069820)[0;0m ERROR 12-05 16:36:05 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4069820)[0;0m ERROR 12-05 16:36:05 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069820)[0;0m ERROR 12-05 16:36:05 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4069820)[0;0m ERROR 12-05 16:36:05 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4069820)[0;0m ERROR 12-05 16:36:05 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069820)[0;0m ERROR 12-05 16:36:05 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4069820)[0;0m ERROR 12-05 16:36:05 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4069820)[0;0m ERROR 12-05 16:36:05 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069820)[0;0m ERROR 12-05 16:36:05 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4069820)[0;0m ERROR 12-05 16:36:05 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4069820)[0;0m ERROR 12-05 16:36:05 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7f8a282a4560>' raised:
[1;36m(EngineCore_DP0 pid=4069820)[0;0m ERROR 12-05 16:36:05 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4069820)[0;0m ERROR 12-05 16:36:05 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4069820)[0;0m ERROR 12-05 16:36:05 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4069820)[0;0m ERROR 12-05 16:36:05 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 16:36:27 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 16:36:27 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 16:36:27 [model.py:1745] Using max model len 131072
INFO 12-05 16:36:27 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4069996)[0;0m INFO 12-05 16:36:35 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4069996)[0;0m INFO 12-05 16:36:36 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:52811 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4069996)[0;0m INFO 12-05 16:36:36 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4069996)[0;0m INFO 12-05 16:36:36 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4069996)[0;0m INFO 12-05 16:36:37 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4069996)[0;0m INFO 12-05 16:36:37 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4069996)[0;0m INFO 12-05 16:36:38 [default_loader.py:314] Loading weights took 0.88 seconds
[1;36m(EngineCore_DP0 pid=4069996)[0;0m INFO 12-05 16:36:38 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.427471 seconds
[1;36m(EngineCore_DP0 pid=4069996)[0;0m INFO 12-05 16:36:43 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4069996)[0;0m INFO 12-05 16:36:43 [backends.py:647] Dynamo bytecode transform time: 4.15 s
[1;36m(EngineCore_DP0 pid=4069996)[0;0m ERROR 12-05 16:36:43 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4069996)[0;0m ERROR 12-05 16:36:43 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4069996)[0;0m ERROR 12-05 16:36:43 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4069996)[0;0m ERROR 12-05 16:36:43 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4069996)[0;0m ERROR 12-05 16:36:43 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069996)[0;0m ERROR 12-05 16:36:43 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4069996)[0;0m ERROR 12-05 16:36:43 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4069996)[0;0m ERROR 12-05 16:36:43 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4069996)[0;0m ERROR 12-05 16:36:43 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4069996)[0;0m ERROR 12-05 16:36:43 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069996)[0;0m ERROR 12-05 16:36:43 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4069996)[0;0m ERROR 12-05 16:36:43 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4069996)[0;0m ERROR 12-05 16:36:43 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069996)[0;0m ERROR 12-05 16:36:43 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4069996)[0;0m ERROR 12-05 16:36:43 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4069996)[0;0m ERROR 12-05 16:36:43 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069996)[0;0m ERROR 12-05 16:36:43 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4069996)[0;0m ERROR 12-05 16:36:43 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4069996)[0;0m ERROR 12-05 16:36:43 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069996)[0;0m ERROR 12-05 16:36:43 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4069996)[0;0m ERROR 12-05 16:36:43 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4069996)[0;0m ERROR 12-05 16:36:43 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069996)[0;0m ERROR 12-05 16:36:43 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4069996)[0;0m ERROR 12-05 16:36:43 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4069996)[0;0m ERROR 12-05 16:36:43 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069996)[0;0m ERROR 12-05 16:36:43 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4069996)[0;0m ERROR 12-05 16:36:43 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4069996)[0;0m ERROR 12-05 16:36:43 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4069996)[0;0m ERROR 12-05 16:36:43 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4069996)[0;0m ERROR 12-05 16:36:43 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069996)[0;0m ERROR 12-05 16:36:43 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4069996)[0;0m ERROR 12-05 16:36:43 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4069996)[0;0m ERROR 12-05 16:36:43 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069996)[0;0m ERROR 12-05 16:36:43 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4069996)[0;0m ERROR 12-05 16:36:43 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4069996)[0;0m ERROR 12-05 16:36:43 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069996)[0;0m ERROR 12-05 16:36:43 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4069996)[0;0m ERROR 12-05 16:36:43 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4069996)[0;0m ERROR 12-05 16:36:43 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069996)[0;0m ERROR 12-05 16:36:43 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4069996)[0;0m ERROR 12-05 16:36:43 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4069996)[0;0m ERROR 12-05 16:36:43 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069996)[0;0m ERROR 12-05 16:36:43 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4069996)[0;0m ERROR 12-05 16:36:43 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4069996)[0;0m ERROR 12-05 16:36:43 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069996)[0;0m ERROR 12-05 16:36:43 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4069996)[0;0m ERROR 12-05 16:36:43 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4069996)[0;0m ERROR 12-05 16:36:43 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069996)[0;0m ERROR 12-05 16:36:43 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4069996)[0;0m ERROR 12-05 16:36:43 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4069996)[0;0m ERROR 12-05 16:36:43 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069996)[0;0m ERROR 12-05 16:36:43 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4069996)[0;0m ERROR 12-05 16:36:43 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4069996)[0;0m ERROR 12-05 16:36:43 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069996)[0;0m ERROR 12-05 16:36:43 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4069996)[0;0m ERROR 12-05 16:36:43 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4069996)[0;0m ERROR 12-05 16:36:43 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069996)[0;0m ERROR 12-05 16:36:43 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4069996)[0;0m ERROR 12-05 16:36:43 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4069996)[0;0m ERROR 12-05 16:36:43 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4069996)[0;0m ERROR 12-05 16:36:43 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4069996)[0;0m ERROR 12-05 16:36:43 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069996)[0;0m ERROR 12-05 16:36:43 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4069996)[0;0m ERROR 12-05 16:36:43 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4069996)[0;0m ERROR 12-05 16:36:43 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069996)[0;0m ERROR 12-05 16:36:43 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4069996)[0;0m ERROR 12-05 16:36:43 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4069996)[0;0m ERROR 12-05 16:36:43 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069996)[0;0m ERROR 12-05 16:36:43 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4069996)[0;0m ERROR 12-05 16:36:43 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4069996)[0;0m ERROR 12-05 16:36:43 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4069996)[0;0m ERROR 12-05 16:36:43 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4069996)[0;0m ERROR 12-05 16:36:43 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069996)[0;0m ERROR 12-05 16:36:43 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4069996)[0;0m ERROR 12-05 16:36:43 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4069996)[0;0m ERROR 12-05 16:36:43 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069996)[0;0m ERROR 12-05 16:36:43 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4069996)[0;0m ERROR 12-05 16:36:43 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4069996)[0;0m ERROR 12-05 16:36:43 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069996)[0;0m ERROR 12-05 16:36:43 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4069996)[0;0m ERROR 12-05 16:36:43 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4069996)[0;0m ERROR 12-05 16:36:43 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4069996)[0;0m ERROR 12-05 16:36:43 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4069996)[0;0m ERROR 12-05 16:36:43 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069996)[0;0m ERROR 12-05 16:36:43 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4069996)[0;0m ERROR 12-05 16:36:43 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4069996)[0;0m ERROR 12-05 16:36:43 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069996)[0;0m ERROR 12-05 16:36:43 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4069996)[0;0m ERROR 12-05 16:36:43 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4069996)[0;0m ERROR 12-05 16:36:43 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069996)[0;0m ERROR 12-05 16:36:43 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4069996)[0;0m ERROR 12-05 16:36:43 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4069996)[0;0m ERROR 12-05 16:36:43 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4069996)[0;0m ERROR 12-05 16:36:43 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4069996)[0;0m ERROR 12-05 16:36:43 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4069996)[0;0m ERROR 12-05 16:36:43 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7f19902c39b0>' raised:
[1;36m(EngineCore_DP0 pid=4069996)[0;0m ERROR 12-05 16:36:43 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4069996)[0;0m ERROR 12-05 16:36:43 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4069996)[0;0m ERROR 12-05 16:36:43 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4069996)[0;0m ERROR 12-05 16:36:43 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 16:37:04 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 16:37:04 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 16:37:04 [model.py:1745] Using max model len 131072
INFO 12-05 16:37:04 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4070084)[0;0m INFO 12-05 16:37:14 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4070084)[0;0m INFO 12-05 16:37:15 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:59691 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4070084)[0;0m INFO 12-05 16:37:15 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4070084)[0;0m INFO 12-05 16:37:15 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4070084)[0;0m INFO 12-05 16:37:15 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4070084)[0;0m INFO 12-05 16:37:15 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4070084)[0;0m INFO 12-05 16:37:17 [default_loader.py:314] Loading weights took 0.87 seconds
[1;36m(EngineCore_DP0 pid=4070084)[0;0m INFO 12-05 16:37:17 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.438804 seconds
[1;36m(EngineCore_DP0 pid=4070084)[0;0m INFO 12-05 16:37:21 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4070084)[0;0m INFO 12-05 16:37:21 [backends.py:647] Dynamo bytecode transform time: 3.85 s
[1;36m(EngineCore_DP0 pid=4070084)[0;0m ERROR 12-05 16:37:21 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4070084)[0;0m ERROR 12-05 16:37:21 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4070084)[0;0m ERROR 12-05 16:37:21 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4070084)[0;0m ERROR 12-05 16:37:21 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4070084)[0;0m ERROR 12-05 16:37:21 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070084)[0;0m ERROR 12-05 16:37:21 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4070084)[0;0m ERROR 12-05 16:37:21 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4070084)[0;0m ERROR 12-05 16:37:21 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4070084)[0;0m ERROR 12-05 16:37:21 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4070084)[0;0m ERROR 12-05 16:37:21 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070084)[0;0m ERROR 12-05 16:37:21 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4070084)[0;0m ERROR 12-05 16:37:21 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4070084)[0;0m ERROR 12-05 16:37:21 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070084)[0;0m ERROR 12-05 16:37:21 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4070084)[0;0m ERROR 12-05 16:37:21 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4070084)[0;0m ERROR 12-05 16:37:21 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070084)[0;0m ERROR 12-05 16:37:21 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4070084)[0;0m ERROR 12-05 16:37:21 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4070084)[0;0m ERROR 12-05 16:37:21 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070084)[0;0m ERROR 12-05 16:37:21 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4070084)[0;0m ERROR 12-05 16:37:21 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4070084)[0;0m ERROR 12-05 16:37:21 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070084)[0;0m ERROR 12-05 16:37:21 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4070084)[0;0m ERROR 12-05 16:37:21 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4070084)[0;0m ERROR 12-05 16:37:21 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070084)[0;0m ERROR 12-05 16:37:21 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4070084)[0;0m ERROR 12-05 16:37:21 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4070084)[0;0m ERROR 12-05 16:37:21 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4070084)[0;0m ERROR 12-05 16:37:21 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4070084)[0;0m ERROR 12-05 16:37:21 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070084)[0;0m ERROR 12-05 16:37:21 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4070084)[0;0m ERROR 12-05 16:37:21 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4070084)[0;0m ERROR 12-05 16:37:21 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070084)[0;0m ERROR 12-05 16:37:21 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4070084)[0;0m ERROR 12-05 16:37:21 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4070084)[0;0m ERROR 12-05 16:37:21 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070084)[0;0m ERROR 12-05 16:37:21 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4070084)[0;0m ERROR 12-05 16:37:21 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4070084)[0;0m ERROR 12-05 16:37:21 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070084)[0;0m ERROR 12-05 16:37:21 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4070084)[0;0m ERROR 12-05 16:37:21 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4070084)[0;0m ERROR 12-05 16:37:21 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070084)[0;0m ERROR 12-05 16:37:21 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4070084)[0;0m ERROR 12-05 16:37:21 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4070084)[0;0m ERROR 12-05 16:37:21 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070084)[0;0m ERROR 12-05 16:37:21 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4070084)[0;0m ERROR 12-05 16:37:21 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4070084)[0;0m ERROR 12-05 16:37:21 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070084)[0;0m ERROR 12-05 16:37:21 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4070084)[0;0m ERROR 12-05 16:37:21 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4070084)[0;0m ERROR 12-05 16:37:21 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070084)[0;0m ERROR 12-05 16:37:21 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4070084)[0;0m ERROR 12-05 16:37:21 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4070084)[0;0m ERROR 12-05 16:37:21 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070084)[0;0m ERROR 12-05 16:37:21 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4070084)[0;0m ERROR 12-05 16:37:21 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4070084)[0;0m ERROR 12-05 16:37:21 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070084)[0;0m ERROR 12-05 16:37:21 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4070084)[0;0m ERROR 12-05 16:37:21 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4070084)[0;0m ERROR 12-05 16:37:21 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4070084)[0;0m ERROR 12-05 16:37:21 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4070084)[0;0m ERROR 12-05 16:37:21 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070084)[0;0m ERROR 12-05 16:37:21 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4070084)[0;0m ERROR 12-05 16:37:21 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4070084)[0;0m ERROR 12-05 16:37:21 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070084)[0;0m ERROR 12-05 16:37:21 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4070084)[0;0m ERROR 12-05 16:37:21 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4070084)[0;0m ERROR 12-05 16:37:21 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070084)[0;0m ERROR 12-05 16:37:21 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4070084)[0;0m ERROR 12-05 16:37:21 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4070084)[0;0m ERROR 12-05 16:37:21 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4070084)[0;0m ERROR 12-05 16:37:21 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4070084)[0;0m ERROR 12-05 16:37:21 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070084)[0;0m ERROR 12-05 16:37:21 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4070084)[0;0m ERROR 12-05 16:37:21 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4070084)[0;0m ERROR 12-05 16:37:21 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070084)[0;0m ERROR 12-05 16:37:21 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4070084)[0;0m ERROR 12-05 16:37:21 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4070084)[0;0m ERROR 12-05 16:37:21 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070084)[0;0m ERROR 12-05 16:37:21 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4070084)[0;0m ERROR 12-05 16:37:21 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4070084)[0;0m ERROR 12-05 16:37:21 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4070084)[0;0m ERROR 12-05 16:37:21 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4070084)[0;0m ERROR 12-05 16:37:21 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070084)[0;0m ERROR 12-05 16:37:21 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4070084)[0;0m ERROR 12-05 16:37:21 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4070084)[0;0m ERROR 12-05 16:37:21 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070084)[0;0m ERROR 12-05 16:37:21 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4070084)[0;0m ERROR 12-05 16:37:21 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4070084)[0;0m ERROR 12-05 16:37:21 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070084)[0;0m ERROR 12-05 16:37:21 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4070084)[0;0m ERROR 12-05 16:37:21 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4070084)[0;0m ERROR 12-05 16:37:21 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070084)[0;0m ERROR 12-05 16:37:21 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4070084)[0;0m ERROR 12-05 16:37:21 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4070084)[0;0m ERROR 12-05 16:37:21 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7f146c1673b0>' raised:
[1;36m(EngineCore_DP0 pid=4070084)[0;0m ERROR 12-05 16:37:21 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4070084)[0;0m ERROR 12-05 16:37:21 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4070084)[0;0m ERROR 12-05 16:37:21 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4070084)[0;0m ERROR 12-05 16:37:21 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 16:37:43 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 16:37:43 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 16:37:43 [model.py:1745] Using max model len 131072
INFO 12-05 16:37:43 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4070165)[0;0m INFO 12-05 16:37:51 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4070165)[0;0m INFO 12-05 16:37:51 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:56695 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4070165)[0;0m INFO 12-05 16:37:51 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4070165)[0;0m INFO 12-05 16:37:52 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4070165)[0;0m INFO 12-05 16:37:52 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4070165)[0;0m INFO 12-05 16:37:52 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4070165)[0;0m INFO 12-05 16:37:53 [default_loader.py:314] Loading weights took 0.87 seconds
[1;36m(EngineCore_DP0 pid=4070165)[0;0m INFO 12-05 16:37:54 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.391475 seconds
[1;36m(EngineCore_DP0 pid=4070165)[0;0m INFO 12-05 16:37:58 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4070165)[0;0m INFO 12-05 16:37:58 [backends.py:647] Dynamo bytecode transform time: 3.79 s
[1;36m(EngineCore_DP0 pid=4070165)[0;0m ERROR 12-05 16:37:58 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4070165)[0;0m ERROR 12-05 16:37:58 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4070165)[0;0m ERROR 12-05 16:37:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4070165)[0;0m ERROR 12-05 16:37:58 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4070165)[0;0m ERROR 12-05 16:37:58 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070165)[0;0m ERROR 12-05 16:37:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4070165)[0;0m ERROR 12-05 16:37:58 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4070165)[0;0m ERROR 12-05 16:37:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4070165)[0;0m ERROR 12-05 16:37:58 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4070165)[0;0m ERROR 12-05 16:37:58 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070165)[0;0m ERROR 12-05 16:37:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4070165)[0;0m ERROR 12-05 16:37:58 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4070165)[0;0m ERROR 12-05 16:37:58 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070165)[0;0m ERROR 12-05 16:37:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4070165)[0;0m ERROR 12-05 16:37:58 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4070165)[0;0m ERROR 12-05 16:37:58 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070165)[0;0m ERROR 12-05 16:37:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4070165)[0;0m ERROR 12-05 16:37:58 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4070165)[0;0m ERROR 12-05 16:37:58 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070165)[0;0m ERROR 12-05 16:37:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4070165)[0;0m ERROR 12-05 16:37:58 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4070165)[0;0m ERROR 12-05 16:37:58 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070165)[0;0m ERROR 12-05 16:37:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4070165)[0;0m ERROR 12-05 16:37:58 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4070165)[0;0m ERROR 12-05 16:37:58 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070165)[0;0m ERROR 12-05 16:37:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4070165)[0;0m ERROR 12-05 16:37:58 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4070165)[0;0m ERROR 12-05 16:37:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4070165)[0;0m ERROR 12-05 16:37:58 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4070165)[0;0m ERROR 12-05 16:37:58 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070165)[0;0m ERROR 12-05 16:37:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4070165)[0;0m ERROR 12-05 16:37:58 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4070165)[0;0m ERROR 12-05 16:37:58 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070165)[0;0m ERROR 12-05 16:37:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4070165)[0;0m ERROR 12-05 16:37:58 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4070165)[0;0m ERROR 12-05 16:37:58 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070165)[0;0m ERROR 12-05 16:37:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4070165)[0;0m ERROR 12-05 16:37:58 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4070165)[0;0m ERROR 12-05 16:37:58 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070165)[0;0m ERROR 12-05 16:37:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4070165)[0;0m ERROR 12-05 16:37:58 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4070165)[0;0m ERROR 12-05 16:37:58 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070165)[0;0m ERROR 12-05 16:37:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4070165)[0;0m ERROR 12-05 16:37:58 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4070165)[0;0m ERROR 12-05 16:37:58 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070165)[0;0m ERROR 12-05 16:37:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4070165)[0;0m ERROR 12-05 16:37:58 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4070165)[0;0m ERROR 12-05 16:37:58 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070165)[0;0m ERROR 12-05 16:37:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4070165)[0;0m ERROR 12-05 16:37:58 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4070165)[0;0m ERROR 12-05 16:37:58 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070165)[0;0m ERROR 12-05 16:37:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4070165)[0;0m ERROR 12-05 16:37:58 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4070165)[0;0m ERROR 12-05 16:37:58 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070165)[0;0m ERROR 12-05 16:37:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4070165)[0;0m ERROR 12-05 16:37:58 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4070165)[0;0m ERROR 12-05 16:37:58 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070165)[0;0m ERROR 12-05 16:37:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4070165)[0;0m ERROR 12-05 16:37:58 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4070165)[0;0m ERROR 12-05 16:37:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4070165)[0;0m ERROR 12-05 16:37:58 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4070165)[0;0m ERROR 12-05 16:37:58 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070165)[0;0m ERROR 12-05 16:37:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4070165)[0;0m ERROR 12-05 16:37:58 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4070165)[0;0m ERROR 12-05 16:37:58 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070165)[0;0m ERROR 12-05 16:37:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4070165)[0;0m ERROR 12-05 16:37:58 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4070165)[0;0m ERROR 12-05 16:37:58 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070165)[0;0m ERROR 12-05 16:37:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4070165)[0;0m ERROR 12-05 16:37:58 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4070165)[0;0m ERROR 12-05 16:37:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4070165)[0;0m ERROR 12-05 16:37:58 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4070165)[0;0m ERROR 12-05 16:37:58 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070165)[0;0m ERROR 12-05 16:37:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4070165)[0;0m ERROR 12-05 16:37:58 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4070165)[0;0m ERROR 12-05 16:37:58 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070165)[0;0m ERROR 12-05 16:37:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4070165)[0;0m ERROR 12-05 16:37:58 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4070165)[0;0m ERROR 12-05 16:37:58 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070165)[0;0m ERROR 12-05 16:37:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4070165)[0;0m ERROR 12-05 16:37:58 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4070165)[0;0m ERROR 12-05 16:37:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4070165)[0;0m ERROR 12-05 16:37:58 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4070165)[0;0m ERROR 12-05 16:37:58 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070165)[0;0m ERROR 12-05 16:37:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4070165)[0;0m ERROR 12-05 16:37:58 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4070165)[0;0m ERROR 12-05 16:37:58 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070165)[0;0m ERROR 12-05 16:37:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4070165)[0;0m ERROR 12-05 16:37:58 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4070165)[0;0m ERROR 12-05 16:37:58 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070165)[0;0m ERROR 12-05 16:37:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4070165)[0;0m ERROR 12-05 16:37:58 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4070165)[0;0m ERROR 12-05 16:37:58 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070165)[0;0m ERROR 12-05 16:37:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4070165)[0;0m ERROR 12-05 16:37:58 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4070165)[0;0m ERROR 12-05 16:37:58 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7f2802e2f2f0>' raised:
[1;36m(EngineCore_DP0 pid=4070165)[0;0m ERROR 12-05 16:37:58 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4070165)[0;0m ERROR 12-05 16:37:58 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4070165)[0;0m ERROR 12-05 16:37:58 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4070165)[0;0m ERROR 12-05 16:37:58 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 16:38:19 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 16:38:20 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 16:38:20 [model.py:1745] Using max model len 131072
INFO 12-05 16:38:20 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4070250)[0;0m INFO 12-05 16:38:29 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4070250)[0;0m INFO 12-05 16:38:30 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:51101 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4070250)[0;0m INFO 12-05 16:38:30 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4070250)[0;0m INFO 12-05 16:38:30 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4070250)[0;0m INFO 12-05 16:38:31 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4070250)[0;0m INFO 12-05 16:38:31 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4070250)[0;0m INFO 12-05 16:38:32 [default_loader.py:314] Loading weights took 0.88 seconds
[1;36m(EngineCore_DP0 pid=4070250)[0;0m INFO 12-05 16:38:32 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.492622 seconds
[1;36m(EngineCore_DP0 pid=4070250)[0;0m INFO 12-05 16:38:36 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4070250)[0;0m INFO 12-05 16:38:36 [backends.py:647] Dynamo bytecode transform time: 3.70 s
[1;36m(EngineCore_DP0 pid=4070250)[0;0m ERROR 12-05 16:38:36 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4070250)[0;0m ERROR 12-05 16:38:36 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4070250)[0;0m ERROR 12-05 16:38:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4070250)[0;0m ERROR 12-05 16:38:36 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4070250)[0;0m ERROR 12-05 16:38:36 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070250)[0;0m ERROR 12-05 16:38:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4070250)[0;0m ERROR 12-05 16:38:36 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4070250)[0;0m ERROR 12-05 16:38:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4070250)[0;0m ERROR 12-05 16:38:36 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4070250)[0;0m ERROR 12-05 16:38:36 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070250)[0;0m ERROR 12-05 16:38:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4070250)[0;0m ERROR 12-05 16:38:36 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4070250)[0;0m ERROR 12-05 16:38:36 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070250)[0;0m ERROR 12-05 16:38:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4070250)[0;0m ERROR 12-05 16:38:36 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4070250)[0;0m ERROR 12-05 16:38:36 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070250)[0;0m ERROR 12-05 16:38:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4070250)[0;0m ERROR 12-05 16:38:36 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4070250)[0;0m ERROR 12-05 16:38:36 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070250)[0;0m ERROR 12-05 16:38:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4070250)[0;0m ERROR 12-05 16:38:36 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4070250)[0;0m ERROR 12-05 16:38:36 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070250)[0;0m ERROR 12-05 16:38:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4070250)[0;0m ERROR 12-05 16:38:36 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4070250)[0;0m ERROR 12-05 16:38:36 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070250)[0;0m ERROR 12-05 16:38:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4070250)[0;0m ERROR 12-05 16:38:36 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4070250)[0;0m ERROR 12-05 16:38:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4070250)[0;0m ERROR 12-05 16:38:36 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4070250)[0;0m ERROR 12-05 16:38:36 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070250)[0;0m ERROR 12-05 16:38:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4070250)[0;0m ERROR 12-05 16:38:36 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4070250)[0;0m ERROR 12-05 16:38:36 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070250)[0;0m ERROR 12-05 16:38:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4070250)[0;0m ERROR 12-05 16:38:36 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4070250)[0;0m ERROR 12-05 16:38:36 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070250)[0;0m ERROR 12-05 16:38:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4070250)[0;0m ERROR 12-05 16:38:36 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4070250)[0;0m ERROR 12-05 16:38:36 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070250)[0;0m ERROR 12-05 16:38:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4070250)[0;0m ERROR 12-05 16:38:36 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4070250)[0;0m ERROR 12-05 16:38:36 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070250)[0;0m ERROR 12-05 16:38:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4070250)[0;0m ERROR 12-05 16:38:36 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4070250)[0;0m ERROR 12-05 16:38:36 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070250)[0;0m ERROR 12-05 16:38:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4070250)[0;0m ERROR 12-05 16:38:36 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4070250)[0;0m ERROR 12-05 16:38:36 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070250)[0;0m ERROR 12-05 16:38:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4070250)[0;0m ERROR 12-05 16:38:36 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4070250)[0;0m ERROR 12-05 16:38:36 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070250)[0;0m ERROR 12-05 16:38:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4070250)[0;0m ERROR 12-05 16:38:36 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4070250)[0;0m ERROR 12-05 16:38:36 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070250)[0;0m ERROR 12-05 16:38:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4070250)[0;0m ERROR 12-05 16:38:36 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4070250)[0;0m ERROR 12-05 16:38:36 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070250)[0;0m ERROR 12-05 16:38:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4070250)[0;0m ERROR 12-05 16:38:36 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4070250)[0;0m ERROR 12-05 16:38:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4070250)[0;0m ERROR 12-05 16:38:36 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4070250)[0;0m ERROR 12-05 16:38:36 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070250)[0;0m ERROR 12-05 16:38:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4070250)[0;0m ERROR 12-05 16:38:36 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4070250)[0;0m ERROR 12-05 16:38:36 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070250)[0;0m ERROR 12-05 16:38:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4070250)[0;0m ERROR 12-05 16:38:36 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4070250)[0;0m ERROR 12-05 16:38:36 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070250)[0;0m ERROR 12-05 16:38:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4070250)[0;0m ERROR 12-05 16:38:36 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4070250)[0;0m ERROR 12-05 16:38:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4070250)[0;0m ERROR 12-05 16:38:36 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4070250)[0;0m ERROR 12-05 16:38:36 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070250)[0;0m ERROR 12-05 16:38:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4070250)[0;0m ERROR 12-05 16:38:36 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4070250)[0;0m ERROR 12-05 16:38:36 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070250)[0;0m ERROR 12-05 16:38:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4070250)[0;0m ERROR 12-05 16:38:36 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4070250)[0;0m ERROR 12-05 16:38:36 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070250)[0;0m ERROR 12-05 16:38:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4070250)[0;0m ERROR 12-05 16:38:36 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4070250)[0;0m ERROR 12-05 16:38:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4070250)[0;0m ERROR 12-05 16:38:36 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4070250)[0;0m ERROR 12-05 16:38:36 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070250)[0;0m ERROR 12-05 16:38:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4070250)[0;0m ERROR 12-05 16:38:36 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4070250)[0;0m ERROR 12-05 16:38:36 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070250)[0;0m ERROR 12-05 16:38:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4070250)[0;0m ERROR 12-05 16:38:36 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4070250)[0;0m ERROR 12-05 16:38:36 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070250)[0;0m ERROR 12-05 16:38:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4070250)[0;0m ERROR 12-05 16:38:36 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4070250)[0;0m ERROR 12-05 16:38:36 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070250)[0;0m ERROR 12-05 16:38:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4070250)[0;0m ERROR 12-05 16:38:36 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4070250)[0;0m ERROR 12-05 16:38:36 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7f01456ceea0>' raised:
[1;36m(EngineCore_DP0 pid=4070250)[0;0m ERROR 12-05 16:38:36 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4070250)[0;0m ERROR 12-05 16:38:36 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4070250)[0;0m ERROR 12-05 16:38:36 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4070250)[0;0m ERROR 12-05 16:38:36 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 16:38:58 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 16:38:58 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 16:38:58 [model.py:1745] Using max model len 131072
INFO 12-05 16:38:58 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4070339)[0;0m INFO 12-05 16:39:06 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4070339)[0;0m INFO 12-05 16:39:06 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:39727 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4070339)[0;0m INFO 12-05 16:39:07 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4070339)[0;0m INFO 12-05 16:39:07 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4070339)[0;0m INFO 12-05 16:39:07 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4070339)[0;0m INFO 12-05 16:39:07 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4070339)[0;0m INFO 12-05 16:39:08 [default_loader.py:314] Loading weights took 0.88 seconds
[1;36m(EngineCore_DP0 pid=4070339)[0;0m INFO 12-05 16:39:09 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.464717 seconds
[1;36m(EngineCore_DP0 pid=4070339)[0;0m INFO 12-05 16:39:13 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4070339)[0;0m INFO 12-05 16:39:13 [backends.py:647] Dynamo bytecode transform time: 3.91 s
[1;36m(EngineCore_DP0 pid=4070339)[0;0m ERROR 12-05 16:39:13 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4070339)[0;0m ERROR 12-05 16:39:13 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4070339)[0;0m ERROR 12-05 16:39:13 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4070339)[0;0m ERROR 12-05 16:39:13 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4070339)[0;0m ERROR 12-05 16:39:13 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070339)[0;0m ERROR 12-05 16:39:13 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4070339)[0;0m ERROR 12-05 16:39:13 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4070339)[0;0m ERROR 12-05 16:39:13 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4070339)[0;0m ERROR 12-05 16:39:13 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4070339)[0;0m ERROR 12-05 16:39:13 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070339)[0;0m ERROR 12-05 16:39:13 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4070339)[0;0m ERROR 12-05 16:39:13 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4070339)[0;0m ERROR 12-05 16:39:13 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070339)[0;0m ERROR 12-05 16:39:13 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4070339)[0;0m ERROR 12-05 16:39:13 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4070339)[0;0m ERROR 12-05 16:39:13 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070339)[0;0m ERROR 12-05 16:39:13 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4070339)[0;0m ERROR 12-05 16:39:13 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4070339)[0;0m ERROR 12-05 16:39:13 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070339)[0;0m ERROR 12-05 16:39:13 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4070339)[0;0m ERROR 12-05 16:39:13 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4070339)[0;0m ERROR 12-05 16:39:13 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070339)[0;0m ERROR 12-05 16:39:13 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4070339)[0;0m ERROR 12-05 16:39:13 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4070339)[0;0m ERROR 12-05 16:39:13 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070339)[0;0m ERROR 12-05 16:39:13 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4070339)[0;0m ERROR 12-05 16:39:13 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4070339)[0;0m ERROR 12-05 16:39:13 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4070339)[0;0m ERROR 12-05 16:39:13 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4070339)[0;0m ERROR 12-05 16:39:13 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070339)[0;0m ERROR 12-05 16:39:13 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4070339)[0;0m ERROR 12-05 16:39:13 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4070339)[0;0m ERROR 12-05 16:39:13 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070339)[0;0m ERROR 12-05 16:39:13 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4070339)[0;0m ERROR 12-05 16:39:13 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4070339)[0;0m ERROR 12-05 16:39:13 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070339)[0;0m ERROR 12-05 16:39:13 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4070339)[0;0m ERROR 12-05 16:39:13 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4070339)[0;0m ERROR 12-05 16:39:13 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070339)[0;0m ERROR 12-05 16:39:13 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4070339)[0;0m ERROR 12-05 16:39:13 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4070339)[0;0m ERROR 12-05 16:39:13 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070339)[0;0m ERROR 12-05 16:39:13 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4070339)[0;0m ERROR 12-05 16:39:13 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4070339)[0;0m ERROR 12-05 16:39:13 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070339)[0;0m ERROR 12-05 16:39:13 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4070339)[0;0m ERROR 12-05 16:39:13 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4070339)[0;0m ERROR 12-05 16:39:13 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070339)[0;0m ERROR 12-05 16:39:13 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4070339)[0;0m ERROR 12-05 16:39:13 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4070339)[0;0m ERROR 12-05 16:39:13 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070339)[0;0m ERROR 12-05 16:39:13 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4070339)[0;0m ERROR 12-05 16:39:13 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4070339)[0;0m ERROR 12-05 16:39:13 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070339)[0;0m ERROR 12-05 16:39:13 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4070339)[0;0m ERROR 12-05 16:39:13 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4070339)[0;0m ERROR 12-05 16:39:13 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070339)[0;0m ERROR 12-05 16:39:13 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4070339)[0;0m ERROR 12-05 16:39:13 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4070339)[0;0m ERROR 12-05 16:39:13 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4070339)[0;0m ERROR 12-05 16:39:13 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4070339)[0;0m ERROR 12-05 16:39:13 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070339)[0;0m ERROR 12-05 16:39:13 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4070339)[0;0m ERROR 12-05 16:39:13 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4070339)[0;0m ERROR 12-05 16:39:13 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070339)[0;0m ERROR 12-05 16:39:13 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4070339)[0;0m ERROR 12-05 16:39:13 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4070339)[0;0m ERROR 12-05 16:39:13 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070339)[0;0m ERROR 12-05 16:39:13 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4070339)[0;0m ERROR 12-05 16:39:13 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4070339)[0;0m ERROR 12-05 16:39:13 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4070339)[0;0m ERROR 12-05 16:39:13 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4070339)[0;0m ERROR 12-05 16:39:13 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070339)[0;0m ERROR 12-05 16:39:13 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4070339)[0;0m ERROR 12-05 16:39:13 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4070339)[0;0m ERROR 12-05 16:39:13 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070339)[0;0m ERROR 12-05 16:39:13 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4070339)[0;0m ERROR 12-05 16:39:13 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4070339)[0;0m ERROR 12-05 16:39:13 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070339)[0;0m ERROR 12-05 16:39:13 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4070339)[0;0m ERROR 12-05 16:39:13 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4070339)[0;0m ERROR 12-05 16:39:13 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4070339)[0;0m ERROR 12-05 16:39:13 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4070339)[0;0m ERROR 12-05 16:39:13 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070339)[0;0m ERROR 12-05 16:39:13 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4070339)[0;0m ERROR 12-05 16:39:13 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4070339)[0;0m ERROR 12-05 16:39:13 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070339)[0;0m ERROR 12-05 16:39:13 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4070339)[0;0m ERROR 12-05 16:39:13 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4070339)[0;0m ERROR 12-05 16:39:13 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070339)[0;0m ERROR 12-05 16:39:13 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4070339)[0;0m ERROR 12-05 16:39:13 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4070339)[0;0m ERROR 12-05 16:39:13 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070339)[0;0m ERROR 12-05 16:39:13 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4070339)[0;0m ERROR 12-05 16:39:13 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4070339)[0;0m ERROR 12-05 16:39:13 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7efe101984a0>' raised:
[1;36m(EngineCore_DP0 pid=4070339)[0;0m ERROR 12-05 16:39:13 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4070339)[0;0m ERROR 12-05 16:39:13 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4070339)[0;0m ERROR 12-05 16:39:13 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4070339)[0;0m ERROR 12-05 16:39:13 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 16:39:35 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 16:39:35 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 16:39:35 [model.py:1745] Using max model len 131072
INFO 12-05 16:39:35 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4070420)[0;0m INFO 12-05 16:39:44 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4070420)[0;0m INFO 12-05 16:39:45 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:40155 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4070420)[0;0m INFO 12-05 16:39:45 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4070420)[0;0m INFO 12-05 16:39:45 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4070420)[0;0m INFO 12-05 16:39:46 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4070420)[0;0m INFO 12-05 16:39:46 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4070420)[0;0m INFO 12-05 16:39:47 [default_loader.py:314] Loading weights took 0.87 seconds
[1;36m(EngineCore_DP0 pid=4070420)[0;0m INFO 12-05 16:39:47 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.511747 seconds
[1;36m(EngineCore_DP0 pid=4070420)[0;0m INFO 12-05 16:39:51 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4070420)[0;0m INFO 12-05 16:39:51 [backends.py:647] Dynamo bytecode transform time: 3.89 s
[1;36m(EngineCore_DP0 pid=4070420)[0;0m ERROR 12-05 16:39:52 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4070420)[0;0m ERROR 12-05 16:39:52 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4070420)[0;0m ERROR 12-05 16:39:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4070420)[0;0m ERROR 12-05 16:39:52 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4070420)[0;0m ERROR 12-05 16:39:52 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070420)[0;0m ERROR 12-05 16:39:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4070420)[0;0m ERROR 12-05 16:39:52 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4070420)[0;0m ERROR 12-05 16:39:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4070420)[0;0m ERROR 12-05 16:39:52 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4070420)[0;0m ERROR 12-05 16:39:52 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070420)[0;0m ERROR 12-05 16:39:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4070420)[0;0m ERROR 12-05 16:39:52 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4070420)[0;0m ERROR 12-05 16:39:52 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070420)[0;0m ERROR 12-05 16:39:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4070420)[0;0m ERROR 12-05 16:39:52 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4070420)[0;0m ERROR 12-05 16:39:52 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070420)[0;0m ERROR 12-05 16:39:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4070420)[0;0m ERROR 12-05 16:39:52 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4070420)[0;0m ERROR 12-05 16:39:52 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070420)[0;0m ERROR 12-05 16:39:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4070420)[0;0m ERROR 12-05 16:39:52 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4070420)[0;0m ERROR 12-05 16:39:52 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070420)[0;0m ERROR 12-05 16:39:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4070420)[0;0m ERROR 12-05 16:39:52 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4070420)[0;0m ERROR 12-05 16:39:52 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070420)[0;0m ERROR 12-05 16:39:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4070420)[0;0m ERROR 12-05 16:39:52 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4070420)[0;0m ERROR 12-05 16:39:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4070420)[0;0m ERROR 12-05 16:39:52 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4070420)[0;0m ERROR 12-05 16:39:52 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070420)[0;0m ERROR 12-05 16:39:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4070420)[0;0m ERROR 12-05 16:39:52 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4070420)[0;0m ERROR 12-05 16:39:52 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070420)[0;0m ERROR 12-05 16:39:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4070420)[0;0m ERROR 12-05 16:39:52 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4070420)[0;0m ERROR 12-05 16:39:52 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070420)[0;0m ERROR 12-05 16:39:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4070420)[0;0m ERROR 12-05 16:39:52 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4070420)[0;0m ERROR 12-05 16:39:52 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070420)[0;0m ERROR 12-05 16:39:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4070420)[0;0m ERROR 12-05 16:39:52 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4070420)[0;0m ERROR 12-05 16:39:52 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070420)[0;0m ERROR 12-05 16:39:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4070420)[0;0m ERROR 12-05 16:39:52 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4070420)[0;0m ERROR 12-05 16:39:52 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070420)[0;0m ERROR 12-05 16:39:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4070420)[0;0m ERROR 12-05 16:39:52 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4070420)[0;0m ERROR 12-05 16:39:52 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070420)[0;0m ERROR 12-05 16:39:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4070420)[0;0m ERROR 12-05 16:39:52 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4070420)[0;0m ERROR 12-05 16:39:52 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070420)[0;0m ERROR 12-05 16:39:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4070420)[0;0m ERROR 12-05 16:39:52 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4070420)[0;0m ERROR 12-05 16:39:52 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070420)[0;0m ERROR 12-05 16:39:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4070420)[0;0m ERROR 12-05 16:39:52 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4070420)[0;0m ERROR 12-05 16:39:52 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070420)[0;0m ERROR 12-05 16:39:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4070420)[0;0m ERROR 12-05 16:39:52 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4070420)[0;0m ERROR 12-05 16:39:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4070420)[0;0m ERROR 12-05 16:39:52 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4070420)[0;0m ERROR 12-05 16:39:52 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070420)[0;0m ERROR 12-05 16:39:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4070420)[0;0m ERROR 12-05 16:39:52 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4070420)[0;0m ERROR 12-05 16:39:52 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070420)[0;0m ERROR 12-05 16:39:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4070420)[0;0m ERROR 12-05 16:39:52 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4070420)[0;0m ERROR 12-05 16:39:52 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070420)[0;0m ERROR 12-05 16:39:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4070420)[0;0m ERROR 12-05 16:39:52 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4070420)[0;0m ERROR 12-05 16:39:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4070420)[0;0m ERROR 12-05 16:39:52 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4070420)[0;0m ERROR 12-05 16:39:52 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070420)[0;0m ERROR 12-05 16:39:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4070420)[0;0m ERROR 12-05 16:39:52 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4070420)[0;0m ERROR 12-05 16:39:52 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070420)[0;0m ERROR 12-05 16:39:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4070420)[0;0m ERROR 12-05 16:39:52 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4070420)[0;0m ERROR 12-05 16:39:52 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070420)[0;0m ERROR 12-05 16:39:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4070420)[0;0m ERROR 12-05 16:39:52 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4070420)[0;0m ERROR 12-05 16:39:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4070420)[0;0m ERROR 12-05 16:39:52 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4070420)[0;0m ERROR 12-05 16:39:52 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070420)[0;0m ERROR 12-05 16:39:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4070420)[0;0m ERROR 12-05 16:39:52 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4070420)[0;0m ERROR 12-05 16:39:52 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070420)[0;0m ERROR 12-05 16:39:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4070420)[0;0m ERROR 12-05 16:39:52 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4070420)[0;0m ERROR 12-05 16:39:52 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070420)[0;0m ERROR 12-05 16:39:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4070420)[0;0m ERROR 12-05 16:39:52 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4070420)[0;0m ERROR 12-05 16:39:52 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070420)[0;0m ERROR 12-05 16:39:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4070420)[0;0m ERROR 12-05 16:39:52 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4070420)[0;0m ERROR 12-05 16:39:52 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7f9fdc13fbc0>' raised:
[1;36m(EngineCore_DP0 pid=4070420)[0;0m ERROR 12-05 16:39:52 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4070420)[0;0m ERROR 12-05 16:39:52 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4070420)[0;0m ERROR 12-05 16:39:52 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4070420)[0;0m ERROR 12-05 16:39:52 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 16:40:13 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 16:40:13 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 16:40:13 [model.py:1745] Using max model len 131072
INFO 12-05 16:40:13 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4070507)[0;0m INFO 12-05 16:40:21 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4070507)[0;0m INFO 12-05 16:40:22 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:33805 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4070507)[0;0m INFO 12-05 16:40:22 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4070507)[0;0m INFO 12-05 16:40:22 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4070507)[0;0m INFO 12-05 16:40:23 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4070507)[0;0m INFO 12-05 16:40:23 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4070507)[0;0m INFO 12-05 16:40:24 [default_loader.py:314] Loading weights took 0.88 seconds
[1;36m(EngineCore_DP0 pid=4070507)[0;0m INFO 12-05 16:40:24 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.435726 seconds
[1;36m(EngineCore_DP0 pid=4070507)[0;0m INFO 12-05 16:40:28 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4070507)[0;0m INFO 12-05 16:40:28 [backends.py:647] Dynamo bytecode transform time: 3.92 s
[1;36m(EngineCore_DP0 pid=4070507)[0;0m ERROR 12-05 16:40:29 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4070507)[0;0m ERROR 12-05 16:40:29 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4070507)[0;0m ERROR 12-05 16:40:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4070507)[0;0m ERROR 12-05 16:40:29 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4070507)[0;0m ERROR 12-05 16:40:29 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070507)[0;0m ERROR 12-05 16:40:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4070507)[0;0m ERROR 12-05 16:40:29 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4070507)[0;0m ERROR 12-05 16:40:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4070507)[0;0m ERROR 12-05 16:40:29 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4070507)[0;0m ERROR 12-05 16:40:29 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070507)[0;0m ERROR 12-05 16:40:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4070507)[0;0m ERROR 12-05 16:40:29 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4070507)[0;0m ERROR 12-05 16:40:29 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070507)[0;0m ERROR 12-05 16:40:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4070507)[0;0m ERROR 12-05 16:40:29 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4070507)[0;0m ERROR 12-05 16:40:29 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070507)[0;0m ERROR 12-05 16:40:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4070507)[0;0m ERROR 12-05 16:40:29 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4070507)[0;0m ERROR 12-05 16:40:29 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070507)[0;0m ERROR 12-05 16:40:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4070507)[0;0m ERROR 12-05 16:40:29 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4070507)[0;0m ERROR 12-05 16:40:29 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070507)[0;0m ERROR 12-05 16:40:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4070507)[0;0m ERROR 12-05 16:40:29 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4070507)[0;0m ERROR 12-05 16:40:29 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070507)[0;0m ERROR 12-05 16:40:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4070507)[0;0m ERROR 12-05 16:40:29 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4070507)[0;0m ERROR 12-05 16:40:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4070507)[0;0m ERROR 12-05 16:40:29 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4070507)[0;0m ERROR 12-05 16:40:29 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070507)[0;0m ERROR 12-05 16:40:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4070507)[0;0m ERROR 12-05 16:40:29 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4070507)[0;0m ERROR 12-05 16:40:29 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070507)[0;0m ERROR 12-05 16:40:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4070507)[0;0m ERROR 12-05 16:40:29 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4070507)[0;0m ERROR 12-05 16:40:29 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070507)[0;0m ERROR 12-05 16:40:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4070507)[0;0m ERROR 12-05 16:40:29 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4070507)[0;0m ERROR 12-05 16:40:29 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070507)[0;0m ERROR 12-05 16:40:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4070507)[0;0m ERROR 12-05 16:40:29 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4070507)[0;0m ERROR 12-05 16:40:29 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070507)[0;0m ERROR 12-05 16:40:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4070507)[0;0m ERROR 12-05 16:40:29 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4070507)[0;0m ERROR 12-05 16:40:29 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070507)[0;0m ERROR 12-05 16:40:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4070507)[0;0m ERROR 12-05 16:40:29 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4070507)[0;0m ERROR 12-05 16:40:29 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070507)[0;0m ERROR 12-05 16:40:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4070507)[0;0m ERROR 12-05 16:40:29 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4070507)[0;0m ERROR 12-05 16:40:29 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070507)[0;0m ERROR 12-05 16:40:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4070507)[0;0m ERROR 12-05 16:40:29 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4070507)[0;0m ERROR 12-05 16:40:29 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070507)[0;0m ERROR 12-05 16:40:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4070507)[0;0m ERROR 12-05 16:40:29 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4070507)[0;0m ERROR 12-05 16:40:29 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070507)[0;0m ERROR 12-05 16:40:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4070507)[0;0m ERROR 12-05 16:40:29 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4070507)[0;0m ERROR 12-05 16:40:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4070507)[0;0m ERROR 12-05 16:40:29 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4070507)[0;0m ERROR 12-05 16:40:29 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070507)[0;0m ERROR 12-05 16:40:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4070507)[0;0m ERROR 12-05 16:40:29 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4070507)[0;0m ERROR 12-05 16:40:29 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070507)[0;0m ERROR 12-05 16:40:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4070507)[0;0m ERROR 12-05 16:40:29 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4070507)[0;0m ERROR 12-05 16:40:29 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070507)[0;0m ERROR 12-05 16:40:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4070507)[0;0m ERROR 12-05 16:40:29 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4070507)[0;0m ERROR 12-05 16:40:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4070507)[0;0m ERROR 12-05 16:40:29 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4070507)[0;0m ERROR 12-05 16:40:29 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070507)[0;0m ERROR 12-05 16:40:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4070507)[0;0m ERROR 12-05 16:40:29 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4070507)[0;0m ERROR 12-05 16:40:29 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070507)[0;0m ERROR 12-05 16:40:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4070507)[0;0m ERROR 12-05 16:40:29 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4070507)[0;0m ERROR 12-05 16:40:29 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070507)[0;0m ERROR 12-05 16:40:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4070507)[0;0m ERROR 12-05 16:40:29 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4070507)[0;0m ERROR 12-05 16:40:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4070507)[0;0m ERROR 12-05 16:40:29 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4070507)[0;0m ERROR 12-05 16:40:29 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070507)[0;0m ERROR 12-05 16:40:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4070507)[0;0m ERROR 12-05 16:40:29 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4070507)[0;0m ERROR 12-05 16:40:29 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070507)[0;0m ERROR 12-05 16:40:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4070507)[0;0m ERROR 12-05 16:40:29 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4070507)[0;0m ERROR 12-05 16:40:29 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070507)[0;0m ERROR 12-05 16:40:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4070507)[0;0m ERROR 12-05 16:40:29 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4070507)[0;0m ERROR 12-05 16:40:29 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070507)[0;0m ERROR 12-05 16:40:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4070507)[0;0m ERROR 12-05 16:40:29 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4070507)[0;0m ERROR 12-05 16:40:29 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7f4d8c2094c0>' raised:
[1;36m(EngineCore_DP0 pid=4070507)[0;0m ERROR 12-05 16:40:29 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4070507)[0;0m ERROR 12-05 16:40:29 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4070507)[0;0m ERROR 12-05 16:40:29 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4070507)[0;0m ERROR 12-05 16:40:29 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 16:40:50 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 16:40:50 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 16:40:50 [model.py:1745] Using max model len 131072
INFO 12-05 16:40:50 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4070593)[0;0m INFO 12-05 16:40:59 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4070593)[0;0m INFO 12-05 16:41:00 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:43033 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4070593)[0;0m INFO 12-05 16:41:00 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4070593)[0;0m INFO 12-05 16:41:00 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4070593)[0;0m INFO 12-05 16:41:01 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4070593)[0;0m INFO 12-05 16:41:01 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4070593)[0;0m INFO 12-05 16:41:02 [default_loader.py:314] Loading weights took 0.89 seconds
[1;36m(EngineCore_DP0 pid=4070593)[0;0m INFO 12-05 16:41:02 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.479244 seconds
[1;36m(EngineCore_DP0 pid=4070593)[0;0m INFO 12-05 16:41:08 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4070593)[0;0m INFO 12-05 16:41:08 [backends.py:647] Dynamo bytecode transform time: 5.35 s
[1;36m(EngineCore_DP0 pid=4070593)[0;0m ERROR 12-05 16:41:08 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4070593)[0;0m ERROR 12-05 16:41:08 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4070593)[0;0m ERROR 12-05 16:41:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4070593)[0;0m ERROR 12-05 16:41:08 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4070593)[0;0m ERROR 12-05 16:41:08 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070593)[0;0m ERROR 12-05 16:41:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4070593)[0;0m ERROR 12-05 16:41:08 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4070593)[0;0m ERROR 12-05 16:41:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4070593)[0;0m ERROR 12-05 16:41:08 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4070593)[0;0m ERROR 12-05 16:41:08 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070593)[0;0m ERROR 12-05 16:41:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4070593)[0;0m ERROR 12-05 16:41:08 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4070593)[0;0m ERROR 12-05 16:41:08 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070593)[0;0m ERROR 12-05 16:41:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4070593)[0;0m ERROR 12-05 16:41:08 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4070593)[0;0m ERROR 12-05 16:41:08 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070593)[0;0m ERROR 12-05 16:41:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4070593)[0;0m ERROR 12-05 16:41:08 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4070593)[0;0m ERROR 12-05 16:41:08 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070593)[0;0m ERROR 12-05 16:41:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4070593)[0;0m ERROR 12-05 16:41:08 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4070593)[0;0m ERROR 12-05 16:41:08 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070593)[0;0m ERROR 12-05 16:41:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4070593)[0;0m ERROR 12-05 16:41:08 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4070593)[0;0m ERROR 12-05 16:41:08 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070593)[0;0m ERROR 12-05 16:41:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4070593)[0;0m ERROR 12-05 16:41:08 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4070593)[0;0m ERROR 12-05 16:41:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4070593)[0;0m ERROR 12-05 16:41:08 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4070593)[0;0m ERROR 12-05 16:41:08 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070593)[0;0m ERROR 12-05 16:41:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4070593)[0;0m ERROR 12-05 16:41:08 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4070593)[0;0m ERROR 12-05 16:41:08 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070593)[0;0m ERROR 12-05 16:41:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4070593)[0;0m ERROR 12-05 16:41:08 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4070593)[0;0m ERROR 12-05 16:41:08 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070593)[0;0m ERROR 12-05 16:41:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4070593)[0;0m ERROR 12-05 16:41:08 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4070593)[0;0m ERROR 12-05 16:41:08 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070593)[0;0m ERROR 12-05 16:41:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4070593)[0;0m ERROR 12-05 16:41:08 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4070593)[0;0m ERROR 12-05 16:41:08 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070593)[0;0m ERROR 12-05 16:41:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4070593)[0;0m ERROR 12-05 16:41:08 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4070593)[0;0m ERROR 12-05 16:41:08 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070593)[0;0m ERROR 12-05 16:41:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4070593)[0;0m ERROR 12-05 16:41:08 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4070593)[0;0m ERROR 12-05 16:41:08 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070593)[0;0m ERROR 12-05 16:41:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4070593)[0;0m ERROR 12-05 16:41:08 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4070593)[0;0m ERROR 12-05 16:41:08 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070593)[0;0m ERROR 12-05 16:41:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4070593)[0;0m ERROR 12-05 16:41:08 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4070593)[0;0m ERROR 12-05 16:41:08 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070593)[0;0m ERROR 12-05 16:41:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4070593)[0;0m ERROR 12-05 16:41:08 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4070593)[0;0m ERROR 12-05 16:41:08 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070593)[0;0m ERROR 12-05 16:41:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4070593)[0;0m ERROR 12-05 16:41:08 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4070593)[0;0m ERROR 12-05 16:41:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4070593)[0;0m ERROR 12-05 16:41:08 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4070593)[0;0m ERROR 12-05 16:41:08 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070593)[0;0m ERROR 12-05 16:41:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4070593)[0;0m ERROR 12-05 16:41:08 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4070593)[0;0m ERROR 12-05 16:41:08 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070593)[0;0m ERROR 12-05 16:41:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4070593)[0;0m ERROR 12-05 16:41:08 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4070593)[0;0m ERROR 12-05 16:41:08 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070593)[0;0m ERROR 12-05 16:41:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4070593)[0;0m ERROR 12-05 16:41:08 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4070593)[0;0m ERROR 12-05 16:41:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4070593)[0;0m ERROR 12-05 16:41:08 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4070593)[0;0m ERROR 12-05 16:41:08 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070593)[0;0m ERROR 12-05 16:41:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4070593)[0;0m ERROR 12-05 16:41:08 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4070593)[0;0m ERROR 12-05 16:41:08 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070593)[0;0m ERROR 12-05 16:41:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4070593)[0;0m ERROR 12-05 16:41:08 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4070593)[0;0m ERROR 12-05 16:41:08 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070593)[0;0m ERROR 12-05 16:41:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4070593)[0;0m ERROR 12-05 16:41:08 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4070593)[0;0m ERROR 12-05 16:41:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4070593)[0;0m ERROR 12-05 16:41:08 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4070593)[0;0m ERROR 12-05 16:41:08 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070593)[0;0m ERROR 12-05 16:41:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4070593)[0;0m ERROR 12-05 16:41:08 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4070593)[0;0m ERROR 12-05 16:41:08 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070593)[0;0m ERROR 12-05 16:41:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4070593)[0;0m ERROR 12-05 16:41:08 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4070593)[0;0m ERROR 12-05 16:41:08 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070593)[0;0m ERROR 12-05 16:41:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4070593)[0;0m ERROR 12-05 16:41:08 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4070593)[0;0m ERROR 12-05 16:41:08 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070593)[0;0m ERROR 12-05 16:41:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4070593)[0;0m ERROR 12-05 16:41:08 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4070593)[0;0m ERROR 12-05 16:41:08 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7f2b702b93d0>' raised:
[1;36m(EngineCore_DP0 pid=4070593)[0;0m ERROR 12-05 16:41:08 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4070593)[0;0m ERROR 12-05 16:41:08 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4070593)[0;0m ERROR 12-05 16:41:08 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4070593)[0;0m ERROR 12-05 16:41:08 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 16:41:31 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 16:41:31 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 16:41:31 [model.py:1745] Using max model len 131072
INFO 12-05 16:41:31 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4070772)[0;0m INFO 12-05 16:41:39 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4070772)[0;0m INFO 12-05 16:41:40 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:32867 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4070772)[0;0m INFO 12-05 16:41:40 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4070772)[0;0m INFO 12-05 16:41:40 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4070772)[0;0m INFO 12-05 16:41:41 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4070772)[0;0m INFO 12-05 16:41:41 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4070772)[0;0m INFO 12-05 16:41:42 [default_loader.py:314] Loading weights took 0.89 seconds
[1;36m(EngineCore_DP0 pid=4070772)[0;0m INFO 12-05 16:41:42 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.530046 seconds
[1;36m(EngineCore_DP0 pid=4070772)[0;0m INFO 12-05 16:41:48 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4070772)[0;0m INFO 12-05 16:41:48 [backends.py:647] Dynamo bytecode transform time: 5.34 s
[1;36m(EngineCore_DP0 pid=4070772)[0;0m ERROR 12-05 16:41:48 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4070772)[0;0m ERROR 12-05 16:41:48 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4070772)[0;0m ERROR 12-05 16:41:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4070772)[0;0m ERROR 12-05 16:41:48 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4070772)[0;0m ERROR 12-05 16:41:48 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070772)[0;0m ERROR 12-05 16:41:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4070772)[0;0m ERROR 12-05 16:41:48 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4070772)[0;0m ERROR 12-05 16:41:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4070772)[0;0m ERROR 12-05 16:41:48 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4070772)[0;0m ERROR 12-05 16:41:48 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070772)[0;0m ERROR 12-05 16:41:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4070772)[0;0m ERROR 12-05 16:41:48 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4070772)[0;0m ERROR 12-05 16:41:48 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070772)[0;0m ERROR 12-05 16:41:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4070772)[0;0m ERROR 12-05 16:41:48 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4070772)[0;0m ERROR 12-05 16:41:48 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070772)[0;0m ERROR 12-05 16:41:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4070772)[0;0m ERROR 12-05 16:41:48 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4070772)[0;0m ERROR 12-05 16:41:48 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070772)[0;0m ERROR 12-05 16:41:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4070772)[0;0m ERROR 12-05 16:41:48 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4070772)[0;0m ERROR 12-05 16:41:48 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070772)[0;0m ERROR 12-05 16:41:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4070772)[0;0m ERROR 12-05 16:41:48 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4070772)[0;0m ERROR 12-05 16:41:48 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070772)[0;0m ERROR 12-05 16:41:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4070772)[0;0m ERROR 12-05 16:41:48 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4070772)[0;0m ERROR 12-05 16:41:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4070772)[0;0m ERROR 12-05 16:41:48 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4070772)[0;0m ERROR 12-05 16:41:48 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070772)[0;0m ERROR 12-05 16:41:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4070772)[0;0m ERROR 12-05 16:41:48 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4070772)[0;0m ERROR 12-05 16:41:48 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070772)[0;0m ERROR 12-05 16:41:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4070772)[0;0m ERROR 12-05 16:41:48 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4070772)[0;0m ERROR 12-05 16:41:48 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070772)[0;0m ERROR 12-05 16:41:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4070772)[0;0m ERROR 12-05 16:41:48 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4070772)[0;0m ERROR 12-05 16:41:48 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070772)[0;0m ERROR 12-05 16:41:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4070772)[0;0m ERROR 12-05 16:41:48 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4070772)[0;0m ERROR 12-05 16:41:48 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070772)[0;0m ERROR 12-05 16:41:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4070772)[0;0m ERROR 12-05 16:41:48 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4070772)[0;0m ERROR 12-05 16:41:48 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070772)[0;0m ERROR 12-05 16:41:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4070772)[0;0m ERROR 12-05 16:41:48 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4070772)[0;0m ERROR 12-05 16:41:48 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070772)[0;0m ERROR 12-05 16:41:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4070772)[0;0m ERROR 12-05 16:41:48 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4070772)[0;0m ERROR 12-05 16:41:48 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070772)[0;0m ERROR 12-05 16:41:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4070772)[0;0m ERROR 12-05 16:41:48 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4070772)[0;0m ERROR 12-05 16:41:48 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070772)[0;0m ERROR 12-05 16:41:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4070772)[0;0m ERROR 12-05 16:41:48 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4070772)[0;0m ERROR 12-05 16:41:48 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070772)[0;0m ERROR 12-05 16:41:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4070772)[0;0m ERROR 12-05 16:41:48 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4070772)[0;0m ERROR 12-05 16:41:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4070772)[0;0m ERROR 12-05 16:41:48 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4070772)[0;0m ERROR 12-05 16:41:48 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070772)[0;0m ERROR 12-05 16:41:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4070772)[0;0m ERROR 12-05 16:41:48 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4070772)[0;0m ERROR 12-05 16:41:48 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070772)[0;0m ERROR 12-05 16:41:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4070772)[0;0m ERROR 12-05 16:41:48 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4070772)[0;0m ERROR 12-05 16:41:48 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070772)[0;0m ERROR 12-05 16:41:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4070772)[0;0m ERROR 12-05 16:41:48 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4070772)[0;0m ERROR 12-05 16:41:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4070772)[0;0m ERROR 12-05 16:41:48 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4070772)[0;0m ERROR 12-05 16:41:48 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070772)[0;0m ERROR 12-05 16:41:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4070772)[0;0m ERROR 12-05 16:41:48 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4070772)[0;0m ERROR 12-05 16:41:48 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070772)[0;0m ERROR 12-05 16:41:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4070772)[0;0m ERROR 12-05 16:41:48 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4070772)[0;0m ERROR 12-05 16:41:48 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070772)[0;0m ERROR 12-05 16:41:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4070772)[0;0m ERROR 12-05 16:41:48 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4070772)[0;0m ERROR 12-05 16:41:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4070772)[0;0m ERROR 12-05 16:41:48 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4070772)[0;0m ERROR 12-05 16:41:48 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070772)[0;0m ERROR 12-05 16:41:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4070772)[0;0m ERROR 12-05 16:41:48 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4070772)[0;0m ERROR 12-05 16:41:48 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070772)[0;0m ERROR 12-05 16:41:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4070772)[0;0m ERROR 12-05 16:41:48 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4070772)[0;0m ERROR 12-05 16:41:48 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070772)[0;0m ERROR 12-05 16:41:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4070772)[0;0m ERROR 12-05 16:41:48 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4070772)[0;0m ERROR 12-05 16:41:48 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070772)[0;0m ERROR 12-05 16:41:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4070772)[0;0m ERROR 12-05 16:41:48 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4070772)[0;0m ERROR 12-05 16:41:48 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7fab34532c90>' raised:
[1;36m(EngineCore_DP0 pid=4070772)[0;0m ERROR 12-05 16:41:48 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4070772)[0;0m ERROR 12-05 16:41:48 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4070772)[0;0m ERROR 12-05 16:41:48 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4070772)[0;0m ERROR 12-05 16:41:48 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 16:42:11 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 16:42:11 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 16:42:11 [model.py:1745] Using max model len 131072
INFO 12-05 16:42:11 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4070857)[0;0m INFO 12-05 16:42:20 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4070857)[0;0m INFO 12-05 16:42:21 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:39115 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4070857)[0;0m INFO 12-05 16:42:21 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4070857)[0;0m INFO 12-05 16:42:22 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4070857)[0;0m INFO 12-05 16:42:22 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4070857)[0;0m INFO 12-05 16:42:22 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4070857)[0;0m INFO 12-05 16:42:24 [default_loader.py:314] Loading weights took 0.89 seconds
[1;36m(EngineCore_DP0 pid=4070857)[0;0m INFO 12-05 16:42:24 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.552071 seconds
[1;36m(EngineCore_DP0 pid=4070857)[0;0m INFO 12-05 16:42:29 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4070857)[0;0m INFO 12-05 16:42:29 [backends.py:647] Dynamo bytecode transform time: 4.62 s
[1;36m(EngineCore_DP0 pid=4070857)[0;0m ERROR 12-05 16:42:29 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4070857)[0;0m ERROR 12-05 16:42:29 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4070857)[0;0m ERROR 12-05 16:42:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4070857)[0;0m ERROR 12-05 16:42:29 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4070857)[0;0m ERROR 12-05 16:42:29 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070857)[0;0m ERROR 12-05 16:42:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4070857)[0;0m ERROR 12-05 16:42:29 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4070857)[0;0m ERROR 12-05 16:42:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4070857)[0;0m ERROR 12-05 16:42:29 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4070857)[0;0m ERROR 12-05 16:42:29 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070857)[0;0m ERROR 12-05 16:42:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4070857)[0;0m ERROR 12-05 16:42:29 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4070857)[0;0m ERROR 12-05 16:42:29 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070857)[0;0m ERROR 12-05 16:42:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4070857)[0;0m ERROR 12-05 16:42:29 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4070857)[0;0m ERROR 12-05 16:42:29 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070857)[0;0m ERROR 12-05 16:42:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4070857)[0;0m ERROR 12-05 16:42:29 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4070857)[0;0m ERROR 12-05 16:42:29 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070857)[0;0m ERROR 12-05 16:42:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4070857)[0;0m ERROR 12-05 16:42:29 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4070857)[0;0m ERROR 12-05 16:42:29 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070857)[0;0m ERROR 12-05 16:42:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4070857)[0;0m ERROR 12-05 16:42:29 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4070857)[0;0m ERROR 12-05 16:42:29 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070857)[0;0m ERROR 12-05 16:42:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4070857)[0;0m ERROR 12-05 16:42:29 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4070857)[0;0m ERROR 12-05 16:42:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4070857)[0;0m ERROR 12-05 16:42:29 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4070857)[0;0m ERROR 12-05 16:42:29 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070857)[0;0m ERROR 12-05 16:42:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4070857)[0;0m ERROR 12-05 16:42:29 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4070857)[0;0m ERROR 12-05 16:42:29 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070857)[0;0m ERROR 12-05 16:42:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4070857)[0;0m ERROR 12-05 16:42:29 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4070857)[0;0m ERROR 12-05 16:42:29 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070857)[0;0m ERROR 12-05 16:42:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4070857)[0;0m ERROR 12-05 16:42:29 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4070857)[0;0m ERROR 12-05 16:42:29 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070857)[0;0m ERROR 12-05 16:42:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4070857)[0;0m ERROR 12-05 16:42:29 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4070857)[0;0m ERROR 12-05 16:42:29 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070857)[0;0m ERROR 12-05 16:42:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4070857)[0;0m ERROR 12-05 16:42:29 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4070857)[0;0m ERROR 12-05 16:42:29 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070857)[0;0m ERROR 12-05 16:42:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4070857)[0;0m ERROR 12-05 16:42:29 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4070857)[0;0m ERROR 12-05 16:42:29 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070857)[0;0m ERROR 12-05 16:42:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4070857)[0;0m ERROR 12-05 16:42:29 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4070857)[0;0m ERROR 12-05 16:42:29 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070857)[0;0m ERROR 12-05 16:42:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4070857)[0;0m ERROR 12-05 16:42:29 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4070857)[0;0m ERROR 12-05 16:42:29 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070857)[0;0m ERROR 12-05 16:42:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4070857)[0;0m ERROR 12-05 16:42:29 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4070857)[0;0m ERROR 12-05 16:42:29 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070857)[0;0m ERROR 12-05 16:42:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4070857)[0;0m ERROR 12-05 16:42:29 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4070857)[0;0m ERROR 12-05 16:42:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4070857)[0;0m ERROR 12-05 16:42:29 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4070857)[0;0m ERROR 12-05 16:42:29 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070857)[0;0m ERROR 12-05 16:42:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4070857)[0;0m ERROR 12-05 16:42:29 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4070857)[0;0m ERROR 12-05 16:42:29 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070857)[0;0m ERROR 12-05 16:42:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4070857)[0;0m ERROR 12-05 16:42:29 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4070857)[0;0m ERROR 12-05 16:42:29 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070857)[0;0m ERROR 12-05 16:42:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4070857)[0;0m ERROR 12-05 16:42:29 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4070857)[0;0m ERROR 12-05 16:42:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4070857)[0;0m ERROR 12-05 16:42:29 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4070857)[0;0m ERROR 12-05 16:42:29 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070857)[0;0m ERROR 12-05 16:42:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4070857)[0;0m ERROR 12-05 16:42:29 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4070857)[0;0m ERROR 12-05 16:42:29 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070857)[0;0m ERROR 12-05 16:42:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4070857)[0;0m ERROR 12-05 16:42:29 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4070857)[0;0m ERROR 12-05 16:42:29 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070857)[0;0m ERROR 12-05 16:42:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4070857)[0;0m ERROR 12-05 16:42:29 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4070857)[0;0m ERROR 12-05 16:42:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4070857)[0;0m ERROR 12-05 16:42:29 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4070857)[0;0m ERROR 12-05 16:42:29 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070857)[0;0m ERROR 12-05 16:42:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4070857)[0;0m ERROR 12-05 16:42:29 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4070857)[0;0m ERROR 12-05 16:42:29 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070857)[0;0m ERROR 12-05 16:42:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4070857)[0;0m ERROR 12-05 16:42:29 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4070857)[0;0m ERROR 12-05 16:42:29 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070857)[0;0m ERROR 12-05 16:42:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4070857)[0;0m ERROR 12-05 16:42:29 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4070857)[0;0m ERROR 12-05 16:42:29 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070857)[0;0m ERROR 12-05 16:42:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4070857)[0;0m ERROR 12-05 16:42:29 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4070857)[0;0m ERROR 12-05 16:42:29 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7fb60d5971d0>' raised:
[1;36m(EngineCore_DP0 pid=4070857)[0;0m ERROR 12-05 16:42:29 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4070857)[0;0m ERROR 12-05 16:42:29 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4070857)[0;0m ERROR 12-05 16:42:29 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4070857)[0;0m ERROR 12-05 16:42:29 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 16:42:52 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 16:42:52 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 16:42:52 [model.py:1745] Using max model len 131072
INFO 12-05 16:42:52 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4070948)[0;0m INFO 12-05 16:43:00 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4070948)[0;0m INFO 12-05 16:43:01 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:49209 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4070948)[0;0m INFO 12-05 16:43:01 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4070948)[0;0m INFO 12-05 16:43:02 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4070948)[0;0m INFO 12-05 16:43:02 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4070948)[0;0m INFO 12-05 16:43:02 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4070948)[0;0m INFO 12-05 16:43:03 [default_loader.py:314] Loading weights took 0.87 seconds
[1;36m(EngineCore_DP0 pid=4070948)[0;0m INFO 12-05 16:43:04 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.579060 seconds
[1;36m(EngineCore_DP0 pid=4070948)[0;0m INFO 12-05 16:43:08 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4070948)[0;0m INFO 12-05 16:43:08 [backends.py:647] Dynamo bytecode transform time: 4.11 s
[1;36m(EngineCore_DP0 pid=4070948)[0;0m ERROR 12-05 16:43:08 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4070948)[0;0m ERROR 12-05 16:43:08 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4070948)[0;0m ERROR 12-05 16:43:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4070948)[0;0m ERROR 12-05 16:43:08 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4070948)[0;0m ERROR 12-05 16:43:08 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070948)[0;0m ERROR 12-05 16:43:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4070948)[0;0m ERROR 12-05 16:43:08 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4070948)[0;0m ERROR 12-05 16:43:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4070948)[0;0m ERROR 12-05 16:43:08 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4070948)[0;0m ERROR 12-05 16:43:08 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070948)[0;0m ERROR 12-05 16:43:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4070948)[0;0m ERROR 12-05 16:43:08 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4070948)[0;0m ERROR 12-05 16:43:08 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070948)[0;0m ERROR 12-05 16:43:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4070948)[0;0m ERROR 12-05 16:43:08 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4070948)[0;0m ERROR 12-05 16:43:08 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070948)[0;0m ERROR 12-05 16:43:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4070948)[0;0m ERROR 12-05 16:43:08 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4070948)[0;0m ERROR 12-05 16:43:08 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070948)[0;0m ERROR 12-05 16:43:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4070948)[0;0m ERROR 12-05 16:43:08 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4070948)[0;0m ERROR 12-05 16:43:08 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070948)[0;0m ERROR 12-05 16:43:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4070948)[0;0m ERROR 12-05 16:43:08 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4070948)[0;0m ERROR 12-05 16:43:08 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070948)[0;0m ERROR 12-05 16:43:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4070948)[0;0m ERROR 12-05 16:43:08 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4070948)[0;0m ERROR 12-05 16:43:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4070948)[0;0m ERROR 12-05 16:43:08 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4070948)[0;0m ERROR 12-05 16:43:08 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070948)[0;0m ERROR 12-05 16:43:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4070948)[0;0m ERROR 12-05 16:43:08 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4070948)[0;0m ERROR 12-05 16:43:08 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070948)[0;0m ERROR 12-05 16:43:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4070948)[0;0m ERROR 12-05 16:43:08 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4070948)[0;0m ERROR 12-05 16:43:08 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070948)[0;0m ERROR 12-05 16:43:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4070948)[0;0m ERROR 12-05 16:43:08 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4070948)[0;0m ERROR 12-05 16:43:08 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070948)[0;0m ERROR 12-05 16:43:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4070948)[0;0m ERROR 12-05 16:43:08 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4070948)[0;0m ERROR 12-05 16:43:08 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070948)[0;0m ERROR 12-05 16:43:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4070948)[0;0m ERROR 12-05 16:43:08 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4070948)[0;0m ERROR 12-05 16:43:08 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070948)[0;0m ERROR 12-05 16:43:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4070948)[0;0m ERROR 12-05 16:43:08 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4070948)[0;0m ERROR 12-05 16:43:08 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070948)[0;0m ERROR 12-05 16:43:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4070948)[0;0m ERROR 12-05 16:43:08 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4070948)[0;0m ERROR 12-05 16:43:08 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070948)[0;0m ERROR 12-05 16:43:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4070948)[0;0m ERROR 12-05 16:43:08 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4070948)[0;0m ERROR 12-05 16:43:08 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070948)[0;0m ERROR 12-05 16:43:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4070948)[0;0m ERROR 12-05 16:43:08 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4070948)[0;0m ERROR 12-05 16:43:08 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070948)[0;0m ERROR 12-05 16:43:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4070948)[0;0m ERROR 12-05 16:43:08 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4070948)[0;0m ERROR 12-05 16:43:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4070948)[0;0m ERROR 12-05 16:43:08 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4070948)[0;0m ERROR 12-05 16:43:08 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070948)[0;0m ERROR 12-05 16:43:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4070948)[0;0m ERROR 12-05 16:43:08 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4070948)[0;0m ERROR 12-05 16:43:08 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070948)[0;0m ERROR 12-05 16:43:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4070948)[0;0m ERROR 12-05 16:43:08 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4070948)[0;0m ERROR 12-05 16:43:08 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070948)[0;0m ERROR 12-05 16:43:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4070948)[0;0m ERROR 12-05 16:43:08 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4070948)[0;0m ERROR 12-05 16:43:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4070948)[0;0m ERROR 12-05 16:43:08 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4070948)[0;0m ERROR 12-05 16:43:08 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070948)[0;0m ERROR 12-05 16:43:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4070948)[0;0m ERROR 12-05 16:43:08 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4070948)[0;0m ERROR 12-05 16:43:08 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070948)[0;0m ERROR 12-05 16:43:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4070948)[0;0m ERROR 12-05 16:43:08 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4070948)[0;0m ERROR 12-05 16:43:08 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070948)[0;0m ERROR 12-05 16:43:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4070948)[0;0m ERROR 12-05 16:43:08 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4070948)[0;0m ERROR 12-05 16:43:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4070948)[0;0m ERROR 12-05 16:43:08 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4070948)[0;0m ERROR 12-05 16:43:08 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070948)[0;0m ERROR 12-05 16:43:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4070948)[0;0m ERROR 12-05 16:43:08 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4070948)[0;0m ERROR 12-05 16:43:08 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070948)[0;0m ERROR 12-05 16:43:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4070948)[0;0m ERROR 12-05 16:43:08 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4070948)[0;0m ERROR 12-05 16:43:08 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070948)[0;0m ERROR 12-05 16:43:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4070948)[0;0m ERROR 12-05 16:43:08 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4070948)[0;0m ERROR 12-05 16:43:08 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4070948)[0;0m ERROR 12-05 16:43:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4070948)[0;0m ERROR 12-05 16:43:08 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4070948)[0;0m ERROR 12-05 16:43:08 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7f39780a68d0>' raised:
[1;36m(EngineCore_DP0 pid=4070948)[0;0m ERROR 12-05 16:43:08 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4070948)[0;0m ERROR 12-05 16:43:08 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4070948)[0;0m ERROR 12-05 16:43:08 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4070948)[0;0m ERROR 12-05 16:43:08 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 16:43:30 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 16:43:30 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 16:43:30 [model.py:1745] Using max model len 131072
INFO 12-05 16:43:30 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4071027)[0;0m INFO 12-05 16:43:38 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4071027)[0;0m INFO 12-05 16:43:39 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:37633 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4071027)[0;0m INFO 12-05 16:43:39 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4071027)[0;0m INFO 12-05 16:43:39 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4071027)[0;0m INFO 12-05 16:43:40 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4071027)[0;0m INFO 12-05 16:43:40 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4071027)[0;0m INFO 12-05 16:43:41 [default_loader.py:314] Loading weights took 0.89 seconds
[1;36m(EngineCore_DP0 pid=4071027)[0;0m INFO 12-05 16:43:42 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.475721 seconds
[1;36m(EngineCore_DP0 pid=4071027)[0;0m INFO 12-05 16:43:46 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4071027)[0;0m INFO 12-05 16:43:46 [backends.py:647] Dynamo bytecode transform time: 3.79 s
[1;36m(EngineCore_DP0 pid=4071027)[0;0m ERROR 12-05 16:43:46 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4071027)[0;0m ERROR 12-05 16:43:46 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4071027)[0;0m ERROR 12-05 16:43:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4071027)[0;0m ERROR 12-05 16:43:46 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4071027)[0;0m ERROR 12-05 16:43:46 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071027)[0;0m ERROR 12-05 16:43:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4071027)[0;0m ERROR 12-05 16:43:46 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4071027)[0;0m ERROR 12-05 16:43:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4071027)[0;0m ERROR 12-05 16:43:46 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4071027)[0;0m ERROR 12-05 16:43:46 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071027)[0;0m ERROR 12-05 16:43:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4071027)[0;0m ERROR 12-05 16:43:46 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4071027)[0;0m ERROR 12-05 16:43:46 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071027)[0;0m ERROR 12-05 16:43:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4071027)[0;0m ERROR 12-05 16:43:46 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4071027)[0;0m ERROR 12-05 16:43:46 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071027)[0;0m ERROR 12-05 16:43:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4071027)[0;0m ERROR 12-05 16:43:46 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4071027)[0;0m ERROR 12-05 16:43:46 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071027)[0;0m ERROR 12-05 16:43:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4071027)[0;0m ERROR 12-05 16:43:46 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4071027)[0;0m ERROR 12-05 16:43:46 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071027)[0;0m ERROR 12-05 16:43:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4071027)[0;0m ERROR 12-05 16:43:46 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4071027)[0;0m ERROR 12-05 16:43:46 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071027)[0;0m ERROR 12-05 16:43:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4071027)[0;0m ERROR 12-05 16:43:46 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4071027)[0;0m ERROR 12-05 16:43:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4071027)[0;0m ERROR 12-05 16:43:46 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4071027)[0;0m ERROR 12-05 16:43:46 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071027)[0;0m ERROR 12-05 16:43:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4071027)[0;0m ERROR 12-05 16:43:46 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4071027)[0;0m ERROR 12-05 16:43:46 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071027)[0;0m ERROR 12-05 16:43:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4071027)[0;0m ERROR 12-05 16:43:46 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4071027)[0;0m ERROR 12-05 16:43:46 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071027)[0;0m ERROR 12-05 16:43:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4071027)[0;0m ERROR 12-05 16:43:46 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4071027)[0;0m ERROR 12-05 16:43:46 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071027)[0;0m ERROR 12-05 16:43:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4071027)[0;0m ERROR 12-05 16:43:46 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4071027)[0;0m ERROR 12-05 16:43:46 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071027)[0;0m ERROR 12-05 16:43:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4071027)[0;0m ERROR 12-05 16:43:46 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4071027)[0;0m ERROR 12-05 16:43:46 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071027)[0;0m ERROR 12-05 16:43:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4071027)[0;0m ERROR 12-05 16:43:46 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4071027)[0;0m ERROR 12-05 16:43:46 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071027)[0;0m ERROR 12-05 16:43:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4071027)[0;0m ERROR 12-05 16:43:46 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4071027)[0;0m ERROR 12-05 16:43:46 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071027)[0;0m ERROR 12-05 16:43:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4071027)[0;0m ERROR 12-05 16:43:46 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4071027)[0;0m ERROR 12-05 16:43:46 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071027)[0;0m ERROR 12-05 16:43:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4071027)[0;0m ERROR 12-05 16:43:46 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4071027)[0;0m ERROR 12-05 16:43:46 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071027)[0;0m ERROR 12-05 16:43:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4071027)[0;0m ERROR 12-05 16:43:46 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4071027)[0;0m ERROR 12-05 16:43:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4071027)[0;0m ERROR 12-05 16:43:46 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4071027)[0;0m ERROR 12-05 16:43:46 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071027)[0;0m ERROR 12-05 16:43:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4071027)[0;0m ERROR 12-05 16:43:46 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4071027)[0;0m ERROR 12-05 16:43:46 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071027)[0;0m ERROR 12-05 16:43:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4071027)[0;0m ERROR 12-05 16:43:46 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4071027)[0;0m ERROR 12-05 16:43:46 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071027)[0;0m ERROR 12-05 16:43:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4071027)[0;0m ERROR 12-05 16:43:46 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4071027)[0;0m ERROR 12-05 16:43:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4071027)[0;0m ERROR 12-05 16:43:46 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4071027)[0;0m ERROR 12-05 16:43:46 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071027)[0;0m ERROR 12-05 16:43:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4071027)[0;0m ERROR 12-05 16:43:46 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4071027)[0;0m ERROR 12-05 16:43:46 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071027)[0;0m ERROR 12-05 16:43:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4071027)[0;0m ERROR 12-05 16:43:46 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4071027)[0;0m ERROR 12-05 16:43:46 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071027)[0;0m ERROR 12-05 16:43:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4071027)[0;0m ERROR 12-05 16:43:46 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4071027)[0;0m ERROR 12-05 16:43:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4071027)[0;0m ERROR 12-05 16:43:46 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4071027)[0;0m ERROR 12-05 16:43:46 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071027)[0;0m ERROR 12-05 16:43:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4071027)[0;0m ERROR 12-05 16:43:46 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4071027)[0;0m ERROR 12-05 16:43:46 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071027)[0;0m ERROR 12-05 16:43:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4071027)[0;0m ERROR 12-05 16:43:46 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4071027)[0;0m ERROR 12-05 16:43:46 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071027)[0;0m ERROR 12-05 16:43:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4071027)[0;0m ERROR 12-05 16:43:46 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4071027)[0;0m ERROR 12-05 16:43:46 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071027)[0;0m ERROR 12-05 16:43:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4071027)[0;0m ERROR 12-05 16:43:46 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4071027)[0;0m ERROR 12-05 16:43:46 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7f3304334620>' raised:
[1;36m(EngineCore_DP0 pid=4071027)[0;0m ERROR 12-05 16:43:46 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4071027)[0;0m ERROR 12-05 16:43:46 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4071027)[0;0m ERROR 12-05 16:43:46 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4071027)[0;0m ERROR 12-05 16:43:46 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 16:44:07 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 16:44:07 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 16:44:07 [model.py:1745] Using max model len 131072
INFO 12-05 16:44:07 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4071114)[0;0m INFO 12-05 16:44:15 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4071114)[0;0m INFO 12-05 16:44:16 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:41003 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4071114)[0;0m INFO 12-05 16:44:16 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4071114)[0;0m INFO 12-05 16:44:16 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4071114)[0;0m INFO 12-05 16:44:17 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4071114)[0;0m INFO 12-05 16:44:17 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4071114)[0;0m INFO 12-05 16:44:18 [default_loader.py:314] Loading weights took 0.88 seconds
[1;36m(EngineCore_DP0 pid=4071114)[0;0m INFO 12-05 16:44:18 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.398624 seconds
[1;36m(EngineCore_DP0 pid=4071114)[0;0m INFO 12-05 16:44:22 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4071114)[0;0m INFO 12-05 16:44:22 [backends.py:647] Dynamo bytecode transform time: 3.93 s
[1;36m(EngineCore_DP0 pid=4071114)[0;0m ERROR 12-05 16:44:23 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4071114)[0;0m ERROR 12-05 16:44:23 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4071114)[0;0m ERROR 12-05 16:44:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4071114)[0;0m ERROR 12-05 16:44:23 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4071114)[0;0m ERROR 12-05 16:44:23 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071114)[0;0m ERROR 12-05 16:44:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4071114)[0;0m ERROR 12-05 16:44:23 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4071114)[0;0m ERROR 12-05 16:44:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4071114)[0;0m ERROR 12-05 16:44:23 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4071114)[0;0m ERROR 12-05 16:44:23 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071114)[0;0m ERROR 12-05 16:44:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4071114)[0;0m ERROR 12-05 16:44:23 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4071114)[0;0m ERROR 12-05 16:44:23 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071114)[0;0m ERROR 12-05 16:44:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4071114)[0;0m ERROR 12-05 16:44:23 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4071114)[0;0m ERROR 12-05 16:44:23 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071114)[0;0m ERROR 12-05 16:44:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4071114)[0;0m ERROR 12-05 16:44:23 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4071114)[0;0m ERROR 12-05 16:44:23 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071114)[0;0m ERROR 12-05 16:44:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4071114)[0;0m ERROR 12-05 16:44:23 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4071114)[0;0m ERROR 12-05 16:44:23 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071114)[0;0m ERROR 12-05 16:44:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4071114)[0;0m ERROR 12-05 16:44:23 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4071114)[0;0m ERROR 12-05 16:44:23 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071114)[0;0m ERROR 12-05 16:44:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4071114)[0;0m ERROR 12-05 16:44:23 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4071114)[0;0m ERROR 12-05 16:44:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4071114)[0;0m ERROR 12-05 16:44:23 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4071114)[0;0m ERROR 12-05 16:44:23 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071114)[0;0m ERROR 12-05 16:44:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4071114)[0;0m ERROR 12-05 16:44:23 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4071114)[0;0m ERROR 12-05 16:44:23 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071114)[0;0m ERROR 12-05 16:44:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4071114)[0;0m ERROR 12-05 16:44:23 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4071114)[0;0m ERROR 12-05 16:44:23 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071114)[0;0m ERROR 12-05 16:44:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4071114)[0;0m ERROR 12-05 16:44:23 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4071114)[0;0m ERROR 12-05 16:44:23 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071114)[0;0m ERROR 12-05 16:44:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4071114)[0;0m ERROR 12-05 16:44:23 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4071114)[0;0m ERROR 12-05 16:44:23 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071114)[0;0m ERROR 12-05 16:44:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4071114)[0;0m ERROR 12-05 16:44:23 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4071114)[0;0m ERROR 12-05 16:44:23 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071114)[0;0m ERROR 12-05 16:44:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4071114)[0;0m ERROR 12-05 16:44:23 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4071114)[0;0m ERROR 12-05 16:44:23 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071114)[0;0m ERROR 12-05 16:44:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4071114)[0;0m ERROR 12-05 16:44:23 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4071114)[0;0m ERROR 12-05 16:44:23 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071114)[0;0m ERROR 12-05 16:44:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4071114)[0;0m ERROR 12-05 16:44:23 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4071114)[0;0m ERROR 12-05 16:44:23 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071114)[0;0m ERROR 12-05 16:44:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4071114)[0;0m ERROR 12-05 16:44:23 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4071114)[0;0m ERROR 12-05 16:44:23 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071114)[0;0m ERROR 12-05 16:44:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4071114)[0;0m ERROR 12-05 16:44:23 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4071114)[0;0m ERROR 12-05 16:44:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4071114)[0;0m ERROR 12-05 16:44:23 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4071114)[0;0m ERROR 12-05 16:44:23 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071114)[0;0m ERROR 12-05 16:44:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4071114)[0;0m ERROR 12-05 16:44:23 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4071114)[0;0m ERROR 12-05 16:44:23 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071114)[0;0m ERROR 12-05 16:44:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4071114)[0;0m ERROR 12-05 16:44:23 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4071114)[0;0m ERROR 12-05 16:44:23 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071114)[0;0m ERROR 12-05 16:44:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4071114)[0;0m ERROR 12-05 16:44:23 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4071114)[0;0m ERROR 12-05 16:44:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4071114)[0;0m ERROR 12-05 16:44:23 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4071114)[0;0m ERROR 12-05 16:44:23 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071114)[0;0m ERROR 12-05 16:44:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4071114)[0;0m ERROR 12-05 16:44:23 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4071114)[0;0m ERROR 12-05 16:44:23 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071114)[0;0m ERROR 12-05 16:44:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4071114)[0;0m ERROR 12-05 16:44:23 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4071114)[0;0m ERROR 12-05 16:44:23 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071114)[0;0m ERROR 12-05 16:44:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4071114)[0;0m ERROR 12-05 16:44:23 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4071114)[0;0m ERROR 12-05 16:44:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4071114)[0;0m ERROR 12-05 16:44:23 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4071114)[0;0m ERROR 12-05 16:44:23 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071114)[0;0m ERROR 12-05 16:44:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4071114)[0;0m ERROR 12-05 16:44:23 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4071114)[0;0m ERROR 12-05 16:44:23 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071114)[0;0m ERROR 12-05 16:44:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4071114)[0;0m ERROR 12-05 16:44:23 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4071114)[0;0m ERROR 12-05 16:44:23 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071114)[0;0m ERROR 12-05 16:44:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4071114)[0;0m ERROR 12-05 16:44:23 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4071114)[0;0m ERROR 12-05 16:44:23 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071114)[0;0m ERROR 12-05 16:44:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4071114)[0;0m ERROR 12-05 16:44:23 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4071114)[0;0m ERROR 12-05 16:44:23 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7fc17d9aeb10>' raised:
[1;36m(EngineCore_DP0 pid=4071114)[0;0m ERROR 12-05 16:44:23 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4071114)[0;0m ERROR 12-05 16:44:23 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4071114)[0;0m ERROR 12-05 16:44:23 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4071114)[0;0m ERROR 12-05 16:44:23 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 16:44:44 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 16:44:44 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 16:44:44 [model.py:1745] Using max model len 131072
INFO 12-05 16:44:44 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4071199)[0;0m INFO 12-05 16:44:53 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4071199)[0;0m INFO 12-05 16:44:54 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:47055 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4071199)[0;0m INFO 12-05 16:44:54 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4071199)[0;0m INFO 12-05 16:44:54 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4071199)[0;0m INFO 12-05 16:44:55 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4071199)[0;0m INFO 12-05 16:44:55 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4071199)[0;0m INFO 12-05 16:44:56 [default_loader.py:314] Loading weights took 0.88 seconds
[1;36m(EngineCore_DP0 pid=4071199)[0;0m INFO 12-05 16:44:57 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.516358 seconds
[1;36m(EngineCore_DP0 pid=4071199)[0;0m INFO 12-05 16:45:01 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4071199)[0;0m INFO 12-05 16:45:01 [backends.py:647] Dynamo bytecode transform time: 4.49 s
[1;36m(EngineCore_DP0 pid=4071199)[0;0m ERROR 12-05 16:45:02 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4071199)[0;0m ERROR 12-05 16:45:02 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4071199)[0;0m ERROR 12-05 16:45:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4071199)[0;0m ERROR 12-05 16:45:02 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4071199)[0;0m ERROR 12-05 16:45:02 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071199)[0;0m ERROR 12-05 16:45:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4071199)[0;0m ERROR 12-05 16:45:02 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4071199)[0;0m ERROR 12-05 16:45:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4071199)[0;0m ERROR 12-05 16:45:02 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4071199)[0;0m ERROR 12-05 16:45:02 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071199)[0;0m ERROR 12-05 16:45:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4071199)[0;0m ERROR 12-05 16:45:02 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4071199)[0;0m ERROR 12-05 16:45:02 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071199)[0;0m ERROR 12-05 16:45:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4071199)[0;0m ERROR 12-05 16:45:02 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4071199)[0;0m ERROR 12-05 16:45:02 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071199)[0;0m ERROR 12-05 16:45:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4071199)[0;0m ERROR 12-05 16:45:02 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4071199)[0;0m ERROR 12-05 16:45:02 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071199)[0;0m ERROR 12-05 16:45:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4071199)[0;0m ERROR 12-05 16:45:02 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4071199)[0;0m ERROR 12-05 16:45:02 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071199)[0;0m ERROR 12-05 16:45:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4071199)[0;0m ERROR 12-05 16:45:02 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4071199)[0;0m ERROR 12-05 16:45:02 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071199)[0;0m ERROR 12-05 16:45:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4071199)[0;0m ERROR 12-05 16:45:02 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4071199)[0;0m ERROR 12-05 16:45:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4071199)[0;0m ERROR 12-05 16:45:02 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4071199)[0;0m ERROR 12-05 16:45:02 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071199)[0;0m ERROR 12-05 16:45:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4071199)[0;0m ERROR 12-05 16:45:02 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4071199)[0;0m ERROR 12-05 16:45:02 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071199)[0;0m ERROR 12-05 16:45:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4071199)[0;0m ERROR 12-05 16:45:02 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4071199)[0;0m ERROR 12-05 16:45:02 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071199)[0;0m ERROR 12-05 16:45:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4071199)[0;0m ERROR 12-05 16:45:02 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4071199)[0;0m ERROR 12-05 16:45:02 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071199)[0;0m ERROR 12-05 16:45:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4071199)[0;0m ERROR 12-05 16:45:02 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4071199)[0;0m ERROR 12-05 16:45:02 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071199)[0;0m ERROR 12-05 16:45:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4071199)[0;0m ERROR 12-05 16:45:02 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4071199)[0;0m ERROR 12-05 16:45:02 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071199)[0;0m ERROR 12-05 16:45:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4071199)[0;0m ERROR 12-05 16:45:02 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4071199)[0;0m ERROR 12-05 16:45:02 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071199)[0;0m ERROR 12-05 16:45:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4071199)[0;0m ERROR 12-05 16:45:02 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4071199)[0;0m ERROR 12-05 16:45:02 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071199)[0;0m ERROR 12-05 16:45:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4071199)[0;0m ERROR 12-05 16:45:02 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4071199)[0;0m ERROR 12-05 16:45:02 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071199)[0;0m ERROR 12-05 16:45:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4071199)[0;0m ERROR 12-05 16:45:02 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4071199)[0;0m ERROR 12-05 16:45:02 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071199)[0;0m ERROR 12-05 16:45:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4071199)[0;0m ERROR 12-05 16:45:02 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4071199)[0;0m ERROR 12-05 16:45:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4071199)[0;0m ERROR 12-05 16:45:02 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4071199)[0;0m ERROR 12-05 16:45:02 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071199)[0;0m ERROR 12-05 16:45:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4071199)[0;0m ERROR 12-05 16:45:02 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4071199)[0;0m ERROR 12-05 16:45:02 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071199)[0;0m ERROR 12-05 16:45:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4071199)[0;0m ERROR 12-05 16:45:02 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4071199)[0;0m ERROR 12-05 16:45:02 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071199)[0;0m ERROR 12-05 16:45:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4071199)[0;0m ERROR 12-05 16:45:02 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4071199)[0;0m ERROR 12-05 16:45:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4071199)[0;0m ERROR 12-05 16:45:02 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4071199)[0;0m ERROR 12-05 16:45:02 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071199)[0;0m ERROR 12-05 16:45:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4071199)[0;0m ERROR 12-05 16:45:02 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4071199)[0;0m ERROR 12-05 16:45:02 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071199)[0;0m ERROR 12-05 16:45:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4071199)[0;0m ERROR 12-05 16:45:02 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4071199)[0;0m ERROR 12-05 16:45:02 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071199)[0;0m ERROR 12-05 16:45:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4071199)[0;0m ERROR 12-05 16:45:02 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4071199)[0;0m ERROR 12-05 16:45:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4071199)[0;0m ERROR 12-05 16:45:02 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4071199)[0;0m ERROR 12-05 16:45:02 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071199)[0;0m ERROR 12-05 16:45:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4071199)[0;0m ERROR 12-05 16:45:02 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4071199)[0;0m ERROR 12-05 16:45:02 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071199)[0;0m ERROR 12-05 16:45:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4071199)[0;0m ERROR 12-05 16:45:02 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4071199)[0;0m ERROR 12-05 16:45:02 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071199)[0;0m ERROR 12-05 16:45:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4071199)[0;0m ERROR 12-05 16:45:02 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4071199)[0;0m ERROR 12-05 16:45:02 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071199)[0;0m ERROR 12-05 16:45:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4071199)[0;0m ERROR 12-05 16:45:02 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4071199)[0;0m ERROR 12-05 16:45:02 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7f6a923bc920>' raised:
[1;36m(EngineCore_DP0 pid=4071199)[0;0m ERROR 12-05 16:45:02 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4071199)[0;0m ERROR 12-05 16:45:02 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4071199)[0;0m ERROR 12-05 16:45:02 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4071199)[0;0m ERROR 12-05 16:45:02 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 16:45:23 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 16:45:23 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 16:45:23 [model.py:1745] Using max model len 131072
INFO 12-05 16:45:23 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4071285)[0;0m INFO 12-05 16:45:31 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4071285)[0;0m INFO 12-05 16:45:31 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:50969 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4071285)[0;0m INFO 12-05 16:45:31 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4071285)[0;0m INFO 12-05 16:45:32 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4071285)[0;0m INFO 12-05 16:45:32 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4071285)[0;0m INFO 12-05 16:45:32 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4071285)[0;0m INFO 12-05 16:45:33 [default_loader.py:314] Loading weights took 0.87 seconds
[1;36m(EngineCore_DP0 pid=4071285)[0;0m INFO 12-05 16:45:34 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.397216 seconds
[1;36m(EngineCore_DP0 pid=4071285)[0;0m INFO 12-05 16:45:38 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4071285)[0;0m INFO 12-05 16:45:38 [backends.py:647] Dynamo bytecode transform time: 3.76 s
[1;36m(EngineCore_DP0 pid=4071285)[0;0m ERROR 12-05 16:45:38 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4071285)[0;0m ERROR 12-05 16:45:38 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4071285)[0;0m ERROR 12-05 16:45:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4071285)[0;0m ERROR 12-05 16:45:38 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4071285)[0;0m ERROR 12-05 16:45:38 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071285)[0;0m ERROR 12-05 16:45:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4071285)[0;0m ERROR 12-05 16:45:38 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4071285)[0;0m ERROR 12-05 16:45:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4071285)[0;0m ERROR 12-05 16:45:38 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4071285)[0;0m ERROR 12-05 16:45:38 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071285)[0;0m ERROR 12-05 16:45:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4071285)[0;0m ERROR 12-05 16:45:38 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4071285)[0;0m ERROR 12-05 16:45:38 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071285)[0;0m ERROR 12-05 16:45:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4071285)[0;0m ERROR 12-05 16:45:38 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4071285)[0;0m ERROR 12-05 16:45:38 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071285)[0;0m ERROR 12-05 16:45:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4071285)[0;0m ERROR 12-05 16:45:38 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4071285)[0;0m ERROR 12-05 16:45:38 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071285)[0;0m ERROR 12-05 16:45:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4071285)[0;0m ERROR 12-05 16:45:38 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4071285)[0;0m ERROR 12-05 16:45:38 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071285)[0;0m ERROR 12-05 16:45:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4071285)[0;0m ERROR 12-05 16:45:38 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4071285)[0;0m ERROR 12-05 16:45:38 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071285)[0;0m ERROR 12-05 16:45:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4071285)[0;0m ERROR 12-05 16:45:38 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4071285)[0;0m ERROR 12-05 16:45:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4071285)[0;0m ERROR 12-05 16:45:38 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4071285)[0;0m ERROR 12-05 16:45:38 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071285)[0;0m ERROR 12-05 16:45:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4071285)[0;0m ERROR 12-05 16:45:38 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4071285)[0;0m ERROR 12-05 16:45:38 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071285)[0;0m ERROR 12-05 16:45:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4071285)[0;0m ERROR 12-05 16:45:38 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4071285)[0;0m ERROR 12-05 16:45:38 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071285)[0;0m ERROR 12-05 16:45:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4071285)[0;0m ERROR 12-05 16:45:38 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4071285)[0;0m ERROR 12-05 16:45:38 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071285)[0;0m ERROR 12-05 16:45:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4071285)[0;0m ERROR 12-05 16:45:38 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4071285)[0;0m ERROR 12-05 16:45:38 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071285)[0;0m ERROR 12-05 16:45:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4071285)[0;0m ERROR 12-05 16:45:38 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4071285)[0;0m ERROR 12-05 16:45:38 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071285)[0;0m ERROR 12-05 16:45:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4071285)[0;0m ERROR 12-05 16:45:38 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4071285)[0;0m ERROR 12-05 16:45:38 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071285)[0;0m ERROR 12-05 16:45:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4071285)[0;0m ERROR 12-05 16:45:38 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4071285)[0;0m ERROR 12-05 16:45:38 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071285)[0;0m ERROR 12-05 16:45:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4071285)[0;0m ERROR 12-05 16:45:38 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4071285)[0;0m ERROR 12-05 16:45:38 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071285)[0;0m ERROR 12-05 16:45:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4071285)[0;0m ERROR 12-05 16:45:38 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4071285)[0;0m ERROR 12-05 16:45:38 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071285)[0;0m ERROR 12-05 16:45:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4071285)[0;0m ERROR 12-05 16:45:38 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4071285)[0;0m ERROR 12-05 16:45:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4071285)[0;0m ERROR 12-05 16:45:38 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4071285)[0;0m ERROR 12-05 16:45:38 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071285)[0;0m ERROR 12-05 16:45:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4071285)[0;0m ERROR 12-05 16:45:38 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4071285)[0;0m ERROR 12-05 16:45:38 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071285)[0;0m ERROR 12-05 16:45:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4071285)[0;0m ERROR 12-05 16:45:38 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4071285)[0;0m ERROR 12-05 16:45:38 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071285)[0;0m ERROR 12-05 16:45:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4071285)[0;0m ERROR 12-05 16:45:38 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4071285)[0;0m ERROR 12-05 16:45:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4071285)[0;0m ERROR 12-05 16:45:38 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4071285)[0;0m ERROR 12-05 16:45:38 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071285)[0;0m ERROR 12-05 16:45:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4071285)[0;0m ERROR 12-05 16:45:38 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4071285)[0;0m ERROR 12-05 16:45:38 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071285)[0;0m ERROR 12-05 16:45:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4071285)[0;0m ERROR 12-05 16:45:38 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4071285)[0;0m ERROR 12-05 16:45:38 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071285)[0;0m ERROR 12-05 16:45:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4071285)[0;0m ERROR 12-05 16:45:38 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4071285)[0;0m ERROR 12-05 16:45:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4071285)[0;0m ERROR 12-05 16:45:38 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4071285)[0;0m ERROR 12-05 16:45:38 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071285)[0;0m ERROR 12-05 16:45:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4071285)[0;0m ERROR 12-05 16:45:38 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4071285)[0;0m ERROR 12-05 16:45:38 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071285)[0;0m ERROR 12-05 16:45:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4071285)[0;0m ERROR 12-05 16:45:38 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4071285)[0;0m ERROR 12-05 16:45:38 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071285)[0;0m ERROR 12-05 16:45:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4071285)[0;0m ERROR 12-05 16:45:38 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4071285)[0;0m ERROR 12-05 16:45:38 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071285)[0;0m ERROR 12-05 16:45:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4071285)[0;0m ERROR 12-05 16:45:38 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4071285)[0;0m ERROR 12-05 16:45:38 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7f22e9f9fd10>' raised:
[1;36m(EngineCore_DP0 pid=4071285)[0;0m ERROR 12-05 16:45:38 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4071285)[0;0m ERROR 12-05 16:45:38 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4071285)[0;0m ERROR 12-05 16:45:38 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4071285)[0;0m ERROR 12-05 16:45:38 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 16:45:59 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 16:46:00 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 16:46:00 [model.py:1745] Using max model len 131072
INFO 12-05 16:46:00 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4071372)[0;0m INFO 12-05 16:46:09 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4071372)[0;0m INFO 12-05 16:46:10 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:45367 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4071372)[0;0m INFO 12-05 16:46:10 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4071372)[0;0m INFO 12-05 16:46:10 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4071372)[0;0m INFO 12-05 16:46:10 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4071372)[0;0m INFO 12-05 16:46:10 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4071372)[0;0m INFO 12-05 16:46:12 [default_loader.py:314] Loading weights took 0.88 seconds
[1;36m(EngineCore_DP0 pid=4071372)[0;0m INFO 12-05 16:46:12 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.449142 seconds
[1;36m(EngineCore_DP0 pid=4071372)[0;0m INFO 12-05 16:46:16 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4071372)[0;0m INFO 12-05 16:46:16 [backends.py:647] Dynamo bytecode transform time: 3.85 s
[1;36m(EngineCore_DP0 pid=4071372)[0;0m ERROR 12-05 16:46:16 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4071372)[0;0m ERROR 12-05 16:46:16 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4071372)[0;0m ERROR 12-05 16:46:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4071372)[0;0m ERROR 12-05 16:46:16 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4071372)[0;0m ERROR 12-05 16:46:16 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071372)[0;0m ERROR 12-05 16:46:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4071372)[0;0m ERROR 12-05 16:46:16 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4071372)[0;0m ERROR 12-05 16:46:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4071372)[0;0m ERROR 12-05 16:46:16 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4071372)[0;0m ERROR 12-05 16:46:16 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071372)[0;0m ERROR 12-05 16:46:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4071372)[0;0m ERROR 12-05 16:46:16 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4071372)[0;0m ERROR 12-05 16:46:16 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071372)[0;0m ERROR 12-05 16:46:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4071372)[0;0m ERROR 12-05 16:46:16 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4071372)[0;0m ERROR 12-05 16:46:16 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071372)[0;0m ERROR 12-05 16:46:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4071372)[0;0m ERROR 12-05 16:46:16 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4071372)[0;0m ERROR 12-05 16:46:16 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071372)[0;0m ERROR 12-05 16:46:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4071372)[0;0m ERROR 12-05 16:46:16 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4071372)[0;0m ERROR 12-05 16:46:16 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071372)[0;0m ERROR 12-05 16:46:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4071372)[0;0m ERROR 12-05 16:46:16 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4071372)[0;0m ERROR 12-05 16:46:16 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071372)[0;0m ERROR 12-05 16:46:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4071372)[0;0m ERROR 12-05 16:46:16 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4071372)[0;0m ERROR 12-05 16:46:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4071372)[0;0m ERROR 12-05 16:46:16 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4071372)[0;0m ERROR 12-05 16:46:16 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071372)[0;0m ERROR 12-05 16:46:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4071372)[0;0m ERROR 12-05 16:46:16 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4071372)[0;0m ERROR 12-05 16:46:16 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071372)[0;0m ERROR 12-05 16:46:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4071372)[0;0m ERROR 12-05 16:46:16 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4071372)[0;0m ERROR 12-05 16:46:16 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071372)[0;0m ERROR 12-05 16:46:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4071372)[0;0m ERROR 12-05 16:46:16 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4071372)[0;0m ERROR 12-05 16:46:16 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071372)[0;0m ERROR 12-05 16:46:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4071372)[0;0m ERROR 12-05 16:46:16 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4071372)[0;0m ERROR 12-05 16:46:16 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071372)[0;0m ERROR 12-05 16:46:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4071372)[0;0m ERROR 12-05 16:46:16 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4071372)[0;0m ERROR 12-05 16:46:16 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071372)[0;0m ERROR 12-05 16:46:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4071372)[0;0m ERROR 12-05 16:46:16 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4071372)[0;0m ERROR 12-05 16:46:16 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071372)[0;0m ERROR 12-05 16:46:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4071372)[0;0m ERROR 12-05 16:46:16 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4071372)[0;0m ERROR 12-05 16:46:16 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071372)[0;0m ERROR 12-05 16:46:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4071372)[0;0m ERROR 12-05 16:46:16 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4071372)[0;0m ERROR 12-05 16:46:16 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071372)[0;0m ERROR 12-05 16:46:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4071372)[0;0m ERROR 12-05 16:46:16 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4071372)[0;0m ERROR 12-05 16:46:16 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071372)[0;0m ERROR 12-05 16:46:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4071372)[0;0m ERROR 12-05 16:46:16 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4071372)[0;0m ERROR 12-05 16:46:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4071372)[0;0m ERROR 12-05 16:46:16 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4071372)[0;0m ERROR 12-05 16:46:16 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071372)[0;0m ERROR 12-05 16:46:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4071372)[0;0m ERROR 12-05 16:46:16 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4071372)[0;0m ERROR 12-05 16:46:16 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071372)[0;0m ERROR 12-05 16:46:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4071372)[0;0m ERROR 12-05 16:46:16 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4071372)[0;0m ERROR 12-05 16:46:16 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071372)[0;0m ERROR 12-05 16:46:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4071372)[0;0m ERROR 12-05 16:46:16 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4071372)[0;0m ERROR 12-05 16:46:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4071372)[0;0m ERROR 12-05 16:46:16 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4071372)[0;0m ERROR 12-05 16:46:16 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071372)[0;0m ERROR 12-05 16:46:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4071372)[0;0m ERROR 12-05 16:46:16 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4071372)[0;0m ERROR 12-05 16:46:16 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071372)[0;0m ERROR 12-05 16:46:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4071372)[0;0m ERROR 12-05 16:46:16 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4071372)[0;0m ERROR 12-05 16:46:16 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071372)[0;0m ERROR 12-05 16:46:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4071372)[0;0m ERROR 12-05 16:46:16 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4071372)[0;0m ERROR 12-05 16:46:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4071372)[0;0m ERROR 12-05 16:46:16 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4071372)[0;0m ERROR 12-05 16:46:16 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071372)[0;0m ERROR 12-05 16:46:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4071372)[0;0m ERROR 12-05 16:46:16 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4071372)[0;0m ERROR 12-05 16:46:16 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071372)[0;0m ERROR 12-05 16:46:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4071372)[0;0m ERROR 12-05 16:46:16 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4071372)[0;0m ERROR 12-05 16:46:16 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071372)[0;0m ERROR 12-05 16:46:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4071372)[0;0m ERROR 12-05 16:46:16 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4071372)[0;0m ERROR 12-05 16:46:16 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071372)[0;0m ERROR 12-05 16:46:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4071372)[0;0m ERROR 12-05 16:46:16 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4071372)[0;0m ERROR 12-05 16:46:16 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7fc461c36e10>' raised:
[1;36m(EngineCore_DP0 pid=4071372)[0;0m ERROR 12-05 16:46:16 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4071372)[0;0m ERROR 12-05 16:46:16 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4071372)[0;0m ERROR 12-05 16:46:16 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4071372)[0;0m ERROR 12-05 16:46:16 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 16:46:38 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 16:46:38 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 16:46:38 [model.py:1745] Using max model len 131072
INFO 12-05 16:46:38 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4071549)[0;0m INFO 12-05 16:46:45 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4071549)[0;0m INFO 12-05 16:46:46 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:45779 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4071549)[0;0m INFO 12-05 16:46:46 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4071549)[0;0m INFO 12-05 16:46:46 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4071549)[0;0m INFO 12-05 16:46:47 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4071549)[0;0m INFO 12-05 16:46:47 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4071549)[0;0m INFO 12-05 16:46:48 [default_loader.py:314] Loading weights took 0.87 seconds
[1;36m(EngineCore_DP0 pid=4071549)[0;0m INFO 12-05 16:46:48 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.357756 seconds
[1;36m(EngineCore_DP0 pid=4071549)[0;0m INFO 12-05 16:46:52 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4071549)[0;0m INFO 12-05 16:46:52 [backends.py:647] Dynamo bytecode transform time: 3.79 s
[1;36m(EngineCore_DP0 pid=4071549)[0;0m ERROR 12-05 16:46:53 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4071549)[0;0m ERROR 12-05 16:46:53 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4071549)[0;0m ERROR 12-05 16:46:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4071549)[0;0m ERROR 12-05 16:46:53 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4071549)[0;0m ERROR 12-05 16:46:53 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071549)[0;0m ERROR 12-05 16:46:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4071549)[0;0m ERROR 12-05 16:46:53 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4071549)[0;0m ERROR 12-05 16:46:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4071549)[0;0m ERROR 12-05 16:46:53 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4071549)[0;0m ERROR 12-05 16:46:53 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071549)[0;0m ERROR 12-05 16:46:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4071549)[0;0m ERROR 12-05 16:46:53 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4071549)[0;0m ERROR 12-05 16:46:53 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071549)[0;0m ERROR 12-05 16:46:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4071549)[0;0m ERROR 12-05 16:46:53 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4071549)[0;0m ERROR 12-05 16:46:53 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071549)[0;0m ERROR 12-05 16:46:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4071549)[0;0m ERROR 12-05 16:46:53 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4071549)[0;0m ERROR 12-05 16:46:53 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071549)[0;0m ERROR 12-05 16:46:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4071549)[0;0m ERROR 12-05 16:46:53 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4071549)[0;0m ERROR 12-05 16:46:53 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071549)[0;0m ERROR 12-05 16:46:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4071549)[0;0m ERROR 12-05 16:46:53 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4071549)[0;0m ERROR 12-05 16:46:53 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071549)[0;0m ERROR 12-05 16:46:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4071549)[0;0m ERROR 12-05 16:46:53 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4071549)[0;0m ERROR 12-05 16:46:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4071549)[0;0m ERROR 12-05 16:46:53 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4071549)[0;0m ERROR 12-05 16:46:53 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071549)[0;0m ERROR 12-05 16:46:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4071549)[0;0m ERROR 12-05 16:46:53 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4071549)[0;0m ERROR 12-05 16:46:53 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071549)[0;0m ERROR 12-05 16:46:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4071549)[0;0m ERROR 12-05 16:46:53 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4071549)[0;0m ERROR 12-05 16:46:53 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071549)[0;0m ERROR 12-05 16:46:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4071549)[0;0m ERROR 12-05 16:46:53 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4071549)[0;0m ERROR 12-05 16:46:53 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071549)[0;0m ERROR 12-05 16:46:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4071549)[0;0m ERROR 12-05 16:46:53 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4071549)[0;0m ERROR 12-05 16:46:53 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071549)[0;0m ERROR 12-05 16:46:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4071549)[0;0m ERROR 12-05 16:46:53 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4071549)[0;0m ERROR 12-05 16:46:53 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071549)[0;0m ERROR 12-05 16:46:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4071549)[0;0m ERROR 12-05 16:46:53 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4071549)[0;0m ERROR 12-05 16:46:53 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071549)[0;0m ERROR 12-05 16:46:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4071549)[0;0m ERROR 12-05 16:46:53 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4071549)[0;0m ERROR 12-05 16:46:53 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071549)[0;0m ERROR 12-05 16:46:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4071549)[0;0m ERROR 12-05 16:46:53 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4071549)[0;0m ERROR 12-05 16:46:53 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071549)[0;0m ERROR 12-05 16:46:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4071549)[0;0m ERROR 12-05 16:46:53 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4071549)[0;0m ERROR 12-05 16:46:53 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071549)[0;0m ERROR 12-05 16:46:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4071549)[0;0m ERROR 12-05 16:46:53 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4071549)[0;0m ERROR 12-05 16:46:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4071549)[0;0m ERROR 12-05 16:46:53 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4071549)[0;0m ERROR 12-05 16:46:53 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071549)[0;0m ERROR 12-05 16:46:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4071549)[0;0m ERROR 12-05 16:46:53 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4071549)[0;0m ERROR 12-05 16:46:53 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071549)[0;0m ERROR 12-05 16:46:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4071549)[0;0m ERROR 12-05 16:46:53 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4071549)[0;0m ERROR 12-05 16:46:53 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071549)[0;0m ERROR 12-05 16:46:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4071549)[0;0m ERROR 12-05 16:46:53 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4071549)[0;0m ERROR 12-05 16:46:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4071549)[0;0m ERROR 12-05 16:46:53 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4071549)[0;0m ERROR 12-05 16:46:53 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071549)[0;0m ERROR 12-05 16:46:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4071549)[0;0m ERROR 12-05 16:46:53 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4071549)[0;0m ERROR 12-05 16:46:53 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071549)[0;0m ERROR 12-05 16:46:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4071549)[0;0m ERROR 12-05 16:46:53 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4071549)[0;0m ERROR 12-05 16:46:53 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071549)[0;0m ERROR 12-05 16:46:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4071549)[0;0m ERROR 12-05 16:46:53 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4071549)[0;0m ERROR 12-05 16:46:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4071549)[0;0m ERROR 12-05 16:46:53 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4071549)[0;0m ERROR 12-05 16:46:53 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071549)[0;0m ERROR 12-05 16:46:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4071549)[0;0m ERROR 12-05 16:46:53 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4071549)[0;0m ERROR 12-05 16:46:53 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071549)[0;0m ERROR 12-05 16:46:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4071549)[0;0m ERROR 12-05 16:46:53 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4071549)[0;0m ERROR 12-05 16:46:53 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071549)[0;0m ERROR 12-05 16:46:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4071549)[0;0m ERROR 12-05 16:46:53 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4071549)[0;0m ERROR 12-05 16:46:53 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071549)[0;0m ERROR 12-05 16:46:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4071549)[0;0m ERROR 12-05 16:46:53 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4071549)[0;0m ERROR 12-05 16:46:53 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7f66e4b9c830>' raised:
[1;36m(EngineCore_DP0 pid=4071549)[0;0m ERROR 12-05 16:46:53 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4071549)[0;0m ERROR 12-05 16:46:53 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4071549)[0;0m ERROR 12-05 16:46:53 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4071549)[0;0m ERROR 12-05 16:46:53 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 16:47:14 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 16:47:14 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 16:47:14 [model.py:1745] Using max model len 131072
INFO 12-05 16:47:14 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4071637)[0;0m INFO 12-05 16:47:23 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4071637)[0;0m INFO 12-05 16:47:24 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.97:43671 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4071637)[0;0m INFO 12-05 16:47:24 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4071637)[0;0m INFO 12-05 16:47:24 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
[1;36m(EngineCore_DP0 pid=4071637)[0;0m INFO 12-05 16:47:24 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4071637)[0;0m INFO 12-05 16:47:24 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=4071637)[0;0m INFO 12-05 16:47:26 [default_loader.py:314] Loading weights took 0.88 seconds
[1;36m(EngineCore_DP0 pid=4071637)[0;0m INFO 12-05 16:47:26 [gpu_model_runner.py:3338] Model loading took 6.0160 GiB memory and 1.427101 seconds
[1;36m(EngineCore_DP0 pid=4071637)[0;0m INFO 12-05 16:47:30 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/11a98d8a81/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4071637)[0;0m INFO 12-05 16:47:30 [backends.py:647] Dynamo bytecode transform time: 3.87 s
[1;36m(EngineCore_DP0 pid=4071637)[0;0m ERROR 12-05 16:47:30 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4071637)[0;0m ERROR 12-05 16:47:30 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4071637)[0;0m ERROR 12-05 16:47:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4071637)[0;0m ERROR 12-05 16:47:30 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4071637)[0;0m ERROR 12-05 16:47:30 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071637)[0;0m ERROR 12-05 16:47:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4071637)[0;0m ERROR 12-05 16:47:30 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4071637)[0;0m ERROR 12-05 16:47:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4071637)[0;0m ERROR 12-05 16:47:30 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4071637)[0;0m ERROR 12-05 16:47:30 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071637)[0;0m ERROR 12-05 16:47:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4071637)[0;0m ERROR 12-05 16:47:30 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=4071637)[0;0m ERROR 12-05 16:47:30 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071637)[0;0m ERROR 12-05 16:47:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4071637)[0;0m ERROR 12-05 16:47:30 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=4071637)[0;0m ERROR 12-05 16:47:30 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071637)[0;0m ERROR 12-05 16:47:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4071637)[0;0m ERROR 12-05 16:47:30 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4071637)[0;0m ERROR 12-05 16:47:30 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071637)[0;0m ERROR 12-05 16:47:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4071637)[0;0m ERROR 12-05 16:47:30 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4071637)[0;0m ERROR 12-05 16:47:30 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071637)[0;0m ERROR 12-05 16:47:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4071637)[0;0m ERROR 12-05 16:47:30 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4071637)[0;0m ERROR 12-05 16:47:30 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071637)[0;0m ERROR 12-05 16:47:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=4071637)[0;0m ERROR 12-05 16:47:30 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=4071637)[0;0m ERROR 12-05 16:47:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=4071637)[0;0m ERROR 12-05 16:47:30 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=4071637)[0;0m ERROR 12-05 16:47:30 [core.py:842]                                         ^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071637)[0;0m ERROR 12-05 16:47:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4071637)[0;0m ERROR 12-05 16:47:30 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4071637)[0;0m ERROR 12-05 16:47:30 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071637)[0;0m ERROR 12-05 16:47:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=4071637)[0;0m ERROR 12-05 16:47:30 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=4071637)[0;0m ERROR 12-05 16:47:30 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071637)[0;0m ERROR 12-05 16:47:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=4071637)[0;0m ERROR 12-05 16:47:30 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4071637)[0;0m ERROR 12-05 16:47:30 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071637)[0;0m ERROR 12-05 16:47:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=4071637)[0;0m ERROR 12-05 16:47:30 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4071637)[0;0m ERROR 12-05 16:47:30 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071637)[0;0m ERROR 12-05 16:47:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=4071637)[0;0m ERROR 12-05 16:47:30 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4071637)[0;0m ERROR 12-05 16:47:30 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071637)[0;0m ERROR 12-05 16:47:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=4071637)[0;0m ERROR 12-05 16:47:30 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=4071637)[0;0m ERROR 12-05 16:47:30 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071637)[0;0m ERROR 12-05 16:47:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 471, in __call__
[1;36m(EngineCore_DP0 pid=4071637)[0;0m ERROR 12-05 16:47:30 [core.py:842]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=4071637)[0;0m ERROR 12-05 16:47:30 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071637)[0;0m ERROR 12-05 16:47:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 149, in __call__
[1;36m(EngineCore_DP0 pid=4071637)[0;0m ERROR 12-05 16:47:30 [core.py:842]     return self._compiled_callable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4071637)[0;0m ERROR 12-05 16:47:30 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071637)[0;0m ERROR 12-05 16:47:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[1;36m(EngineCore_DP0 pid=4071637)[0;0m ERROR 12-05 16:47:30 [core.py:842]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[1;36m(EngineCore_DP0 pid=4071637)[0;0m ERROR 12-05 16:47:30 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071637)[0;0m ERROR 12-05 16:47:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2196, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4071637)[0;0m ERROR 12-05 16:47:30 [core.py:842]     raise BackendCompilerFailed(
[1;36m(EngineCore_DP0 pid=4071637)[0;0m ERROR 12-05 16:47:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 2171, in _call_user_compiler
[1;36m(EngineCore_DP0 pid=4071637)[0;0m ERROR 12-05 16:47:30 [core.py:842]     compiled_fn = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4071637)[0;0m ERROR 12-05 16:47:30 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071637)[0;0m ERROR 12-05 16:47:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 156, in __call__
[1;36m(EngineCore_DP0 pid=4071637)[0;0m ERROR 12-05 16:47:30 [core.py:842]     compiled_gm = compiler_fn(gm, example_inputs)
[1;36m(EngineCore_DP0 pid=4071637)[0;0m ERROR 12-05 16:47:30 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071637)[0;0m ERROR 12-05 16:47:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/__init__.py", line 2437, in __call__
[1;36m(EngineCore_DP0 pid=4071637)[0;0m ERROR 12-05 16:47:30 [core.py:842]     return self.compiler_fn(model_, inputs_, **self.kwargs)
[1;36m(EngineCore_DP0 pid=4071637)[0;0m ERROR 12-05 16:47:30 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071637)[0;0m ERROR 12-05 16:47:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 683, in __call__
[1;36m(EngineCore_DP0 pid=4071637)[0;0m ERROR 12-05 16:47:30 [core.py:842]     PiecewiseCompileInterpreter(
[1;36m(EngineCore_DP0 pid=4071637)[0;0m ERROR 12-05 16:47:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 404, in run
[1;36m(EngineCore_DP0 pid=4071637)[0;0m ERROR 12-05 16:47:30 [core.py:842]     return super().run(*fake_args)
[1;36m(EngineCore_DP0 pid=4071637)[0;0m ERROR 12-05 16:47:30 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071637)[0;0m ERROR 12-05 16:47:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 174, in run
[1;36m(EngineCore_DP0 pid=4071637)[0;0m ERROR 12-05 16:47:30 [core.py:842]     self.env[node] = self.run_node(node)
[1;36m(EngineCore_DP0 pid=4071637)[0;0m ERROR 12-05 16:47:30 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071637)[0;0m ERROR 12-05 16:47:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/interpreter.py", line 256, in run_node
[1;36m(EngineCore_DP0 pid=4071637)[0;0m ERROR 12-05 16:47:30 [core.py:842]     return getattr(self, n.op)(n.target, args, kwargs)
[1;36m(EngineCore_DP0 pid=4071637)[0;0m ERROR 12-05 16:47:30 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071637)[0;0m ERROR 12-05 16:47:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 424, in call_module
[1;36m(EngineCore_DP0 pid=4071637)[0;0m ERROR 12-05 16:47:30 [core.py:842]     self.vllm_backend.compiler_manager.compile(
[1;36m(EngineCore_DP0 pid=4071637)[0;0m ERROR 12-05 16:47:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 201, in compile
[1;36m(EngineCore_DP0 pid=4071637)[0;0m ERROR 12-05 16:47:30 [core.py:842]     compiled_graph = self.load(graph, example_inputs, graph_index, runtime_shape)
[1;36m(EngineCore_DP0 pid=4071637)[0;0m ERROR 12-05 16:47:30 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071637)[0;0m ERROR 12-05 16:47:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/backends.py", line 161, in load
[1;36m(EngineCore_DP0 pid=4071637)[0;0m ERROR 12-05 16:47:30 [core.py:842]     compiled_graph = self.compiler.load(
[1;36m(EngineCore_DP0 pid=4071637)[0;0m ERROR 12-05 16:47:30 [core.py:842]                      ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071637)[0;0m ERROR 12-05 16:47:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 260, in load
[1;36m(EngineCore_DP0 pid=4071637)[0;0m ERROR 12-05 16:47:30 [core.py:842]     inductor_compiled_graph = torch._inductor.CompiledArtifact.load(
[1;36m(EngineCore_DP0 pid=4071637)[0;0m ERROR 12-05 16:47:30 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071637)[0;0m ERROR 12-05 16:47:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 131, in load
[1;36m(EngineCore_DP0 pid=4071637)[0;0m ERROR 12-05 16:47:30 [core.py:842]     reader = BytesReader(artifacts)
[1;36m(EngineCore_DP0 pid=4071637)[0;0m ERROR 12-05 16:47:30 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4071637)[0;0m ERROR 12-05 16:47:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_appending_byte_serializer.py", line 55, in __init__
[1;36m(EngineCore_DP0 pid=4071637)[0;0m ERROR 12-05 16:47:30 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=4071637)[0;0m ERROR 12-05 16:47:30 [core.py:842] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7f9ae00eb860>' raised:
[1;36m(EngineCore_DP0 pid=4071637)[0;0m ERROR 12-05 16:47:30 [core.py:842] RuntimeError: Bytes object is corrupted, checksum does not match. Expected: b' \xdc9\x91', Got: b'\x89\xfc\x9d\xa9'
[1;36m(EngineCore_DP0 pid=4071637)[0;0m ERROR 12-05 16:47:30 [core.py:842] 
[1;36m(EngineCore_DP0 pid=4071637)[0;0m ERROR 12-05 16:47:30 [core.py:842] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_DP0 pid=4071637)[0;0m ERROR 12-05 16:47:30 [core.py:842] 
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: meta-llama/Llama-3.2-3B-Instruct (backend=vllm)
[ModelCache] Using max_model_len=131072 for meta-llama/Llama-3.2-3B-Instruct
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-05 16:47:52 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'meta-llama/Llama-3.2-3B-Instruct'}
INFO 12-05 16:47:52 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 12-05 16:47:52 [model.py:1745] Using max model len 131072
INFO 12-05 16:47:52 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
