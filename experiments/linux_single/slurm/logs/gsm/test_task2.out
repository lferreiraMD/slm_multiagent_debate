Using persona diversity with 3 different personas
============================================================
GSM Task - Multiagent Debate
============================================================
Model: Qwen/Qwen3-14B
Persona diversity mode:
  Agent 1: an enigma machine operator whose primary filter is signal-to...
  Agent 2: a Zen master who communicates only through non-sequiturs, ko...
  Agent 3: a deep-sea volcanologist focused on extremes of pressure, he...
Agents: 3
Rounds: 3
Problems: 2
Dataset: /home/ch269957/projects/slm_multiagent_debate/data/gsm8k/test.jsonl
Generation params: {'temperature': 1.0, 'max_tokens': None, 'top_p': 1.0, 'n': 1}
============================================================

--- Problem 1/2, Round 1, Agent 1/3 ---
[ModelCache] Loading model: Qwen/Qwen3-14B (backend=vllm)
[ModelCache] Using max_model_len=40960 for Qwen/Qwen3-14B
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 13:12:04 [utils.py:253] non-default args: {'max_model_len': 40960, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'Qwen/Qwen3-14B'}
Using persona diversity with 3 different personas
============================================================
GSM Task - Multiagent Debate
============================================================
Model: Qwen/Qwen3-14B
Persona diversity mode:
  Agent 1: an enigma machine operator whose primary filter is signal-to...
  Agent 2: a Zen master who communicates only through non-sequiturs, ko...
  Agent 3: a deep-sea volcanologist focused on extremes of pressure, he...
Agents: 3
Rounds: 3
Problems: 2
Dataset: /home/ch269957/projects/slm_multiagent_debate/data/gsm8k/test.jsonl
Generation params: {'temperature': 1.0, 'max_tokens': None, 'top_p': 1.0, 'n': 1}
============================================================

--- Problem 1/2, Round 1, Agent 1/3 ---
[ModelCache] Loading model: Qwen/Qwen3-14B (backend=vllm)
[ModelCache] Using max_model_len=40960 for Qwen/Qwen3-14B
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 13:12:05 [utils.py:253] non-default args: {'max_model_len': 40960, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'Qwen/Qwen3-14B'}
INFO 12-04 13:12:05 [model.py:631] Resolved architecture: Qwen3ForCausalLM
INFO 12-04 13:12:05 [model.py:1745] Using max model len 40960
INFO 12-04 13:12:05 [model.py:631] Resolved architecture: Qwen3ForCausalLM
INFO 12-04 13:12:05 [model.py:1745] Using max model len 40960
INFO 12-04 13:12:06 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
INFO 12-04 13:12:06 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-04 13:12:07 [system_utils.py:103] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: CUDA is initialized
WARNING 12-04 13:12:07 [system_utils.py:103] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: CUDA is initialized
[1;36m(EngineCore_DP0 pid=367256)[0;0m INFO 12-04 13:12:23 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='Qwen/Qwen3-14B', speculative_config=None, tokenizer='Qwen/Qwen3-14B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-14B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=367264)[0;0m INFO 12-04 13:12:23 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='Qwen/Qwen3-14B', speculative_config=None, tokenizer='Qwen/Qwen3-14B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-14B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=367264)[0;0m INFO 12-04 13:12:25 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.98:42849 backend=nccl
[1;36m(EngineCore_DP0 pid=367256)[0;0m INFO 12-04 13:12:25 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.98:49071 backend=nccl
[W1204 13:12:25.341683316 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-25-0.rc.tch.harvard.edu]:49071 (errno: 97 - Address family not supported by protocol).
[W1204 13:12:25.341683182 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-25-0.rc.tch.harvard.edu]:42849 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=367256)[0;0m INFO 12-04 13:12:25 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=367264)[0;0m INFO 12-04 13:12:25 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=367256)[0;0m INFO 12-04 13:12:26 [gpu_model_runner.py:3259] Starting to load model Qwen/Qwen3-14B...
[1;36m(EngineCore_DP0 pid=367264)[0;0m INFO 12-04 13:12:26 [gpu_model_runner.py:3259] Starting to load model Qwen/Qwen3-14B...
[1;36m(EngineCore_DP0 pid=367264)[0;0m INFO 12-04 13:12:27 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=367264)[0;0m INFO 12-04 13:12:27 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=367256)[0;0m INFO 12-04 13:12:27 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=367256)[0;0m INFO 12-04 13:12:27 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=367256)[0;0m ERROR 12-04 13:12:28 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=367256)[0;0m ERROR 12-04 13:12:28 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=367256)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=367256)[0;0m ERROR 12-04 13:12:28 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=367256)[0;0m ERROR 12-04 13:12:28 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367256)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=367256)[0;0m ERROR 12-04 13:12:28 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=367256)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=367256)[0;0m ERROR 12-04 13:12:28 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=367256)[0;0m ERROR 12-04 13:12:28 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367256)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=367256)[0;0m ERROR 12-04 13:12:28 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=367256)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[1;36m(EngineCore_DP0 pid=367256)[0;0m ERROR 12-04 13:12:28 [core.py:842]     self.driver_worker.load_model()
[1;36m(EngineCore_DP0 pid=367256)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 273, in load_model
[1;36m(EngineCore_DP0 pid=367256)[0;0m ERROR 12-04 13:12:28 [core.py:842]     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[1;36m(EngineCore_DP0 pid=367256)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3276, in load_model
[1;36m(EngineCore_DP0 pid=367256)[0;0m ERROR 12-04 13:12:28 [core.py:842]     self.model = model_loader.load_model(
[1;36m(EngineCore_DP0 pid=367256)[0;0m ERROR 12-04 13:12:28 [core.py:842]                  ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367256)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
[1;36m(EngineCore_DP0 pid=367256)[0;0m ERROR 12-04 13:12:28 [core.py:842]     model = initialize_model(
[1;36m(EngineCore_DP0 pid=367256)[0;0m ERROR 12-04 13:12:28 [core.py:842]             ^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367256)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py", line 55, in initialize_model
[1;36m(EngineCore_DP0 pid=367256)[0;0m ERROR 12-04 13:12:28 [core.py:842]     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(EngineCore_DP0 pid=367256)[0;0m ERROR 12-04 13:12:28 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367256)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 279, in __init__
[1;36m(EngineCore_DP0 pid=367256)[0;0m ERROR 12-04 13:12:28 [core.py:842]     self.model = Qwen3Model(
[1;36m(EngineCore_DP0 pid=367264)[0;0m ERROR 12-04 13:12:28 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=367264)[0;0m ERROR 12-04 13:12:28 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=367264)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=367264)[0;0m ERROR 12-04 13:12:28 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=367264)[0;0m ERROR 12-04 13:12:28 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367264)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=367264)[0;0m ERROR 12-04 13:12:28 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=367264)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=367264)[0;0m ERROR 12-04 13:12:28 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=367264)[0;0m ERROR 12-04 13:12:28 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367264)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=367264)[0;0m ERROR 12-04 13:12:28 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=367264)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[1;36m(EngineCore_DP0 pid=367264)[0;0m ERROR 12-04 13:12:28 [core.py:842]     self.driver_worker.load_model()
[1;36m(EngineCore_DP0 pid=367264)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 273, in load_model
[1;36m(EngineCore_DP0 pid=367264)[0;0m ERROR 12-04 13:12:28 [core.py:842]     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[1;36m(EngineCore_DP0 pid=367264)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3276, in load_model
[1;36m(EngineCore_DP0 pid=367264)[0;0m ERROR 12-04 13:12:28 [core.py:842]     self.model = model_loader.load_model(
[1;36m(EngineCore_DP0 pid=367264)[0;0m ERROR 12-04 13:12:28 [core.py:842]                  ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367264)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
[1;36m(EngineCore_DP0 pid=367264)[0;0m ERROR 12-04 13:12:28 [core.py:842]     model = initialize_model(
[1;36m(EngineCore_DP0 pid=367264)[0;0m ERROR 12-04 13:12:28 [core.py:842]             ^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367264)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py", line 55, in initialize_model
[1;36m(EngineCore_DP0 pid=367264)[0;0m ERROR 12-04 13:12:28 [core.py:842]     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(EngineCore_DP0 pid=367264)[0;0m ERROR 12-04 13:12:28 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367264)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 279, in __init__
[1;36m(EngineCore_DP0 pid=367264)[0;0m ERROR 12-04 13:12:28 [core.py:842]     self.model = Qwen3Model(
[1;36m(EngineCore_DP0 pid=367256)[0;0m ERROR 12-04 13:12:28 [core.py:842]                  ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367256)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=367256)[0;0m ERROR 12-04 13:12:28 [core.py:842]     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=367256)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 253, in __init__
[1;36m(EngineCore_DP0 pid=367256)[0;0m ERROR 12-04 13:12:28 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=367256)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=367256)[0;0m ERROR 12-04 13:12:28 [core.py:842]     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=367256)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 337, in __init__
[1;36m(EngineCore_DP0 pid=367256)[0;0m ERROR 12-04 13:12:28 [core.py:842]     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(EngineCore_DP0 pid=367256)[0;0m ERROR 12-04 13:12:28 [core.py:842]                                                     ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367256)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 651, in make_layers
[1;36m(EngineCore_DP0 pid=367256)[0;0m ERROR 12-04 13:12:28 [core.py:842]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(EngineCore_DP0 pid=367256)[0;0m ERROR 12-04 13:12:28 [core.py:842]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367256)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 339, in <lambda>
[1;36m(EngineCore_DP0 pid=367256)[0;0m ERROR 12-04 13:12:28 [core.py:842]     lambda prefix: decoder_layer_type(
[1;36m(EngineCore_DP0 pid=367256)[0;0m ERROR 12-04 13:12:28 [core.py:842]                    ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367256)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 201, in __init__
[1;36m(EngineCore_DP0 pid=367256)[0;0m ERROR 12-04 13:12:28 [core.py:842]     self.mlp = Qwen3MLP(
[1;36m(EngineCore_DP0 pid=367256)[0;0m ERROR 12-04 13:12:28 [core.py:842]                ^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367256)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 84, in __init__
[1;36m(EngineCore_DP0 pid=367256)[0;0m ERROR 12-04 13:12:28 [core.py:842]     self.gate_up_proj = MergedColumnParallelLinear(
[1;36m(EngineCore_DP0 pid=367256)[0;0m ERROR 12-04 13:12:28 [core.py:842]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367256)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 631, in __init__
[1;36m(EngineCore_DP0 pid=367256)[0;0m ERROR 12-04 13:12:28 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=367256)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 484, in __init__
[1;36m(EngineCore_DP0 pid=367256)[0;0m ERROR 12-04 13:12:28 [core.py:842]     self.quant_method.create_weights(
[1;36m(EngineCore_DP0 pid=367256)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=367264)[0;0m ERROR 12-04 13:12:28 [core.py:842]                  ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367264)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=367264)[0;0m ERROR 12-04 13:12:28 [core.py:842]     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=367264)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 253, in __init__
[1;36m(EngineCore_DP0 pid=367264)[0;0m ERROR 12-04 13:12:28 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=367264)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=367264)[0;0m ERROR 12-04 13:12:28 [core.py:842]     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=367264)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 337, in __init__
[1;36m(EngineCore_DP0 pid=367264)[0;0m ERROR 12-04 13:12:28 [core.py:842]     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(EngineCore_DP0 pid=367264)[0;0m ERROR 12-04 13:12:28 [core.py:842]                                                     ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367264)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 651, in make_layers
[1;36m(EngineCore_DP0 pid=367264)[0;0m ERROR 12-04 13:12:28 [core.py:842]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(EngineCore_DP0 pid=367264)[0;0m ERROR 12-04 13:12:28 [core.py:842]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367264)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 339, in <lambda>
[1;36m(EngineCore_DP0 pid=367264)[0;0m ERROR 12-04 13:12:28 [core.py:842]     lambda prefix: decoder_layer_type(
[1;36m(EngineCore_DP0 pid=367264)[0;0m ERROR 12-04 13:12:28 [core.py:842]                    ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367264)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 201, in __init__
[1;36m(EngineCore_DP0 pid=367264)[0;0m ERROR 12-04 13:12:28 [core.py:842]     self.mlp = Qwen3MLP(
[1;36m(EngineCore_DP0 pid=367264)[0;0m ERROR 12-04 13:12:28 [core.py:842]                ^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367264)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 84, in __init__
[1;36m(EngineCore_DP0 pid=367264)[0;0m ERROR 12-04 13:12:28 [core.py:842]     self.gate_up_proj = MergedColumnParallelLinear(
[1;36m(EngineCore_DP0 pid=367264)[0;0m ERROR 12-04 13:12:28 [core.py:842]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367264)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 631, in __init__
[1;36m(EngineCore_DP0 pid=367264)[0;0m ERROR 12-04 13:12:28 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=367264)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 484, in __init__
[1;36m(EngineCore_DP0 pid=367264)[0;0m ERROR 12-04 13:12:28 [core.py:842]     self.quant_method.create_weights(
[1;36m(EngineCore_DP0 pid=367264)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=367256)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 215, in create_weights
[1;36m(EngineCore_DP0 pid=367256)[0;0m ERROR 12-04 13:12:28 [core.py:842]     data=torch.empty(
[1;36m(EngineCore_DP0 pid=367256)[0;0m ERROR 12-04 13:12:28 [core.py:842]          ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367256)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_device.py", line 103, in __torch_function__
[1;36m(EngineCore_DP0 pid=367256)[0;0m ERROR 12-04 13:12:28 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=367256)[0;0m ERROR 12-04 13:12:28 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367256)[0;0m ERROR 12-04 13:12:28 [core.py:842] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 340.00 MiB. GPU 0 has a total capacity of 44.39 GiB of which 34.00 MiB is free. Process 367264 has 21.87 GiB memory in use. Including non-PyTorch memory, this process has 22.48 GiB memory in use. Of the allocated memory 21.96 GiB is allocated by PyTorch, and 18.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=367264)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 215, in create_weights
[1;36m(EngineCore_DP0 pid=367264)[0;0m ERROR 12-04 13:12:28 [core.py:842]     data=torch.empty(
[1;36m(EngineCore_DP0 pid=367264)[0;0m ERROR 12-04 13:12:28 [core.py:842]          ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367264)[0;0m ERROR 12-04 13:12:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_device.py", line 103, in __torch_function__
[1;36m(EngineCore_DP0 pid=367264)[0;0m ERROR 12-04 13:12:28 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=367264)[0;0m ERROR 12-04 13:12:28 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367264)[0;0m ERROR 12-04 13:12:28 [core.py:842] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 340.00 MiB. GPU 0 has a total capacity of 44.39 GiB of which 124.00 MiB is free. Including non-PyTorch memory, this process has 21.87 GiB memory in use. Process 367256 has 22.51 GiB memory in use. Of the allocated memory 21.35 GiB is allocated by PyTorch, and 18.55 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=367256)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=367264)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=367256)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=367256)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=367256)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=367256)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=367256)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=367256)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=367256)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=367256)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=367256)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367256)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=367256)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=367264)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=367264)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=367264)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=367264)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=367264)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=367264)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=367264)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=367264)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=367264)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367264)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=367264)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=367264)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=367264)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=367264)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367264)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=367264)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=367264)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[1;36m(EngineCore_DP0 pid=367264)[0;0m     self.driver_worker.load_model()
[1;36m(EngineCore_DP0 pid=367256)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=367256)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=367256)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367256)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=367256)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=367256)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[1;36m(EngineCore_DP0 pid=367256)[0;0m     self.driver_worker.load_model()
[1;36m(EngineCore_DP0 pid=367256)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 273, in load_model
[1;36m(EngineCore_DP0 pid=367256)[0;0m     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[1;36m(EngineCore_DP0 pid=367256)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3276, in load_model
[1;36m(EngineCore_DP0 pid=367256)[0;0m     self.model = model_loader.load_model(
[1;36m(EngineCore_DP0 pid=367256)[0;0m                  ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367256)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
[1;36m(EngineCore_DP0 pid=367256)[0;0m     model = initialize_model(
[1;36m(EngineCore_DP0 pid=367256)[0;0m             ^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367256)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py", line 55, in initialize_model
[1;36m(EngineCore_DP0 pid=367256)[0;0m     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(EngineCore_DP0 pid=367256)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367256)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 279, in __init__
[1;36m(EngineCore_DP0 pid=367256)[0;0m     self.model = Qwen3Model(
[1;36m(EngineCore_DP0 pid=367256)[0;0m                  ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367256)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=367256)[0;0m     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=367256)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 253, in __init__
[1;36m(EngineCore_DP0 pid=367256)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=367256)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=367256)[0;0m     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=367256)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 337, in __init__
[1;36m(EngineCore_DP0 pid=367256)[0;0m     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(EngineCore_DP0 pid=367256)[0;0m                                                     ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367256)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 651, in make_layers
[1;36m(EngineCore_DP0 pid=367256)[0;0m     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(EngineCore_DP0 pid=367256)[0;0m                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367256)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 339, in <lambda>
[1;36m(EngineCore_DP0 pid=367264)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 273, in load_model
[1;36m(EngineCore_DP0 pid=367264)[0;0m     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[1;36m(EngineCore_DP0 pid=367264)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3276, in load_model
[1;36m(EngineCore_DP0 pid=367264)[0;0m     self.model = model_loader.load_model(
[1;36m(EngineCore_DP0 pid=367264)[0;0m                  ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367264)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
[1;36m(EngineCore_DP0 pid=367264)[0;0m     model = initialize_model(
[1;36m(EngineCore_DP0 pid=367264)[0;0m             ^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367264)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py", line 55, in initialize_model
[1;36m(EngineCore_DP0 pid=367264)[0;0m     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(EngineCore_DP0 pid=367264)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367264)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 279, in __init__
[1;36m(EngineCore_DP0 pid=367264)[0;0m     self.model = Qwen3Model(
[1;36m(EngineCore_DP0 pid=367264)[0;0m                  ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367264)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=367264)[0;0m     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=367264)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 253, in __init__
[1;36m(EngineCore_DP0 pid=367264)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=367264)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=367264)[0;0m     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=367264)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 337, in __init__
[1;36m(EngineCore_DP0 pid=367264)[0;0m     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(EngineCore_DP0 pid=367264)[0;0m                                                     ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367264)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 651, in make_layers
[1;36m(EngineCore_DP0 pid=367264)[0;0m     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(EngineCore_DP0 pid=367264)[0;0m                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367264)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 339, in <lambda>
[1;36m(EngineCore_DP0 pid=367264)[0;0m     lambda prefix: decoder_layer_type(
[1;36m(EngineCore_DP0 pid=367264)[0;0m                    ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367264)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 201, in __init__
[1;36m(EngineCore_DP0 pid=367264)[0;0m     self.mlp = Qwen3MLP(
[1;36m(EngineCore_DP0 pid=367264)[0;0m                ^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367264)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 84, in __init__
[1;36m(EngineCore_DP0 pid=367264)[0;0m     self.gate_up_proj = MergedColumnParallelLinear(
[1;36m(EngineCore_DP0 pid=367264)[0;0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367256)[0;0m     lambda prefix: decoder_layer_type(
[1;36m(EngineCore_DP0 pid=367256)[0;0m                    ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367256)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 201, in __init__
[1;36m(EngineCore_DP0 pid=367256)[0;0m     self.mlp = Qwen3MLP(
[1;36m(EngineCore_DP0 pid=367256)[0;0m                ^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367256)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 84, in __init__
[1;36m(EngineCore_DP0 pid=367256)[0;0m     self.gate_up_proj = MergedColumnParallelLinear(
[1;36m(EngineCore_DP0 pid=367256)[0;0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367256)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 631, in __init__
[1;36m(EngineCore_DP0 pid=367256)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=367256)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 484, in __init__
[1;36m(EngineCore_DP0 pid=367256)[0;0m     self.quant_method.create_weights(
[1;36m(EngineCore_DP0 pid=367256)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 215, in create_weights
[1;36m(EngineCore_DP0 pid=367256)[0;0m     data=torch.empty(
[1;36m(EngineCore_DP0 pid=367256)[0;0m          ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367256)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_device.py", line 103, in __torch_function__
[1;36m(EngineCore_DP0 pid=367256)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=367256)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367256)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 340.00 MiB. GPU 0 has a total capacity of 44.39 GiB of which 34.00 MiB is free. Process 367264 has 21.87 GiB memory in use. Including non-PyTorch memory, this process has 22.48 GiB memory in use. Of the allocated memory 21.96 GiB is allocated by PyTorch, and 18.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=367264)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 631, in __init__
[1;36m(EngineCore_DP0 pid=367264)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=367264)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 484, in __init__
[1;36m(EngineCore_DP0 pid=367264)[0;0m     self.quant_method.create_weights(
[1;36m(EngineCore_DP0 pid=367264)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 215, in create_weights
[1;36m(EngineCore_DP0 pid=367264)[0;0m     data=torch.empty(
[1;36m(EngineCore_DP0 pid=367264)[0;0m          ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367264)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_device.py", line 103, in __torch_function__
[1;36m(EngineCore_DP0 pid=367264)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=367264)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367264)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 340.00 MiB. GPU 0 has a total capacity of 44.39 GiB of which 124.00 MiB is free. Including non-PyTorch memory, this process has 21.87 GiB memory in use. Process 367256 has 22.51 GiB memory in use. Of the allocated memory 21.35 GiB is allocated by PyTorch, and 18.55 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W1204 13:12:29.873553557 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank0]:[W1204 13:12:29.873477699 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: Qwen/Qwen3-14B (backend=vllm)
[ModelCache] Using max_model_len=40960 for Qwen/Qwen3-14B
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 13:12:50 [utils.py:253] non-default args: {'max_model_len': 40960, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'Qwen/Qwen3-14B'}
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: Qwen/Qwen3-14B (backend=vllm)
[ModelCache] Using max_model_len=40960 for Qwen/Qwen3-14B
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 13:12:50 [utils.py:253] non-default args: {'max_model_len': 40960, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'Qwen/Qwen3-14B'}
INFO 12-04 13:12:51 [model.py:631] Resolved architecture: Qwen3ForCausalLM
INFO 12-04 13:12:51 [model.py:1745] Using max model len 40960
INFO 12-04 13:12:51 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
INFO 12-04 13:12:51 [model.py:631] Resolved architecture: Qwen3ForCausalLM
INFO 12-04 13:12:51 [model.py:1745] Using max model len 40960
INFO 12-04 13:12:51 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=367755)[0;0m INFO 12-04 13:13:11 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='Qwen/Qwen3-14B', speculative_config=None, tokenizer='Qwen/Qwen3-14B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-14B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=367758)[0;0m INFO 12-04 13:13:11 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='Qwen/Qwen3-14B', speculative_config=None, tokenizer='Qwen/Qwen3-14B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-14B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=367755)[0;0m INFO 12-04 13:13:13 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.98:48895 backend=nccl
[1;36m(EngineCore_DP0 pid=367758)[0;0m INFO 12-04 13:13:13 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.98:37775 backend=nccl
[W1204 13:13:13.828778207 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-25-0.rc.tch.harvard.edu]:48895 (errno: 97 - Address family not supported by protocol).
[W1204 13:13:13.832745147 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-25-0.rc.tch.harvard.edu]:37775 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=367758)[0;0m INFO 12-04 13:13:13 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=367755)[0;0m INFO 12-04 13:13:13 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=367758)[0;0m INFO 12-04 13:13:13 [gpu_model_runner.py:3259] Starting to load model Qwen/Qwen3-14B...
[1;36m(EngineCore_DP0 pid=367755)[0;0m INFO 12-04 13:13:13 [gpu_model_runner.py:3259] Starting to load model Qwen/Qwen3-14B...
[1;36m(EngineCore_DP0 pid=367758)[0;0m INFO 12-04 13:13:14 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=367758)[0;0m INFO 12-04 13:13:14 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=367755)[0;0m INFO 12-04 13:13:14 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=367755)[0;0m INFO 12-04 13:13:14 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=367755)[0;0m ERROR 12-04 13:13:16 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=367755)[0;0m ERROR 12-04 13:13:16 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=367755)[0;0m ERROR 12-04 13:13:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=367755)[0;0m ERROR 12-04 13:13:16 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=367755)[0;0m ERROR 12-04 13:13:16 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367755)[0;0m ERROR 12-04 13:13:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=367755)[0;0m ERROR 12-04 13:13:16 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=367755)[0;0m ERROR 12-04 13:13:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=367755)[0;0m ERROR 12-04 13:13:16 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=367755)[0;0m ERROR 12-04 13:13:16 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367755)[0;0m ERROR 12-04 13:13:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=367755)[0;0m ERROR 12-04 13:13:16 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=367755)[0;0m ERROR 12-04 13:13:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[1;36m(EngineCore_DP0 pid=367755)[0;0m ERROR 12-04 13:13:16 [core.py:842]     self.driver_worker.load_model()
[1;36m(EngineCore_DP0 pid=367755)[0;0m ERROR 12-04 13:13:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 273, in load_model
[1;36m(EngineCore_DP0 pid=367755)[0;0m ERROR 12-04 13:13:16 [core.py:842]     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[1;36m(EngineCore_DP0 pid=367755)[0;0m ERROR 12-04 13:13:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3276, in load_model
[1;36m(EngineCore_DP0 pid=367755)[0;0m ERROR 12-04 13:13:16 [core.py:842]     self.model = model_loader.load_model(
[1;36m(EngineCore_DP0 pid=367755)[0;0m ERROR 12-04 13:13:16 [core.py:842]                  ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367755)[0;0m ERROR 12-04 13:13:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
[1;36m(EngineCore_DP0 pid=367755)[0;0m ERROR 12-04 13:13:16 [core.py:842]     model = initialize_model(
[1;36m(EngineCore_DP0 pid=367755)[0;0m ERROR 12-04 13:13:16 [core.py:842]             ^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367755)[0;0m ERROR 12-04 13:13:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py", line 55, in initialize_model
[1;36m(EngineCore_DP0 pid=367755)[0;0m ERROR 12-04 13:13:16 [core.py:842]     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(EngineCore_DP0 pid=367755)[0;0m ERROR 12-04 13:13:16 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367755)[0;0m ERROR 12-04 13:13:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 279, in __init__
[1;36m(EngineCore_DP0 pid=367755)[0;0m ERROR 12-04 13:13:16 [core.py:842]     self.model = Qwen3Model(
[1;36m(EngineCore_DP0 pid=367755)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=367755)[0;0m ERROR 12-04 13:13:16 [core.py:842]                  ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367755)[0;0m ERROR 12-04 13:13:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=367755)[0;0m ERROR 12-04 13:13:16 [core.py:842]     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=367755)[0;0m ERROR 12-04 13:13:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 253, in __init__
[1;36m(EngineCore_DP0 pid=367755)[0;0m ERROR 12-04 13:13:16 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=367755)[0;0m ERROR 12-04 13:13:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=367755)[0;0m ERROR 12-04 13:13:16 [core.py:842]     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=367755)[0;0m ERROR 12-04 13:13:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 337, in __init__
[1;36m(EngineCore_DP0 pid=367755)[0;0m ERROR 12-04 13:13:16 [core.py:842]     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(EngineCore_DP0 pid=367755)[0;0m ERROR 12-04 13:13:16 [core.py:842]                                                     ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367755)[0;0m ERROR 12-04 13:13:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 651, in make_layers
[1;36m(EngineCore_DP0 pid=367755)[0;0m ERROR 12-04 13:13:16 [core.py:842]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(EngineCore_DP0 pid=367755)[0;0m ERROR 12-04 13:13:16 [core.py:842]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367755)[0;0m ERROR 12-04 13:13:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 339, in <lambda>
[1;36m(EngineCore_DP0 pid=367755)[0;0m ERROR 12-04 13:13:16 [core.py:842]     lambda prefix: decoder_layer_type(
[1;36m(EngineCore_DP0 pid=367755)[0;0m ERROR 12-04 13:13:16 [core.py:842]                    ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367755)[0;0m ERROR 12-04 13:13:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 201, in __init__
[1;36m(EngineCore_DP0 pid=367755)[0;0m ERROR 12-04 13:13:16 [core.py:842]     self.mlp = Qwen3MLP(
[1;36m(EngineCore_DP0 pid=367755)[0;0m ERROR 12-04 13:13:16 [core.py:842]                ^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367755)[0;0m ERROR 12-04 13:13:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 84, in __init__
[1;36m(EngineCore_DP0 pid=367755)[0;0m ERROR 12-04 13:13:16 [core.py:842]     self.gate_up_proj = MergedColumnParallelLinear(
[1;36m(EngineCore_DP0 pid=367755)[0;0m ERROR 12-04 13:13:16 [core.py:842]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367755)[0;0m ERROR 12-04 13:13:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 631, in __init__
[1;36m(EngineCore_DP0 pid=367755)[0;0m ERROR 12-04 13:13:16 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=367755)[0;0m ERROR 12-04 13:13:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 484, in __init__
[1;36m(EngineCore_DP0 pid=367755)[0;0m ERROR 12-04 13:13:16 [core.py:842]     self.quant_method.create_weights(
[1;36m(EngineCore_DP0 pid=367755)[0;0m ERROR 12-04 13:13:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 215, in create_weights
[1;36m(EngineCore_DP0 pid=367755)[0;0m ERROR 12-04 13:13:16 [core.py:842]     data=torch.empty(
[1;36m(EngineCore_DP0 pid=367755)[0;0m ERROR 12-04 13:13:16 [core.py:842]          ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367755)[0;0m ERROR 12-04 13:13:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_device.py", line 103, in __torch_function__
[1;36m(EngineCore_DP0 pid=367755)[0;0m ERROR 12-04 13:13:16 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=367755)[0;0m ERROR 12-04 13:13:16 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367755)[0;0m ERROR 12-04 13:13:16 [core.py:842] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 340.00 MiB. GPU 0 has a total capacity of 44.39 GiB of which 124.00 MiB is free. Process 367758 has 22.51 GiB memory in use. Including non-PyTorch memory, this process has 21.87 GiB memory in use. Of the allocated memory 21.35 GiB is allocated by PyTorch, and 18.55 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=367755)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=367755)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=367755)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=367755)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=367755)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=367755)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=367755)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=367755)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=367755)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=367755)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367755)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=367755)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=367755)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=367755)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=367755)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367755)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=367755)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=367755)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[1;36m(EngineCore_DP0 pid=367755)[0;0m     self.driver_worker.load_model()
[1;36m(EngineCore_DP0 pid=367755)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 273, in load_model
[1;36m(EngineCore_DP0 pid=367755)[0;0m     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[1;36m(EngineCore_DP0 pid=367755)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3276, in load_model
[1;36m(EngineCore_DP0 pid=367755)[0;0m     self.model = model_loader.load_model(
[1;36m(EngineCore_DP0 pid=367755)[0;0m                  ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367755)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
[1;36m(EngineCore_DP0 pid=367755)[0;0m     model = initialize_model(
[1;36m(EngineCore_DP0 pid=367755)[0;0m             ^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367755)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py", line 55, in initialize_model
[1;36m(EngineCore_DP0 pid=367755)[0;0m     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(EngineCore_DP0 pid=367755)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367755)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 279, in __init__
[1;36m(EngineCore_DP0 pid=367755)[0;0m     self.model = Qwen3Model(
[1;36m(EngineCore_DP0 pid=367755)[0;0m                  ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367755)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=367755)[0;0m     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=367755)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 253, in __init__
[1;36m(EngineCore_DP0 pid=367755)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=367755)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=367755)[0;0m     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=367755)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 337, in __init__
[1;36m(EngineCore_DP0 pid=367755)[0;0m     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(EngineCore_DP0 pid=367755)[0;0m                                                     ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367755)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 651, in make_layers
[1;36m(EngineCore_DP0 pid=367755)[0;0m     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(EngineCore_DP0 pid=367755)[0;0m                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367755)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 339, in <lambda>
[1;36m(EngineCore_DP0 pid=367755)[0;0m     lambda prefix: decoder_layer_type(
[1;36m(EngineCore_DP0 pid=367755)[0;0m                    ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367755)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 201, in __init__
[1;36m(EngineCore_DP0 pid=367755)[0;0m     self.mlp = Qwen3MLP(
[1;36m(EngineCore_DP0 pid=367755)[0;0m                ^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367755)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 84, in __init__
[1;36m(EngineCore_DP0 pid=367755)[0;0m     self.gate_up_proj = MergedColumnParallelLinear(
[1;36m(EngineCore_DP0 pid=367755)[0;0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367755)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 631, in __init__
[1;36m(EngineCore_DP0 pid=367755)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=367755)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 484, in __init__
[1;36m(EngineCore_DP0 pid=367755)[0;0m     self.quant_method.create_weights(
[1;36m(EngineCore_DP0 pid=367755)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 215, in create_weights
[1;36m(EngineCore_DP0 pid=367755)[0;0m     data=torch.empty(
[1;36m(EngineCore_DP0 pid=367755)[0;0m          ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367755)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_device.py", line 103, in __torch_function__
[1;36m(EngineCore_DP0 pid=367755)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=367755)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367755)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 340.00 MiB. GPU 0 has a total capacity of 44.39 GiB of which 124.00 MiB is free. Process 367758 has 22.51 GiB memory in use. Including non-PyTorch memory, this process has 21.87 GiB memory in use. Of the allocated memory 21.35 GiB is allocated by PyTorch, and 18.55 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=367758)[0;0m ERROR 12-04 13:13:16 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=367758)[0;0m ERROR 12-04 13:13:16 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=367758)[0;0m ERROR 12-04 13:13:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=367758)[0;0m ERROR 12-04 13:13:16 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=367758)[0;0m ERROR 12-04 13:13:16 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367758)[0;0m ERROR 12-04 13:13:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=367758)[0;0m ERROR 12-04 13:13:16 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=367758)[0;0m ERROR 12-04 13:13:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=367758)[0;0m ERROR 12-04 13:13:16 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=367758)[0;0m ERROR 12-04 13:13:16 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367758)[0;0m ERROR 12-04 13:13:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=367758)[0;0m ERROR 12-04 13:13:16 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=367758)[0;0m ERROR 12-04 13:13:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[1;36m(EngineCore_DP0 pid=367758)[0;0m ERROR 12-04 13:13:16 [core.py:842]     self.driver_worker.load_model()
[1;36m(EngineCore_DP0 pid=367758)[0;0m ERROR 12-04 13:13:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 273, in load_model
[1;36m(EngineCore_DP0 pid=367758)[0;0m ERROR 12-04 13:13:16 [core.py:842]     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[1;36m(EngineCore_DP0 pid=367758)[0;0m ERROR 12-04 13:13:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3276, in load_model
[1;36m(EngineCore_DP0 pid=367758)[0;0m ERROR 12-04 13:13:16 [core.py:842]     self.model = model_loader.load_model(
[1;36m(EngineCore_DP0 pid=367758)[0;0m ERROR 12-04 13:13:16 [core.py:842]                  ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367758)[0;0m ERROR 12-04 13:13:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
[1;36m(EngineCore_DP0 pid=367758)[0;0m ERROR 12-04 13:13:16 [core.py:842]     model = initialize_model(
[1;36m(EngineCore_DP0 pid=367758)[0;0m ERROR 12-04 13:13:16 [core.py:842]             ^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367758)[0;0m ERROR 12-04 13:13:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py", line 55, in initialize_model
[1;36m(EngineCore_DP0 pid=367758)[0;0m ERROR 12-04 13:13:16 [core.py:842]     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(EngineCore_DP0 pid=367758)[0;0m ERROR 12-04 13:13:16 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367758)[0;0m ERROR 12-04 13:13:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 279, in __init__
[1;36m(EngineCore_DP0 pid=367758)[0;0m ERROR 12-04 13:13:16 [core.py:842]     self.model = Qwen3Model(
[1;36m(EngineCore_DP0 pid=367758)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=367758)[0;0m ERROR 12-04 13:13:16 [core.py:842]                  ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367758)[0;0m ERROR 12-04 13:13:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=367758)[0;0m ERROR 12-04 13:13:16 [core.py:842]     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=367758)[0;0m ERROR 12-04 13:13:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 253, in __init__
[1;36m(EngineCore_DP0 pid=367758)[0;0m ERROR 12-04 13:13:16 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=367758)[0;0m ERROR 12-04 13:13:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=367758)[0;0m ERROR 12-04 13:13:16 [core.py:842]     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=367758)[0;0m ERROR 12-04 13:13:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 337, in __init__
[1;36m(EngineCore_DP0 pid=367758)[0;0m ERROR 12-04 13:13:16 [core.py:842]     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(EngineCore_DP0 pid=367758)[0;0m ERROR 12-04 13:13:16 [core.py:842]                                                     ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367758)[0;0m ERROR 12-04 13:13:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 651, in make_layers
[1;36m(EngineCore_DP0 pid=367758)[0;0m ERROR 12-04 13:13:16 [core.py:842]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(EngineCore_DP0 pid=367758)[0;0m ERROR 12-04 13:13:16 [core.py:842]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367758)[0;0m ERROR 12-04 13:13:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 339, in <lambda>
[1;36m(EngineCore_DP0 pid=367758)[0;0m ERROR 12-04 13:13:16 [core.py:842]     lambda prefix: decoder_layer_type(
[1;36m(EngineCore_DP0 pid=367758)[0;0m ERROR 12-04 13:13:16 [core.py:842]                    ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367758)[0;0m ERROR 12-04 13:13:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 201, in __init__
[1;36m(EngineCore_DP0 pid=367758)[0;0m ERROR 12-04 13:13:16 [core.py:842]     self.mlp = Qwen3MLP(
[1;36m(EngineCore_DP0 pid=367758)[0;0m ERROR 12-04 13:13:16 [core.py:842]                ^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367758)[0;0m ERROR 12-04 13:13:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 84, in __init__
[1;36m(EngineCore_DP0 pid=367758)[0;0m ERROR 12-04 13:13:16 [core.py:842]     self.gate_up_proj = MergedColumnParallelLinear(
[1;36m(EngineCore_DP0 pid=367758)[0;0m ERROR 12-04 13:13:16 [core.py:842]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367758)[0;0m ERROR 12-04 13:13:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 631, in __init__
[1;36m(EngineCore_DP0 pid=367758)[0;0m ERROR 12-04 13:13:16 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=367758)[0;0m ERROR 12-04 13:13:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 484, in __init__
[1;36m(EngineCore_DP0 pid=367758)[0;0m ERROR 12-04 13:13:16 [core.py:842]     self.quant_method.create_weights(
[1;36m(EngineCore_DP0 pid=367758)[0;0m ERROR 12-04 13:13:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 215, in create_weights
[1;36m(EngineCore_DP0 pid=367758)[0;0m ERROR 12-04 13:13:16 [core.py:842]     data=torch.empty(
[1;36m(EngineCore_DP0 pid=367758)[0;0m ERROR 12-04 13:13:16 [core.py:842]          ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367758)[0;0m ERROR 12-04 13:13:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_device.py", line 103, in __torch_function__
[1;36m(EngineCore_DP0 pid=367758)[0;0m ERROR 12-04 13:13:16 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=367758)[0;0m ERROR 12-04 13:13:16 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367758)[0;0m ERROR 12-04 13:13:16 [core.py:842] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 340.00 MiB. GPU 0 has a total capacity of 44.39 GiB of which 34.00 MiB is free. Including non-PyTorch memory, this process has 22.48 GiB memory in use. Process 367755 has 21.87 GiB memory in use. Of the allocated memory 21.96 GiB is allocated by PyTorch, and 18.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=367758)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=367758)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=367758)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=367758)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=367758)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=367758)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=367758)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=367758)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=367758)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=367758)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367758)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=367758)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=367758)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=367758)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=367758)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367758)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=367758)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=367758)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[1;36m(EngineCore_DP0 pid=367758)[0;0m     self.driver_worker.load_model()
[1;36m(EngineCore_DP0 pid=367758)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 273, in load_model
[1;36m(EngineCore_DP0 pid=367758)[0;0m     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[1;36m(EngineCore_DP0 pid=367758)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3276, in load_model
[1;36m(EngineCore_DP0 pid=367758)[0;0m     self.model = model_loader.load_model(
[1;36m(EngineCore_DP0 pid=367758)[0;0m                  ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367758)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
[1;36m(EngineCore_DP0 pid=367758)[0;0m     model = initialize_model(
[1;36m(EngineCore_DP0 pid=367758)[0;0m             ^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367758)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py", line 55, in initialize_model
[1;36m(EngineCore_DP0 pid=367758)[0;0m     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(EngineCore_DP0 pid=367758)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367758)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 279, in __init__
[1;36m(EngineCore_DP0 pid=367758)[0;0m     self.model = Qwen3Model(
[1;36m(EngineCore_DP0 pid=367758)[0;0m                  ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367758)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=367758)[0;0m     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=367758)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 253, in __init__
[1;36m(EngineCore_DP0 pid=367758)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=367758)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=367758)[0;0m     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=367758)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 337, in __init__
[1;36m(EngineCore_DP0 pid=367758)[0;0m     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(EngineCore_DP0 pid=367758)[0;0m                                                     ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367758)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 651, in make_layers
[1;36m(EngineCore_DP0 pid=367758)[0;0m     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(EngineCore_DP0 pid=367758)[0;0m                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367758)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 339, in <lambda>
[1;36m(EngineCore_DP0 pid=367758)[0;0m     lambda prefix: decoder_layer_type(
[1;36m(EngineCore_DP0 pid=367758)[0;0m                    ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367758)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 201, in __init__
[1;36m(EngineCore_DP0 pid=367758)[0;0m     self.mlp = Qwen3MLP(
[1;36m(EngineCore_DP0 pid=367758)[0;0m                ^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367758)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 84, in __init__
[1;36m(EngineCore_DP0 pid=367758)[0;0m     self.gate_up_proj = MergedColumnParallelLinear(
[1;36m(EngineCore_DP0 pid=367758)[0;0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367758)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 631, in __init__
[1;36m(EngineCore_DP0 pid=367758)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=367758)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 484, in __init__
[1;36m(EngineCore_DP0 pid=367758)[0;0m     self.quant_method.create_weights(
[1;36m(EngineCore_DP0 pid=367758)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 215, in create_weights
[1;36m(EngineCore_DP0 pid=367758)[0;0m     data=torch.empty(
[1;36m(EngineCore_DP0 pid=367758)[0;0m          ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367758)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_device.py", line 103, in __torch_function__
[1;36m(EngineCore_DP0 pid=367758)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=367758)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=367758)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 340.00 MiB. GPU 0 has a total capacity of 44.39 GiB of which 34.00 MiB is free. Including non-PyTorch memory, this process has 22.48 GiB memory in use. Process 367755 has 21.87 GiB memory in use. Of the allocated memory 21.96 GiB is allocated by PyTorch, and 18.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W1204 13:13:16.267241051 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank0]:[W1204 13:13:16.274331809 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: Qwen/Qwen3-14B (backend=vllm)
[ModelCache] Using max_model_len=40960 for Qwen/Qwen3-14B
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 13:13:38 [utils.py:253] non-default args: {'max_model_len': 40960, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'Qwen/Qwen3-14B'}
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: Qwen/Qwen3-14B (backend=vllm)
[ModelCache] Using max_model_len=40960 for Qwen/Qwen3-14B
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 13:13:38 [utils.py:253] non-default args: {'max_model_len': 40960, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'Qwen/Qwen3-14B'}
INFO 12-04 13:13:38 [model.py:631] Resolved architecture: Qwen3ForCausalLM
INFO 12-04 13:13:38 [model.py:1745] Using max model len 40960
INFO 12-04 13:13:38 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
INFO 12-04 13:13:38 [model.py:631] Resolved architecture: Qwen3ForCausalLM
INFO 12-04 13:13:38 [model.py:1745] Using max model len 40960
INFO 12-04 13:13:38 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=368277)[0;0m INFO 12-04 13:13:54 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='Qwen/Qwen3-14B', speculative_config=None, tokenizer='Qwen/Qwen3-14B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-14B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=368280)[0;0m INFO 12-04 13:13:54 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='Qwen/Qwen3-14B', speculative_config=None, tokenizer='Qwen/Qwen3-14B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-14B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=368280)[0;0m INFO 12-04 13:13:56 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.98:45145 backend=nccl
[W1204 13:13:56.500981223 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-25-0.rc.tch.harvard.edu]:45145 (errno: 97 - Address family not supported by protocol).
[1;36m(EngineCore_DP0 pid=368277)[0;0m INFO 12-04 13:13:56 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.98:50675 backend=nccl
[W1204 13:13:57.513069455 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-25-0.rc.tch.harvard.edu]:50675 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=368277)[0;0m INFO 12-04 13:13:57 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=368280)[0;0m INFO 12-04 13:13:57 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=368277)[0;0m INFO 12-04 13:13:57 [gpu_model_runner.py:3259] Starting to load model Qwen/Qwen3-14B...
[1;36m(EngineCore_DP0 pid=368280)[0;0m INFO 12-04 13:13:57 [gpu_model_runner.py:3259] Starting to load model Qwen/Qwen3-14B...
[1;36m(EngineCore_DP0 pid=368277)[0;0m INFO 12-04 13:13:58 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=368277)[0;0m INFO 12-04 13:13:58 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=368280)[0;0m INFO 12-04 13:13:58 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=368280)[0;0m INFO 12-04 13:13:58 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=368280)[0;0m ERROR 12-04 13:13:59 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=368280)[0;0m ERROR 12-04 13:13:59 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=368280)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=368280)[0;0m ERROR 12-04 13:13:59 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=368280)[0;0m ERROR 12-04 13:13:59 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368280)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=368280)[0;0m ERROR 12-04 13:13:59 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=368280)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=368280)[0;0m ERROR 12-04 13:13:59 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=368280)[0;0m ERROR 12-04 13:13:59 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368280)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=368280)[0;0m ERROR 12-04 13:13:59 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=368280)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[1;36m(EngineCore_DP0 pid=368280)[0;0m ERROR 12-04 13:13:59 [core.py:842]     self.driver_worker.load_model()
[1;36m(EngineCore_DP0 pid=368280)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 273, in load_model
[1;36m(EngineCore_DP0 pid=368280)[0;0m ERROR 12-04 13:13:59 [core.py:842]     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[1;36m(EngineCore_DP0 pid=368280)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3276, in load_model
[1;36m(EngineCore_DP0 pid=368280)[0;0m ERROR 12-04 13:13:59 [core.py:842]     self.model = model_loader.load_model(
[1;36m(EngineCore_DP0 pid=368280)[0;0m ERROR 12-04 13:13:59 [core.py:842]                  ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368280)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
[1;36m(EngineCore_DP0 pid=368280)[0;0m ERROR 12-04 13:13:59 [core.py:842]     model = initialize_model(
[1;36m(EngineCore_DP0 pid=368280)[0;0m ERROR 12-04 13:13:59 [core.py:842]             ^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368280)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py", line 55, in initialize_model
[1;36m(EngineCore_DP0 pid=368280)[0;0m ERROR 12-04 13:13:59 [core.py:842]     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(EngineCore_DP0 pid=368280)[0;0m ERROR 12-04 13:13:59 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368280)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 279, in __init__
[1;36m(EngineCore_DP0 pid=368280)[0;0m ERROR 12-04 13:13:59 [core.py:842]     self.model = Qwen3Model(
[1;36m(EngineCore_DP0 pid=368280)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=368280)[0;0m ERROR 12-04 13:13:59 [core.py:842]                  ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368280)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=368280)[0;0m ERROR 12-04 13:13:59 [core.py:842]     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=368280)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 253, in __init__
[1;36m(EngineCore_DP0 pid=368280)[0;0m ERROR 12-04 13:13:59 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=368280)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=368280)[0;0m ERROR 12-04 13:13:59 [core.py:842]     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=368280)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 337, in __init__
[1;36m(EngineCore_DP0 pid=368280)[0;0m ERROR 12-04 13:13:59 [core.py:842]     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(EngineCore_DP0 pid=368280)[0;0m ERROR 12-04 13:13:59 [core.py:842]                                                     ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368280)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 651, in make_layers
[1;36m(EngineCore_DP0 pid=368280)[0;0m ERROR 12-04 13:13:59 [core.py:842]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(EngineCore_DP0 pid=368280)[0;0m ERROR 12-04 13:13:59 [core.py:842]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368280)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 339, in <lambda>
[1;36m(EngineCore_DP0 pid=368280)[0;0m ERROR 12-04 13:13:59 [core.py:842]     lambda prefix: decoder_layer_type(
[1;36m(EngineCore_DP0 pid=368280)[0;0m ERROR 12-04 13:13:59 [core.py:842]                    ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368280)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 201, in __init__
[1;36m(EngineCore_DP0 pid=368280)[0;0m ERROR 12-04 13:13:59 [core.py:842]     self.mlp = Qwen3MLP(
[1;36m(EngineCore_DP0 pid=368280)[0;0m ERROR 12-04 13:13:59 [core.py:842]                ^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368280)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 84, in __init__
[1;36m(EngineCore_DP0 pid=368280)[0;0m ERROR 12-04 13:13:59 [core.py:842]     self.gate_up_proj = MergedColumnParallelLinear(
[1;36m(EngineCore_DP0 pid=368280)[0;0m ERROR 12-04 13:13:59 [core.py:842]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368280)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 631, in __init__
[1;36m(EngineCore_DP0 pid=368280)[0;0m ERROR 12-04 13:13:59 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=368280)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 484, in __init__
[1;36m(EngineCore_DP0 pid=368280)[0;0m ERROR 12-04 13:13:59 [core.py:842]     self.quant_method.create_weights(
[1;36m(EngineCore_DP0 pid=368280)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 215, in create_weights
[1;36m(EngineCore_DP0 pid=368280)[0;0m ERROR 12-04 13:13:59 [core.py:842]     data=torch.empty(
[1;36m(EngineCore_DP0 pid=368280)[0;0m ERROR 12-04 13:13:59 [core.py:842]          ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368280)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_device.py", line 103, in __torch_function__
[1;36m(EngineCore_DP0 pid=368280)[0;0m ERROR 12-04 13:13:59 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=368280)[0;0m ERROR 12-04 13:13:59 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368280)[0;0m ERROR 12-04 13:13:59 [core.py:842] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 340.00 MiB. GPU 0 has a total capacity of 44.39 GiB of which 34.00 MiB is free. Process 368277 has 21.87 GiB memory in use. Including non-PyTorch memory, this process has 22.48 GiB memory in use. Of the allocated memory 21.96 GiB is allocated by PyTorch, and 18.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=368280)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=368280)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=368280)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=368280)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=368280)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=368280)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=368280)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=368280)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=368280)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=368280)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368280)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=368280)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=368280)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=368280)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=368280)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368280)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=368280)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=368280)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[1;36m(EngineCore_DP0 pid=368280)[0;0m     self.driver_worker.load_model()
[1;36m(EngineCore_DP0 pid=368280)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 273, in load_model
[1;36m(EngineCore_DP0 pid=368280)[0;0m     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[1;36m(EngineCore_DP0 pid=368280)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3276, in load_model
[1;36m(EngineCore_DP0 pid=368280)[0;0m     self.model = model_loader.load_model(
[1;36m(EngineCore_DP0 pid=368280)[0;0m                  ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368280)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
[1;36m(EngineCore_DP0 pid=368280)[0;0m     model = initialize_model(
[1;36m(EngineCore_DP0 pid=368280)[0;0m             ^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368280)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py", line 55, in initialize_model
[1;36m(EngineCore_DP0 pid=368280)[0;0m     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(EngineCore_DP0 pid=368280)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368280)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 279, in __init__
[1;36m(EngineCore_DP0 pid=368280)[0;0m     self.model = Qwen3Model(
[1;36m(EngineCore_DP0 pid=368280)[0;0m                  ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368280)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=368280)[0;0m     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=368280)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 253, in __init__
[1;36m(EngineCore_DP0 pid=368280)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=368280)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=368280)[0;0m     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=368280)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 337, in __init__
[1;36m(EngineCore_DP0 pid=368280)[0;0m     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(EngineCore_DP0 pid=368280)[0;0m                                                     ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368280)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 651, in make_layers
[1;36m(EngineCore_DP0 pid=368280)[0;0m     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(EngineCore_DP0 pid=368280)[0;0m                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368280)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 339, in <lambda>
[1;36m(EngineCore_DP0 pid=368280)[0;0m     lambda prefix: decoder_layer_type(
[1;36m(EngineCore_DP0 pid=368280)[0;0m                    ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368280)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 201, in __init__
[1;36m(EngineCore_DP0 pid=368280)[0;0m     self.mlp = Qwen3MLP(
[1;36m(EngineCore_DP0 pid=368280)[0;0m                ^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368280)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 84, in __init__
[1;36m(EngineCore_DP0 pid=368280)[0;0m     self.gate_up_proj = MergedColumnParallelLinear(
[1;36m(EngineCore_DP0 pid=368280)[0;0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368280)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 631, in __init__
[1;36m(EngineCore_DP0 pid=368280)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=368280)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 484, in __init__
[1;36m(EngineCore_DP0 pid=368280)[0;0m     self.quant_method.create_weights(
[1;36m(EngineCore_DP0 pid=368280)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 215, in create_weights
[1;36m(EngineCore_DP0 pid=368280)[0;0m     data=torch.empty(
[1;36m(EngineCore_DP0 pid=368280)[0;0m          ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368280)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_device.py", line 103, in __torch_function__
[1;36m(EngineCore_DP0 pid=368280)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=368280)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368280)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 340.00 MiB. GPU 0 has a total capacity of 44.39 GiB of which 34.00 MiB is free. Process 368277 has 21.87 GiB memory in use. Including non-PyTorch memory, this process has 22.48 GiB memory in use. Of the allocated memory 21.96 GiB is allocated by PyTorch, and 18.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=368277)[0;0m ERROR 12-04 13:13:59 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=368277)[0;0m ERROR 12-04 13:13:59 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=368277)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=368277)[0;0m ERROR 12-04 13:13:59 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=368277)[0;0m ERROR 12-04 13:13:59 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368277)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=368277)[0;0m ERROR 12-04 13:13:59 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=368277)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=368277)[0;0m ERROR 12-04 13:13:59 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=368277)[0;0m ERROR 12-04 13:13:59 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368277)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=368277)[0;0m ERROR 12-04 13:13:59 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=368277)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[1;36m(EngineCore_DP0 pid=368277)[0;0m ERROR 12-04 13:13:59 [core.py:842]     self.driver_worker.load_model()
[1;36m(EngineCore_DP0 pid=368277)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 273, in load_model
[1;36m(EngineCore_DP0 pid=368277)[0;0m ERROR 12-04 13:13:59 [core.py:842]     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[1;36m(EngineCore_DP0 pid=368277)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3276, in load_model
[1;36m(EngineCore_DP0 pid=368277)[0;0m ERROR 12-04 13:13:59 [core.py:842]     self.model = model_loader.load_model(
[1;36m(EngineCore_DP0 pid=368277)[0;0m ERROR 12-04 13:13:59 [core.py:842]                  ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368277)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
[1;36m(EngineCore_DP0 pid=368277)[0;0m ERROR 12-04 13:13:59 [core.py:842]     model = initialize_model(
[1;36m(EngineCore_DP0 pid=368277)[0;0m ERROR 12-04 13:13:59 [core.py:842]             ^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368277)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py", line 55, in initialize_model
[1;36m(EngineCore_DP0 pid=368277)[0;0m ERROR 12-04 13:13:59 [core.py:842]     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(EngineCore_DP0 pid=368277)[0;0m ERROR 12-04 13:13:59 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368277)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 279, in __init__
[1;36m(EngineCore_DP0 pid=368277)[0;0m ERROR 12-04 13:13:59 [core.py:842]     self.model = Qwen3Model(
[1;36m(EngineCore_DP0 pid=368277)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=368277)[0;0m ERROR 12-04 13:13:59 [core.py:842]                  ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368277)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=368277)[0;0m ERROR 12-04 13:13:59 [core.py:842]     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=368277)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 253, in __init__
[1;36m(EngineCore_DP0 pid=368277)[0;0m ERROR 12-04 13:13:59 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=368277)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=368277)[0;0m ERROR 12-04 13:13:59 [core.py:842]     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=368277)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 337, in __init__
[1;36m(EngineCore_DP0 pid=368277)[0;0m ERROR 12-04 13:13:59 [core.py:842]     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(EngineCore_DP0 pid=368277)[0;0m ERROR 12-04 13:13:59 [core.py:842]                                                     ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368277)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 651, in make_layers
[1;36m(EngineCore_DP0 pid=368277)[0;0m ERROR 12-04 13:13:59 [core.py:842]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(EngineCore_DP0 pid=368277)[0;0m ERROR 12-04 13:13:59 [core.py:842]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368277)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 339, in <lambda>
[1;36m(EngineCore_DP0 pid=368277)[0;0m ERROR 12-04 13:13:59 [core.py:842]     lambda prefix: decoder_layer_type(
[1;36m(EngineCore_DP0 pid=368277)[0;0m ERROR 12-04 13:13:59 [core.py:842]                    ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368277)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 201, in __init__
[1;36m(EngineCore_DP0 pid=368277)[0;0m ERROR 12-04 13:13:59 [core.py:842]     self.mlp = Qwen3MLP(
[1;36m(EngineCore_DP0 pid=368277)[0;0m ERROR 12-04 13:13:59 [core.py:842]                ^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368277)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 84, in __init__
[1;36m(EngineCore_DP0 pid=368277)[0;0m ERROR 12-04 13:13:59 [core.py:842]     self.gate_up_proj = MergedColumnParallelLinear(
[1;36m(EngineCore_DP0 pid=368277)[0;0m ERROR 12-04 13:13:59 [core.py:842]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368277)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 631, in __init__
[1;36m(EngineCore_DP0 pid=368277)[0;0m ERROR 12-04 13:13:59 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=368277)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 484, in __init__
[1;36m(EngineCore_DP0 pid=368277)[0;0m ERROR 12-04 13:13:59 [core.py:842]     self.quant_method.create_weights(
[1;36m(EngineCore_DP0 pid=368277)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 215, in create_weights
[1;36m(EngineCore_DP0 pid=368277)[0;0m ERROR 12-04 13:13:59 [core.py:842]     data=torch.empty(
[1;36m(EngineCore_DP0 pid=368277)[0;0m ERROR 12-04 13:13:59 [core.py:842]          ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368277)[0;0m ERROR 12-04 13:13:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_device.py", line 103, in __torch_function__
[1;36m(EngineCore_DP0 pid=368277)[0;0m ERROR 12-04 13:13:59 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=368277)[0;0m ERROR 12-04 13:13:59 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368277)[0;0m ERROR 12-04 13:13:59 [core.py:842] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 340.00 MiB. GPU 0 has a total capacity of 44.39 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 21.87 GiB memory in use. Process 368280 has 22.51 GiB memory in use. Of the allocated memory 21.35 GiB is allocated by PyTorch, and 18.55 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=368277)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=368277)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=368277)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=368277)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=368277)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=368277)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=368277)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=368277)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=368277)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=368277)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368277)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=368277)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=368277)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=368277)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=368277)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368277)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=368277)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=368277)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[1;36m(EngineCore_DP0 pid=368277)[0;0m     self.driver_worker.load_model()
[1;36m(EngineCore_DP0 pid=368277)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 273, in load_model
[1;36m(EngineCore_DP0 pid=368277)[0;0m     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[1;36m(EngineCore_DP0 pid=368277)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3276, in load_model
[1;36m(EngineCore_DP0 pid=368277)[0;0m     self.model = model_loader.load_model(
[1;36m(EngineCore_DP0 pid=368277)[0;0m                  ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368277)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
[1;36m(EngineCore_DP0 pid=368277)[0;0m     model = initialize_model(
[1;36m(EngineCore_DP0 pid=368277)[0;0m             ^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368277)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py", line 55, in initialize_model
[1;36m(EngineCore_DP0 pid=368277)[0;0m     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(EngineCore_DP0 pid=368277)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368277)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 279, in __init__
[1;36m(EngineCore_DP0 pid=368277)[0;0m     self.model = Qwen3Model(
[1;36m(EngineCore_DP0 pid=368277)[0;0m                  ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368277)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=368277)[0;0m     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=368277)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 253, in __init__
[1;36m(EngineCore_DP0 pid=368277)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=368277)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=368277)[0;0m     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=368277)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 337, in __init__
[1;36m(EngineCore_DP0 pid=368277)[0;0m     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(EngineCore_DP0 pid=368277)[0;0m                                                     ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368277)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 651, in make_layers
[1;36m(EngineCore_DP0 pid=368277)[0;0m     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(EngineCore_DP0 pid=368277)[0;0m                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368277)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 339, in <lambda>
[1;36m(EngineCore_DP0 pid=368277)[0;0m     lambda prefix: decoder_layer_type(
[1;36m(EngineCore_DP0 pid=368277)[0;0m                    ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368277)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 201, in __init__
[1;36m(EngineCore_DP0 pid=368277)[0;0m     self.mlp = Qwen3MLP(
[1;36m(EngineCore_DP0 pid=368277)[0;0m                ^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368277)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 84, in __init__
[1;36m(EngineCore_DP0 pid=368277)[0;0m     self.gate_up_proj = MergedColumnParallelLinear(
[1;36m(EngineCore_DP0 pid=368277)[0;0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368277)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 631, in __init__
[1;36m(EngineCore_DP0 pid=368277)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=368277)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 484, in __init__
[1;36m(EngineCore_DP0 pid=368277)[0;0m     self.quant_method.create_weights(
[1;36m(EngineCore_DP0 pid=368277)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 215, in create_weights
[1;36m(EngineCore_DP0 pid=368277)[0;0m     data=torch.empty(
[1;36m(EngineCore_DP0 pid=368277)[0;0m          ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368277)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_device.py", line 103, in __torch_function__
[1;36m(EngineCore_DP0 pid=368277)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=368277)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368277)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 340.00 MiB. GPU 0 has a total capacity of 44.39 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 21.87 GiB memory in use. Process 368280 has 22.51 GiB memory in use. Of the allocated memory 21.35 GiB is allocated by PyTorch, and 18.55 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W1204 13:14:00.193012251 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank0]:[W1204 13:14:00.193139733 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: Qwen/Qwen3-14B (backend=vllm)
[ModelCache] Using max_model_len=40960 for Qwen/Qwen3-14B
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 13:14:22 [utils.py:253] non-default args: {'max_model_len': 40960, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'Qwen/Qwen3-14B'}
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: Qwen/Qwen3-14B (backend=vllm)
[ModelCache] Using max_model_len=40960 for Qwen/Qwen3-14B
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 13:14:22 [utils.py:253] non-default args: {'max_model_len': 40960, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'Qwen/Qwen3-14B'}
INFO 12-04 13:14:22 [model.py:631] Resolved architecture: Qwen3ForCausalLM
INFO 12-04 13:14:22 [model.py:1745] Using max model len 40960
INFO 12-04 13:14:22 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
INFO 12-04 13:14:22 [model.py:631] Resolved architecture: Qwen3ForCausalLM
INFO 12-04 13:14:22 [model.py:1745] Using max model len 40960
INFO 12-04 13:14:22 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=368875)[0;0m INFO 12-04 13:14:41 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='Qwen/Qwen3-14B', speculative_config=None, tokenizer='Qwen/Qwen3-14B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-14B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=368878)[0;0m INFO 12-04 13:14:41 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='Qwen/Qwen3-14B', speculative_config=None, tokenizer='Qwen/Qwen3-14B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-14B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=368875)[0;0m INFO 12-04 13:14:43 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.98:43071 backend=nccl
[1;36m(EngineCore_DP0 pid=368878)[0;0m INFO 12-04 13:14:43 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.98:45031 backend=nccl
[W1204 13:14:43.186549969 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-25-0.rc.tch.harvard.edu]:43071 (errno: 97 - Address family not supported by protocol).
[W1204 13:14:43.192553452 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-25-0.rc.tch.harvard.edu]:45031 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=368878)[0;0m INFO 12-04 13:14:43 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=368875)[0;0m INFO 12-04 13:14:43 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=368878)[0;0m INFO 12-04 13:14:44 [gpu_model_runner.py:3259] Starting to load model Qwen/Qwen3-14B...
[1;36m(EngineCore_DP0 pid=368875)[0;0m INFO 12-04 13:14:44 [gpu_model_runner.py:3259] Starting to load model Qwen/Qwen3-14B...
[1;36m(EngineCore_DP0 pid=368875)[0;0m INFO 12-04 13:14:45 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=368875)[0;0m INFO 12-04 13:14:45 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=368878)[0;0m INFO 12-04 13:14:45 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=368878)[0;0m INFO 12-04 13:14:45 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=368875)[0;0m ERROR 12-04 13:14:46 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=368875)[0;0m ERROR 12-04 13:14:46 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=368875)[0;0m ERROR 12-04 13:14:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=368875)[0;0m ERROR 12-04 13:14:46 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=368875)[0;0m ERROR 12-04 13:14:46 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368875)[0;0m ERROR 12-04 13:14:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=368875)[0;0m ERROR 12-04 13:14:46 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=368875)[0;0m ERROR 12-04 13:14:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=368875)[0;0m ERROR 12-04 13:14:46 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=368875)[0;0m ERROR 12-04 13:14:46 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368875)[0;0m ERROR 12-04 13:14:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=368875)[0;0m ERROR 12-04 13:14:46 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=368875)[0;0m ERROR 12-04 13:14:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[1;36m(EngineCore_DP0 pid=368875)[0;0m ERROR 12-04 13:14:46 [core.py:842]     self.driver_worker.load_model()
[1;36m(EngineCore_DP0 pid=368875)[0;0m ERROR 12-04 13:14:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 273, in load_model
[1;36m(EngineCore_DP0 pid=368875)[0;0m ERROR 12-04 13:14:46 [core.py:842]     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[1;36m(EngineCore_DP0 pid=368875)[0;0m ERROR 12-04 13:14:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3276, in load_model
[1;36m(EngineCore_DP0 pid=368875)[0;0m ERROR 12-04 13:14:46 [core.py:842]     self.model = model_loader.load_model(
[1;36m(EngineCore_DP0 pid=368875)[0;0m ERROR 12-04 13:14:46 [core.py:842]                  ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368875)[0;0m ERROR 12-04 13:14:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
[1;36m(EngineCore_DP0 pid=368875)[0;0m ERROR 12-04 13:14:46 [core.py:842]     model = initialize_model(
[1;36m(EngineCore_DP0 pid=368875)[0;0m ERROR 12-04 13:14:46 [core.py:842]             ^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368875)[0;0m ERROR 12-04 13:14:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py", line 55, in initialize_model
[1;36m(EngineCore_DP0 pid=368875)[0;0m ERROR 12-04 13:14:46 [core.py:842]     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(EngineCore_DP0 pid=368875)[0;0m ERROR 12-04 13:14:46 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368875)[0;0m ERROR 12-04 13:14:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 287, in __init__
[1;36m(EngineCore_DP0 pid=368875)[0;0m ERROR 12-04 13:14:46 [core.py:842]     self.lm_head = ParallelLMHead(
[1;36m(EngineCore_DP0 pid=368875)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=368875)[0;0m ERROR 12-04 13:14:46 [core.py:842]                    ^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368875)[0;0m ERROR 12-04 13:14:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/vocab_parallel_embedding.py", line 523, in __init__
[1;36m(EngineCore_DP0 pid=368875)[0;0m ERROR 12-04 13:14:46 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=368875)[0;0m ERROR 12-04 13:14:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/vocab_parallel_embedding.py", line 301, in __init__
[1;36m(EngineCore_DP0 pid=368875)[0;0m ERROR 12-04 13:14:46 [core.py:842]     self.quant_method.create_weights(
[1;36m(EngineCore_DP0 pid=368875)[0;0m ERROR 12-04 13:14:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/vocab_parallel_embedding.py", line 46, in create_weights
[1;36m(EngineCore_DP0 pid=368875)[0;0m ERROR 12-04 13:14:46 [core.py:842]     torch.empty(
[1;36m(EngineCore_DP0 pid=368875)[0;0m ERROR 12-04 13:14:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_device.py", line 103, in __torch_function__
[1;36m(EngineCore_DP0 pid=368875)[0;0m ERROR 12-04 13:14:46 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=368875)[0;0m ERROR 12-04 13:14:46 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368875)[0;0m ERROR 12-04 13:14:46 [core.py:842] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.45 GiB. GPU 0 has a total capacity of 44.39 GiB of which 154.00 MiB is free. Including non-PyTorch memory, this process has 26.67 GiB memory in use. Process 368878 has 17.56 GiB memory in use. Of the allocated memory 26.15 GiB is allocated by PyTorch, and 18.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=368875)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=368875)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=368875)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=368875)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=368875)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=368875)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=368875)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=368875)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=368875)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=368875)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368875)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=368875)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=368875)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=368875)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=368875)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368875)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=368875)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=368875)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[1;36m(EngineCore_DP0 pid=368875)[0;0m     self.driver_worker.load_model()
[1;36m(EngineCore_DP0 pid=368875)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 273, in load_model
[1;36m(EngineCore_DP0 pid=368875)[0;0m     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[1;36m(EngineCore_DP0 pid=368875)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3276, in load_model
[1;36m(EngineCore_DP0 pid=368875)[0;0m     self.model = model_loader.load_model(
[1;36m(EngineCore_DP0 pid=368875)[0;0m                  ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368875)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
[1;36m(EngineCore_DP0 pid=368875)[0;0m     model = initialize_model(
[1;36m(EngineCore_DP0 pid=368875)[0;0m             ^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368875)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py", line 55, in initialize_model
[1;36m(EngineCore_DP0 pid=368875)[0;0m     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(EngineCore_DP0 pid=368875)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368875)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 287, in __init__
[1;36m(EngineCore_DP0 pid=368875)[0;0m     self.lm_head = ParallelLMHead(
[1;36m(EngineCore_DP0 pid=368875)[0;0m                    ^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368875)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/vocab_parallel_embedding.py", line 523, in __init__
[1;36m(EngineCore_DP0 pid=368875)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=368875)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/vocab_parallel_embedding.py", line 301, in __init__
[1;36m(EngineCore_DP0 pid=368875)[0;0m     self.quant_method.create_weights(
[1;36m(EngineCore_DP0 pid=368875)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/vocab_parallel_embedding.py", line 46, in create_weights
[1;36m(EngineCore_DP0 pid=368875)[0;0m     torch.empty(
[1;36m(EngineCore_DP0 pid=368875)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_device.py", line 103, in __torch_function__
[1;36m(EngineCore_DP0 pid=368875)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=368875)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368875)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.45 GiB. GPU 0 has a total capacity of 44.39 GiB of which 154.00 MiB is free. Including non-PyTorch memory, this process has 26.67 GiB memory in use. Process 368878 has 17.56 GiB memory in use. Of the allocated memory 26.15 GiB is allocated by PyTorch, and 18.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=368878)[0;0m ERROR 12-04 13:14:46 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=368878)[0;0m ERROR 12-04 13:14:46 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=368878)[0;0m ERROR 12-04 13:14:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=368878)[0;0m ERROR 12-04 13:14:46 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=368878)[0;0m ERROR 12-04 13:14:46 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368878)[0;0m ERROR 12-04 13:14:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=368878)[0;0m ERROR 12-04 13:14:46 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=368878)[0;0m ERROR 12-04 13:14:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=368878)[0;0m ERROR 12-04 13:14:46 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=368878)[0;0m ERROR 12-04 13:14:46 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368878)[0;0m ERROR 12-04 13:14:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=368878)[0;0m ERROR 12-04 13:14:46 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=368878)[0;0m ERROR 12-04 13:14:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[1;36m(EngineCore_DP0 pid=368878)[0;0m ERROR 12-04 13:14:46 [core.py:842]     self.driver_worker.load_model()
[1;36m(EngineCore_DP0 pid=368878)[0;0m ERROR 12-04 13:14:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 273, in load_model
[1;36m(EngineCore_DP0 pid=368878)[0;0m ERROR 12-04 13:14:46 [core.py:842]     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[1;36m(EngineCore_DP0 pid=368878)[0;0m ERROR 12-04 13:14:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3276, in load_model
[1;36m(EngineCore_DP0 pid=368878)[0;0m ERROR 12-04 13:14:46 [core.py:842]     self.model = model_loader.load_model(
[1;36m(EngineCore_DP0 pid=368878)[0;0m ERROR 12-04 13:14:46 [core.py:842]                  ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368878)[0;0m ERROR 12-04 13:14:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
[1;36m(EngineCore_DP0 pid=368878)[0;0m ERROR 12-04 13:14:46 [core.py:842]     model = initialize_model(
[1;36m(EngineCore_DP0 pid=368878)[0;0m ERROR 12-04 13:14:46 [core.py:842]             ^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368878)[0;0m ERROR 12-04 13:14:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py", line 55, in initialize_model
[1;36m(EngineCore_DP0 pid=368878)[0;0m ERROR 12-04 13:14:46 [core.py:842]     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(EngineCore_DP0 pid=368878)[0;0m ERROR 12-04 13:14:46 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368878)[0;0m ERROR 12-04 13:14:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 279, in __init__
[1;36m(EngineCore_DP0 pid=368878)[0;0m ERROR 12-04 13:14:46 [core.py:842]     self.model = Qwen3Model(
[1;36m(EngineCore_DP0 pid=368878)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=368878)[0;0m ERROR 12-04 13:14:46 [core.py:842]                  ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368878)[0;0m ERROR 12-04 13:14:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=368878)[0;0m ERROR 12-04 13:14:46 [core.py:842]     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=368878)[0;0m ERROR 12-04 13:14:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 253, in __init__
[1;36m(EngineCore_DP0 pid=368878)[0;0m ERROR 12-04 13:14:46 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=368878)[0;0m ERROR 12-04 13:14:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=368878)[0;0m ERROR 12-04 13:14:46 [core.py:842]     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=368878)[0;0m ERROR 12-04 13:14:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 337, in __init__
[1;36m(EngineCore_DP0 pid=368878)[0;0m ERROR 12-04 13:14:46 [core.py:842]     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(EngineCore_DP0 pid=368878)[0;0m ERROR 12-04 13:14:46 [core.py:842]                                                     ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368878)[0;0m ERROR 12-04 13:14:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 651, in make_layers
[1;36m(EngineCore_DP0 pid=368878)[0;0m ERROR 12-04 13:14:46 [core.py:842]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(EngineCore_DP0 pid=368878)[0;0m ERROR 12-04 13:14:46 [core.py:842]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368878)[0;0m ERROR 12-04 13:14:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 339, in <lambda>
[1;36m(EngineCore_DP0 pid=368878)[0;0m ERROR 12-04 13:14:46 [core.py:842]     lambda prefix: decoder_layer_type(
[1;36m(EngineCore_DP0 pid=368878)[0;0m ERROR 12-04 13:14:46 [core.py:842]                    ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368878)[0;0m ERROR 12-04 13:14:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 201, in __init__
[1;36m(EngineCore_DP0 pid=368878)[0;0m ERROR 12-04 13:14:46 [core.py:842]     self.mlp = Qwen3MLP(
[1;36m(EngineCore_DP0 pid=368878)[0;0m ERROR 12-04 13:14:46 [core.py:842]                ^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368878)[0;0m ERROR 12-04 13:14:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 84, in __init__
[1;36m(EngineCore_DP0 pid=368878)[0;0m ERROR 12-04 13:14:46 [core.py:842]     self.gate_up_proj = MergedColumnParallelLinear(
[1;36m(EngineCore_DP0 pid=368878)[0;0m ERROR 12-04 13:14:46 [core.py:842]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368878)[0;0m ERROR 12-04 13:14:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 631, in __init__
[1;36m(EngineCore_DP0 pid=368878)[0;0m ERROR 12-04 13:14:46 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=368878)[0;0m ERROR 12-04 13:14:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 484, in __init__
[1;36m(EngineCore_DP0 pid=368878)[0;0m ERROR 12-04 13:14:46 [core.py:842]     self.quant_method.create_weights(
[1;36m(EngineCore_DP0 pid=368878)[0;0m ERROR 12-04 13:14:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 215, in create_weights
[1;36m(EngineCore_DP0 pid=368878)[0;0m ERROR 12-04 13:14:46 [core.py:842]     data=torch.empty(
[1;36m(EngineCore_DP0 pid=368878)[0;0m ERROR 12-04 13:14:46 [core.py:842]          ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368878)[0;0m ERROR 12-04 13:14:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_device.py", line 103, in __torch_function__
[1;36m(EngineCore_DP0 pid=368878)[0;0m ERROR 12-04 13:14:46 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=368878)[0;0m ERROR 12-04 13:14:46 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368878)[0;0m ERROR 12-04 13:14:46 [core.py:842] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 340.00 MiB. GPU 0 has a total capacity of 44.39 GiB of which 124.00 MiB is free. Process 368875 has 26.70 GiB memory in use. Including non-PyTorch memory, this process has 17.56 GiB memory in use. Of the allocated memory 17.04 GiB is allocated by PyTorch, and 18.72 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=368878)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=368878)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=368878)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=368878)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=368878)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=368878)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=368878)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=368878)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=368878)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=368878)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368878)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=368878)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=368878)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=368878)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=368878)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368878)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=368878)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=368878)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[1;36m(EngineCore_DP0 pid=368878)[0;0m     self.driver_worker.load_model()
[1;36m(EngineCore_DP0 pid=368878)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 273, in load_model
[1;36m(EngineCore_DP0 pid=368878)[0;0m     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[1;36m(EngineCore_DP0 pid=368878)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3276, in load_model
[1;36m(EngineCore_DP0 pid=368878)[0;0m     self.model = model_loader.load_model(
[1;36m(EngineCore_DP0 pid=368878)[0;0m                  ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368878)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
[1;36m(EngineCore_DP0 pid=368878)[0;0m     model = initialize_model(
[1;36m(EngineCore_DP0 pid=368878)[0;0m             ^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368878)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py", line 55, in initialize_model
[1;36m(EngineCore_DP0 pid=368878)[0;0m     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(EngineCore_DP0 pid=368878)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368878)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 279, in __init__
[1;36m(EngineCore_DP0 pid=368878)[0;0m     self.model = Qwen3Model(
[1;36m(EngineCore_DP0 pid=368878)[0;0m                  ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368878)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=368878)[0;0m     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=368878)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 253, in __init__
[1;36m(EngineCore_DP0 pid=368878)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=368878)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=368878)[0;0m     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=368878)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 337, in __init__
[1;36m(EngineCore_DP0 pid=368878)[0;0m     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(EngineCore_DP0 pid=368878)[0;0m                                                     ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368878)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 651, in make_layers
[1;36m(EngineCore_DP0 pid=368878)[0;0m     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(EngineCore_DP0 pid=368878)[0;0m                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368878)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 339, in <lambda>
[1;36m(EngineCore_DP0 pid=368878)[0;0m     lambda prefix: decoder_layer_type(
[1;36m(EngineCore_DP0 pid=368878)[0;0m                    ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368878)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 201, in __init__
[1;36m(EngineCore_DP0 pid=368878)[0;0m     self.mlp = Qwen3MLP(
[1;36m(EngineCore_DP0 pid=368878)[0;0m                ^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368878)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 84, in __init__
[1;36m(EngineCore_DP0 pid=368878)[0;0m     self.gate_up_proj = MergedColumnParallelLinear(
[1;36m(EngineCore_DP0 pid=368878)[0;0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368878)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 631, in __init__
[1;36m(EngineCore_DP0 pid=368878)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=368878)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 484, in __init__
[1;36m(EngineCore_DP0 pid=368878)[0;0m     self.quant_method.create_weights(
[1;36m(EngineCore_DP0 pid=368878)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 215, in create_weights
[1;36m(EngineCore_DP0 pid=368878)[0;0m     data=torch.empty(
[1;36m(EngineCore_DP0 pid=368878)[0;0m          ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368878)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_device.py", line 103, in __torch_function__
[1;36m(EngineCore_DP0 pid=368878)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=368878)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=368878)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 340.00 MiB. GPU 0 has a total capacity of 44.39 GiB of which 124.00 MiB is free. Process 368875 has 26.70 GiB memory in use. Including non-PyTorch memory, this process has 17.56 GiB memory in use. Of the allocated memory 17.04 GiB is allocated by PyTorch, and 18.72 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W1204 13:14:46.502778896 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank0]:[W1204 13:14:47.519071838 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: Qwen/Qwen3-14B (backend=vllm)
[ModelCache] Using max_model_len=40960 for Qwen/Qwen3-14B
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 13:15:08 [utils.py:253] non-default args: {'max_model_len': 40960, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'Qwen/Qwen3-14B'}
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: Qwen/Qwen3-14B (backend=vllm)
[ModelCache] Using max_model_len=40960 for Qwen/Qwen3-14B
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 13:15:08 [utils.py:253] non-default args: {'max_model_len': 40960, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'Qwen/Qwen3-14B'}
INFO 12-04 13:15:08 [model.py:631] Resolved architecture: Qwen3ForCausalLM
INFO 12-04 13:15:08 [model.py:1745] Using max model len 40960
INFO 12-04 13:15:08 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
INFO 12-04 13:15:08 [model.py:631] Resolved architecture: Qwen3ForCausalLM
INFO 12-04 13:15:08 [model.py:1745] Using max model len 40960
INFO 12-04 13:15:08 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=369382)[0;0m INFO 12-04 13:15:25 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='Qwen/Qwen3-14B', speculative_config=None, tokenizer='Qwen/Qwen3-14B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-14B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=369379)[0;0m INFO 12-04 13:15:25 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='Qwen/Qwen3-14B', speculative_config=None, tokenizer='Qwen/Qwen3-14B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-14B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=369382)[0;0m INFO 12-04 13:15:27 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.98:58183 backend=nccl
[W1204 13:15:27.829616149 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-25-0.rc.tch.harvard.edu]:58183 (errno: 97 - Address family not supported by protocol).
[1;36m(EngineCore_DP0 pid=369379)[0;0m INFO 12-04 13:15:27 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.98:55177 backend=nccl
[W1204 13:15:27.846753421 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-25-0.rc.tch.harvard.edu]:55177 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=369382)[0;0m INFO 12-04 13:15:27 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=369379)[0;0m INFO 12-04 13:15:27 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=369379)[0;0m INFO 12-04 13:15:27 [gpu_model_runner.py:3259] Starting to load model Qwen/Qwen3-14B...
[1;36m(EngineCore_DP0 pid=369382)[0;0m INFO 12-04 13:15:27 [gpu_model_runner.py:3259] Starting to load model Qwen/Qwen3-14B...
[1;36m(EngineCore_DP0 pid=369382)[0;0m INFO 12-04 13:15:29 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=369382)[0;0m INFO 12-04 13:15:29 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=369379)[0;0m INFO 12-04 13:15:29 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=369379)[0;0m INFO 12-04 13:15:29 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=369379)[0;0m ERROR 12-04 13:15:30 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=369379)[0;0m ERROR 12-04 13:15:30 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=369379)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=369379)[0;0m ERROR 12-04 13:15:30 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=369379)[0;0m ERROR 12-04 13:15:30 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369379)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=369379)[0;0m ERROR 12-04 13:15:30 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=369379)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=369379)[0;0m ERROR 12-04 13:15:30 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=369379)[0;0m ERROR 12-04 13:15:30 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369379)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=369379)[0;0m ERROR 12-04 13:15:30 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=369379)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[1;36m(EngineCore_DP0 pid=369379)[0;0m ERROR 12-04 13:15:30 [core.py:842]     self.driver_worker.load_model()
[1;36m(EngineCore_DP0 pid=369379)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 273, in load_model
[1;36m(EngineCore_DP0 pid=369379)[0;0m ERROR 12-04 13:15:30 [core.py:842]     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[1;36m(EngineCore_DP0 pid=369379)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3276, in load_model
[1;36m(EngineCore_DP0 pid=369379)[0;0m ERROR 12-04 13:15:30 [core.py:842]     self.model = model_loader.load_model(
[1;36m(EngineCore_DP0 pid=369379)[0;0m ERROR 12-04 13:15:30 [core.py:842]                  ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369379)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
[1;36m(EngineCore_DP0 pid=369379)[0;0m ERROR 12-04 13:15:30 [core.py:842]     model = initialize_model(
[1;36m(EngineCore_DP0 pid=369379)[0;0m ERROR 12-04 13:15:30 [core.py:842]             ^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369379)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py", line 55, in initialize_model
[1;36m(EngineCore_DP0 pid=369379)[0;0m ERROR 12-04 13:15:30 [core.py:842]     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(EngineCore_DP0 pid=369379)[0;0m ERROR 12-04 13:15:30 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369379)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 279, in __init__
[1;36m(EngineCore_DP0 pid=369379)[0;0m ERROR 12-04 13:15:30 [core.py:842]     self.model = Qwen3Model(
[1;36m(EngineCore_DP0 pid=369379)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=369379)[0;0m ERROR 12-04 13:15:30 [core.py:842]                  ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369379)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=369379)[0;0m ERROR 12-04 13:15:30 [core.py:842]     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=369379)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 253, in __init__
[1;36m(EngineCore_DP0 pid=369379)[0;0m ERROR 12-04 13:15:30 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=369379)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=369379)[0;0m ERROR 12-04 13:15:30 [core.py:842]     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=369379)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 337, in __init__
[1;36m(EngineCore_DP0 pid=369379)[0;0m ERROR 12-04 13:15:30 [core.py:842]     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(EngineCore_DP0 pid=369379)[0;0m ERROR 12-04 13:15:30 [core.py:842]                                                     ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369379)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 651, in make_layers
[1;36m(EngineCore_DP0 pid=369379)[0;0m ERROR 12-04 13:15:30 [core.py:842]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(EngineCore_DP0 pid=369379)[0;0m ERROR 12-04 13:15:30 [core.py:842]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369379)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 339, in <lambda>
[1;36m(EngineCore_DP0 pid=369379)[0;0m ERROR 12-04 13:15:30 [core.py:842]     lambda prefix: decoder_layer_type(
[1;36m(EngineCore_DP0 pid=369379)[0;0m ERROR 12-04 13:15:30 [core.py:842]                    ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369379)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 201, in __init__
[1;36m(EngineCore_DP0 pid=369379)[0;0m ERROR 12-04 13:15:30 [core.py:842]     self.mlp = Qwen3MLP(
[1;36m(EngineCore_DP0 pid=369379)[0;0m ERROR 12-04 13:15:30 [core.py:842]                ^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369379)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 84, in __init__
[1;36m(EngineCore_DP0 pid=369379)[0;0m ERROR 12-04 13:15:30 [core.py:842]     self.gate_up_proj = MergedColumnParallelLinear(
[1;36m(EngineCore_DP0 pid=369379)[0;0m ERROR 12-04 13:15:30 [core.py:842]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369379)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 631, in __init__
[1;36m(EngineCore_DP0 pid=369379)[0;0m ERROR 12-04 13:15:30 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=369379)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 484, in __init__
[1;36m(EngineCore_DP0 pid=369379)[0;0m ERROR 12-04 13:15:30 [core.py:842]     self.quant_method.create_weights(
[1;36m(EngineCore_DP0 pid=369379)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 215, in create_weights
[1;36m(EngineCore_DP0 pid=369379)[0;0m ERROR 12-04 13:15:30 [core.py:842]     data=torch.empty(
[1;36m(EngineCore_DP0 pid=369379)[0;0m ERROR 12-04 13:15:30 [core.py:842]          ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369379)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_device.py", line 103, in __torch_function__
[1;36m(EngineCore_DP0 pid=369379)[0;0m ERROR 12-04 13:15:30 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=369379)[0;0m ERROR 12-04 13:15:30 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369379)[0;0m ERROR 12-04 13:15:30 [core.py:842] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 340.00 MiB. GPU 0 has a total capacity of 44.39 GiB of which 34.00 MiB is free. Process 369382 has 22.48 GiB memory in use. Including non-PyTorch memory, this process has 21.87 GiB memory in use. Of the allocated memory 21.35 GiB is allocated by PyTorch, and 18.55 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=369379)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=369379)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=369379)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=369379)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=369379)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=369379)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=369379)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=369379)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=369379)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=369379)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369379)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=369379)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=369379)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=369379)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=369379)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369379)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=369379)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=369379)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[1;36m(EngineCore_DP0 pid=369379)[0;0m     self.driver_worker.load_model()
[1;36m(EngineCore_DP0 pid=369379)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 273, in load_model
[1;36m(EngineCore_DP0 pid=369379)[0;0m     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[1;36m(EngineCore_DP0 pid=369379)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3276, in load_model
[1;36m(EngineCore_DP0 pid=369379)[0;0m     self.model = model_loader.load_model(
[1;36m(EngineCore_DP0 pid=369379)[0;0m                  ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369379)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
[1;36m(EngineCore_DP0 pid=369379)[0;0m     model = initialize_model(
[1;36m(EngineCore_DP0 pid=369379)[0;0m             ^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369379)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py", line 55, in initialize_model
[1;36m(EngineCore_DP0 pid=369379)[0;0m     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(EngineCore_DP0 pid=369379)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369379)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 279, in __init__
[1;36m(EngineCore_DP0 pid=369379)[0;0m     self.model = Qwen3Model(
[1;36m(EngineCore_DP0 pid=369379)[0;0m                  ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369379)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=369379)[0;0m     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=369379)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 253, in __init__
[1;36m(EngineCore_DP0 pid=369379)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=369379)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=369379)[0;0m     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=369379)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 337, in __init__
[1;36m(EngineCore_DP0 pid=369379)[0;0m     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(EngineCore_DP0 pid=369379)[0;0m                                                     ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369379)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 651, in make_layers
[1;36m(EngineCore_DP0 pid=369379)[0;0m     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(EngineCore_DP0 pid=369379)[0;0m                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369379)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 339, in <lambda>
[1;36m(EngineCore_DP0 pid=369379)[0;0m     lambda prefix: decoder_layer_type(
[1;36m(EngineCore_DP0 pid=369379)[0;0m                    ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369379)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 201, in __init__
[1;36m(EngineCore_DP0 pid=369379)[0;0m     self.mlp = Qwen3MLP(
[1;36m(EngineCore_DP0 pid=369379)[0;0m                ^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369379)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 84, in __init__
[1;36m(EngineCore_DP0 pid=369379)[0;0m     self.gate_up_proj = MergedColumnParallelLinear(
[1;36m(EngineCore_DP0 pid=369379)[0;0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369379)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 631, in __init__
[1;36m(EngineCore_DP0 pid=369379)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=369379)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 484, in __init__
[1;36m(EngineCore_DP0 pid=369379)[0;0m     self.quant_method.create_weights(
[1;36m(EngineCore_DP0 pid=369379)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 215, in create_weights
[1;36m(EngineCore_DP0 pid=369379)[0;0m     data=torch.empty(
[1;36m(EngineCore_DP0 pid=369379)[0;0m          ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369379)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_device.py", line 103, in __torch_function__
[1;36m(EngineCore_DP0 pid=369379)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=369379)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369379)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 340.00 MiB. GPU 0 has a total capacity of 44.39 GiB of which 34.00 MiB is free. Process 369382 has 22.48 GiB memory in use. Including non-PyTorch memory, this process has 21.87 GiB memory in use. Of the allocated memory 21.35 GiB is allocated by PyTorch, and 18.55 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=369382)[0;0m ERROR 12-04 13:15:30 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=369382)[0;0m ERROR 12-04 13:15:30 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=369382)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=369382)[0;0m ERROR 12-04 13:15:30 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=369382)[0;0m ERROR 12-04 13:15:30 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369382)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=369382)[0;0m ERROR 12-04 13:15:30 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=369382)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=369382)[0;0m ERROR 12-04 13:15:30 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=369382)[0;0m ERROR 12-04 13:15:30 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369382)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=369382)[0;0m ERROR 12-04 13:15:30 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=369382)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[1;36m(EngineCore_DP0 pid=369382)[0;0m ERROR 12-04 13:15:30 [core.py:842]     self.driver_worker.load_model()
[1;36m(EngineCore_DP0 pid=369382)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 273, in load_model
[1;36m(EngineCore_DP0 pid=369382)[0;0m ERROR 12-04 13:15:30 [core.py:842]     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[1;36m(EngineCore_DP0 pid=369382)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3276, in load_model
[1;36m(EngineCore_DP0 pid=369382)[0;0m ERROR 12-04 13:15:30 [core.py:842]     self.model = model_loader.load_model(
[1;36m(EngineCore_DP0 pid=369382)[0;0m ERROR 12-04 13:15:30 [core.py:842]                  ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369382)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
[1;36m(EngineCore_DP0 pid=369382)[0;0m ERROR 12-04 13:15:30 [core.py:842]     model = initialize_model(
[1;36m(EngineCore_DP0 pid=369382)[0;0m ERROR 12-04 13:15:30 [core.py:842]             ^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369382)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py", line 55, in initialize_model
[1;36m(EngineCore_DP0 pid=369382)[0;0m ERROR 12-04 13:15:30 [core.py:842]     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(EngineCore_DP0 pid=369382)[0;0m ERROR 12-04 13:15:30 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369382)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 279, in __init__
[1;36m(EngineCore_DP0 pid=369382)[0;0m ERROR 12-04 13:15:30 [core.py:842]     self.model = Qwen3Model(
[1;36m(EngineCore_DP0 pid=369382)[0;0m ERROR 12-04 13:15:30 [core.py:842]                  ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369382)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=369382)[0;0m ERROR 12-04 13:15:30 [core.py:842]     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=369382)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 253, in __init__
[1;36m(EngineCore_DP0 pid=369382)[0;0m ERROR 12-04 13:15:30 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=369382)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=369382)[0;0m ERROR 12-04 13:15:30 [core.py:842]     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=369382)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 337, in __init__
[1;36m(EngineCore_DP0 pid=369382)[0;0m ERROR 12-04 13:15:30 [core.py:842]     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(EngineCore_DP0 pid=369382)[0;0m ERROR 12-04 13:15:30 [core.py:842]                                                     ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369382)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 651, in make_layers
[1;36m(EngineCore_DP0 pid=369382)[0;0m ERROR 12-04 13:15:30 [core.py:842]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(EngineCore_DP0 pid=369382)[0;0m ERROR 12-04 13:15:30 [core.py:842]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369382)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 339, in <lambda>
[1;36m(EngineCore_DP0 pid=369382)[0;0m ERROR 12-04 13:15:30 [core.py:842]     lambda prefix: decoder_layer_type(
[1;36m(EngineCore_DP0 pid=369382)[0;0m ERROR 12-04 13:15:30 [core.py:842]                    ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369382)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 201, in __init__
[1;36m(EngineCore_DP0 pid=369382)[0;0m ERROR 12-04 13:15:30 [core.py:842]     self.mlp = Qwen3MLP(
[1;36m(EngineCore_DP0 pid=369382)[0;0m ERROR 12-04 13:15:30 [core.py:842]                ^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369382)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 84, in __init__
[1;36m(EngineCore_DP0 pid=369382)[0;0m ERROR 12-04 13:15:30 [core.py:842]     self.gate_up_proj = MergedColumnParallelLinear(
[1;36m(EngineCore_DP0 pid=369382)[0;0m ERROR 12-04 13:15:30 [core.py:842]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369382)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 631, in __init__
[1;36m(EngineCore_DP0 pid=369382)[0;0m ERROR 12-04 13:15:30 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=369382)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 484, in __init__
[1;36m(EngineCore_DP0 pid=369382)[0;0m ERROR 12-04 13:15:30 [core.py:842]     self.quant_method.create_weights(
[1;36m(EngineCore_DP0 pid=369382)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=369382)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 215, in create_weights
[1;36m(EngineCore_DP0 pid=369382)[0;0m ERROR 12-04 13:15:30 [core.py:842]     data=torch.empty(
[1;36m(EngineCore_DP0 pid=369382)[0;0m ERROR 12-04 13:15:30 [core.py:842]          ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369382)[0;0m ERROR 12-04 13:15:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_device.py", line 103, in __torch_function__
[1;36m(EngineCore_DP0 pid=369382)[0;0m ERROR 12-04 13:15:30 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=369382)[0;0m ERROR 12-04 13:15:30 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369382)[0;0m ERROR 12-04 13:15:30 [core.py:842] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 340.00 MiB. GPU 0 has a total capacity of 44.39 GiB of which 54.00 MiB is free. Including non-PyTorch memory, this process has 22.48 GiB memory in use. Process 369379 has 21.89 GiB memory in use. Of the allocated memory 21.96 GiB is allocated by PyTorch, and 18.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=369382)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=369382)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=369382)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=369382)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=369382)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=369382)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=369382)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=369382)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=369382)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=369382)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369382)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=369382)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=369382)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=369382)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=369382)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369382)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=369382)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=369382)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[1;36m(EngineCore_DP0 pid=369382)[0;0m     self.driver_worker.load_model()
[1;36m(EngineCore_DP0 pid=369382)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 273, in load_model
[1;36m(EngineCore_DP0 pid=369382)[0;0m     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[1;36m(EngineCore_DP0 pid=369382)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3276, in load_model
[1;36m(EngineCore_DP0 pid=369382)[0;0m     self.model = model_loader.load_model(
[1;36m(EngineCore_DP0 pid=369382)[0;0m                  ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369382)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
[1;36m(EngineCore_DP0 pid=369382)[0;0m     model = initialize_model(
[1;36m(EngineCore_DP0 pid=369382)[0;0m             ^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369382)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py", line 55, in initialize_model
[1;36m(EngineCore_DP0 pid=369382)[0;0m     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(EngineCore_DP0 pid=369382)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369382)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 279, in __init__
[1;36m(EngineCore_DP0 pid=369382)[0;0m     self.model = Qwen3Model(
[1;36m(EngineCore_DP0 pid=369382)[0;0m                  ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369382)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=369382)[0;0m     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=369382)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 253, in __init__
[1;36m(EngineCore_DP0 pid=369382)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=369382)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=369382)[0;0m     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=369382)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 337, in __init__
[1;36m(EngineCore_DP0 pid=369382)[0;0m     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(EngineCore_DP0 pid=369382)[0;0m                                                     ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369382)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 651, in make_layers
[1;36m(EngineCore_DP0 pid=369382)[0;0m     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(EngineCore_DP0 pid=369382)[0;0m                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369382)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 339, in <lambda>
[1;36m(EngineCore_DP0 pid=369382)[0;0m     lambda prefix: decoder_layer_type(
[1;36m(EngineCore_DP0 pid=369382)[0;0m                    ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369382)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 201, in __init__
[1;36m(EngineCore_DP0 pid=369382)[0;0m     self.mlp = Qwen3MLP(
[1;36m(EngineCore_DP0 pid=369382)[0;0m                ^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369382)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 84, in __init__
[1;36m(EngineCore_DP0 pid=369382)[0;0m     self.gate_up_proj = MergedColumnParallelLinear(
[1;36m(EngineCore_DP0 pid=369382)[0;0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369382)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 631, in __init__
[1;36m(EngineCore_DP0 pid=369382)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=369382)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 484, in __init__
[1;36m(EngineCore_DP0 pid=369382)[0;0m     self.quant_method.create_weights(
[1;36m(EngineCore_DP0 pid=369382)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 215, in create_weights
[1;36m(EngineCore_DP0 pid=369382)[0;0m     data=torch.empty(
[1;36m(EngineCore_DP0 pid=369382)[0;0m          ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369382)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_device.py", line 103, in __torch_function__
[1;36m(EngineCore_DP0 pid=369382)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=369382)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369382)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 340.00 MiB. GPU 0 has a total capacity of 44.39 GiB of which 54.00 MiB is free. Including non-PyTorch memory, this process has 22.48 GiB memory in use. Process 369379 has 21.89 GiB memory in use. Of the allocated memory 21.96 GiB is allocated by PyTorch, and 18.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W1204 13:15:30.342894129 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank0]:[W1204 13:15:30.342890119 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: Qwen/Qwen3-14B (backend=vllm)
[ModelCache] Using max_model_len=40960 for Qwen/Qwen3-14B
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 13:15:52 [utils.py:253] non-default args: {'max_model_len': 40960, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'Qwen/Qwen3-14B'}
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: Qwen/Qwen3-14B (backend=vllm)
[ModelCache] Using max_model_len=40960 for Qwen/Qwen3-14B
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 13:15:52 [utils.py:253] non-default args: {'max_model_len': 40960, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'Qwen/Qwen3-14B'}
INFO 12-04 13:15:52 [model.py:631] Resolved architecture: Qwen3ForCausalLM
INFO 12-04 13:15:52 [model.py:1745] Using max model len 40960
INFO 12-04 13:15:52 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
INFO 12-04 13:15:52 [model.py:631] Resolved architecture: Qwen3ForCausalLM
INFO 12-04 13:15:52 [model.py:1745] Using max model len 40960
INFO 12-04 13:15:52 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=369896)[0;0m INFO 12-04 13:16:11 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='Qwen/Qwen3-14B', speculative_config=None, tokenizer='Qwen/Qwen3-14B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-14B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=369887)[0;0m INFO 12-04 13:16:11 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='Qwen/Qwen3-14B', speculative_config=None, tokenizer='Qwen/Qwen3-14B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-14B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=369896)[0;0m INFO 12-04 13:16:13 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.98:36737 backend=nccl
[1;36m(EngineCore_DP0 pid=369887)[0;0m INFO 12-04 13:16:13 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.98:39249 backend=nccl
[W1204 13:16:13.037482747 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-25-0.rc.tch.harvard.edu]:36737 (errno: 97 - Address family not supported by protocol).
[W1204 13:16:13.038501112 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-25-0.rc.tch.harvard.edu]:39249 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=369887)[0;0m INFO 12-04 13:16:13 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=369896)[0;0m INFO 12-04 13:16:13 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=369887)[0;0m INFO 12-04 13:16:14 [gpu_model_runner.py:3259] Starting to load model Qwen/Qwen3-14B...
[1;36m(EngineCore_DP0 pid=369896)[0;0m INFO 12-04 13:16:14 [gpu_model_runner.py:3259] Starting to load model Qwen/Qwen3-14B...
[1;36m(EngineCore_DP0 pid=369887)[0;0m INFO 12-04 13:16:15 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=369887)[0;0m INFO 12-04 13:16:15 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=369896)[0;0m INFO 12-04 13:16:15 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=369896)[0;0m INFO 12-04 13:16:15 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=369896)[0;0m ERROR 12-04 13:16:16 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=369896)[0;0m ERROR 12-04 13:16:16 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=369896)[0;0m ERROR 12-04 13:16:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=369896)[0;0m ERROR 12-04 13:16:16 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=369896)[0;0m ERROR 12-04 13:16:16 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369896)[0;0m ERROR 12-04 13:16:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=369896)[0;0m ERROR 12-04 13:16:16 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=369896)[0;0m ERROR 12-04 13:16:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=369896)[0;0m ERROR 12-04 13:16:16 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=369896)[0;0m ERROR 12-04 13:16:16 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369896)[0;0m ERROR 12-04 13:16:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=369896)[0;0m ERROR 12-04 13:16:16 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=369896)[0;0m ERROR 12-04 13:16:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[1;36m(EngineCore_DP0 pid=369896)[0;0m ERROR 12-04 13:16:16 [core.py:842]     self.driver_worker.load_model()
[1;36m(EngineCore_DP0 pid=369896)[0;0m ERROR 12-04 13:16:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 273, in load_model
[1;36m(EngineCore_DP0 pid=369896)[0;0m ERROR 12-04 13:16:16 [core.py:842]     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[1;36m(EngineCore_DP0 pid=369896)[0;0m ERROR 12-04 13:16:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3276, in load_model
[1;36m(EngineCore_DP0 pid=369896)[0;0m ERROR 12-04 13:16:16 [core.py:842]     self.model = model_loader.load_model(
[1;36m(EngineCore_DP0 pid=369896)[0;0m ERROR 12-04 13:16:16 [core.py:842]                  ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369896)[0;0m ERROR 12-04 13:16:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
[1;36m(EngineCore_DP0 pid=369896)[0;0m ERROR 12-04 13:16:16 [core.py:842]     model = initialize_model(
[1;36m(EngineCore_DP0 pid=369896)[0;0m ERROR 12-04 13:16:16 [core.py:842]             ^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369896)[0;0m ERROR 12-04 13:16:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py", line 55, in initialize_model
[1;36m(EngineCore_DP0 pid=369896)[0;0m ERROR 12-04 13:16:16 [core.py:842]     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(EngineCore_DP0 pid=369896)[0;0m ERROR 12-04 13:16:16 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369896)[0;0m ERROR 12-04 13:16:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 279, in __init__
[1;36m(EngineCore_DP0 pid=369896)[0;0m ERROR 12-04 13:16:16 [core.py:842]     self.model = Qwen3Model(
[1;36m(EngineCore_DP0 pid=369896)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=369896)[0;0m ERROR 12-04 13:16:16 [core.py:842]                  ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369896)[0;0m ERROR 12-04 13:16:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=369896)[0;0m ERROR 12-04 13:16:16 [core.py:842]     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=369896)[0;0m ERROR 12-04 13:16:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 253, in __init__
[1;36m(EngineCore_DP0 pid=369896)[0;0m ERROR 12-04 13:16:16 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=369896)[0;0m ERROR 12-04 13:16:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=369896)[0;0m ERROR 12-04 13:16:16 [core.py:842]     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=369896)[0;0m ERROR 12-04 13:16:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 337, in __init__
[1;36m(EngineCore_DP0 pid=369896)[0;0m ERROR 12-04 13:16:16 [core.py:842]     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(EngineCore_DP0 pid=369896)[0;0m ERROR 12-04 13:16:16 [core.py:842]                                                     ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369896)[0;0m ERROR 12-04 13:16:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 651, in make_layers
[1;36m(EngineCore_DP0 pid=369896)[0;0m ERROR 12-04 13:16:16 [core.py:842]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(EngineCore_DP0 pid=369896)[0;0m ERROR 12-04 13:16:16 [core.py:842]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369896)[0;0m ERROR 12-04 13:16:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 339, in <lambda>
[1;36m(EngineCore_DP0 pid=369896)[0;0m ERROR 12-04 13:16:16 [core.py:842]     lambda prefix: decoder_layer_type(
[1;36m(EngineCore_DP0 pid=369896)[0;0m ERROR 12-04 13:16:16 [core.py:842]                    ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369896)[0;0m ERROR 12-04 13:16:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 201, in __init__
[1;36m(EngineCore_DP0 pid=369896)[0;0m ERROR 12-04 13:16:16 [core.py:842]     self.mlp = Qwen3MLP(
[1;36m(EngineCore_DP0 pid=369896)[0;0m ERROR 12-04 13:16:16 [core.py:842]                ^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369896)[0;0m ERROR 12-04 13:16:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 84, in __init__
[1;36m(EngineCore_DP0 pid=369896)[0;0m ERROR 12-04 13:16:16 [core.py:842]     self.gate_up_proj = MergedColumnParallelLinear(
[1;36m(EngineCore_DP0 pid=369896)[0;0m ERROR 12-04 13:16:16 [core.py:842]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369896)[0;0m ERROR 12-04 13:16:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 631, in __init__
[1;36m(EngineCore_DP0 pid=369896)[0;0m ERROR 12-04 13:16:16 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=369896)[0;0m ERROR 12-04 13:16:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 484, in __init__
[1;36m(EngineCore_DP0 pid=369896)[0;0m ERROR 12-04 13:16:16 [core.py:842]     self.quant_method.create_weights(
[1;36m(EngineCore_DP0 pid=369896)[0;0m ERROR 12-04 13:16:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 215, in create_weights
[1;36m(EngineCore_DP0 pid=369896)[0;0m ERROR 12-04 13:16:16 [core.py:842]     data=torch.empty(
[1;36m(EngineCore_DP0 pid=369896)[0;0m ERROR 12-04 13:16:16 [core.py:842]          ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369896)[0;0m ERROR 12-04 13:16:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_device.py", line 103, in __torch_function__
[1;36m(EngineCore_DP0 pid=369896)[0;0m ERROR 12-04 13:16:16 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=369896)[0;0m ERROR 12-04 13:16:16 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369896)[0;0m ERROR 12-04 13:16:16 [core.py:842] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 340.00 MiB. GPU 0 has a total capacity of 44.39 GiB of which 34.00 MiB is free. Including non-PyTorch memory, this process has 20.63 GiB memory in use. Process 369887 has 23.71 GiB memory in use. Of the allocated memory 20.11 GiB is allocated by PyTorch, and 18.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=369896)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=369896)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=369896)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=369896)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=369896)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=369896)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=369896)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=369896)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=369896)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=369896)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369896)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=369896)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=369896)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=369896)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=369896)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369896)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=369896)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=369896)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[1;36m(EngineCore_DP0 pid=369896)[0;0m     self.driver_worker.load_model()
[1;36m(EngineCore_DP0 pid=369896)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 273, in load_model
[1;36m(EngineCore_DP0 pid=369896)[0;0m     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[1;36m(EngineCore_DP0 pid=369896)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3276, in load_model
[1;36m(EngineCore_DP0 pid=369896)[0;0m     self.model = model_loader.load_model(
[1;36m(EngineCore_DP0 pid=369896)[0;0m                  ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369896)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
[1;36m(EngineCore_DP0 pid=369896)[0;0m     model = initialize_model(
[1;36m(EngineCore_DP0 pid=369896)[0;0m             ^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369896)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py", line 55, in initialize_model
[1;36m(EngineCore_DP0 pid=369896)[0;0m     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(EngineCore_DP0 pid=369896)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369896)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 279, in __init__
[1;36m(EngineCore_DP0 pid=369896)[0;0m     self.model = Qwen3Model(
[1;36m(EngineCore_DP0 pid=369896)[0;0m                  ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369896)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=369896)[0;0m     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=369896)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 253, in __init__
[1;36m(EngineCore_DP0 pid=369896)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=369896)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=369896)[0;0m     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=369896)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 337, in __init__
[1;36m(EngineCore_DP0 pid=369896)[0;0m     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(EngineCore_DP0 pid=369896)[0;0m                                                     ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369896)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 651, in make_layers
[1;36m(EngineCore_DP0 pid=369896)[0;0m     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(EngineCore_DP0 pid=369896)[0;0m                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369896)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 339, in <lambda>
[1;36m(EngineCore_DP0 pid=369896)[0;0m     lambda prefix: decoder_layer_type(
[1;36m(EngineCore_DP0 pid=369896)[0;0m                    ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369896)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 201, in __init__
[1;36m(EngineCore_DP0 pid=369896)[0;0m     self.mlp = Qwen3MLP(
[1;36m(EngineCore_DP0 pid=369896)[0;0m                ^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369896)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 84, in __init__
[1;36m(EngineCore_DP0 pid=369896)[0;0m     self.gate_up_proj = MergedColumnParallelLinear(
[1;36m(EngineCore_DP0 pid=369896)[0;0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369896)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 631, in __init__
[1;36m(EngineCore_DP0 pid=369896)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=369896)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 484, in __init__
[1;36m(EngineCore_DP0 pid=369896)[0;0m     self.quant_method.create_weights(
[1;36m(EngineCore_DP0 pid=369896)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 215, in create_weights
[1;36m(EngineCore_DP0 pid=369896)[0;0m     data=torch.empty(
[1;36m(EngineCore_DP0 pid=369896)[0;0m          ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369896)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_device.py", line 103, in __torch_function__
[1;36m(EngineCore_DP0 pid=369896)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=369896)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369896)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 340.00 MiB. GPU 0 has a total capacity of 44.39 GiB of which 34.00 MiB is free. Including non-PyTorch memory, this process has 20.63 GiB memory in use. Process 369887 has 23.71 GiB memory in use. Of the allocated memory 20.11 GiB is allocated by PyTorch, and 18.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=369887)[0;0m ERROR 12-04 13:16:16 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=369887)[0;0m ERROR 12-04 13:16:16 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=369887)[0;0m ERROR 12-04 13:16:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=369887)[0;0m ERROR 12-04 13:16:16 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=369887)[0;0m ERROR 12-04 13:16:16 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369887)[0;0m ERROR 12-04 13:16:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=369887)[0;0m ERROR 12-04 13:16:16 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=369887)[0;0m ERROR 12-04 13:16:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=369887)[0;0m ERROR 12-04 13:16:16 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=369887)[0;0m ERROR 12-04 13:16:16 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369887)[0;0m ERROR 12-04 13:16:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=369887)[0;0m ERROR 12-04 13:16:16 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=369887)[0;0m ERROR 12-04 13:16:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[1;36m(EngineCore_DP0 pid=369887)[0;0m ERROR 12-04 13:16:16 [core.py:842]     self.driver_worker.load_model()
[1;36m(EngineCore_DP0 pid=369887)[0;0m ERROR 12-04 13:16:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 273, in load_model
[1;36m(EngineCore_DP0 pid=369887)[0;0m ERROR 12-04 13:16:16 [core.py:842]     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[1;36m(EngineCore_DP0 pid=369887)[0;0m ERROR 12-04 13:16:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3276, in load_model
[1;36m(EngineCore_DP0 pid=369887)[0;0m ERROR 12-04 13:16:16 [core.py:842]     self.model = model_loader.load_model(
[1;36m(EngineCore_DP0 pid=369887)[0;0m ERROR 12-04 13:16:16 [core.py:842]                  ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369887)[0;0m ERROR 12-04 13:16:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
[1;36m(EngineCore_DP0 pid=369887)[0;0m ERROR 12-04 13:16:16 [core.py:842]     model = initialize_model(
[1;36m(EngineCore_DP0 pid=369887)[0;0m ERROR 12-04 13:16:16 [core.py:842]             ^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369887)[0;0m ERROR 12-04 13:16:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py", line 55, in initialize_model
[1;36m(EngineCore_DP0 pid=369887)[0;0m ERROR 12-04 13:16:16 [core.py:842]     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(EngineCore_DP0 pid=369887)[0;0m ERROR 12-04 13:16:16 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369887)[0;0m ERROR 12-04 13:16:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 279, in __init__
[1;36m(EngineCore_DP0 pid=369887)[0;0m ERROR 12-04 13:16:16 [core.py:842]     self.model = Qwen3Model(
[1;36m(EngineCore_DP0 pid=369887)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=369887)[0;0m ERROR 12-04 13:16:16 [core.py:842]                  ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369887)[0;0m ERROR 12-04 13:16:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=369887)[0;0m ERROR 12-04 13:16:16 [core.py:842]     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=369887)[0;0m ERROR 12-04 13:16:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 253, in __init__
[1;36m(EngineCore_DP0 pid=369887)[0;0m ERROR 12-04 13:16:16 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=369887)[0;0m ERROR 12-04 13:16:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=369887)[0;0m ERROR 12-04 13:16:16 [core.py:842]     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=369887)[0;0m ERROR 12-04 13:16:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 337, in __init__
[1;36m(EngineCore_DP0 pid=369887)[0;0m ERROR 12-04 13:16:16 [core.py:842]     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(EngineCore_DP0 pid=369887)[0;0m ERROR 12-04 13:16:16 [core.py:842]                                                     ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369887)[0;0m ERROR 12-04 13:16:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 651, in make_layers
[1;36m(EngineCore_DP0 pid=369887)[0;0m ERROR 12-04 13:16:16 [core.py:842]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(EngineCore_DP0 pid=369887)[0;0m ERROR 12-04 13:16:16 [core.py:842]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369887)[0;0m ERROR 12-04 13:16:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 339, in <lambda>
[1;36m(EngineCore_DP0 pid=369887)[0;0m ERROR 12-04 13:16:16 [core.py:842]     lambda prefix: decoder_layer_type(
[1;36m(EngineCore_DP0 pid=369887)[0;0m ERROR 12-04 13:16:16 [core.py:842]                    ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369887)[0;0m ERROR 12-04 13:16:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 201, in __init__
[1;36m(EngineCore_DP0 pid=369887)[0;0m ERROR 12-04 13:16:16 [core.py:842]     self.mlp = Qwen3MLP(
[1;36m(EngineCore_DP0 pid=369887)[0;0m ERROR 12-04 13:16:16 [core.py:842]                ^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369887)[0;0m ERROR 12-04 13:16:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 84, in __init__
[1;36m(EngineCore_DP0 pid=369887)[0;0m ERROR 12-04 13:16:16 [core.py:842]     self.gate_up_proj = MergedColumnParallelLinear(
[1;36m(EngineCore_DP0 pid=369887)[0;0m ERROR 12-04 13:16:16 [core.py:842]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369887)[0;0m ERROR 12-04 13:16:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 631, in __init__
[1;36m(EngineCore_DP0 pid=369887)[0;0m ERROR 12-04 13:16:16 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=369887)[0;0m ERROR 12-04 13:16:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 484, in __init__
[1;36m(EngineCore_DP0 pid=369887)[0;0m ERROR 12-04 13:16:16 [core.py:842]     self.quant_method.create_weights(
[1;36m(EngineCore_DP0 pid=369887)[0;0m ERROR 12-04 13:16:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 215, in create_weights
[1;36m(EngineCore_DP0 pid=369887)[0;0m ERROR 12-04 13:16:16 [core.py:842]     data=torch.empty(
[1;36m(EngineCore_DP0 pid=369887)[0;0m ERROR 12-04 13:16:16 [core.py:842]          ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369887)[0;0m ERROR 12-04 13:16:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_device.py", line 103, in __torch_function__
[1;36m(EngineCore_DP0 pid=369887)[0;0m ERROR 12-04 13:16:16 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=369887)[0;0m ERROR 12-04 13:16:16 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369887)[0;0m ERROR 12-04 13:16:16 [core.py:842] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 340.00 MiB. GPU 0 has a total capacity of 44.39 GiB of which 64.00 MiB is free. Process 369896 has 20.63 GiB memory in use. Including non-PyTorch memory, this process has 23.71 GiB memory in use. Of the allocated memory 23.19 GiB is allocated by PyTorch, and 18.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=369887)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=369887)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=369887)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=369887)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=369887)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=369887)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=369887)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=369887)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=369887)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=369887)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369887)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=369887)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=369887)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=369887)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=369887)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369887)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=369887)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=369887)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[1;36m(EngineCore_DP0 pid=369887)[0;0m     self.driver_worker.load_model()
[1;36m(EngineCore_DP0 pid=369887)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 273, in load_model
[1;36m(EngineCore_DP0 pid=369887)[0;0m     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[1;36m(EngineCore_DP0 pid=369887)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3276, in load_model
[1;36m(EngineCore_DP0 pid=369887)[0;0m     self.model = model_loader.load_model(
[1;36m(EngineCore_DP0 pid=369887)[0;0m                  ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369887)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
[1;36m(EngineCore_DP0 pid=369887)[0;0m     model = initialize_model(
[1;36m(EngineCore_DP0 pid=369887)[0;0m             ^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369887)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py", line 55, in initialize_model
[1;36m(EngineCore_DP0 pid=369887)[0;0m     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(EngineCore_DP0 pid=369887)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369887)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 279, in __init__
[1;36m(EngineCore_DP0 pid=369887)[0;0m     self.model = Qwen3Model(
[1;36m(EngineCore_DP0 pid=369887)[0;0m                  ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369887)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=369887)[0;0m     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=369887)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 253, in __init__
[1;36m(EngineCore_DP0 pid=369887)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=369887)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=369887)[0;0m     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=369887)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 337, in __init__
[1;36m(EngineCore_DP0 pid=369887)[0;0m     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(EngineCore_DP0 pid=369887)[0;0m                                                     ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369887)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 651, in make_layers
[1;36m(EngineCore_DP0 pid=369887)[0;0m     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(EngineCore_DP0 pid=369887)[0;0m                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369887)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 339, in <lambda>
[1;36m(EngineCore_DP0 pid=369887)[0;0m     lambda prefix: decoder_layer_type(
[1;36m(EngineCore_DP0 pid=369887)[0;0m                    ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369887)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 201, in __init__
[1;36m(EngineCore_DP0 pid=369887)[0;0m     self.mlp = Qwen3MLP(
[1;36m(EngineCore_DP0 pid=369887)[0;0m                ^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369887)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 84, in __init__
[1;36m(EngineCore_DP0 pid=369887)[0;0m     self.gate_up_proj = MergedColumnParallelLinear(
[1;36m(EngineCore_DP0 pid=369887)[0;0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369887)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 631, in __init__
[1;36m(EngineCore_DP0 pid=369887)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=369887)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 484, in __init__
[1;36m(EngineCore_DP0 pid=369887)[0;0m     self.quant_method.create_weights(
[1;36m(EngineCore_DP0 pid=369887)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 215, in create_weights
[1;36m(EngineCore_DP0 pid=369887)[0;0m     data=torch.empty(
[1;36m(EngineCore_DP0 pid=369887)[0;0m          ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369887)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_device.py", line 103, in __torch_function__
[1;36m(EngineCore_DP0 pid=369887)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=369887)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=369887)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 340.00 MiB. GPU 0 has a total capacity of 44.39 GiB of which 64.00 MiB is free. Process 369896 has 20.63 GiB memory in use. Including non-PyTorch memory, this process has 23.71 GiB memory in use. Of the allocated memory 23.19 GiB is allocated by PyTorch, and 18.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W1204 13:16:16.436487119 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank0]:[W1204 13:16:16.477503380 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: Qwen/Qwen3-14B (backend=vllm)
[ModelCache] Using max_model_len=40960 for Qwen/Qwen3-14B
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 13:16:38 [utils.py:253] non-default args: {'max_model_len': 40960, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'Qwen/Qwen3-14B'}
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: Qwen/Qwen3-14B (backend=vllm)
[ModelCache] Using max_model_len=40960 for Qwen/Qwen3-14B
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 13:16:38 [utils.py:253] non-default args: {'max_model_len': 40960, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'Qwen/Qwen3-14B'}
INFO 12-04 13:16:38 [model.py:631] Resolved architecture: Qwen3ForCausalLM
INFO 12-04 13:16:38 [model.py:1745] Using max model len 40960
INFO 12-04 13:16:38 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
INFO 12-04 13:16:38 [model.py:631] Resolved architecture: Qwen3ForCausalLM
INFO 12-04 13:16:38 [model.py:1745] Using max model len 40960
INFO 12-04 13:16:38 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=372311)[0;0m INFO 12-04 13:16:52 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='Qwen/Qwen3-14B', speculative_config=None, tokenizer='Qwen/Qwen3-14B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-14B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=372308)[0;0m INFO 12-04 13:16:52 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='Qwen/Qwen3-14B', speculative_config=None, tokenizer='Qwen/Qwen3-14B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-14B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=372311)[0;0m INFO 12-04 13:16:53 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.98:46617 backend=nccl
[1;36m(EngineCore_DP0 pid=372308)[0;0m INFO 12-04 13:16:53 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.98:39593 backend=nccl
[W1204 13:16:53.328876345 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-25-0.rc.tch.harvard.edu]:46617 (errno: 97 - Address family not supported by protocol).
[W1204 13:16:53.331425398 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-25-0.rc.tch.harvard.edu]:39593 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=372308)[0;0m INFO 12-04 13:16:53 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=372311)[0;0m INFO 12-04 13:16:53 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=372308)[0;0m INFO 12-04 13:16:54 [gpu_model_runner.py:3259] Starting to load model Qwen/Qwen3-14B...
[1;36m(EngineCore_DP0 pid=372311)[0;0m INFO 12-04 13:16:54 [gpu_model_runner.py:3259] Starting to load model Qwen/Qwen3-14B...
[1;36m(EngineCore_DP0 pid=372311)[0;0m INFO 12-04 13:16:55 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=372311)[0;0m INFO 12-04 13:16:55 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=372308)[0;0m INFO 12-04 13:16:55 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=372308)[0;0m INFO 12-04 13:16:55 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=372308)[0;0m ERROR 12-04 13:16:56 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=372308)[0;0m ERROR 12-04 13:16:56 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=372308)[0;0m ERROR 12-04 13:16:56 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=372308)[0;0m ERROR 12-04 13:16:56 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=372308)[0;0m ERROR 12-04 13:16:56 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372308)[0;0m ERROR 12-04 13:16:56 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=372308)[0;0m ERROR 12-04 13:16:56 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=372308)[0;0m ERROR 12-04 13:16:56 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=372308)[0;0m ERROR 12-04 13:16:56 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=372308)[0;0m ERROR 12-04 13:16:56 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372308)[0;0m ERROR 12-04 13:16:56 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=372308)[0;0m ERROR 12-04 13:16:56 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=372308)[0;0m ERROR 12-04 13:16:56 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[1;36m(EngineCore_DP0 pid=372308)[0;0m ERROR 12-04 13:16:56 [core.py:842]     self.driver_worker.load_model()
[1;36m(EngineCore_DP0 pid=372308)[0;0m ERROR 12-04 13:16:56 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 273, in load_model
[1;36m(EngineCore_DP0 pid=372308)[0;0m ERROR 12-04 13:16:56 [core.py:842]     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[1;36m(EngineCore_DP0 pid=372308)[0;0m ERROR 12-04 13:16:56 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3276, in load_model
[1;36m(EngineCore_DP0 pid=372308)[0;0m ERROR 12-04 13:16:56 [core.py:842]     self.model = model_loader.load_model(
[1;36m(EngineCore_DP0 pid=372308)[0;0m ERROR 12-04 13:16:56 [core.py:842]                  ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372308)[0;0m ERROR 12-04 13:16:56 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
[1;36m(EngineCore_DP0 pid=372308)[0;0m ERROR 12-04 13:16:56 [core.py:842]     model = initialize_model(
[1;36m(EngineCore_DP0 pid=372308)[0;0m ERROR 12-04 13:16:56 [core.py:842]             ^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372308)[0;0m ERROR 12-04 13:16:56 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py", line 55, in initialize_model
[1;36m(EngineCore_DP0 pid=372308)[0;0m ERROR 12-04 13:16:56 [core.py:842]     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(EngineCore_DP0 pid=372308)[0;0m ERROR 12-04 13:16:56 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372308)[0;0m ERROR 12-04 13:16:56 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 279, in __init__
[1;36m(EngineCore_DP0 pid=372308)[0;0m ERROR 12-04 13:16:56 [core.py:842]     self.model = Qwen3Model(
[1;36m(EngineCore_DP0 pid=372308)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=372308)[0;0m ERROR 12-04 13:16:56 [core.py:842]                  ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372308)[0;0m ERROR 12-04 13:16:56 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=372308)[0;0m ERROR 12-04 13:16:56 [core.py:842]     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=372308)[0;0m ERROR 12-04 13:16:56 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 253, in __init__
[1;36m(EngineCore_DP0 pid=372308)[0;0m ERROR 12-04 13:16:56 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=372308)[0;0m ERROR 12-04 13:16:56 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=372308)[0;0m ERROR 12-04 13:16:56 [core.py:842]     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=372308)[0;0m ERROR 12-04 13:16:56 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 337, in __init__
[1;36m(EngineCore_DP0 pid=372308)[0;0m ERROR 12-04 13:16:56 [core.py:842]     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(EngineCore_DP0 pid=372308)[0;0m ERROR 12-04 13:16:56 [core.py:842]                                                     ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372308)[0;0m ERROR 12-04 13:16:56 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 651, in make_layers
[1;36m(EngineCore_DP0 pid=372308)[0;0m ERROR 12-04 13:16:56 [core.py:842]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(EngineCore_DP0 pid=372308)[0;0m ERROR 12-04 13:16:56 [core.py:842]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372308)[0;0m ERROR 12-04 13:16:56 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 339, in <lambda>
[1;36m(EngineCore_DP0 pid=372308)[0;0m ERROR 12-04 13:16:56 [core.py:842]     lambda prefix: decoder_layer_type(
[1;36m(EngineCore_DP0 pid=372308)[0;0m ERROR 12-04 13:16:56 [core.py:842]                    ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372308)[0;0m ERROR 12-04 13:16:56 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 201, in __init__
[1;36m(EngineCore_DP0 pid=372308)[0;0m ERROR 12-04 13:16:56 [core.py:842]     self.mlp = Qwen3MLP(
[1;36m(EngineCore_DP0 pid=372308)[0;0m ERROR 12-04 13:16:56 [core.py:842]                ^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372308)[0;0m ERROR 12-04 13:16:56 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 84, in __init__
[1;36m(EngineCore_DP0 pid=372308)[0;0m ERROR 12-04 13:16:56 [core.py:842]     self.gate_up_proj = MergedColumnParallelLinear(
[1;36m(EngineCore_DP0 pid=372308)[0;0m ERROR 12-04 13:16:56 [core.py:842]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372308)[0;0m ERROR 12-04 13:16:56 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 631, in __init__
[1;36m(EngineCore_DP0 pid=372308)[0;0m ERROR 12-04 13:16:56 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=372308)[0;0m ERROR 12-04 13:16:56 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 484, in __init__
[1;36m(EngineCore_DP0 pid=372308)[0;0m ERROR 12-04 13:16:56 [core.py:842]     self.quant_method.create_weights(
[1;36m(EngineCore_DP0 pid=372308)[0;0m ERROR 12-04 13:16:56 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 215, in create_weights
[1;36m(EngineCore_DP0 pid=372308)[0;0m ERROR 12-04 13:16:56 [core.py:842]     data=torch.empty(
[1;36m(EngineCore_DP0 pid=372308)[0;0m ERROR 12-04 13:16:56 [core.py:842]          ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372308)[0;0m ERROR 12-04 13:16:56 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_device.py", line 103, in __torch_function__
[1;36m(EngineCore_DP0 pid=372308)[0;0m ERROR 12-04 13:16:56 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=372308)[0;0m ERROR 12-04 13:16:56 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372308)[0;0m ERROR 12-04 13:16:56 [core.py:842] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 340.00 MiB. GPU 0 has a total capacity of 44.39 GiB of which 54.00 MiB is free. Process 372311 has 23.12 GiB memory in use. Including non-PyTorch memory, this process has 21.25 GiB memory in use. Of the allocated memory 20.73 GiB is allocated by PyTorch, and 18.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=372308)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=372308)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=372308)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=372308)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=372308)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=372308)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=372308)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=372308)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=372308)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=372308)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372308)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=372308)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=372308)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=372308)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=372308)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372308)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=372308)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=372308)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[1;36m(EngineCore_DP0 pid=372308)[0;0m     self.driver_worker.load_model()
[1;36m(EngineCore_DP0 pid=372308)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 273, in load_model
[1;36m(EngineCore_DP0 pid=372308)[0;0m     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[1;36m(EngineCore_DP0 pid=372308)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3276, in load_model
[1;36m(EngineCore_DP0 pid=372308)[0;0m     self.model = model_loader.load_model(
[1;36m(EngineCore_DP0 pid=372308)[0;0m                  ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372308)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
[1;36m(EngineCore_DP0 pid=372308)[0;0m     model = initialize_model(
[1;36m(EngineCore_DP0 pid=372308)[0;0m             ^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372308)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py", line 55, in initialize_model
[1;36m(EngineCore_DP0 pid=372308)[0;0m     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(EngineCore_DP0 pid=372308)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372308)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 279, in __init__
[1;36m(EngineCore_DP0 pid=372308)[0;0m     self.model = Qwen3Model(
[1;36m(EngineCore_DP0 pid=372308)[0;0m                  ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372308)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=372308)[0;0m     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=372308)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 253, in __init__
[1;36m(EngineCore_DP0 pid=372308)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=372308)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=372308)[0;0m     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=372308)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 337, in __init__
[1;36m(EngineCore_DP0 pid=372308)[0;0m     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(EngineCore_DP0 pid=372308)[0;0m                                                     ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372308)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 651, in make_layers
[1;36m(EngineCore_DP0 pid=372308)[0;0m     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(EngineCore_DP0 pid=372308)[0;0m                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372308)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 339, in <lambda>
[1;36m(EngineCore_DP0 pid=372308)[0;0m     lambda prefix: decoder_layer_type(
[1;36m(EngineCore_DP0 pid=372308)[0;0m                    ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372308)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 201, in __init__
[1;36m(EngineCore_DP0 pid=372308)[0;0m     self.mlp = Qwen3MLP(
[1;36m(EngineCore_DP0 pid=372308)[0;0m                ^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372308)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 84, in __init__
[1;36m(EngineCore_DP0 pid=372308)[0;0m     self.gate_up_proj = MergedColumnParallelLinear(
[1;36m(EngineCore_DP0 pid=372308)[0;0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372308)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 631, in __init__
[1;36m(EngineCore_DP0 pid=372308)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=372308)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 484, in __init__
[1;36m(EngineCore_DP0 pid=372308)[0;0m     self.quant_method.create_weights(
[1;36m(EngineCore_DP0 pid=372308)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 215, in create_weights
[1;36m(EngineCore_DP0 pid=372308)[0;0m     data=torch.empty(
[1;36m(EngineCore_DP0 pid=372308)[0;0m          ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372308)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_device.py", line 103, in __torch_function__
[1;36m(EngineCore_DP0 pid=372308)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=372308)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372308)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 340.00 MiB. GPU 0 has a total capacity of 44.39 GiB of which 54.00 MiB is free. Process 372311 has 23.12 GiB memory in use. Including non-PyTorch memory, this process has 21.25 GiB memory in use. Of the allocated memory 20.73 GiB is allocated by PyTorch, and 18.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=372311)[0;0m ERROR 12-04 13:16:56 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=372311)[0;0m ERROR 12-04 13:16:56 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=372311)[0;0m ERROR 12-04 13:16:56 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=372311)[0;0m ERROR 12-04 13:16:56 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=372311)[0;0m ERROR 12-04 13:16:56 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372311)[0;0m ERROR 12-04 13:16:56 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=372311)[0;0m ERROR 12-04 13:16:56 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=372311)[0;0m ERROR 12-04 13:16:56 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=372311)[0;0m ERROR 12-04 13:16:56 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=372311)[0;0m ERROR 12-04 13:16:56 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372311)[0;0m ERROR 12-04 13:16:56 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=372311)[0;0m ERROR 12-04 13:16:56 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=372311)[0;0m ERROR 12-04 13:16:56 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[1;36m(EngineCore_DP0 pid=372311)[0;0m ERROR 12-04 13:16:56 [core.py:842]     self.driver_worker.load_model()
[1;36m(EngineCore_DP0 pid=372311)[0;0m ERROR 12-04 13:16:56 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 273, in load_model
[1;36m(EngineCore_DP0 pid=372311)[0;0m ERROR 12-04 13:16:56 [core.py:842]     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[1;36m(EngineCore_DP0 pid=372311)[0;0m ERROR 12-04 13:16:56 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3276, in load_model
[1;36m(EngineCore_DP0 pid=372311)[0;0m ERROR 12-04 13:16:56 [core.py:842]     self.model = model_loader.load_model(
[1;36m(EngineCore_DP0 pid=372311)[0;0m ERROR 12-04 13:16:56 [core.py:842]                  ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372311)[0;0m ERROR 12-04 13:16:56 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
[1;36m(EngineCore_DP0 pid=372311)[0;0m ERROR 12-04 13:16:56 [core.py:842]     model = initialize_model(
[1;36m(EngineCore_DP0 pid=372311)[0;0m ERROR 12-04 13:16:56 [core.py:842]             ^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372311)[0;0m ERROR 12-04 13:16:56 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py", line 55, in initialize_model
[1;36m(EngineCore_DP0 pid=372311)[0;0m ERROR 12-04 13:16:56 [core.py:842]     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(EngineCore_DP0 pid=372311)[0;0m ERROR 12-04 13:16:56 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372311)[0;0m ERROR 12-04 13:16:56 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 279, in __init__
[1;36m(EngineCore_DP0 pid=372311)[0;0m ERROR 12-04 13:16:56 [core.py:842]     self.model = Qwen3Model(
[1;36m(EngineCore_DP0 pid=372311)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=372311)[0;0m ERROR 12-04 13:16:56 [core.py:842]                  ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372311)[0;0m ERROR 12-04 13:16:56 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=372311)[0;0m ERROR 12-04 13:16:56 [core.py:842]     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=372311)[0;0m ERROR 12-04 13:16:56 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 253, in __init__
[1;36m(EngineCore_DP0 pid=372311)[0;0m ERROR 12-04 13:16:56 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=372311)[0;0m ERROR 12-04 13:16:56 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=372311)[0;0m ERROR 12-04 13:16:56 [core.py:842]     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=372311)[0;0m ERROR 12-04 13:16:56 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 337, in __init__
[1;36m(EngineCore_DP0 pid=372311)[0;0m ERROR 12-04 13:16:56 [core.py:842]     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(EngineCore_DP0 pid=372311)[0;0m ERROR 12-04 13:16:56 [core.py:842]                                                     ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372311)[0;0m ERROR 12-04 13:16:56 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 651, in make_layers
[1;36m(EngineCore_DP0 pid=372311)[0;0m ERROR 12-04 13:16:56 [core.py:842]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(EngineCore_DP0 pid=372311)[0;0m ERROR 12-04 13:16:56 [core.py:842]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372311)[0;0m ERROR 12-04 13:16:56 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 339, in <lambda>
[1;36m(EngineCore_DP0 pid=372311)[0;0m ERROR 12-04 13:16:56 [core.py:842]     lambda prefix: decoder_layer_type(
[1;36m(EngineCore_DP0 pid=372311)[0;0m ERROR 12-04 13:16:56 [core.py:842]                    ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372311)[0;0m ERROR 12-04 13:16:56 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 201, in __init__
[1;36m(EngineCore_DP0 pid=372311)[0;0m ERROR 12-04 13:16:56 [core.py:842]     self.mlp = Qwen3MLP(
[1;36m(EngineCore_DP0 pid=372311)[0;0m ERROR 12-04 13:16:56 [core.py:842]                ^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372311)[0;0m ERROR 12-04 13:16:56 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 84, in __init__
[1;36m(EngineCore_DP0 pid=372311)[0;0m ERROR 12-04 13:16:56 [core.py:842]     self.gate_up_proj = MergedColumnParallelLinear(
[1;36m(EngineCore_DP0 pid=372311)[0;0m ERROR 12-04 13:16:56 [core.py:842]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372311)[0;0m ERROR 12-04 13:16:56 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 631, in __init__
[1;36m(EngineCore_DP0 pid=372311)[0;0m ERROR 12-04 13:16:56 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=372311)[0;0m ERROR 12-04 13:16:56 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 484, in __init__
[1;36m(EngineCore_DP0 pid=372311)[0;0m ERROR 12-04 13:16:56 [core.py:842]     self.quant_method.create_weights(
[1;36m(EngineCore_DP0 pid=372311)[0;0m ERROR 12-04 13:16:56 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 215, in create_weights
[1;36m(EngineCore_DP0 pid=372311)[0;0m ERROR 12-04 13:16:56 [core.py:842]     data=torch.empty(
[1;36m(EngineCore_DP0 pid=372311)[0;0m ERROR 12-04 13:16:56 [core.py:842]          ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372311)[0;0m ERROR 12-04 13:16:56 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_device.py", line 103, in __torch_function__
[1;36m(EngineCore_DP0 pid=372311)[0;0m ERROR 12-04 13:16:56 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=372311)[0;0m ERROR 12-04 13:16:56 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372311)[0;0m ERROR 12-04 13:16:56 [core.py:842] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 340.00 MiB. GPU 0 has a total capacity of 44.39 GiB of which 34.00 MiB is free. Including non-PyTorch memory, this process has 23.10 GiB memory in use. Process 372308 has 21.25 GiB memory in use. Of the allocated memory 22.58 GiB is allocated by PyTorch, and 18.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=372311)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=372311)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=372311)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=372311)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=372311)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=372311)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=372311)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=372311)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=372311)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=372311)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372311)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=372311)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=372311)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=372311)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=372311)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372311)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=372311)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=372311)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[1;36m(EngineCore_DP0 pid=372311)[0;0m     self.driver_worker.load_model()
[1;36m(EngineCore_DP0 pid=372311)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 273, in load_model
[1;36m(EngineCore_DP0 pid=372311)[0;0m     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[1;36m(EngineCore_DP0 pid=372311)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3276, in load_model
[1;36m(EngineCore_DP0 pid=372311)[0;0m     self.model = model_loader.load_model(
[1;36m(EngineCore_DP0 pid=372311)[0;0m                  ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372311)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
[1;36m(EngineCore_DP0 pid=372311)[0;0m     model = initialize_model(
[1;36m(EngineCore_DP0 pid=372311)[0;0m             ^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372311)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py", line 55, in initialize_model
[1;36m(EngineCore_DP0 pid=372311)[0;0m     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(EngineCore_DP0 pid=372311)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372311)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 279, in __init__
[1;36m(EngineCore_DP0 pid=372311)[0;0m     self.model = Qwen3Model(
[1;36m(EngineCore_DP0 pid=372311)[0;0m                  ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372311)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=372311)[0;0m     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=372311)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 253, in __init__
[1;36m(EngineCore_DP0 pid=372311)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=372311)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=372311)[0;0m     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=372311)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 337, in __init__
[1;36m(EngineCore_DP0 pid=372311)[0;0m     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(EngineCore_DP0 pid=372311)[0;0m                                                     ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372311)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 651, in make_layers
[1;36m(EngineCore_DP0 pid=372311)[0;0m     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(EngineCore_DP0 pid=372311)[0;0m                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372311)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 339, in <lambda>
[1;36m(EngineCore_DP0 pid=372311)[0;0m     lambda prefix: decoder_layer_type(
[1;36m(EngineCore_DP0 pid=372311)[0;0m                    ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372311)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 201, in __init__
[1;36m(EngineCore_DP0 pid=372311)[0;0m     self.mlp = Qwen3MLP(
[1;36m(EngineCore_DP0 pid=372311)[0;0m                ^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372311)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 84, in __init__
[1;36m(EngineCore_DP0 pid=372311)[0;0m     self.gate_up_proj = MergedColumnParallelLinear(
[1;36m(EngineCore_DP0 pid=372311)[0;0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372311)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 631, in __init__
[1;36m(EngineCore_DP0 pid=372311)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=372311)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 484, in __init__
[1;36m(EngineCore_DP0 pid=372311)[0;0m     self.quant_method.create_weights(
[1;36m(EngineCore_DP0 pid=372311)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 215, in create_weights
[1;36m(EngineCore_DP0 pid=372311)[0;0m     data=torch.empty(
[1;36m(EngineCore_DP0 pid=372311)[0;0m          ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372311)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_device.py", line 103, in __torch_function__
[1;36m(EngineCore_DP0 pid=372311)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=372311)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=372311)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 340.00 MiB. GPU 0 has a total capacity of 44.39 GiB of which 34.00 MiB is free. Including non-PyTorch memory, this process has 23.10 GiB memory in use. Process 372308 has 21.25 GiB memory in use. Of the allocated memory 22.58 GiB is allocated by PyTorch, and 18.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W1204 13:16:56.372346232 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank0]:[W1204 13:16:56.396264893 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: Qwen/Qwen3-14B (backend=vllm)
[ModelCache] Using max_model_len=40960 for Qwen/Qwen3-14B
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 13:17:18 [utils.py:253] non-default args: {'max_model_len': 40960, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'Qwen/Qwen3-14B'}
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: Qwen/Qwen3-14B (backend=vllm)
[ModelCache] Using max_model_len=40960 for Qwen/Qwen3-14B
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 13:17:18 [utils.py:253] non-default args: {'max_model_len': 40960, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'Qwen/Qwen3-14B'}
INFO 12-04 13:17:18 [model.py:631] Resolved architecture: Qwen3ForCausalLM
INFO 12-04 13:17:18 [model.py:1745] Using max model len 40960
INFO 12-04 13:17:18 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
INFO 12-04 13:17:18 [model.py:631] Resolved architecture: Qwen3ForCausalLM
INFO 12-04 13:17:18 [model.py:1745] Using max model len 40960
INFO 12-04 13:17:18 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=374414)[0;0m INFO 12-04 13:17:31 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='Qwen/Qwen3-14B', speculative_config=None, tokenizer='Qwen/Qwen3-14B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-14B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=374410)[0;0m INFO 12-04 13:17:32 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='Qwen/Qwen3-14B', speculative_config=None, tokenizer='Qwen/Qwen3-14B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-14B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=374414)[0;0m INFO 12-04 13:17:33 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.98:49071 backend=nccl
[W1204 13:17:33.924688976 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-25-0.rc.tch.harvard.edu]:49071 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=374410)[0;0m INFO 12-04 13:17:33 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.98:45575 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[W1204 13:17:33.978997989 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-25-0.rc.tch.harvard.edu]:45575 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=374414)[0;0m INFO 12-04 13:17:33 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=374410)[0;0m INFO 12-04 13:17:33 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=374414)[0;0m INFO 12-04 13:17:33 [gpu_model_runner.py:3259] Starting to load model Qwen/Qwen3-14B...
[1;36m(EngineCore_DP0 pid=374410)[0;0m INFO 12-04 13:17:33 [gpu_model_runner.py:3259] Starting to load model Qwen/Qwen3-14B...
[1;36m(EngineCore_DP0 pid=374414)[0;0m INFO 12-04 13:17:34 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=374414)[0;0m INFO 12-04 13:17:34 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=374410)[0;0m INFO 12-04 13:17:34 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=374410)[0;0m INFO 12-04 13:17:34 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=374410)[0;0m ERROR 12-04 13:17:35 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=374410)[0;0m ERROR 12-04 13:17:35 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=374410)[0;0m ERROR 12-04 13:17:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=374410)[0;0m ERROR 12-04 13:17:35 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=374410)[0;0m ERROR 12-04 13:17:35 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374410)[0;0m ERROR 12-04 13:17:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=374410)[0;0m ERROR 12-04 13:17:35 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=374410)[0;0m ERROR 12-04 13:17:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=374410)[0;0m ERROR 12-04 13:17:35 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=374410)[0;0m ERROR 12-04 13:17:35 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374410)[0;0m ERROR 12-04 13:17:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=374410)[0;0m ERROR 12-04 13:17:35 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=374410)[0;0m ERROR 12-04 13:17:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[1;36m(EngineCore_DP0 pid=374410)[0;0m ERROR 12-04 13:17:35 [core.py:842]     self.driver_worker.load_model()
[1;36m(EngineCore_DP0 pid=374410)[0;0m ERROR 12-04 13:17:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 273, in load_model
[1;36m(EngineCore_DP0 pid=374410)[0;0m ERROR 12-04 13:17:35 [core.py:842]     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[1;36m(EngineCore_DP0 pid=374410)[0;0m ERROR 12-04 13:17:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3276, in load_model
[1;36m(EngineCore_DP0 pid=374410)[0;0m ERROR 12-04 13:17:35 [core.py:842]     self.model = model_loader.load_model(
[1;36m(EngineCore_DP0 pid=374410)[0;0m ERROR 12-04 13:17:35 [core.py:842]                  ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374410)[0;0m ERROR 12-04 13:17:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
[1;36m(EngineCore_DP0 pid=374410)[0;0m ERROR 12-04 13:17:35 [core.py:842]     model = initialize_model(
[1;36m(EngineCore_DP0 pid=374410)[0;0m ERROR 12-04 13:17:35 [core.py:842]             ^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374410)[0;0m ERROR 12-04 13:17:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py", line 55, in initialize_model
[1;36m(EngineCore_DP0 pid=374410)[0;0m ERROR 12-04 13:17:35 [core.py:842]     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(EngineCore_DP0 pid=374410)[0;0m ERROR 12-04 13:17:35 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374410)[0;0m ERROR 12-04 13:17:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 279, in __init__
[1;36m(EngineCore_DP0 pid=374410)[0;0m ERROR 12-04 13:17:35 [core.py:842]     self.model = Qwen3Model(
[1;36m(EngineCore_DP0 pid=374410)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=374410)[0;0m ERROR 12-04 13:17:35 [core.py:842]                  ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374410)[0;0m ERROR 12-04 13:17:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=374410)[0;0m ERROR 12-04 13:17:35 [core.py:842]     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=374410)[0;0m ERROR 12-04 13:17:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 253, in __init__
[1;36m(EngineCore_DP0 pid=374410)[0;0m ERROR 12-04 13:17:35 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=374410)[0;0m ERROR 12-04 13:17:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=374410)[0;0m ERROR 12-04 13:17:35 [core.py:842]     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=374410)[0;0m ERROR 12-04 13:17:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 337, in __init__
[1;36m(EngineCore_DP0 pid=374410)[0;0m ERROR 12-04 13:17:35 [core.py:842]     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(EngineCore_DP0 pid=374410)[0;0m ERROR 12-04 13:17:35 [core.py:842]                                                     ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374410)[0;0m ERROR 12-04 13:17:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 651, in make_layers
[1;36m(EngineCore_DP0 pid=374410)[0;0m ERROR 12-04 13:17:35 [core.py:842]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(EngineCore_DP0 pid=374410)[0;0m ERROR 12-04 13:17:35 [core.py:842]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374410)[0;0m ERROR 12-04 13:17:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 339, in <lambda>
[1;36m(EngineCore_DP0 pid=374410)[0;0m ERROR 12-04 13:17:35 [core.py:842]     lambda prefix: decoder_layer_type(
[1;36m(EngineCore_DP0 pid=374410)[0;0m ERROR 12-04 13:17:35 [core.py:842]                    ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374410)[0;0m ERROR 12-04 13:17:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 201, in __init__
[1;36m(EngineCore_DP0 pid=374410)[0;0m ERROR 12-04 13:17:35 [core.py:842]     self.mlp = Qwen3MLP(
[1;36m(EngineCore_DP0 pid=374410)[0;0m ERROR 12-04 13:17:35 [core.py:842]                ^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374410)[0;0m ERROR 12-04 13:17:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 84, in __init__
[1;36m(EngineCore_DP0 pid=374410)[0;0m ERROR 12-04 13:17:35 [core.py:842]     self.gate_up_proj = MergedColumnParallelLinear(
[1;36m(EngineCore_DP0 pid=374410)[0;0m ERROR 12-04 13:17:35 [core.py:842]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374410)[0;0m ERROR 12-04 13:17:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 631, in __init__
[1;36m(EngineCore_DP0 pid=374410)[0;0m ERROR 12-04 13:17:35 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=374410)[0;0m ERROR 12-04 13:17:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 484, in __init__
[1;36m(EngineCore_DP0 pid=374410)[0;0m ERROR 12-04 13:17:35 [core.py:842]     self.quant_method.create_weights(
[1;36m(EngineCore_DP0 pid=374410)[0;0m ERROR 12-04 13:17:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 215, in create_weights
[1;36m(EngineCore_DP0 pid=374410)[0;0m ERROR 12-04 13:17:35 [core.py:842]     data=torch.empty(
[1;36m(EngineCore_DP0 pid=374410)[0;0m ERROR 12-04 13:17:35 [core.py:842]          ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374410)[0;0m ERROR 12-04 13:17:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_device.py", line 103, in __torch_function__
[1;36m(EngineCore_DP0 pid=374410)[0;0m ERROR 12-04 13:17:35 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=374410)[0;0m ERROR 12-04 13:17:35 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374410)[0;0m ERROR 12-04 13:17:35 [core.py:842] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 340.00 MiB. GPU 0 has a total capacity of 44.39 GiB of which 34.00 MiB is free. Process 374414 has 23.71 GiB memory in use. Including non-PyTorch memory, this process has 20.63 GiB memory in use. Of the allocated memory 20.11 GiB is allocated by PyTorch, and 18.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=374410)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=374410)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=374410)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=374410)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=374410)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=374410)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=374410)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=374410)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=374410)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=374410)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374410)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=374410)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=374410)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=374410)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=374410)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374410)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=374410)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=374410)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[1;36m(EngineCore_DP0 pid=374410)[0;0m     self.driver_worker.load_model()
[1;36m(EngineCore_DP0 pid=374410)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 273, in load_model
[1;36m(EngineCore_DP0 pid=374410)[0;0m     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[1;36m(EngineCore_DP0 pid=374410)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3276, in load_model
[1;36m(EngineCore_DP0 pid=374410)[0;0m     self.model = model_loader.load_model(
[1;36m(EngineCore_DP0 pid=374410)[0;0m                  ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374410)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
[1;36m(EngineCore_DP0 pid=374410)[0;0m     model = initialize_model(
[1;36m(EngineCore_DP0 pid=374410)[0;0m             ^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374410)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py", line 55, in initialize_model
[1;36m(EngineCore_DP0 pid=374410)[0;0m     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(EngineCore_DP0 pid=374410)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374410)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 279, in __init__
[1;36m(EngineCore_DP0 pid=374410)[0;0m     self.model = Qwen3Model(
[1;36m(EngineCore_DP0 pid=374410)[0;0m                  ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374410)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=374410)[0;0m     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=374410)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 253, in __init__
[1;36m(EngineCore_DP0 pid=374410)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=374410)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=374410)[0;0m     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=374410)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 337, in __init__
[1;36m(EngineCore_DP0 pid=374410)[0;0m     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(EngineCore_DP0 pid=374410)[0;0m                                                     ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374410)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 651, in make_layers
[1;36m(EngineCore_DP0 pid=374410)[0;0m     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(EngineCore_DP0 pid=374410)[0;0m                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374410)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 339, in <lambda>
[1;36m(EngineCore_DP0 pid=374410)[0;0m     lambda prefix: decoder_layer_type(
[1;36m(EngineCore_DP0 pid=374410)[0;0m                    ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374410)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 201, in __init__
[1;36m(EngineCore_DP0 pid=374410)[0;0m     self.mlp = Qwen3MLP(
[1;36m(EngineCore_DP0 pid=374410)[0;0m                ^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374410)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 84, in __init__
[1;36m(EngineCore_DP0 pid=374410)[0;0m     self.gate_up_proj = MergedColumnParallelLinear(
[1;36m(EngineCore_DP0 pid=374410)[0;0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374410)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 631, in __init__
[1;36m(EngineCore_DP0 pid=374410)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=374410)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 484, in __init__
[1;36m(EngineCore_DP0 pid=374410)[0;0m     self.quant_method.create_weights(
[1;36m(EngineCore_DP0 pid=374410)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 215, in create_weights
[1;36m(EngineCore_DP0 pid=374410)[0;0m     data=torch.empty(
[1;36m(EngineCore_DP0 pid=374410)[0;0m          ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374410)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_device.py", line 103, in __torch_function__
[1;36m(EngineCore_DP0 pid=374410)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=374410)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374410)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 340.00 MiB. GPU 0 has a total capacity of 44.39 GiB of which 34.00 MiB is free. Process 374414 has 23.71 GiB memory in use. Including non-PyTorch memory, this process has 20.63 GiB memory in use. Of the allocated memory 20.11 GiB is allocated by PyTorch, and 18.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=374414)[0;0m ERROR 12-04 13:17:35 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=374414)[0;0m ERROR 12-04 13:17:35 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=374414)[0;0m ERROR 12-04 13:17:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=374414)[0;0m ERROR 12-04 13:17:35 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=374414)[0;0m ERROR 12-04 13:17:35 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374414)[0;0m ERROR 12-04 13:17:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=374414)[0;0m ERROR 12-04 13:17:35 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=374414)[0;0m ERROR 12-04 13:17:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=374414)[0;0m ERROR 12-04 13:17:35 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=374414)[0;0m ERROR 12-04 13:17:35 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374414)[0;0m ERROR 12-04 13:17:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=374414)[0;0m ERROR 12-04 13:17:35 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=374414)[0;0m ERROR 12-04 13:17:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[1;36m(EngineCore_DP0 pid=374414)[0;0m ERROR 12-04 13:17:35 [core.py:842]     self.driver_worker.load_model()
[1;36m(EngineCore_DP0 pid=374414)[0;0m ERROR 12-04 13:17:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 273, in load_model
[1;36m(EngineCore_DP0 pid=374414)[0;0m ERROR 12-04 13:17:35 [core.py:842]     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[1;36m(EngineCore_DP0 pid=374414)[0;0m ERROR 12-04 13:17:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3276, in load_model
[1;36m(EngineCore_DP0 pid=374414)[0;0m ERROR 12-04 13:17:35 [core.py:842]     self.model = model_loader.load_model(
[1;36m(EngineCore_DP0 pid=374414)[0;0m ERROR 12-04 13:17:35 [core.py:842]                  ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374414)[0;0m ERROR 12-04 13:17:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
[1;36m(EngineCore_DP0 pid=374414)[0;0m ERROR 12-04 13:17:35 [core.py:842]     model = initialize_model(
[1;36m(EngineCore_DP0 pid=374414)[0;0m ERROR 12-04 13:17:35 [core.py:842]             ^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374414)[0;0m ERROR 12-04 13:17:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py", line 55, in initialize_model
[1;36m(EngineCore_DP0 pid=374414)[0;0m ERROR 12-04 13:17:35 [core.py:842]     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(EngineCore_DP0 pid=374414)[0;0m ERROR 12-04 13:17:35 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374414)[0;0m ERROR 12-04 13:17:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 279, in __init__
[1;36m(EngineCore_DP0 pid=374414)[0;0m ERROR 12-04 13:17:35 [core.py:842]     self.model = Qwen3Model(
[1;36m(EngineCore_DP0 pid=374414)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=374414)[0;0m ERROR 12-04 13:17:35 [core.py:842]                  ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374414)[0;0m ERROR 12-04 13:17:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=374414)[0;0m ERROR 12-04 13:17:35 [core.py:842]     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=374414)[0;0m ERROR 12-04 13:17:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 253, in __init__
[1;36m(EngineCore_DP0 pid=374414)[0;0m ERROR 12-04 13:17:35 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=374414)[0;0m ERROR 12-04 13:17:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=374414)[0;0m ERROR 12-04 13:17:35 [core.py:842]     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=374414)[0;0m ERROR 12-04 13:17:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 337, in __init__
[1;36m(EngineCore_DP0 pid=374414)[0;0m ERROR 12-04 13:17:35 [core.py:842]     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(EngineCore_DP0 pid=374414)[0;0m ERROR 12-04 13:17:35 [core.py:842]                                                     ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374414)[0;0m ERROR 12-04 13:17:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 651, in make_layers
[1;36m(EngineCore_DP0 pid=374414)[0;0m ERROR 12-04 13:17:35 [core.py:842]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(EngineCore_DP0 pid=374414)[0;0m ERROR 12-04 13:17:35 [core.py:842]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374414)[0;0m ERROR 12-04 13:17:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 339, in <lambda>
[1;36m(EngineCore_DP0 pid=374414)[0;0m ERROR 12-04 13:17:35 [core.py:842]     lambda prefix: decoder_layer_type(
[1;36m(EngineCore_DP0 pid=374414)[0;0m ERROR 12-04 13:17:35 [core.py:842]                    ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374414)[0;0m ERROR 12-04 13:17:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 201, in __init__
[1;36m(EngineCore_DP0 pid=374414)[0;0m ERROR 12-04 13:17:35 [core.py:842]     self.mlp = Qwen3MLP(
[1;36m(EngineCore_DP0 pid=374414)[0;0m ERROR 12-04 13:17:35 [core.py:842]                ^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374414)[0;0m ERROR 12-04 13:17:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 84, in __init__
[1;36m(EngineCore_DP0 pid=374414)[0;0m ERROR 12-04 13:17:35 [core.py:842]     self.gate_up_proj = MergedColumnParallelLinear(
[1;36m(EngineCore_DP0 pid=374414)[0;0m ERROR 12-04 13:17:35 [core.py:842]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374414)[0;0m ERROR 12-04 13:17:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 631, in __init__
[1;36m(EngineCore_DP0 pid=374414)[0;0m ERROR 12-04 13:17:35 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=374414)[0;0m ERROR 12-04 13:17:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 484, in __init__
[1;36m(EngineCore_DP0 pid=374414)[0;0m ERROR 12-04 13:17:35 [core.py:842]     self.quant_method.create_weights(
[1;36m(EngineCore_DP0 pid=374414)[0;0m ERROR 12-04 13:17:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 215, in create_weights
[1;36m(EngineCore_DP0 pid=374414)[0;0m ERROR 12-04 13:17:35 [core.py:842]     data=torch.empty(
[1;36m(EngineCore_DP0 pid=374414)[0;0m ERROR 12-04 13:17:35 [core.py:842]          ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374414)[0;0m ERROR 12-04 13:17:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_device.py", line 103, in __torch_function__
[1;36m(EngineCore_DP0 pid=374414)[0;0m ERROR 12-04 13:17:35 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=374414)[0;0m ERROR 12-04 13:17:35 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374414)[0;0m ERROR 12-04 13:17:35 [core.py:842] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 340.00 MiB. GPU 0 has a total capacity of 44.39 GiB of which 64.00 MiB is free. Including non-PyTorch memory, this process has 23.71 GiB memory in use. Process 374410 has 20.63 GiB memory in use. Of the allocated memory 23.19 GiB is allocated by PyTorch, and 18.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=374414)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=374414)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=374414)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=374414)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=374414)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=374414)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=374414)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=374414)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=374414)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=374414)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374414)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=374414)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=374414)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=374414)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=374414)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374414)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=374414)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=374414)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[1;36m(EngineCore_DP0 pid=374414)[0;0m     self.driver_worker.load_model()
[1;36m(EngineCore_DP0 pid=374414)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 273, in load_model
[1;36m(EngineCore_DP0 pid=374414)[0;0m     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[1;36m(EngineCore_DP0 pid=374414)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3276, in load_model
[1;36m(EngineCore_DP0 pid=374414)[0;0m     self.model = model_loader.load_model(
[1;36m(EngineCore_DP0 pid=374414)[0;0m                  ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374414)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
[1;36m(EngineCore_DP0 pid=374414)[0;0m     model = initialize_model(
[1;36m(EngineCore_DP0 pid=374414)[0;0m             ^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374414)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py", line 55, in initialize_model
[1;36m(EngineCore_DP0 pid=374414)[0;0m     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(EngineCore_DP0 pid=374414)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374414)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 279, in __init__
[1;36m(EngineCore_DP0 pid=374414)[0;0m     self.model = Qwen3Model(
[1;36m(EngineCore_DP0 pid=374414)[0;0m                  ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374414)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=374414)[0;0m     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=374414)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 253, in __init__
[1;36m(EngineCore_DP0 pid=374414)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=374414)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=374414)[0;0m     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=374414)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 337, in __init__
[1;36m(EngineCore_DP0 pid=374414)[0;0m     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(EngineCore_DP0 pid=374414)[0;0m                                                     ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374414)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 651, in make_layers
[1;36m(EngineCore_DP0 pid=374414)[0;0m     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(EngineCore_DP0 pid=374414)[0;0m                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374414)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 339, in <lambda>
[1;36m(EngineCore_DP0 pid=374414)[0;0m     lambda prefix: decoder_layer_type(
[1;36m(EngineCore_DP0 pid=374414)[0;0m                    ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374414)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 201, in __init__
[1;36m(EngineCore_DP0 pid=374414)[0;0m     self.mlp = Qwen3MLP(
[1;36m(EngineCore_DP0 pid=374414)[0;0m                ^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374414)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 84, in __init__
[1;36m(EngineCore_DP0 pid=374414)[0;0m     self.gate_up_proj = MergedColumnParallelLinear(
[1;36m(EngineCore_DP0 pid=374414)[0;0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374414)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 631, in __init__
[1;36m(EngineCore_DP0 pid=374414)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=374414)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 484, in __init__
[1;36m(EngineCore_DP0 pid=374414)[0;0m     self.quant_method.create_weights(
[1;36m(EngineCore_DP0 pid=374414)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 215, in create_weights
[1;36m(EngineCore_DP0 pid=374414)[0;0m     data=torch.empty(
[1;36m(EngineCore_DP0 pid=374414)[0;0m          ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374414)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_device.py", line 103, in __torch_function__
[1;36m(EngineCore_DP0 pid=374414)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=374414)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=374414)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 340.00 MiB. GPU 0 has a total capacity of 44.39 GiB of which 64.00 MiB is free. Including non-PyTorch memory, this process has 23.71 GiB memory in use. Process 374410 has 20.63 GiB memory in use. Of the allocated memory 23.19 GiB is allocated by PyTorch, and 18.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W1204 13:17:36.972128934 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank0]:[W1204 13:17:36.999962254 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: Qwen/Qwen3-14B (backend=vllm)
[ModelCache] Using max_model_len=40960 for Qwen/Qwen3-14B
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 13:17:57 [utils.py:253] non-default args: {'max_model_len': 40960, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'Qwen/Qwen3-14B'}
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: Qwen/Qwen3-14B (backend=vllm)
[ModelCache] Using max_model_len=40960 for Qwen/Qwen3-14B
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 13:17:57 [utils.py:253] non-default args: {'max_model_len': 40960, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'Qwen/Qwen3-14B'}
INFO 12-04 13:17:57 [model.py:631] Resolved architecture: Qwen3ForCausalLM
INFO 12-04 13:17:57 [model.py:1745] Using max model len 40960
INFO 12-04 13:17:57 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
INFO 12-04 13:17:57 [model.py:631] Resolved architecture: Qwen3ForCausalLM
INFO 12-04 13:17:57 [model.py:1745] Using max model len 40960
INFO 12-04 13:17:57 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=377950)[0;0m INFO 12-04 13:18:11 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='Qwen/Qwen3-14B', speculative_config=None, tokenizer='Qwen/Qwen3-14B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-14B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=377953)[0;0m INFO 12-04 13:18:11 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='Qwen/Qwen3-14B', speculative_config=None, tokenizer='Qwen/Qwen3-14B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-14B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=377950)[0;0m INFO 12-04 13:18:12 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.98:52281 backend=nccl
[W1204 13:18:12.917661994 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-25-0.rc.tch.harvard.edu]:52281 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=377950)[0;0m INFO 12-04 13:18:12 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=377953)[0;0m INFO 12-04 13:18:12 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.98:45427 backend=nccl
[W1204 13:18:12.049331260 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-25-0.rc.tch.harvard.edu]:45427 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=377953)[0;0m INFO 12-04 13:18:12 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=377950)[0;0m INFO 12-04 13:18:12 [gpu_model_runner.py:3259] Starting to load model Qwen/Qwen3-14B...
[1;36m(EngineCore_DP0 pid=377953)[0;0m INFO 12-04 13:18:12 [gpu_model_runner.py:3259] Starting to load model Qwen/Qwen3-14B...
[1;36m(EngineCore_DP0 pid=377950)[0;0m INFO 12-04 13:18:13 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=377950)[0;0m INFO 12-04 13:18:13 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=377953)[0;0m INFO 12-04 13:18:13 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=377953)[0;0m INFO 12-04 13:18:13 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=377950)[0;0m ERROR 12-04 13:18:14 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=377950)[0;0m ERROR 12-04 13:18:14 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=377950)[0;0m ERROR 12-04 13:18:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=377950)[0;0m ERROR 12-04 13:18:14 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=377950)[0;0m ERROR 12-04 13:18:14 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=377950)[0;0m ERROR 12-04 13:18:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=377950)[0;0m ERROR 12-04 13:18:14 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=377950)[0;0m ERROR 12-04 13:18:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=377950)[0;0m ERROR 12-04 13:18:14 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=377950)[0;0m ERROR 12-04 13:18:14 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=377950)[0;0m ERROR 12-04 13:18:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=377950)[0;0m ERROR 12-04 13:18:14 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=377950)[0;0m ERROR 12-04 13:18:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[1;36m(EngineCore_DP0 pid=377950)[0;0m ERROR 12-04 13:18:14 [core.py:842]     self.driver_worker.load_model()
[1;36m(EngineCore_DP0 pid=377950)[0;0m ERROR 12-04 13:18:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 273, in load_model
[1;36m(EngineCore_DP0 pid=377950)[0;0m ERROR 12-04 13:18:14 [core.py:842]     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[1;36m(EngineCore_DP0 pid=377950)[0;0m ERROR 12-04 13:18:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3276, in load_model
[1;36m(EngineCore_DP0 pid=377950)[0;0m ERROR 12-04 13:18:14 [core.py:842]     self.model = model_loader.load_model(
[1;36m(EngineCore_DP0 pid=377950)[0;0m ERROR 12-04 13:18:14 [core.py:842]                  ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=377950)[0;0m ERROR 12-04 13:18:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
[1;36m(EngineCore_DP0 pid=377950)[0;0m ERROR 12-04 13:18:14 [core.py:842]     model = initialize_model(
[1;36m(EngineCore_DP0 pid=377950)[0;0m ERROR 12-04 13:18:14 [core.py:842]             ^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=377950)[0;0m ERROR 12-04 13:18:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py", line 55, in initialize_model
[1;36m(EngineCore_DP0 pid=377950)[0;0m ERROR 12-04 13:18:14 [core.py:842]     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(EngineCore_DP0 pid=377950)[0;0m ERROR 12-04 13:18:14 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=377950)[0;0m ERROR 12-04 13:18:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 287, in __init__
[1;36m(EngineCore_DP0 pid=377950)[0;0m ERROR 12-04 13:18:14 [core.py:842]     self.lm_head = ParallelLMHead(
[1;36m(EngineCore_DP0 pid=377950)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=377950)[0;0m ERROR 12-04 13:18:14 [core.py:842]                    ^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=377950)[0;0m ERROR 12-04 13:18:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/vocab_parallel_embedding.py", line 523, in __init__
[1;36m(EngineCore_DP0 pid=377950)[0;0m ERROR 12-04 13:18:14 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=377950)[0;0m ERROR 12-04 13:18:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/vocab_parallel_embedding.py", line 301, in __init__
[1;36m(EngineCore_DP0 pid=377950)[0;0m ERROR 12-04 13:18:14 [core.py:842]     self.quant_method.create_weights(
[1;36m(EngineCore_DP0 pid=377950)[0;0m ERROR 12-04 13:18:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/vocab_parallel_embedding.py", line 46, in create_weights
[1;36m(EngineCore_DP0 pid=377950)[0;0m ERROR 12-04 13:18:14 [core.py:842]     torch.empty(
[1;36m(EngineCore_DP0 pid=377950)[0;0m ERROR 12-04 13:18:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_device.py", line 103, in __torch_function__
[1;36m(EngineCore_DP0 pid=377950)[0;0m ERROR 12-04 13:18:14 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=377950)[0;0m ERROR 12-04 13:18:14 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=377950)[0;0m ERROR 12-04 13:18:14 [core.py:842] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.45 GiB. GPU 0 has a total capacity of 44.39 GiB of which 154.00 MiB is free. Including non-PyTorch memory, this process has 26.67 GiB memory in use. Process 377953 has 17.56 GiB memory in use. Of the allocated memory 26.15 GiB is allocated by PyTorch, and 18.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=377950)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=377950)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=377950)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=377950)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=377950)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=377950)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=377950)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=377950)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=377950)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=377950)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=377950)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=377950)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=377950)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=377950)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=377950)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=377950)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=377950)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=377950)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[1;36m(EngineCore_DP0 pid=377950)[0;0m     self.driver_worker.load_model()
[1;36m(EngineCore_DP0 pid=377950)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 273, in load_model
[1;36m(EngineCore_DP0 pid=377950)[0;0m     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[1;36m(EngineCore_DP0 pid=377950)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3276, in load_model
[1;36m(EngineCore_DP0 pid=377950)[0;0m     self.model = model_loader.load_model(
[1;36m(EngineCore_DP0 pid=377950)[0;0m                  ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=377950)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
[1;36m(EngineCore_DP0 pid=377950)[0;0m     model = initialize_model(
[1;36m(EngineCore_DP0 pid=377950)[0;0m             ^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=377950)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py", line 55, in initialize_model
[1;36m(EngineCore_DP0 pid=377950)[0;0m     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(EngineCore_DP0 pid=377950)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=377950)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 287, in __init__
[1;36m(EngineCore_DP0 pid=377950)[0;0m     self.lm_head = ParallelLMHead(
[1;36m(EngineCore_DP0 pid=377950)[0;0m                    ^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=377950)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/vocab_parallel_embedding.py", line 523, in __init__
[1;36m(EngineCore_DP0 pid=377950)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=377950)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/vocab_parallel_embedding.py", line 301, in __init__
[1;36m(EngineCore_DP0 pid=377950)[0;0m     self.quant_method.create_weights(
[1;36m(EngineCore_DP0 pid=377950)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/vocab_parallel_embedding.py", line 46, in create_weights
[1;36m(EngineCore_DP0 pid=377950)[0;0m     torch.empty(
[1;36m(EngineCore_DP0 pid=377950)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_device.py", line 103, in __torch_function__
[1;36m(EngineCore_DP0 pid=377950)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=377950)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=377950)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.45 GiB. GPU 0 has a total capacity of 44.39 GiB of which 154.00 MiB is free. Including non-PyTorch memory, this process has 26.67 GiB memory in use. Process 377953 has 17.56 GiB memory in use. Of the allocated memory 26.15 GiB is allocated by PyTorch, and 18.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=377953)[0;0m ERROR 12-04 13:18:14 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=377953)[0;0m ERROR 12-04 13:18:14 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=377953)[0;0m ERROR 12-04 13:18:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=377953)[0;0m ERROR 12-04 13:18:14 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=377953)[0;0m ERROR 12-04 13:18:14 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=377953)[0;0m ERROR 12-04 13:18:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=377953)[0;0m ERROR 12-04 13:18:14 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=377953)[0;0m ERROR 12-04 13:18:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=377953)[0;0m ERROR 12-04 13:18:14 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=377953)[0;0m ERROR 12-04 13:18:14 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=377953)[0;0m ERROR 12-04 13:18:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=377953)[0;0m ERROR 12-04 13:18:14 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=377953)[0;0m ERROR 12-04 13:18:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[1;36m(EngineCore_DP0 pid=377953)[0;0m ERROR 12-04 13:18:14 [core.py:842]     self.driver_worker.load_model()
[1;36m(EngineCore_DP0 pid=377953)[0;0m ERROR 12-04 13:18:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 273, in load_model
[1;36m(EngineCore_DP0 pid=377953)[0;0m ERROR 12-04 13:18:14 [core.py:842]     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[1;36m(EngineCore_DP0 pid=377953)[0;0m ERROR 12-04 13:18:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3276, in load_model
[1;36m(EngineCore_DP0 pid=377953)[0;0m ERROR 12-04 13:18:14 [core.py:842]     self.model = model_loader.load_model(
[1;36m(EngineCore_DP0 pid=377953)[0;0m ERROR 12-04 13:18:14 [core.py:842]                  ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=377953)[0;0m ERROR 12-04 13:18:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
[1;36m(EngineCore_DP0 pid=377953)[0;0m ERROR 12-04 13:18:14 [core.py:842]     model = initialize_model(
[1;36m(EngineCore_DP0 pid=377953)[0;0m ERROR 12-04 13:18:14 [core.py:842]             ^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=377953)[0;0m ERROR 12-04 13:18:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py", line 55, in initialize_model
[1;36m(EngineCore_DP0 pid=377953)[0;0m ERROR 12-04 13:18:14 [core.py:842]     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(EngineCore_DP0 pid=377953)[0;0m ERROR 12-04 13:18:14 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=377953)[0;0m ERROR 12-04 13:18:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 279, in __init__
[1;36m(EngineCore_DP0 pid=377953)[0;0m ERROR 12-04 13:18:14 [core.py:842]     self.model = Qwen3Model(
[1;36m(EngineCore_DP0 pid=377953)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=377953)[0;0m ERROR 12-04 13:18:14 [core.py:842]                  ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=377953)[0;0m ERROR 12-04 13:18:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=377953)[0;0m ERROR 12-04 13:18:14 [core.py:842]     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=377953)[0;0m ERROR 12-04 13:18:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 253, in __init__
[1;36m(EngineCore_DP0 pid=377953)[0;0m ERROR 12-04 13:18:14 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=377953)[0;0m ERROR 12-04 13:18:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=377953)[0;0m ERROR 12-04 13:18:14 [core.py:842]     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=377953)[0;0m ERROR 12-04 13:18:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 337, in __init__
[1;36m(EngineCore_DP0 pid=377953)[0;0m ERROR 12-04 13:18:14 [core.py:842]     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(EngineCore_DP0 pid=377953)[0;0m ERROR 12-04 13:18:14 [core.py:842]                                                     ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=377953)[0;0m ERROR 12-04 13:18:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 651, in make_layers
[1;36m(EngineCore_DP0 pid=377953)[0;0m ERROR 12-04 13:18:14 [core.py:842]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(EngineCore_DP0 pid=377953)[0;0m ERROR 12-04 13:18:14 [core.py:842]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=377953)[0;0m ERROR 12-04 13:18:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 339, in <lambda>
[1;36m(EngineCore_DP0 pid=377953)[0;0m ERROR 12-04 13:18:14 [core.py:842]     lambda prefix: decoder_layer_type(
[1;36m(EngineCore_DP0 pid=377953)[0;0m ERROR 12-04 13:18:14 [core.py:842]                    ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=377953)[0;0m ERROR 12-04 13:18:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 201, in __init__
[1;36m(EngineCore_DP0 pid=377953)[0;0m ERROR 12-04 13:18:14 [core.py:842]     self.mlp = Qwen3MLP(
[1;36m(EngineCore_DP0 pid=377953)[0;0m ERROR 12-04 13:18:14 [core.py:842]                ^^^^^^^^^
[1;36m(EngineCore_DP0 pid=377953)[0;0m ERROR 12-04 13:18:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 84, in __init__
[1;36m(EngineCore_DP0 pid=377953)[0;0m ERROR 12-04 13:18:14 [core.py:842]     self.gate_up_proj = MergedColumnParallelLinear(
[1;36m(EngineCore_DP0 pid=377953)[0;0m ERROR 12-04 13:18:14 [core.py:842]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=377953)[0;0m ERROR 12-04 13:18:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 631, in __init__
[1;36m(EngineCore_DP0 pid=377953)[0;0m ERROR 12-04 13:18:14 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=377953)[0;0m ERROR 12-04 13:18:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 484, in __init__
[1;36m(EngineCore_DP0 pid=377953)[0;0m ERROR 12-04 13:18:14 [core.py:842]     self.quant_method.create_weights(
[1;36m(EngineCore_DP0 pid=377953)[0;0m ERROR 12-04 13:18:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 215, in create_weights
[1;36m(EngineCore_DP0 pid=377953)[0;0m ERROR 12-04 13:18:14 [core.py:842]     data=torch.empty(
[1;36m(EngineCore_DP0 pid=377953)[0;0m ERROR 12-04 13:18:14 [core.py:842]          ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=377953)[0;0m ERROR 12-04 13:18:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_device.py", line 103, in __torch_function__
[1;36m(EngineCore_DP0 pid=377953)[0;0m ERROR 12-04 13:18:14 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=377953)[0;0m ERROR 12-04 13:18:14 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=377953)[0;0m ERROR 12-04 13:18:14 [core.py:842] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 340.00 MiB. GPU 0 has a total capacity of 44.39 GiB of which 124.00 MiB is free. Process 377950 has 26.70 GiB memory in use. Including non-PyTorch memory, this process has 17.56 GiB memory in use. Of the allocated memory 17.04 GiB is allocated by PyTorch, and 18.72 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=377953)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=377953)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=377953)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=377953)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=377953)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=377953)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=377953)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=377953)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=377953)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=377953)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=377953)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=377953)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=377953)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=377953)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=377953)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=377953)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=377953)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=377953)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[1;36m(EngineCore_DP0 pid=377953)[0;0m     self.driver_worker.load_model()
[1;36m(EngineCore_DP0 pid=377953)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 273, in load_model
[1;36m(EngineCore_DP0 pid=377953)[0;0m     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[1;36m(EngineCore_DP0 pid=377953)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3276, in load_model
[1;36m(EngineCore_DP0 pid=377953)[0;0m     self.model = model_loader.load_model(
[1;36m(EngineCore_DP0 pid=377953)[0;0m                  ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=377953)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
[1;36m(EngineCore_DP0 pid=377953)[0;0m     model = initialize_model(
[1;36m(EngineCore_DP0 pid=377953)[0;0m             ^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=377953)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py", line 55, in initialize_model
[1;36m(EngineCore_DP0 pid=377953)[0;0m     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(EngineCore_DP0 pid=377953)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=377953)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 279, in __init__
[1;36m(EngineCore_DP0 pid=377953)[0;0m     self.model = Qwen3Model(
[1;36m(EngineCore_DP0 pid=377953)[0;0m                  ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=377953)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=377953)[0;0m     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=377953)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 253, in __init__
[1;36m(EngineCore_DP0 pid=377953)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=377953)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=377953)[0;0m     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=377953)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 337, in __init__
[1;36m(EngineCore_DP0 pid=377953)[0;0m     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(EngineCore_DP0 pid=377953)[0;0m                                                     ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=377953)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 651, in make_layers
[1;36m(EngineCore_DP0 pid=377953)[0;0m     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(EngineCore_DP0 pid=377953)[0;0m                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=377953)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 339, in <lambda>
[1;36m(EngineCore_DP0 pid=377953)[0;0m     lambda prefix: decoder_layer_type(
[1;36m(EngineCore_DP0 pid=377953)[0;0m                    ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=377953)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 201, in __init__
[1;36m(EngineCore_DP0 pid=377953)[0;0m     self.mlp = Qwen3MLP(
[1;36m(EngineCore_DP0 pid=377953)[0;0m                ^^^^^^^^^
[1;36m(EngineCore_DP0 pid=377953)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 84, in __init__
[1;36m(EngineCore_DP0 pid=377953)[0;0m     self.gate_up_proj = MergedColumnParallelLinear(
[1;36m(EngineCore_DP0 pid=377953)[0;0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=377953)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 631, in __init__
[1;36m(EngineCore_DP0 pid=377953)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=377953)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 484, in __init__
[1;36m(EngineCore_DP0 pid=377953)[0;0m     self.quant_method.create_weights(
[1;36m(EngineCore_DP0 pid=377953)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 215, in create_weights
[1;36m(EngineCore_DP0 pid=377953)[0;0m     data=torch.empty(
[1;36m(EngineCore_DP0 pid=377953)[0;0m          ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=377953)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_device.py", line 103, in __torch_function__
[1;36m(EngineCore_DP0 pid=377953)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=377953)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=377953)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 340.00 MiB. GPU 0 has a total capacity of 44.39 GiB of which 124.00 MiB is free. Process 377950 has 26.70 GiB memory in use. Including non-PyTorch memory, this process has 17.56 GiB memory in use. Of the allocated memory 17.04 GiB is allocated by PyTorch, and 18.72 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W1204 13:18:15.797768470 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank0]:[W1204 13:18:15.945553322 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: Qwen/Qwen3-14B (backend=vllm)
[ModelCache] Using max_model_len=40960 for Qwen/Qwen3-14B
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 13:18:36 [utils.py:253] non-default args: {'max_model_len': 40960, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'Qwen/Qwen3-14B'}
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: Qwen/Qwen3-14B (backend=vllm)
[ModelCache] Using max_model_len=40960 for Qwen/Qwen3-14B
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 13:18:36 [utils.py:253] non-default args: {'max_model_len': 40960, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'Qwen/Qwen3-14B'}
INFO 12-04 13:18:36 [model.py:631] Resolved architecture: Qwen3ForCausalLM
INFO 12-04 13:18:36 [model.py:1745] Using max model len 40960
INFO 12-04 13:18:36 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
INFO 12-04 13:18:36 [model.py:631] Resolved architecture: Qwen3ForCausalLM
INFO 12-04 13:18:36 [model.py:1745] Using max model len 40960
INFO 12-04 13:18:36 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=379898)[0;0m INFO 12-04 13:18:50 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='Qwen/Qwen3-14B', speculative_config=None, tokenizer='Qwen/Qwen3-14B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-14B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=379904)[0;0m INFO 12-04 13:18:50 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='Qwen/Qwen3-14B', speculative_config=None, tokenizer='Qwen/Qwen3-14B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-14B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=379898)[0;0m INFO 12-04 13:18:51 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.98:44127 backend=nccl
[W1204 13:18:51.340949037 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-25-0.rc.tch.harvard.edu]:44127 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=379898)[0;0m INFO 12-04 13:18:51 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=379904)[0;0m INFO 12-04 13:18:52 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.98:40561 backend=nccl
[W1204 13:18:52.518985129 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-25-0.rc.tch.harvard.edu]:40561 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=379904)[0;0m INFO 12-04 13:18:52 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=379898)[0;0m INFO 12-04 13:18:52 [gpu_model_runner.py:3259] Starting to load model Qwen/Qwen3-14B...
[1;36m(EngineCore_DP0 pid=379904)[0;0m INFO 12-04 13:18:52 [gpu_model_runner.py:3259] Starting to load model Qwen/Qwen3-14B...
[1;36m(EngineCore_DP0 pid=379898)[0;0m INFO 12-04 13:18:53 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=379898)[0;0m INFO 12-04 13:18:53 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=379904)[0;0m INFO 12-04 13:18:53 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=379904)[0;0m INFO 12-04 13:18:53 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=379898)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/8 [00:00<?, ?it/s]
[1;36m(EngineCore_DP0 pid=379904)[0;0m ERROR 12-04 13:18:54 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=379904)[0;0m ERROR 12-04 13:18:54 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=379904)[0;0m ERROR 12-04 13:18:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=379904)[0;0m ERROR 12-04 13:18:54 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=379904)[0;0m ERROR 12-04 13:18:54 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=379904)[0;0m ERROR 12-04 13:18:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=379904)[0;0m ERROR 12-04 13:18:54 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=379904)[0;0m ERROR 12-04 13:18:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=379904)[0;0m ERROR 12-04 13:18:54 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=379904)[0;0m ERROR 12-04 13:18:54 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=379904)[0;0m ERROR 12-04 13:18:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=379904)[0;0m ERROR 12-04 13:18:54 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=379904)[0;0m ERROR 12-04 13:18:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[1;36m(EngineCore_DP0 pid=379904)[0;0m ERROR 12-04 13:18:54 [core.py:842]     self.driver_worker.load_model()
[1;36m(EngineCore_DP0 pid=379904)[0;0m ERROR 12-04 13:18:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 273, in load_model
[1;36m(EngineCore_DP0 pid=379904)[0;0m ERROR 12-04 13:18:54 [core.py:842]     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[1;36m(EngineCore_DP0 pid=379904)[0;0m ERROR 12-04 13:18:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3276, in load_model
[1;36m(EngineCore_DP0 pid=379904)[0;0m ERROR 12-04 13:18:54 [core.py:842]     self.model = model_loader.load_model(
[1;36m(EngineCore_DP0 pid=379904)[0;0m ERROR 12-04 13:18:54 [core.py:842]                  ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=379904)[0;0m ERROR 12-04 13:18:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
[1;36m(EngineCore_DP0 pid=379904)[0;0m ERROR 12-04 13:18:54 [core.py:842]     model = initialize_model(
[1;36m(EngineCore_DP0 pid=379904)[0;0m ERROR 12-04 13:18:54 [core.py:842]             ^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=379904)[0;0m ERROR 12-04 13:18:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py", line 55, in initialize_model
[1;36m(EngineCore_DP0 pid=379904)[0;0m ERROR 12-04 13:18:54 [core.py:842]     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(EngineCore_DP0 pid=379904)[0;0m ERROR 12-04 13:18:54 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=379904)[0;0m ERROR 12-04 13:18:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 279, in __init__
[1;36m(EngineCore_DP0 pid=379904)[0;0m ERROR 12-04 13:18:54 [core.py:842]     self.model = Qwen3Model(
[1;36m(EngineCore_DP0 pid=379904)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=379904)[0;0m ERROR 12-04 13:18:54 [core.py:842]                  ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=379904)[0;0m ERROR 12-04 13:18:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=379904)[0;0m ERROR 12-04 13:18:54 [core.py:842]     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=379904)[0;0m ERROR 12-04 13:18:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 253, in __init__
[1;36m(EngineCore_DP0 pid=379904)[0;0m ERROR 12-04 13:18:54 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=379904)[0;0m ERROR 12-04 13:18:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=379904)[0;0m ERROR 12-04 13:18:54 [core.py:842]     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=379904)[0;0m ERROR 12-04 13:18:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 337, in __init__
[1;36m(EngineCore_DP0 pid=379904)[0;0m ERROR 12-04 13:18:54 [core.py:842]     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(EngineCore_DP0 pid=379904)[0;0m ERROR 12-04 13:18:54 [core.py:842]                                                     ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=379904)[0;0m ERROR 12-04 13:18:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 651, in make_layers
[1;36m(EngineCore_DP0 pid=379904)[0;0m ERROR 12-04 13:18:54 [core.py:842]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(EngineCore_DP0 pid=379904)[0;0m ERROR 12-04 13:18:54 [core.py:842]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=379904)[0;0m ERROR 12-04 13:18:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 339, in <lambda>
[1;36m(EngineCore_DP0 pid=379904)[0;0m ERROR 12-04 13:18:54 [core.py:842]     lambda prefix: decoder_layer_type(
[1;36m(EngineCore_DP0 pid=379904)[0;0m ERROR 12-04 13:18:54 [core.py:842]                    ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=379904)[0;0m ERROR 12-04 13:18:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 185, in __init__
[1;36m(EngineCore_DP0 pid=379904)[0;0m ERROR 12-04 13:18:54 [core.py:842]     self.self_attn = Qwen3Attention(
[1;36m(EngineCore_DP0 pid=379904)[0;0m ERROR 12-04 13:18:54 [core.py:842]                      ^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=379904)[0;0m ERROR 12-04 13:18:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 95, in __init__
[1;36m(EngineCore_DP0 pid=379904)[0;0m ERROR 12-04 13:18:54 [core.py:842]     self.qkv_proj = QKVParallelLinear(
[1;36m(EngineCore_DP0 pid=379904)[0;0m ERROR 12-04 13:18:54 [core.py:842]                     ^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=379904)[0;0m ERROR 12-04 13:18:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 935, in __init__
[1;36m(EngineCore_DP0 pid=379904)[0;0m ERROR 12-04 13:18:54 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=379904)[0;0m ERROR 12-04 13:18:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 484, in __init__
[1;36m(EngineCore_DP0 pid=379904)[0;0m ERROR 12-04 13:18:54 [core.py:842]     self.quant_method.create_weights(
[1;36m(EngineCore_DP0 pid=379904)[0;0m ERROR 12-04 13:18:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 215, in create_weights
[1;36m(EngineCore_DP0 pid=379904)[0;0m ERROR 12-04 13:18:54 [core.py:842]     data=torch.empty(
[1;36m(EngineCore_DP0 pid=379904)[0;0m ERROR 12-04 13:18:54 [core.py:842]          ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=379904)[0;0m ERROR 12-04 13:18:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_device.py", line 103, in __torch_function__
[1;36m(EngineCore_DP0 pid=379904)[0;0m ERROR 12-04 13:18:54 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=379904)[0;0m ERROR 12-04 13:18:54 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=379904)[0;0m ERROR 12-04 13:18:54 [core.py:842] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 70.00 MiB. GPU 0 has a total capacity of 44.39 GiB of which 20.00 MiB is free. Process 379898 has 28.15 GiB memory in use. Including non-PyTorch memory, this process has 16.21 GiB memory in use. Of the allocated memory 15.69 GiB is allocated by PyTorch, and 18.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=379904)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=379904)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=379904)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=379904)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=379904)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=379904)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=379904)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=379904)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=379904)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=379904)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=379904)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=379904)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=379904)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=379904)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=379904)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=379904)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=379904)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=379904)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[1;36m(EngineCore_DP0 pid=379904)[0;0m     self.driver_worker.load_model()
[1;36m(EngineCore_DP0 pid=379904)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 273, in load_model
[1;36m(EngineCore_DP0 pid=379904)[0;0m     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[1;36m(EngineCore_DP0 pid=379904)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3276, in load_model
[1;36m(EngineCore_DP0 pid=379904)[0;0m     self.model = model_loader.load_model(
[1;36m(EngineCore_DP0 pid=379904)[0;0m                  ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=379904)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
[1;36m(EngineCore_DP0 pid=379904)[0;0m     model = initialize_model(
[1;36m(EngineCore_DP0 pid=379904)[0;0m             ^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=379904)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py", line 55, in initialize_model
[1;36m(EngineCore_DP0 pid=379904)[0;0m     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(EngineCore_DP0 pid=379904)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=379904)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 279, in __init__
[1;36m(EngineCore_DP0 pid=379904)[0;0m     self.model = Qwen3Model(
[1;36m(EngineCore_DP0 pid=379904)[0;0m                  ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=379904)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=379904)[0;0m     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=379904)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 253, in __init__
[1;36m(EngineCore_DP0 pid=379904)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=379904)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 276, in __init__
[1;36m(EngineCore_DP0 pid=379904)[0;0m     old_init(self, **kwargs)
[1;36m(EngineCore_DP0 pid=379904)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 337, in __init__
[1;36m(EngineCore_DP0 pid=379904)[0;0m     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(EngineCore_DP0 pid=379904)[0;0m                                                     ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=379904)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 651, in make_layers
[1;36m(EngineCore_DP0 pid=379904)[0;0m     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(EngineCore_DP0 pid=379904)[0;0m                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=379904)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 339, in <lambda>
[1;36m(EngineCore_DP0 pid=379904)[0;0m     lambda prefix: decoder_layer_type(
[1;36m(EngineCore_DP0 pid=379904)[0;0m                    ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=379904)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 185, in __init__
[1;36m(EngineCore_DP0 pid=379904)[0;0m     self.self_attn = Qwen3Attention(
[1;36m(EngineCore_DP0 pid=379904)[0;0m                      ^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=379904)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py", line 95, in __init__
[1;36m(EngineCore_DP0 pid=379904)[0;0m     self.qkv_proj = QKVParallelLinear(
[1;36m(EngineCore_DP0 pid=379904)[0;0m                     ^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=379904)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 935, in __init__
[1;36m(EngineCore_DP0 pid=379904)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=379904)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 484, in __init__
[1;36m(EngineCore_DP0 pid=379904)[0;0m     self.quant_method.create_weights(
[1;36m(EngineCore_DP0 pid=379904)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 215, in create_weights
[1;36m(EngineCore_DP0 pid=379904)[0;0m     data=torch.empty(
[1;36m(EngineCore_DP0 pid=379904)[0;0m          ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=379904)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_device.py", line 103, in __torch_function__
[1;36m(EngineCore_DP0 pid=379904)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=379904)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=379904)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 70.00 MiB. GPU 0 has a total capacity of 44.39 GiB of which 20.00 MiB is free. Process 379898 has 28.15 GiB memory in use. Including non-PyTorch memory, this process has 16.21 GiB memory in use. Of the allocated memory 15.69 GiB is allocated by PyTorch, and 18.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=379898)[0;0m Loading safetensors checkpoint shards:  12% Completed | 1/8 [00:00<00:06,  1.06it/s]
[rank0]:[W1204 13:18:54.230489330 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[1;36m(EngineCore_DP0 pid=379898)[0;0m Loading safetensors checkpoint shards:  25% Completed | 2/8 [00:01<00:05,  1.06it/s]
[1;36m(EngineCore_DP0 pid=379898)[0;0m Loading safetensors checkpoint shards:  38% Completed | 3/8 [00:02<00:04,  1.13it/s]
[1;36m(EngineCore_DP0 pid=379898)[0;0m Loading safetensors checkpoint shards:  50% Completed | 4/8 [00:03<00:03,  1.21it/s]
[1;36m(EngineCore_DP0 pid=379898)[0;0m Loading safetensors checkpoint shards:  62% Completed | 5/8 [00:04<00:02,  1.26it/s]
[1;36m(EngineCore_DP0 pid=379898)[0;0m Loading safetensors checkpoint shards:  75% Completed | 6/8 [00:04<00:01,  1.31it/s]
[1;36m(EngineCore_DP0 pid=379898)[0;0m Loading safetensors checkpoint shards:  88% Completed | 7/8 [00:05<00:00,  1.36it/s]
[1;36m(EngineCore_DP0 pid=379898)[0;0m Loading safetensors checkpoint shards: 100% Completed | 8/8 [00:05<00:00,  1.61it/s]
[1;36m(EngineCore_DP0 pid=379898)[0;0m Loading safetensors checkpoint shards: 100% Completed | 8/8 [00:05<00:00,  1.35it/s]
[1;36m(EngineCore_DP0 pid=379898)[0;0m 
[1;36m(EngineCore_DP0 pid=379898)[0;0m INFO 12-04 13:18:59 [default_loader.py:314] Loading weights took 6.00 seconds
[1;36m(EngineCore_DP0 pid=379898)[0;0m INFO 12-04 13:19:00 [gpu_model_runner.py:3338] Model loading took 27.5185 GiB memory and 7.123652 seconds
[1;36m(EngineCore_DP0 pid=379898)[0;0m INFO 12-04 13:19:08 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/76690ce19a/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=379898)[0;0m INFO 12-04 13:19:08 [backends.py:647] Dynamo bytecode transform time: 8.34 s
[1;36m(EngineCore_DP0 pid=379898)[0;0m INFO 12-04 13:19:12 [backends.py:210] Directly load the compiled graph(s) for dynamic shape from the cache, took 3.970 s
[1;36m(EngineCore_DP0 pid=379898)[0;0m INFO 12-04 13:19:14 [monitor.py:34] torch.compile takes 12.31 s in total
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: Qwen/Qwen3-14B (backend=vllm)
[ModelCache] Using max_model_len=40960 for Qwen/Qwen3-14B
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 13:19:15 [utils.py:253] non-default args: {'max_model_len': 40960, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'Qwen/Qwen3-14B'}
INFO 12-04 13:19:16 [model.py:631] Resolved architecture: Qwen3ForCausalLM
INFO 12-04 13:19:16 [model.py:1745] Using max model len 40960
INFO 12-04 13:19:16 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=379898)[0;0m INFO 12-04 13:19:17 [gpu_worker.py:359] Available KV cache memory: 11.42 GiB
[1;36m(EngineCore_DP0 pid=379898)[0;0m INFO 12-04 13:19:17 [kv_cache_utils.py:1229] GPU KV cache size: 74,832 tokens
[1;36m(EngineCore_DP0 pid=379898)[0;0m INFO 12-04 13:19:17 [kv_cache_utils.py:1234] Maximum concurrency for 40,960 tokens per request: 1.83x
[1;36m(EngineCore_DP0 pid=379898)[0;0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/51 [00:00<?, ?it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   2%|▏         | 1/51 [00:00<00:07,  7.12it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   4%|▍         | 2/51 [00:00<00:09,  5.00it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 3/51 [00:00<00:08,  5.54it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|▊         | 4/51 [00:00<00:07,  6.07it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  10%|▉         | 5/51 [00:00<00:07,  6.43it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  12%|█▏        | 6/51 [00:00<00:06,  6.71it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▎        | 7/51 [00:01<00:06,  7.02it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|█▌        | 8/51 [00:01<00:05,  7.20it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 9/51 [00:01<00:05,  7.72it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|█▉        | 10/51 [00:01<00:05,  6.91it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|██▏       | 11/51 [00:01<00:06,  6.31it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  24%|██▎       | 12/51 [00:01<00:05,  6.98it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 13/51 [00:01<00:05,  7.42it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|██▋       | 14/51 [00:02<00:04,  7.81it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▉       | 15/51 [00:02<00:04,  8.16it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███▏      | 16/51 [00:02<00:04,  8.34it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 17/51 [00:02<00:04,  7.30it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 19/51 [00:02<00:03,  8.69it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  41%|████      | 21/51 [00:02<00:03,  9.48it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|████▌     | 23/51 [00:02<00:02, 10.08it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|████▉     | 25/51 [00:03<00:02, 10.43it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 27/51 [00:03<00:02, 10.78it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 29/51 [00:03<00:01, 11.02it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  61%|██████    | 31/51 [00:03<00:01, 11.27it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  65%|██████▍   | 33/51 [00:03<00:01, 11.46it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|██████▊   | 35/51 [00:04<00:01, 11.38it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 37/51 [00:04<00:01, 11.56it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  76%|███████▋  | 39/51 [00:04<00:01, 11.80it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 41/51 [00:04<00:00, 12.00it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  84%|████████▍ | 43/51 [00:04<00:00, 12.17it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  88%|████████▊ | 45/51 [00:04<00:00, 12.37it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  92%|█████████▏| 47/51 [00:04<00:00, 12.73it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  96%|█████████▌| 49/51 [00:05<00:00, 13.13it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:05<00:00, 13.21it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:05<00:00,  9.72it/s]
[1;36m(EngineCore_DP0 pid=379898)[0;0m Capturing CUDA graphs (decode, FULL):   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graphs (decode, FULL):   6%|▌         | 2/35 [00:00<00:02, 11.19it/s]Capturing CUDA graphs (decode, FULL):  11%|█▏        | 4/35 [00:00<00:02, 11.40it/s]Capturing CUDA graphs (decode, FULL):  17%|█▋        | 6/35 [00:00<00:02, 11.53it/s]Capturing CUDA graphs (decode, FULL):  23%|██▎       | 8/35 [00:00<00:02, 11.69it/s]Capturing CUDA graphs (decode, FULL):  29%|██▊       | 10/35 [00:00<00:02, 12.07it/s]Capturing CUDA graphs (decode, FULL):  34%|███▍      | 12/35 [00:01<00:01, 12.22it/s]Capturing CUDA graphs (decode, FULL):  40%|████      | 14/35 [00:01<00:01, 12.40it/s]Capturing CUDA graphs (decode, FULL):  46%|████▌     | 16/35 [00:01<00:01, 12.45it/s]Capturing CUDA graphs (decode, FULL):  51%|█████▏    | 18/35 [00:01<00:01, 12.75it/s]Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 20/35 [00:01<00:01, 12.86it/s]Capturing CUDA graphs (decode, FULL):  63%|██████▎   | 22/35 [00:01<00:01, 12.98it/s]Capturing CUDA graphs (decode, FULL):  69%|██████▊   | 24/35 [00:01<00:00, 13.03it/s]Capturing CUDA graphs (decode, FULL):  74%|███████▍  | 26/35 [00:02<00:00, 13.21it/s]Capturing CUDA graphs (decode, FULL):  80%|████████  | 28/35 [00:02<00:00, 13.39it/s]Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 30/35 [00:02<00:00, 13.59it/s]Capturing CUDA graphs (decode, FULL):  91%|█████████▏| 32/35 [00:02<00:00, 13.98it/s]Capturing CUDA graphs (decode, FULL):  97%|█████████▋| 34/35 [00:02<00:00, 14.10it/s]Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:02<00:00, 12.99it/s]
[1;36m(EngineCore_DP0 pid=379898)[0;0m INFO 12-04 13:19:26 [gpu_model_runner.py:4244] Graph capturing finished in 9 secs, took 0.65 GiB
[1;36m(EngineCore_DP0 pid=379898)[0;0m INFO 12-04 13:19:26 [core.py:250] init engine (profile, create kv cache, warmup model) took 26.21 seconds
[1;36m(EngineCore_DP0 pid=381211)[0;0m INFO 12-04 13:19:27 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='Qwen/Qwen3-14B', speculative_config=None, tokenizer='Qwen/Qwen3-14B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-14B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
INFO 12-04 13:19:27 [llm.py:352] Supported tasks: ['generate']
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 381.06it/s]
[1;36m(EngineCore_DP0 pid=381211)[0;0m INFO 12-04 13:19:28 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.98:36545 backend=nccl
[W1204 13:19:28.242313924 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-25-0.rc.tch.harvard.edu]:36545 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=381211)[0;0m INFO 12-04 13:19:28 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=381211)[0;0m ERROR 12-04 13:19:28 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=381211)[0;0m ERROR 12-04 13:19:28 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=381211)[0;0m ERROR 12-04 13:19:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=381211)[0;0m ERROR 12-04 13:19:28 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=381211)[0;0m ERROR 12-04 13:19:28 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=381211)[0;0m ERROR 12-04 13:19:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=381211)[0;0m ERROR 12-04 13:19:28 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=381211)[0;0m ERROR 12-04 13:19:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=381211)[0;0m ERROR 12-04 13:19:28 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=381211)[0;0m ERROR 12-04 13:19:28 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=381211)[0;0m ERROR 12-04 13:19:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=381211)[0;0m ERROR 12-04 13:19:28 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=381211)[0;0m ERROR 12-04 13:19:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=381211)[0;0m ERROR 12-04 13:19:28 [core.py:842]     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=381211)[0;0m ERROR 12-04 13:19:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=381211)[0;0m ERROR 12-04 13:19:28 [core.py:842]     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=381211)[0;0m ERROR 12-04 13:19:28 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=381211)[0;0m ERROR 12-04 13:19:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=381211)[0;0m ERROR 12-04 13:19:28 [core.py:842]     raise ValueError(
[1;36m(EngineCore_DP0 pid=381211)[0;0m ERROR 12-04 13:19:28 [core.py:842] ValueError: Free memory on device (2.24/44.39 GiB) on startup is less than desired GPU memory utilization (0.9, 39.95 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[1;36m(EngineCore_DP0 pid=381211)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=381211)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=381211)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=381211)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=381211)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=381211)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=381211)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=381211)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=381211)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=381211)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=381211)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=381211)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=381211)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=381211)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=381211)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=381211)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=381211)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=381211)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=381211)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=381211)[0;0m     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=381211)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=381211)[0;0m     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=381211)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=381211)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=381211)[0;0m     raise ValueError(
[1;36m(EngineCore_DP0 pid=381211)[0;0m ValueError: Free memory on device (2.24/44.39 GiB) on startup is less than desired GPU memory utilization (0.9, 39.95 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W1204 13:19:29.054658305 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:15<00:00, 15.20s/it, est. speed input: 9.54 toks/s, output: 25.13 toks/s]Processed prompts: 100%|██████████| 1/1 [00:15<00:00, 15.20s/it, est. speed input: 9.54 toks/s, output: 25.13 toks/s]Processed prompts: 100%|██████████| 1/1 [00:15<00:00, 15.20s/it, est. speed input: 9.54 toks/s, output: 25.13 toks/s]
Agent 1 response: The three typing speed measurements are 47 WPM, 52 WPM, and 57 WPM (52 + 5). Adding them: 47 + 52 + ...

--- Problem 1/2, Round 1, Agent 2/3 ---
[ModelCache] Using cached model: Qwen/Qwen3-14B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1064.81it/s]
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: Qwen/Qwen3-14B (backend=vllm)
[ModelCache] Using max_model_len=40960 for Qwen/Qwen3-14B
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 13:19:50 [utils.py:253] non-default args: {'max_model_len': 40960, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'Qwen/Qwen3-14B'}
INFO 12-04 13:19:50 [model.py:631] Resolved architecture: Qwen3ForCausalLM
INFO 12-04 13:19:50 [model.py:1745] Using max model len 40960
INFO 12-04 13:19:50 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:17<00:00, 17.73s/it, est. speed input: 8.41 toks/s, output: 25.05 toks/s]Processed prompts: 100%|██████████| 1/1 [00:17<00:00, 17.73s/it, est. speed input: 8.41 toks/s, output: 25.05 toks/s]Processed prompts: 100%|██████████| 1/1 [00:17<00:00, 17.73s/it, est. speed input: 8.41 toks/s, output: 25.05 toks/s]
Agent 2 response: The average is found by summing all measurements and dividing by three. The three speeds are 47, 52,...

--- Problem 1/2, Round 1, Agent 3/3 ---
[ModelCache] Using cached model: Qwen/Qwen3-14B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 928.35it/s]
[1;36m(EngineCore_DP0 pid=382528)[0;0m INFO 12-04 13:20:01 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='Qwen/Qwen3-14B', speculative_config=None, tokenizer='Qwen/Qwen3-14B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-14B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=382528)[0;0m INFO 12-04 13:20:03 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.98:55957 backend=nccl
[W1204 13:20:03.767165546 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-25-0.rc.tch.harvard.edu]:55957 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=382528)[0;0m INFO 12-04 13:20:03 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=382528)[0;0m ERROR 12-04 13:20:03 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=382528)[0;0m ERROR 12-04 13:20:03 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=382528)[0;0m ERROR 12-04 13:20:03 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=382528)[0;0m ERROR 12-04 13:20:03 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=382528)[0;0m ERROR 12-04 13:20:03 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=382528)[0;0m ERROR 12-04 13:20:03 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=382528)[0;0m ERROR 12-04 13:20:03 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=382528)[0;0m ERROR 12-04 13:20:03 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=382528)[0;0m ERROR 12-04 13:20:03 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=382528)[0;0m ERROR 12-04 13:20:03 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=382528)[0;0m ERROR 12-04 13:20:03 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=382528)[0;0m ERROR 12-04 13:20:03 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=382528)[0;0m ERROR 12-04 13:20:03 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=382528)[0;0m ERROR 12-04 13:20:03 [core.py:842]     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=382528)[0;0m ERROR 12-04 13:20:03 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=382528)[0;0m ERROR 12-04 13:20:03 [core.py:842]     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=382528)[0;0m ERROR 12-04 13:20:03 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=382528)[0;0m ERROR 12-04 13:20:03 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=382528)[0;0m ERROR 12-04 13:20:03 [core.py:842]     raise ValueError(
[1;36m(EngineCore_DP0 pid=382528)[0;0m ERROR 12-04 13:20:03 [core.py:842] ValueError: Free memory on device (2.24/44.39 GiB) on startup is less than desired GPU memory utilization (0.9, 39.95 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[1;36m(EngineCore_DP0 pid=382528)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=382528)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=382528)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=382528)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=382528)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=382528)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=382528)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=382528)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=382528)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=382528)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=382528)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=382528)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=382528)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=382528)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=382528)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=382528)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=382528)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=382528)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=382528)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=382528)[0;0m     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=382528)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=382528)[0;0m     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=382528)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=382528)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=382528)[0;0m     raise ValueError(
[1;36m(EngineCore_DP0 pid=382528)[0;0m ValueError: Free memory on device (2.24/44.39 GiB) on startup is less than desired GPU memory utilization (0.9, 39.95 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W1204 13:20:04.640716439 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:21<00:00, 21.40s/it, est. speed input: 6.82 toks/s, output: 25.18 toks/s]Processed prompts: 100%|██████████| 1/1 [00:21<00:00, 21.40s/it, est. speed input: 6.82 toks/s, output: 25.18 toks/s]Processed prompts: 100%|██████████| 1/1 [00:21<00:00, 21.40s/it, est. speed input: 6.82 toks/s, output: 25.18 toks/s]
Agent 3 response: The three typing speeds are 47 WPM, 52 WPM, and 57 WPM (52 + 5). Adding these: 47 + 52 + 57 = 156. D...

--- Problem 1/2, Round 2, Agent 1/3 ---
Agent 1 receiving other agents' responses...
[ModelCache] Using cached model: Qwen/Qwen3-14B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 590.08it/s]
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: Qwen/Qwen3-14B (backend=vllm)
[ModelCache] Using max_model_len=40960 for Qwen/Qwen3-14B
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 13:20:25 [utils.py:253] non-default args: {'max_model_len': 40960, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'Qwen/Qwen3-14B'}
INFO 12-04 13:20:25 [model.py:631] Resolved architecture: Qwen3ForCausalLM
INFO 12-04 13:20:25 [model.py:1745] Using max model len 40960
INFO 12-04 13:20:25 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=383403)[0;0m INFO 12-04 13:20:36 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='Qwen/Qwen3-14B', speculative_config=None, tokenizer='Qwen/Qwen3-14B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-14B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=383403)[0;0m INFO 12-04 13:20:37 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.98:38103 backend=nccl
[W1204 13:20:37.341230613 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-25-0.rc.tch.harvard.edu]:38103 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:15<00:00, 15.08s/it, est. speed input: 35.40 toks/s, output: 24.93 toks/s]Processed prompts: 100%|██████████| 1/1 [00:15<00:00, 15.08s/it, est. speed input: 35.40 toks/s, output: 24.93 toks/s]Processed prompts: 100%|██████████| 1/1 [00:15<00:00, 15.08s/it, est. speed input: 35.40 toks/s, output: 24.93 toks/s]
Agent 1 response: The three typing speeds are 47 WPM, 52 WPM, and 57 WPM (52 + 5). Adding them: 47 + 52 + 57 = 156. Di...

--- Problem 1/2, Round 2, Agent 2/3 ---
Agent 2 receiving other agents' responses...
[ModelCache] Using cached model: Qwen/Qwen3-14B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 468.53it/s]
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=383403)[0;0m INFO 12-04 13:20:37 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=383403)[0;0m ERROR 12-04 13:20:38 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=383403)[0;0m ERROR 12-04 13:20:38 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=383403)[0;0m ERROR 12-04 13:20:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=383403)[0;0m ERROR 12-04 13:20:38 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=383403)[0;0m ERROR 12-04 13:20:38 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=383403)[0;0m ERROR 12-04 13:20:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=383403)[0;0m ERROR 12-04 13:20:38 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=383403)[0;0m ERROR 12-04 13:20:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=383403)[0;0m ERROR 12-04 13:20:38 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=383403)[0;0m ERROR 12-04 13:20:38 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=383403)[0;0m ERROR 12-04 13:20:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=383403)[0;0m ERROR 12-04 13:20:38 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=383403)[0;0m ERROR 12-04 13:20:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=383403)[0;0m ERROR 12-04 13:20:38 [core.py:842]     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=383403)[0;0m ERROR 12-04 13:20:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=383403)[0;0m ERROR 12-04 13:20:38 [core.py:842]     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=383403)[0;0m ERROR 12-04 13:20:38 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=383403)[0;0m ERROR 12-04 13:20:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=383403)[0;0m ERROR 12-04 13:20:38 [core.py:842]     raise ValueError(
[1;36m(EngineCore_DP0 pid=383403)[0;0m ERROR 12-04 13:20:38 [core.py:842] ValueError: Free memory on device (2.24/44.39 GiB) on startup is less than desired GPU memory utilization (0.9, 39.95 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[1;36m(EngineCore_DP0 pid=383403)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=383403)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=383403)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=383403)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=383403)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=383403)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=383403)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=383403)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=383403)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=383403)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=383403)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=383403)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=383403)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=383403)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=383403)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=383403)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=383403)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=383403)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=383403)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=383403)[0;0m     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=383403)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=383403)[0;0m     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=383403)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=383403)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=383403)[0;0m     raise ValueError(
[1;36m(EngineCore_DP0 pid=383403)[0;0m ValueError: Free memory on device (2.24/44.39 GiB) on startup is less than desired GPU memory utilization (0.9, 39.95 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W1204 13:20:38.150385659 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:12<00:00, 12.50s/it, est. speed input: 43.03 toks/s, output: 25.03 toks/s]Processed prompts: 100%|██████████| 1/1 [00:12<00:00, 12.50s/it, est. speed input: 43.03 toks/s, output: 25.03 toks/s]Processed prompts: 100%|██████████| 1/1 [00:12<00:00, 12.50s/it, est. speed input: 43.03 toks/s, output: 25.03 toks/s]
Agent 2 response: The three typing speeds are 47, 52, and 57. Adding them: 47 + 52 + 57 = 156. Dividing by 3: 156 ÷ 3 ...

--- Problem 1/2, Round 2, Agent 3/3 ---
Agent 3 receiving other agents' responses...
[ModelCache] Using cached model: Qwen/Qwen3-14B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 592.33it/s]
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: Qwen/Qwen3-14B (backend=vllm)
[ModelCache] Using max_model_len=40960 for Qwen/Qwen3-14B
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 13:20:59 [utils.py:253] non-default args: {'max_model_len': 40960, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'Qwen/Qwen3-14B'}
INFO 12-04 13:20:59 [model.py:631] Resolved architecture: Qwen3ForCausalLM
INFO 12-04 13:20:59 [model.py:1745] Using max model len 40960
INFO 12-04 13:20:59 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:12<00:00, 12.52s/it, est. speed input: 42.73 toks/s, output: 24.92 toks/s]Processed prompts: 100%|██████████| 1/1 [00:12<00:00, 12.52s/it, est. speed input: 42.73 toks/s, output: 24.92 toks/s]Processed prompts: 100%|██████████| 1/1 [00:12<00:00, 12.52s/it, est. speed input: 42.73 toks/s, output: 24.92 toks/s]
Agent 3 response: The three typing speeds are 47 WPM, 52 WPM, and 57 WPM (52 + 5). Summing them: 47 + 52 + 57 = 156. D...

--- Problem 1/2, Round 3, Agent 1/3 ---
Agent 1 receiving other agents' responses...
[ModelCache] Using cached model: Qwen/Qwen3-14B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 363.36it/s]
[1;36m(EngineCore_DP0 pid=385785)[0;0m INFO 12-04 13:21:10 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='Qwen/Qwen3-14B', speculative_config=None, tokenizer='Qwen/Qwen3-14B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-14B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=385785)[0;0m INFO 12-04 13:21:11 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.98:47779 backend=nccl
[W1204 13:21:11.314256668 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-25-0.rc.tch.harvard.edu]:47779 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=385785)[0;0m INFO 12-04 13:21:11 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=385785)[0;0m ERROR 12-04 13:21:12 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=385785)[0;0m ERROR 12-04 13:21:12 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=385785)[0;0m ERROR 12-04 13:21:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=385785)[0;0m ERROR 12-04 13:21:12 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=385785)[0;0m ERROR 12-04 13:21:12 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=385785)[0;0m ERROR 12-04 13:21:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=385785)[0;0m ERROR 12-04 13:21:12 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=385785)[0;0m ERROR 12-04 13:21:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=385785)[0;0m ERROR 12-04 13:21:12 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=385785)[0;0m ERROR 12-04 13:21:12 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=385785)[0;0m ERROR 12-04 13:21:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=385785)[0;0m ERROR 12-04 13:21:12 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=385785)[0;0m ERROR 12-04 13:21:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=385785)[0;0m ERROR 12-04 13:21:12 [core.py:842]     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=385785)[0;0m ERROR 12-04 13:21:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=385785)[0;0m ERROR 12-04 13:21:12 [core.py:842]     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=385785)[0;0m ERROR 12-04 13:21:12 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=385785)[0;0m ERROR 12-04 13:21:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=385785)[0;0m ERROR 12-04 13:21:12 [core.py:842]     raise ValueError(
[1;36m(EngineCore_DP0 pid=385785)[0;0m ERROR 12-04 13:21:12 [core.py:842] ValueError: Free memory on device (2.24/44.39 GiB) on startup is less than desired GPU memory utilization (0.9, 39.95 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[1;36m(EngineCore_DP0 pid=385785)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=385785)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=385785)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=385785)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=385785)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=385785)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=385785)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=385785)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=385785)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=385785)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=385785)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=385785)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=385785)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=385785)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=385785)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=385785)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=385785)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=385785)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=385785)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=385785)[0;0m     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=385785)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=385785)[0;0m     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=385785)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=385785)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=385785)[0;0m     raise ValueError(
[1;36m(EngineCore_DP0 pid=385785)[0;0m ValueError: Free memory on device (2.24/44.39 GiB) on startup is less than desired GPU memory utilization (0.9, 39.95 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W1204 13:21:12.136747568 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:12<00:00, 12.04s/it, est. speed input: 75.48 toks/s, output: 24.83 toks/s]Processed prompts: 100%|██████████| 1/1 [00:12<00:00, 12.04s/it, est. speed input: 75.48 toks/s, output: 24.83 toks/s]Processed prompts: 100%|██████████| 1/1 [00:12<00:00, 12.04s/it, est. speed input: 75.48 toks/s, output: 24.83 toks/s]
Agent 1 response: The three typing speeds are 47 WPM, 52 WPM, and 57 WPM (52 + 5). Summing them: 47 + 52 + 57 = 156. D...

--- Problem 1/2, Round 3, Agent 2/3 ---
Agent 2 receiving other agents' responses...
[ModelCache] Using cached model: Qwen/Qwen3-14B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 461.98it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:14<00:00, 14.48s/it, est. speed input: 63.07 toks/s, output: 25.01 toks/s]Processed prompts: 100%|██████████| 1/1 [00:14<00:00, 14.48s/it, est. speed input: 63.07 toks/s, output: 25.01 toks/s]Processed prompts: 100%|██████████| 1/1 [00:14<00:00, 14.48s/it, est. speed input: 63.07 toks/s, output: 25.01 toks/s]
Agent 2 response: The three measurements are 47, 52, and 57. Their sum is 156. Dividing by 3 yields 52.  

\boxed{52}...

--- Problem 1/2, Round 3, Agent 3/3 ---
Agent 3 receiving other agents' responses...
[ModelCache] Using cached model: Qwen/Qwen3-14B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 464.13it/s]
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: Qwen/Qwen3-14B (backend=vllm)
[ModelCache] Using max_model_len=40960 for Qwen/Qwen3-14B
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 13:21:33 [utils.py:253] non-default args: {'max_model_len': 40960, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'Qwen/Qwen3-14B'}
INFO 12-04 13:21:33 [model.py:631] Resolved architecture: Qwen3ForCausalLM
INFO 12-04 13:21:33 [model.py:1745] Using max model len 40960
INFO 12-04 13:21:33 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=387584)[0;0m INFO 12-04 13:21:44 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='Qwen/Qwen3-14B', speculative_config=None, tokenizer='Qwen/Qwen3-14B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-14B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=387584)[0;0m INFO 12-04 13:21:46 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.98:38413 backend=nccl
[W1204 13:21:46.622914864 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-25-0.rc.tch.harvard.edu]:38413 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=387584)[0;0m INFO 12-04 13:21:46 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:16<00:00, 16.75s/it, est. speed input: 54.33 toks/s, output: 24.83 toks/s]Processed prompts: 100%|██████████| 1/1 [00:16<00:00, 16.75s/it, est. speed input: 54.33 toks/s, output: 24.83 toks/s]Processed prompts: 100%|██████████| 1/1 [00:16<00:00, 16.75s/it, est. speed input: 54.33 toks/s, output: 24.83 toks/s]
Agent 3 response: The three typing speeds are 47 WPM, 52 WPM, and 57 WPM (52 + 5). Adding them: 47 + 52 + 57 = 156. Di...

Running accuracy: 1.000 ± 0.000

--- Problem 2/2, Round 1, Agent 1/3 ---
[ModelCache] Using cached model: Qwen/Qwen3-14B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1138.52it/s]
[1;36m(EngineCore_DP0 pid=387584)[0;0m ERROR 12-04 13:21:46 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=387584)[0;0m ERROR 12-04 13:21:46 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=387584)[0;0m ERROR 12-04 13:21:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=387584)[0;0m ERROR 12-04 13:21:46 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=387584)[0;0m ERROR 12-04 13:21:46 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=387584)[0;0m ERROR 12-04 13:21:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=387584)[0;0m ERROR 12-04 13:21:46 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=387584)[0;0m ERROR 12-04 13:21:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=387584)[0;0m ERROR 12-04 13:21:46 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=387584)[0;0m ERROR 12-04 13:21:46 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=387584)[0;0m ERROR 12-04 13:21:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=387584)[0;0m ERROR 12-04 13:21:46 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=387584)[0;0m ERROR 12-04 13:21:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=387584)[0;0m ERROR 12-04 13:21:46 [core.py:842]     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=387584)[0;0m ERROR 12-04 13:21:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=387584)[0;0m ERROR 12-04 13:21:46 [core.py:842]     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=387584)[0;0m ERROR 12-04 13:21:46 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=387584)[0;0m ERROR 12-04 13:21:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=387584)[0;0m ERROR 12-04 13:21:46 [core.py:842]     raise ValueError(
[1;36m(EngineCore_DP0 pid=387584)[0;0m ERROR 12-04 13:21:46 [core.py:842] ValueError: Free memory on device (2.24/44.39 GiB) on startup is less than desired GPU memory utilization (0.9, 39.95 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[1;36m(EngineCore_DP0 pid=387584)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=387584)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=387584)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=387584)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=387584)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=387584)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=387584)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=387584)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=387584)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=387584)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=387584)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=387584)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=387584)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=387584)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=387584)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=387584)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=387584)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=387584)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=387584)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=387584)[0;0m     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=387584)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=387584)[0;0m     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=387584)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=387584)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=387584)[0;0m     raise ValueError(
[1;36m(EngineCore_DP0 pid=387584)[0;0m ValueError: Free memory on device (2.24/44.39 GiB) on startup is less than desired GPU memory utilization (0.9, 39.95 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W1204 13:21:46.473685902 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.81s/it, est. speed input: 14.98 toks/s, output: 25.22 toks/s]Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.81s/it, est. speed input: 14.98 toks/s, output: 25.22 toks/s]Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.81s/it, est. speed input: 14.98 toks/s, output: 25.22 toks/s]
Agent 1 response: Jordan has 2 children, each requiring 5 diaper changes per day, resulting in $2 \times 5 = 10$ total...

--- Problem 2/2, Round 1, Agent 2/3 ---
[ModelCache] Using cached model: Qwen/Qwen3-14B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1324.38it/s]
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: Qwen/Qwen3-14B (backend=vllm)
[ModelCache] Using max_model_len=40960 for Qwen/Qwen3-14B
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 13:22:08 [utils.py:253] non-default args: {'max_model_len': 40960, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'Qwen/Qwen3-14B'}
INFO 12-04 13:22:08 [model.py:631] Resolved architecture: Qwen3ForCausalLM
INFO 12-04 13:22:08 [model.py:1745] Using max model len 40960
INFO 12-04 13:22:08 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:15<00:00, 15.90s/it, est. speed input: 7.61 toks/s, output: 25.15 toks/s]Processed prompts: 100%|██████████| 1/1 [00:15<00:00, 15.90s/it, est. speed input: 7.61 toks/s, output: 25.15 toks/s]Processed prompts: 100%|██████████| 1/1 [00:15<00:00, 15.90s/it, est. speed input: 7.61 toks/s, output: 25.15 toks/s]
Agent 2 response: The river flows uphill.  
A stone cannot be both heavy and light.  
What is the sound of one hand cl...

--- Problem 2/2, Round 1, Agent 3/3 ---
[ModelCache] Using cached model: Qwen/Qwen3-14B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1065.63it/s]
[1;36m(EngineCore_DP0 pid=388953)[0;0m INFO 12-04 13:22:19 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='Qwen/Qwen3-14B', speculative_config=None, tokenizer='Qwen/Qwen3-14B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-14B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=388953)[0;0m INFO 12-04 13:22:20 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.98:42761 backend=nccl
[W1204 13:22:20.086612518 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-25-0.rc.tch.harvard.edu]:42761 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=388953)[0;0m INFO 12-04 13:22:20 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:10<00:00, 10.83s/it, est. speed input: 10.90 toks/s, output: 25.03 toks/s]Processed prompts: 100%|██████████| 1/1 [00:10<00:00, 10.83s/it, est. speed input: 10.90 toks/s, output: 25.03 toks/s]Processed prompts: 100%|██████████| 1/1 [00:10<00:00, 10.83s/it, est. speed input: 10.90 toks/s, output: 25.03 toks/s]
Agent 3 response: Jordan has 2 children, each requiring 5 diaper changes per day, totaling $2 \times 5 = 10$ diapers p...

--- Problem 2/2, Round 2, Agent 1/3 ---
Agent 1 receiving other agents' responses...
[ModelCache] Using cached model: Qwen/Qwen3-14B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 477.71it/s]
[1;36m(EngineCore_DP0 pid=388953)[0;0m ERROR 12-04 13:22:20 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=388953)[0;0m ERROR 12-04 13:22:20 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=388953)[0;0m ERROR 12-04 13:22:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=388953)[0;0m ERROR 12-04 13:22:20 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=388953)[0;0m ERROR 12-04 13:22:20 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=388953)[0;0m ERROR 12-04 13:22:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=388953)[0;0m ERROR 12-04 13:22:20 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=388953)[0;0m ERROR 12-04 13:22:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=388953)[0;0m ERROR 12-04 13:22:20 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=388953)[0;0m ERROR 12-04 13:22:20 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=388953)[0;0m ERROR 12-04 13:22:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=388953)[0;0m ERROR 12-04 13:22:20 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=388953)[0;0m ERROR 12-04 13:22:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=388953)[0;0m ERROR 12-04 13:22:20 [core.py:842]     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=388953)[0;0m ERROR 12-04 13:22:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=388953)[0;0m ERROR 12-04 13:22:20 [core.py:842]     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=388953)[0;0m ERROR 12-04 13:22:20 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=388953)[0;0m ERROR 12-04 13:22:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=388953)[0;0m ERROR 12-04 13:22:20 [core.py:842]     raise ValueError(
[1;36m(EngineCore_DP0 pid=388953)[0;0m ERROR 12-04 13:22:20 [core.py:842] ValueError: Free memory on device (2.24/44.39 GiB) on startup is less than desired GPU memory utilization (0.9, 39.95 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[1;36m(EngineCore_DP0 pid=388953)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=388953)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=388953)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=388953)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=388953)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=388953)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=388953)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=388953)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=388953)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=388953)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=388953)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=388953)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=388953)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=388953)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=388953)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=388953)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=388953)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=388953)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=388953)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=388953)[0;0m     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=388953)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=388953)[0;0m     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=388953)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=388953)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=388953)[0;0m     raise ValueError(
[1;36m(EngineCore_DP0 pid=388953)[0;0m ValueError: Free memory on device (2.24/44.39 GiB) on startup is less than desired GPU memory utilization (0.9, 39.95 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W1204 13:22:21.891725259 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:12<00:00, 12.97s/it, est. speed input: 32.07 toks/s, output: 25.13 toks/s]Processed prompts: 100%|██████████| 1/1 [00:12<00:00, 12.97s/it, est. speed input: 32.07 toks/s, output: 25.13 toks/s]Processed prompts: 100%|██████████| 1/1 [00:12<00:00, 12.97s/it, est. speed input: 32.07 toks/s, output: 25.13 toks/s]
Agent 1 response: Jordan has 2 children, each requiring 5 diaper changes per day, totaling $2 \times 5 = 10$ diapers. ...

--- Problem 2/2, Round 2, Agent 2/3 ---
Agent 2 receiving other agents' responses...
[ModelCache] Using cached model: Qwen/Qwen3-14B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 700.33it/s]
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: Qwen/Qwen3-14B (backend=vllm)
[ModelCache] Using max_model_len=40960 for Qwen/Qwen3-14B
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 13:22:42 [utils.py:253] non-default args: {'max_model_len': 40960, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'Qwen/Qwen3-14B'}
INFO 12-04 13:22:42 [model.py:631] Resolved architecture: Qwen3ForCausalLM
INFO 12-04 13:22:42 [model.py:1745] Using max model len 40960
INFO 12-04 13:22:42 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:09<00:00,  9.81s/it, est. speed input: 42.80 toks/s, output: 24.96 toks/s]Processed prompts: 100%|██████████| 1/1 [00:09<00:00,  9.81s/it, est. speed input: 42.80 toks/s, output: 24.96 toks/s]Processed prompts: 100%|██████████| 1/1 [00:09<00:00,  9.81s/it, est. speed input: 42.80 toks/s, output: 24.96 toks/s]
Agent 2 response: The moon reflects the sun's light.  
A bell is silent when no one rings it.  
What is the shape of a...

--- Problem 2/2, Round 2, Agent 3/3 ---
Agent 3 receiving other agents' responses...
[ModelCache] Using cached model: Qwen/Qwen3-14B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 502.43it/s]
[1;36m(EngineCore_DP0 pid=390897)[0;0m INFO 12-04 13:22:53 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='Qwen/Qwen3-14B', speculative_config=None, tokenizer='Qwen/Qwen3-14B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-14B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=390897)[0;0m INFO 12-04 13:22:54 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.98:47875 backend=nccl
[W1204 13:22:54.339800975 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-25-0.rc.tch.harvard.edu]:47875 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=390897)[0;0m INFO 12-04 13:22:54 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=390897)[0;0m ERROR 12-04 13:22:55 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=390897)[0;0m ERROR 12-04 13:22:55 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=390897)[0;0m ERROR 12-04 13:22:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=390897)[0;0m ERROR 12-04 13:22:55 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=390897)[0;0m ERROR 12-04 13:22:55 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=390897)[0;0m ERROR 12-04 13:22:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=390897)[0;0m ERROR 12-04 13:22:55 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=390897)[0;0m ERROR 12-04 13:22:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=390897)[0;0m ERROR 12-04 13:22:55 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=390897)[0;0m ERROR 12-04 13:22:55 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=390897)[0;0m ERROR 12-04 13:22:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=390897)[0;0m ERROR 12-04 13:22:55 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=390897)[0;0m ERROR 12-04 13:22:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=390897)[0;0m ERROR 12-04 13:22:55 [core.py:842]     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=390897)[0;0m ERROR 12-04 13:22:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=390897)[0;0m ERROR 12-04 13:22:55 [core.py:842]     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=390897)[0;0m ERROR 12-04 13:22:55 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=390897)[0;0m ERROR 12-04 13:22:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=390897)[0;0m ERROR 12-04 13:22:55 [core.py:842]     raise ValueError(
[1;36m(EngineCore_DP0 pid=390897)[0;0m ERROR 12-04 13:22:55 [core.py:842] ValueError: Free memory on device (2.24/44.39 GiB) on startup is less than desired GPU memory utilization (0.9, 39.95 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[1;36m(EngineCore_DP0 pid=390897)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=390897)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=390897)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=390897)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=390897)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=390897)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=390897)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=390897)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=390897)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=390897)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=390897)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=390897)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=390897)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=390897)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=390897)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=390897)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=390897)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=390897)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=390897)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=390897)[0;0m     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=390897)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=390897)[0;0m     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=390897)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=390897)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=390897)[0;0m     raise ValueError(
[1;36m(EngineCore_DP0 pid=390897)[0;0m ValueError: Free memory on device (2.24/44.39 GiB) on startup is less than desired GPU memory utilization (0.9, 39.95 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:11<00:00, 11.91s/it, est. speed input: 35.02 toks/s, output: 24.86 toks/s]Processed prompts: 100%|██████████| 1/1 [00:11<00:00, 11.91s/it, est. speed input: 35.02 toks/s, output: 24.86 toks/s]Processed prompts: 100%|██████████| 1/1 [00:11<00:00, 11.91s/it, est. speed input: 35.02 toks/s, output: 24.86 toks/s]
Agent 3 response: Jordan has 2 children, each requiring 5 diaper changes per day, totaling $2 \times 5 = 10$ diapers p...

--- Problem 2/2, Round 3, Agent 1/3 ---
Agent 1 receiving other agents' responses...
[ModelCache] Using cached model: Qwen/Qwen3-14B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 348.42it/s]
[rank0]:[W1204 13:22:55.125681147 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:08<00:00,  8.66s/it, est. speed input: 82.03 toks/s, output: 25.07 toks/s]Processed prompts: 100%|██████████| 1/1 [00:08<00:00,  8.66s/it, est. speed input: 82.03 toks/s, output: 25.07 toks/s]Processed prompts: 100%|██████████| 1/1 [00:08<00:00,  8.66s/it, est. speed input: 82.03 toks/s, output: 25.07 toks/s]
Agent 1 response: Jordan has 2 children, each requiring 5 diaper changes per day, totaling $2 \times 5 = 10$ diapers. ...

--- Problem 2/2, Round 3, Agent 2/3 ---
Agent 2 receiving other agents' responses...
[ModelCache] Using cached model: Qwen/Qwen3-14B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 529.38it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.89s/it, est. speed input: 90.52 toks/s, output: 24.98 toks/s]Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.89s/it, est. speed input: 90.52 toks/s, output: 24.98 toks/s]Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.89s/it, est. speed input: 90.52 toks/s, output: 24.98 toks/s]
Agent 2 response: The wind does not carry words.  
A mirror shows what is not there.  
What is the color of silence?  ...

--- Problem 2/2, Round 3, Agent 3/3 ---
Agent 3 receiving other agents' responses...
[ModelCache] Using cached model: Qwen/Qwen3-14B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 511.13it/s]
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: Qwen/Qwen3-14B (backend=vllm)
[ModelCache] Using max_model_len=40960 for Qwen/Qwen3-14B
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 13:23:16 [utils.py:253] non-default args: {'max_model_len': 40960, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'Qwen/Qwen3-14B'}
INFO 12-04 13:23:16 [model.py:631] Resolved architecture: Qwen3ForCausalLM
INFO 12-04 13:23:16 [model.py:1745] Using max model len 40960
INFO 12-04 13:23:16 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:09<00:00,  9.56s/it, est. speed input: 74.33 toks/s, output: 24.88 toks/s]Processed prompts: 100%|██████████| 1/1 [00:09<00:00,  9.56s/it, est. speed input: 74.33 toks/s, output: 24.88 toks/s]Processed prompts: 100%|██████████| 1/1 [00:09<00:00,  9.57s/it, est. speed input: 74.33 toks/s, output: 24.88 toks/s]
[rank0]:[W1204 13:23:21.146981943 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Agent 3 response: Jordan has 2 children, each requiring 5 diaper changes per day, totaling $2 \times 5 = 10$ diapers p...

Running accuracy: 1.000 ± 0.000

============================================================
GENERATION & EVALUATION COMPLETE
============================================================
Results saved to: /home/ch269957/projects/slm_multiagent_debate/experiments/linux_single/results/gsm/gsm_Qwen3-14B_persona_enigma+zen+deep-sea_agents3_rounds3.json
Problems processed: 2
Final accuracy: 1.000 ± 0.000
============================================================
[ModelCache] Shut down vLLM model: vllm:Qwen/Qwen3-14B
[ModelCache] All models shut down
[1;36m(EngineCore_DP0 pid=392186)[0;0m INFO 12-04 13:23:28 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='Qwen/Qwen3-14B', speculative_config=None, tokenizer='Qwen/Qwen3-14B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-14B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=392186)[0;0m INFO 12-04 13:23:29 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.98:39141 backend=nccl
[W1204 13:23:29.558633988 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-25-0.rc.tch.harvard.edu]:39141 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=392186)[0;0m INFO 12-04 13:23:34 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=392186)[0;0m INFO 12-04 13:23:34 [gpu_model_runner.py:3259] Starting to load model Qwen/Qwen3-14B...
[1;36m(EngineCore_DP0 pid=392186)[0;0m INFO 12-04 13:23:35 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=392186)[0;0m INFO 12-04 13:23:35 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=392186)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/8 [00:00<?, ?it/s]
[1;36m(EngineCore_DP0 pid=392186)[0;0m Loading safetensors checkpoint shards:  12% Completed | 1/8 [00:00<00:04,  1.69it/s]
[1;36m(EngineCore_DP0 pid=392186)[0;0m Loading safetensors checkpoint shards:  25% Completed | 2/8 [00:01<00:03,  1.56it/s]
[1;36m(EngineCore_DP0 pid=392186)[0;0m Loading safetensors checkpoint shards:  38% Completed | 3/8 [00:01<00:03,  1.52it/s]
[1;36m(EngineCore_DP0 pid=392186)[0;0m Loading safetensors checkpoint shards:  50% Completed | 4/8 [00:02<00:02,  1.47it/s]
[1;36m(EngineCore_DP0 pid=392186)[0;0m Loading safetensors checkpoint shards:  62% Completed | 5/8 [00:03<00:02,  1.46it/s]
[1;36m(EngineCore_DP0 pid=392186)[0;0m Loading safetensors checkpoint shards:  75% Completed | 6/8 [00:04<00:01,  1.46it/s]
[1;36m(EngineCore_DP0 pid=392186)[0;0m Loading safetensors checkpoint shards:  88% Completed | 7/8 [00:04<00:00,  1.47it/s]
[1;36m(EngineCore_DP0 pid=392186)[0;0m Loading safetensors checkpoint shards: 100% Completed | 8/8 [00:05<00:00,  1.71it/s]
[1;36m(EngineCore_DP0 pid=392186)[0;0m Loading safetensors checkpoint shards: 100% Completed | 8/8 [00:05<00:00,  1.57it/s]
[1;36m(EngineCore_DP0 pid=392186)[0;0m 
[1;36m(EngineCore_DP0 pid=392186)[0;0m INFO 12-04 13:23:40 [default_loader.py:314] Loading weights took 5.14 seconds
[1;36m(EngineCore_DP0 pid=392186)[0;0m INFO 12-04 13:23:41 [gpu_model_runner.py:3338] Model loading took 27.5185 GiB memory and 6.080153 seconds
[1;36m(EngineCore_DP0 pid=392186)[0;0m INFO 12-04 13:23:49 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/76690ce19a/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=392186)[0;0m INFO 12-04 13:23:49 [backends.py:647] Dynamo bytecode transform time: 8.61 s
[1;36m(EngineCore_DP0 pid=392186)[0;0m INFO 12-04 13:23:55 [backends.py:210] Directly load the compiled graph(s) for dynamic shape from the cache, took 4.829 s
[1;36m(EngineCore_DP0 pid=392186)[0;0m INFO 12-04 13:23:56 [monitor.py:34] torch.compile takes 13.44 s in total
[1;36m(EngineCore_DP0 pid=392186)[0;0m INFO 12-04 13:23:59 [gpu_worker.py:359] Available KV cache memory: 10.93 GiB
[1;36m(EngineCore_DP0 pid=392186)[0;0m INFO 12-04 13:23:59 [kv_cache_utils.py:1229] GPU KV cache size: 71,616 tokens
[1;36m(EngineCore_DP0 pid=392186)[0;0m INFO 12-04 13:23:59 [kv_cache_utils.py:1234] Maximum concurrency for 40,960 tokens per request: 1.75x
[1;36m(EngineCore_DP0 pid=392186)[0;0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/51 [00:00<?, ?it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   2%|▏         | 1/51 [00:00<00:06,  7.45it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   4%|▍         | 2/51 [00:00<00:06,  7.40it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 3/51 [00:00<00:06,  7.39it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|▊         | 4/51 [00:00<00:06,  7.36it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  10%|▉         | 5/51 [00:00<00:06,  6.79it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  12%|█▏        | 6/51 [00:00<00:06,  7.14it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▎        | 7/51 [00:00<00:06,  7.32it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|█▌        | 8/51 [00:01<00:05,  7.50it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 9/51 [00:01<00:05,  8.00it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|█▉        | 10/51 [00:01<00:04,  8.31it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|██▏       | 11/51 [00:01<00:04,  8.64it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  24%|██▎       | 12/51 [00:01<00:04,  8.86it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 13/51 [00:01<00:04,  9.01it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|██▋       | 14/51 [00:01<00:04,  9.16it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▉       | 15/51 [00:01<00:03,  9.35it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███▏      | 16/51 [00:01<00:03,  9.07it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  35%|███▌      | 18/51 [00:02<00:03, 10.16it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  39%|███▉      | 20/51 [00:02<00:02, 10.57it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 22/51 [00:02<00:03,  8.84it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 24/51 [00:02<00:02,  9.80it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|█████     | 26/51 [00:02<00:02, 10.69it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▍    | 28/51 [00:03<00:02, 11.36it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  59%|█████▉    | 30/51 [00:03<00:01, 11.86it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 32/51 [00:03<00:01, 12.26it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 34/51 [00:03<00:01, 12.44it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████   | 36/51 [00:03<00:01, 12.73it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▍  | 38/51 [00:03<00:00, 13.09it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  78%|███████▊  | 40/51 [00:03<00:00, 13.37it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|████████▏ | 42/51 [00:04<00:00, 13.54it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▋ | 44/51 [00:04<00:00, 13.79it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  90%|█████████ | 46/51 [00:04<00:00, 12.99it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|█████████▍| 48/51 [00:04<00:00, 10.75it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  98%|█████████▊| 50/51 [00:04<00:00, 11.05it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:04<00:00, 10.35it/s]
[1;36m(EngineCore_DP0 pid=392186)[0;0m Capturing CUDA graphs (decode, FULL):   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graphs (decode, FULL):   6%|▌         | 2/35 [00:00<00:02, 11.64it/s]Capturing CUDA graphs (decode, FULL):  11%|█▏        | 4/35 [00:00<00:02, 12.03it/s]Capturing CUDA graphs (decode, FULL):  17%|█▋        | 6/35 [00:00<00:02, 12.19it/s]Capturing CUDA graphs (decode, FULL):  23%|██▎       | 8/35 [00:00<00:02, 12.34it/s]Capturing CUDA graphs (decode, FULL):  29%|██▊       | 10/35 [00:00<00:01, 12.72it/s]Capturing CUDA graphs (decode, FULL):  34%|███▍      | 12/35 [00:00<00:01, 12.97it/s]Capturing CUDA graphs (decode, FULL):  40%|████      | 14/35 [00:01<00:01, 13.16it/s]Capturing CUDA graphs (decode, FULL):  46%|████▌     | 16/35 [00:01<00:01, 13.31it/s]Capturing CUDA graphs (decode, FULL):  51%|█████▏    | 18/35 [00:01<00:01, 13.63it/s]Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 20/35 [00:01<00:01, 13.89it/s]Capturing CUDA graphs (decode, FULL):  63%|██████▎   | 22/35 [00:01<00:00, 14.20it/s]Capturing CUDA graphs (decode, FULL):  69%|██████▊   | 24/35 [00:01<00:00, 14.46it/s]Capturing CUDA graphs (decode, FULL):  74%|███████▍  | 26/35 [00:01<00:00, 14.74it/s]Capturing CUDA graphs (decode, FULL):  80%|████████  | 28/35 [00:02<00:00, 14.95it/s]Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 30/35 [00:02<00:00, 15.19it/s]Capturing CUDA graphs (decode, FULL):  91%|█████████▏| 32/35 [00:02<00:00, 15.70it/s]Capturing CUDA graphs (decode, FULL):  97%|█████████▋| 34/35 [00:02<00:00, 16.04it/s]Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:02<00:00, 14.19it/s]
[1;36m(EngineCore_DP0 pid=392186)[0;0m INFO 12-04 13:24:07 [gpu_model_runner.py:4244] Graph capturing finished in 8 secs, took 0.65 GiB
[1;36m(EngineCore_DP0 pid=392186)[0;0m INFO 12-04 13:24:07 [core.py:250] init engine (profile, create kv cache, warmup model) took 26.51 seconds
INFO 12-04 13:24:08 [llm.py:352] Supported tasks: ['generate']
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 490.45it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:15<00:00, 15.16s/it, est. speed input: 9.57 toks/s, output: 25.21 toks/s]Processed prompts: 100%|██████████| 1/1 [00:15<00:00, 15.16s/it, est. speed input: 9.57 toks/s, output: 25.21 toks/s]Processed prompts: 100%|██████████| 1/1 [00:15<00:00, 15.16s/it, est. speed input: 9.57 toks/s, output: 25.21 toks/s]
Agent 1 response: The three typing speed measurements are 47 WPM, 52 WPM, and 57 WPM (52 + 5). Adding them: 47 + 52 + ...

--- Problem 1/2, Round 1, Agent 2/3 ---
[ModelCache] Using cached model: Qwen/Qwen3-14B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1086.89it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:17<00:00, 17.62s/it, est. speed input: 8.46 toks/s, output: 25.20 toks/s]Processed prompts: 100%|██████████| 1/1 [00:17<00:00, 17.62s/it, est. speed input: 8.46 toks/s, output: 25.20 toks/s]Processed prompts: 100%|██████████| 1/1 [00:17<00:00, 17.62s/it, est. speed input: 8.46 toks/s, output: 25.20 toks/s]
Agent 2 response: The average is found by summing all measurements and dividing by three. The three speeds are 47, 52,...

--- Problem 1/2, Round 1, Agent 3/3 ---
[ModelCache] Using cached model: Qwen/Qwen3-14B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1095.40it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:21<00:00, 21.39s/it, est. speed input: 6.83 toks/s, output: 25.20 toks/s]Processed prompts: 100%|██████████| 1/1 [00:21<00:00, 21.39s/it, est. speed input: 6.83 toks/s, output: 25.20 toks/s]Processed prompts: 100%|██████████| 1/1 [00:21<00:00, 21.39s/it, est. speed input: 6.83 toks/s, output: 25.20 toks/s]
Agent 3 response: The three typing speeds are 47 WPM, 52 WPM, and 57 WPM (52 + 5). Adding these: 47 + 52 + 57 = 156. D...

--- Problem 1/2, Round 2, Agent 1/3 ---
Agent 1 receiving other agents' responses...
[ModelCache] Using cached model: Qwen/Qwen3-14B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 586.21it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:15<00:00, 15.00s/it, est. speed input: 35.60 toks/s, output: 25.07 toks/s]Processed prompts: 100%|██████████| 1/1 [00:15<00:00, 15.00s/it, est. speed input: 35.60 toks/s, output: 25.07 toks/s]Processed prompts: 100%|██████████| 1/1 [00:15<00:00, 15.00s/it, est. speed input: 35.60 toks/s, output: 25.07 toks/s]
Agent 1 response: The three typing speeds are 47 WPM, 52 WPM, and 57 WPM (52 + 5). Adding them: 47 + 52 + 57 = 156. Di...

--- Problem 1/2, Round 2, Agent 2/3 ---
Agent 2 receiving other agents' responses...
[ModelCache] Using cached model: Qwen/Qwen3-14B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 636.66it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:12<00:00, 12.48s/it, est. speed input: 43.12 toks/s, output: 25.09 toks/s]Processed prompts: 100%|██████████| 1/1 [00:12<00:00, 12.48s/it, est. speed input: 43.12 toks/s, output: 25.09 toks/s]Processed prompts: 100%|██████████| 1/1 [00:12<00:00, 12.48s/it, est. speed input: 43.12 toks/s, output: 25.09 toks/s]
Agent 2 response: The three typing speeds are 47, 52, and 57. Adding them: 47 + 52 + 57 = 156. Dividing by 3: 156 ÷ 3 ...

--- Problem 1/2, Round 2, Agent 3/3 ---
Agent 3 receiving other agents' responses...
[ModelCache] Using cached model: Qwen/Qwen3-14B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 675.52it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:12<00:00, 12.47s/it, est. speed input: 42.92 toks/s, output: 25.03 toks/s]Processed prompts: 100%|██████████| 1/1 [00:12<00:00, 12.47s/it, est. speed input: 42.92 toks/s, output: 25.03 toks/s]Processed prompts: 100%|██████████| 1/1 [00:12<00:00, 12.47s/it, est. speed input: 42.92 toks/s, output: 25.03 toks/s]
Agent 3 response: The three typing speeds are 47 WPM, 52 WPM, and 57 WPM (52 + 5). Summing them: 47 + 52 + 57 = 156. D...

--- Problem 1/2, Round 3, Agent 1/3 ---
Agent 1 receiving other agents' responses...
[ModelCache] Using cached model: Qwen/Qwen3-14B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 425.13it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:11<00:00, 11.98s/it, est. speed input: 75.87 toks/s, output: 24.96 toks/s]Processed prompts: 100%|██████████| 1/1 [00:11<00:00, 11.98s/it, est. speed input: 75.87 toks/s, output: 24.96 toks/s]Processed prompts: 100%|██████████| 1/1 [00:11<00:00, 11.98s/it, est. speed input: 75.87 toks/s, output: 24.96 toks/s]
Agent 1 response: The three typing speeds are 47 WPM, 52 WPM, and 57 WPM (52 + 5). Summing them: 47 + 52 + 57 = 156. D...

--- Problem 1/2, Round 3, Agent 2/3 ---
Agent 2 receiving other agents' responses...
[ModelCache] Using cached model: Qwen/Qwen3-14B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 468.53it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:14<00:00, 14.48s/it, est. speed input: 63.06 toks/s, output: 25.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:14<00:00, 14.48s/it, est. speed input: 63.06 toks/s, output: 25.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:14<00:00, 14.48s/it, est. speed input: 63.06 toks/s, output: 25.00 toks/s]
Agent 2 response: The three measurements are 47, 52, and 57. Their sum is 156. Dividing by 3 yields 52.  

\boxed{52}...

--- Problem 1/2, Round 3, Agent 3/3 ---
Agent 3 receiving other agents' responses...
[ModelCache] Using cached model: Qwen/Qwen3-14B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 480.83it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:16<00:00, 16.56s/it, est. speed input: 54.94 toks/s, output: 25.12 toks/s]Processed prompts: 100%|██████████| 1/1 [00:16<00:00, 16.56s/it, est. speed input: 54.94 toks/s, output: 25.12 toks/s]Processed prompts: 100%|██████████| 1/1 [00:16<00:00, 16.56s/it, est. speed input: 54.94 toks/s, output: 25.12 toks/s]
Agent 3 response: The three typing speeds are 47 WPM, 52 WPM, and 57 WPM (52 + 5). Adding them: 47 + 52 + 57 = 156. Di...

Running accuracy: 1.000 ± 0.000

--- Problem 2/2, Round 1, Agent 1/3 ---
[ModelCache] Using cached model: Qwen/Qwen3-14B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1236.16it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.77s/it, est. speed input: 15.05 toks/s, output: 25.34 toks/s]Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.77s/it, est. speed input: 15.05 toks/s, output: 25.34 toks/s]Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.77s/it, est. speed input: 15.05 toks/s, output: 25.34 toks/s]
Agent 1 response: Jordan has 2 children, each requiring 5 diaper changes per day, resulting in $2 \times 5 = 10$ total...

--- Problem 2/2, Round 1, Agent 2/3 ---
[ModelCache] Using cached model: Qwen/Qwen3-14B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1348.65it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:15<00:00, 15.81s/it, est. speed input: 7.66 toks/s, output: 25.31 toks/s]Processed prompts: 100%|██████████| 1/1 [00:15<00:00, 15.81s/it, est. speed input: 7.66 toks/s, output: 25.31 toks/s]Processed prompts: 100%|██████████| 1/1 [00:15<00:00, 15.81s/it, est. speed input: 7.66 toks/s, output: 25.31 toks/s]
Agent 2 response: The river flows uphill.  
A stone cannot be both heavy and light.  
What is the sound of one hand cl...

--- Problem 2/2, Round 1, Agent 3/3 ---
[ModelCache] Using cached model: Qwen/Qwen3-14B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1324.80it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:10<00:00, 10.76s/it, est. speed input: 10.97 toks/s, output: 25.19 toks/s]Processed prompts: 100%|██████████| 1/1 [00:10<00:00, 10.76s/it, est. speed input: 10.97 toks/s, output: 25.19 toks/s]Processed prompts: 100%|██████████| 1/1 [00:10<00:00, 10.76s/it, est. speed input: 10.97 toks/s, output: 25.19 toks/s]
Agent 3 response: Jordan has 2 children, each requiring 5 diaper changes per day, totaling $2 \times 5 = 10$ diapers p...

--- Problem 2/2, Round 2, Agent 1/3 ---
Agent 1 receiving other agents' responses...
[ModelCache] Using cached model: Qwen/Qwen3-14B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 660.83it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:12<00:00, 12.96s/it, est. speed input: 32.11 toks/s, output: 25.16 toks/s]Processed prompts: 100%|██████████| 1/1 [00:12<00:00, 12.96s/it, est. speed input: 32.11 toks/s, output: 25.16 toks/s]Processed prompts: 100%|██████████| 1/1 [00:12<00:00, 12.96s/it, est. speed input: 32.11 toks/s, output: 25.16 toks/s]
Agent 1 response: Jordan has 2 children, each requiring 5 diaper changes per day, totaling $2 \times 5 = 10$ diapers. ...

--- Problem 2/2, Round 2, Agent 2/3 ---
Agent 2 receiving other agents' responses...
[ModelCache] Using cached model: Qwen/Qwen3-14B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 726.29it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:09<00:00,  9.80s/it, est. speed input: 42.85 toks/s, output: 24.99 toks/s]Processed prompts: 100%|██████████| 1/1 [00:09<00:00,  9.80s/it, est. speed input: 42.85 toks/s, output: 24.99 toks/s]Processed prompts: 100%|██████████| 1/1 [00:09<00:00,  9.80s/it, est. speed input: 42.85 toks/s, output: 24.99 toks/s]
Agent 2 response: The moon reflects the sun's light.  
A bell is silent when no one rings it.  
What is the shape of a...

--- Problem 2/2, Round 2, Agent 3/3 ---
Agent 3 receiving other agents' responses...
[ModelCache] Using cached model: Qwen/Qwen3-14B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 720.05it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:11<00:00, 11.84s/it, est. speed input: 35.21 toks/s, output: 24.99 toks/s]Processed prompts: 100%|██████████| 1/1 [00:11<00:00, 11.84s/it, est. speed input: 35.21 toks/s, output: 24.99 toks/s]Processed prompts: 100%|██████████| 1/1 [00:11<00:00, 11.84s/it, est. speed input: 35.21 toks/s, output: 24.99 toks/s]
Agent 3 response: Jordan has 2 children, each requiring 5 diaper changes per day, totaling $2 \times 5 = 10$ diapers p...

--- Problem 2/2, Round 3, Agent 1/3 ---
Agent 1 receiving other agents' responses...
[ModelCache] Using cached model: Qwen/Qwen3-14B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 488.73it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:08<00:00,  8.69s/it, est. speed input: 81.68 toks/s, output: 24.96 toks/s]Processed prompts: 100%|██████████| 1/1 [00:08<00:00,  8.69s/it, est. speed input: 81.68 toks/s, output: 24.96 toks/s]Processed prompts: 100%|██████████| 1/1 [00:08<00:00,  8.69s/it, est. speed input: 81.68 toks/s, output: 24.96 toks/s]
Agent 1 response: Jordan has 2 children, each requiring 5 diaper changes per day, totaling $2 \times 5 = 10$ diapers. ...

--- Problem 2/2, Round 3, Agent 2/3 ---
Agent 2 receiving other agents' responses...
[ModelCache] Using cached model: Qwen/Qwen3-14B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 494.44it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.88s/it, est. speed input: 90.65 toks/s, output: 25.01 toks/s]Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.88s/it, est. speed input: 90.65 toks/s, output: 25.01 toks/s]Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.88s/it, est. speed input: 90.65 toks/s, output: 25.01 toks/s]
Agent 2 response: The wind does not carry words.  
A mirror shows what is not there.  
What is the color of silence?  ...

--- Problem 2/2, Round 3, Agent 3/3 ---
Agent 3 receiving other agents' responses...
[ModelCache] Using cached model: Qwen/Qwen3-14B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 511.56it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:09<00:00,  9.52s/it, est. speed input: 74.72 toks/s, output: 25.01 toks/s]Processed prompts: 100%|██████████| 1/1 [00:09<00:00,  9.52s/it, est. speed input: 74.72 toks/s, output: 25.01 toks/s]Processed prompts: 100%|██████████| 1/1 [00:09<00:00,  9.52s/it, est. speed input: 74.72 toks/s, output: 25.01 toks/s]
[rank0]:[W1204 13:28:01.339519789 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Agent 3 response: Jordan has 2 children, each requiring 5 diaper changes per day, totaling $2 \times 5 = 10$ diapers p...

Running accuracy: 1.000 ± 0.000

============================================================
GENERATION & EVALUATION COMPLETE
============================================================
Results saved to: /home/ch269957/projects/slm_multiagent_debate/experiments/linux_single/results/gsm/gsm_Qwen3-14B_persona_enigma+zen+deep-sea_agents3_rounds3.json
Problems processed: 2
Final accuracy: 1.000 ± 0.000
============================================================
[ModelCache] Shut down vLLM model: vllm:Qwen/Qwen3-14B
[ModelCache] All models shut down
