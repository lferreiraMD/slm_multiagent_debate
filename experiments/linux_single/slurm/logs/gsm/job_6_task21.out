Using persona diversity with 7 different personas
============================================================
GSM Task - Multiagent Debate
============================================================
Model: WeiboAI/VibeThinker-1.5B
Persona diversity mode:
  Agent 1: a radical anarchist who views all imposed structures and hie...
  Agent 2: an enigma machine operator whose primary filter is signal-to...
  Agent 3: a forensic pathologist who works backward from the failure s...
  Agent 4: a stand-up comedian who evaluates suggestions based on their...
  Agent 5: an expert chess grandmaster who analyzes all moves based on ...
  Agent 6: a Renaissance painter who values perspective, light, shadow,...
  Agent 7: a hermetic alchemist who seeks to transmute the problem into...
Agents: 7
Rounds: 3
Problems: 20
Dataset: /home/ch269957/projects/slm_multiagent_debate/data/gsm8k/test.jsonl
Generation params: {'temperature': 1.0, 'max_tokens': None, 'top_p': 1.0, 'n': 1}
============================================================

--- Problem 1/20, Round 1, Agent 1/7 ---
[ModelCache] Loading model: WeiboAI/VibeThinker-1.5B (backend=vllm)
[ModelCache] Using max_model_len=131072 for WeiboAI/VibeThinker-1.5B
[ModelCache] Detected 1 GPU(s), 44.3GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 13:58:51 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'WeiboAI/VibeThinker-1.5B'}
Using persona diversity with 7 different personas
============================================================
GSM Task - Multiagent Debate
============================================================
Model: WeiboAI/VibeThinker-1.5B
Persona diversity mode:
  Agent 1: a radical anarchist who views all imposed structures and hie...
  Agent 2: an enigma machine operator whose primary filter is signal-to...
  Agent 3: a forensic pathologist who works backward from the failure s...
  Agent 4: a stand-up comedian who evaluates suggestions based on their...
  Agent 5: an expert chess grandmaster who analyzes all moves based on ...
  Agent 6: a Renaissance painter who values perspective, light, shadow,...
  Agent 7: a hermetic alchemist who seeks to transmute the problem into...
Agents: 7
Rounds: 3
Problems: 20
Dataset: /home/ch269957/projects/slm_multiagent_debate/data/gsm8k/test.jsonl
Generation params: {'temperature': 1.0, 'max_tokens': None, 'top_p': 1.0, 'n': 1}
============================================================

--- Problem 1/20, Round 1, Agent 1/7 ---
[ModelCache] Loading model: WeiboAI/VibeThinker-1.5B (backend=vllm)
[ModelCache] Using max_model_len=131072 for WeiboAI/VibeThinker-1.5B
[ModelCache] Detected 1 GPU(s), 44.3GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 13:58:51 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'WeiboAI/VibeThinker-1.5B'}
INFO 12-04 13:58:52 [model.py:631] Resolved architecture: Qwen2ForCausalLM
WARNING 12-04 13:58:52 [model.py:1921] Your device 'Quadro RTX 8000' (with compute capability 7.5) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.
WARNING 12-04 13:58:52 [model.py:1971] Casting torch.bfloat16 to torch.float16.
INFO 12-04 13:58:52 [model.py:1745] Using max model len 131072
INFO 12-04 13:58:52 [model.py:631] Resolved architecture: Qwen2ForCausalLM
WARNING 12-04 13:58:52 [model.py:1921] Your device 'Quadro RTX 8000' (with compute capability 7.5) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.
WARNING 12-04 13:58:52 [model.py:1971] Casting torch.bfloat16 to torch.float16.
INFO 12-04 13:58:52 [model.py:1745] Using max model len 131072
INFO 12-04 13:58:53 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
INFO 12-04 13:58:53 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=3851658)[0;0m INFO 12-04 13:59:09 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='WeiboAI/VibeThinker-1.5B', speculative_config=None, tokenizer='WeiboAI/VibeThinker-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=WeiboAI/VibeThinker-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=3851657)[0;0m INFO 12-04 13:59:09 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='WeiboAI/VibeThinker-1.5B', speculative_config=None, tokenizer='WeiboAI/VibeThinker-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=WeiboAI/VibeThinker-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=3851657)[0;0m ERROR 12-04 13:59:10 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8
[1;36m(EngineCore_DP0 pid=3851658)[0;0m ERROR 12-04 13:59:10 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8
[1;36m(EngineCore_DP0 pid=3851657)[0;0m INFO 12-04 13:59:12 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.7:43279 backend=nccl
[1;36m(EngineCore_DP0 pid=3851658)[0;0m INFO 12-04 13:59:12 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.7:57413 backend=nccl
[W1204 13:59:12.382783682 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-23-2.rc.tch.harvard.edu]:43279 (errno: 97 - Address family not supported by protocol).
[W1204 13:59:12.382783691 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-23-2.rc.tch.harvard.edu]:57413 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=3851658)[0;0m INFO 12-04 13:59:12 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=3851657)[0;0m INFO 12-04 13:59:12 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=3851657)[0;0m INFO 12-04 13:59:13 [gpu_model_runner.py:3259] Starting to load model WeiboAI/VibeThinker-1.5B...
[1;36m(EngineCore_DP0 pid=3851658)[0;0m INFO 12-04 13:59:13 [gpu_model_runner.py:3259] Starting to load model WeiboAI/VibeThinker-1.5B...
[1;36m(EngineCore_DP0 pid=3851657)[0;0m INFO 12-04 13:59:14 [cuda.py:418] Valid backends: ['FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=3851657)[0;0m INFO 12-04 13:59:14 [cuda.py:427] Using FLASHINFER backend.
[1;36m(EngineCore_DP0 pid=3851658)[0;0m INFO 12-04 13:59:14 [cuda.py:418] Valid backends: ['FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=3851658)[0;0m INFO 12-04 13:59:14 [cuda.py:427] Using FLASHINFER backend.
[1;36m(EngineCore_DP0 pid=3851658)[0;0m INFO 12-04 13:59:15 [weight_utils.py:481] No model.safetensors.index.json found in remote.
[1;36m(EngineCore_DP0 pid=3851658)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
[1;36m(EngineCore_DP0 pid=3851657)[0;0m INFO 12-04 13:59:15 [weight_utils.py:481] No model.safetensors.index.json found in remote.
[1;36m(EngineCore_DP0 pid=3851657)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
[1;36m(EngineCore_DP0 pid=3851657)[0;0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:19<00:00, 19.42s/it]
[1;36m(EngineCore_DP0 pid=3851657)[0;0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:19<00:00, 19.42s/it]
[1;36m(EngineCore_DP0 pid=3851657)[0;0m 
[1;36m(EngineCore_DP0 pid=3851658)[0;0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:19<00:00, 19.56s/it]
[1;36m(EngineCore_DP0 pid=3851658)[0;0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:19<00:00, 19.56s/it]
[1;36m(EngineCore_DP0 pid=3851658)[0;0m 
[1;36m(EngineCore_DP0 pid=3851657)[0;0m INFO 12-04 13:59:35 [default_loader.py:314] Loading weights took 19.58 seconds
[1;36m(EngineCore_DP0 pid=3851658)[0;0m INFO 12-04 13:59:35 [default_loader.py:314] Loading weights took 19.86 seconds
[1;36m(EngineCore_DP0 pid=3851657)[0;0m INFO 12-04 13:59:35 [gpu_model_runner.py:3338] Model loading took 2.9110 GiB memory and 21.574242 seconds
[1;36m(EngineCore_DP0 pid=3851658)[0;0m INFO 12-04 13:59:36 [gpu_model_runner.py:3338] Model loading took 2.9110 GiB memory and 21.719679 seconds
[1;36m(EngineCore_DP0 pid=3851657)[0;0m INFO 12-04 13:59:52 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/6847b7a26b/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=3851658)[0;0m INFO 12-04 13:59:52 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/6847b7a26b/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=3851657)[0;0m INFO 12-04 13:59:52 [backends.py:647] Dynamo bytecode transform time: 15.97 s
[1;36m(EngineCore_DP0 pid=3851658)[0;0m INFO 12-04 13:59:52 [backends.py:647] Dynamo bytecode transform time: 15.84 s
[1;36m(EngineCore_DP0 pid=3851657)[0;0m INFO 12-04 13:59:59 [backends.py:251] Cache the graph for dynamic shape for later use
[1;36m(EngineCore_DP0 pid=3851658)[0;0m INFO 12-04 13:59:59 [backends.py:251] Cache the graph for dynamic shape for later use
[1;36m(EngineCore_DP0 pid=3851657)[0;0m INFO 12-04 14:00:11 [backends.py:282] Compiling a graph for dynamic shape takes 18.52 s
[1;36m(EngineCore_DP0 pid=3851658)[0;0m INFO 12-04 14:00:11 [backends.py:282] Compiling a graph for dynamic shape takes 18.55 s
[1;36m(EngineCore_DP0 pid=3851657)[0;0m INFO 12-04 14:00:13 [monitor.py:34] torch.compile takes 34.50 s in total
[1;36m(EngineCore_DP0 pid=3851658)[0;0m INFO 12-04 14:00:14 [monitor.py:34] torch.compile takes 34.39 s in total
[1;36m(EngineCore_DP0 pid=3851657)[0;0m INFO 12-04 14:00:15 [gpu_worker.py:359] Available KV cache memory: 30.72 GiB
[1;36m(EngineCore_DP0 pid=3851658)[0;0m INFO 12-04 14:00:15 [gpu_worker.py:359] Available KV cache memory: 32.40 GiB
[1;36m(EngineCore_DP0 pid=3851657)[0;0m INFO 12-04 14:00:16 [kv_cache_utils.py:1229] GPU KV cache size: 1,150,416 tokens
[1;36m(EngineCore_DP0 pid=3851657)[0;0m INFO 12-04 14:00:16 [kv_cache_utils.py:1234] Maximum concurrency for 131,072 tokens per request: 8.78x
[1;36m(EngineCore_DP0 pid=3851657)[0;0m INFO 12-04 14:00:16 [kernel_warmup.py:65] Warming up FlashInfer attention.
[1;36m(EngineCore_DP0 pid=3851658)[0;0m INFO 12-04 14:00:16 [kv_cache_utils.py:1229] GPU KV cache size: 1,213,248 tokens
[1;36m(EngineCore_DP0 pid=3851658)[0;0m INFO 12-04 14:00:16 [kv_cache_utils.py:1234] Maximum concurrency for 131,072 tokens per request: 9.26x
[1;36m(EngineCore_DP0 pid=3851658)[0;0m ERROR 12-04 14:00:16 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=3851658)[0;0m ERROR 12-04 14:00:16 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=3851658)[0;0m ERROR 12-04 14:00:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=3851658)[0;0m ERROR 12-04 14:00:16 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3851658)[0;0m ERROR 12-04 14:00:16 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3851658)[0;0m ERROR 12-04 14:00:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=3851658)[0;0m ERROR 12-04 14:00:16 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=3851658)[0;0m ERROR 12-04 14:00:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=3851658)[0;0m ERROR 12-04 14:00:16 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=3851658)[0;0m ERROR 12-04 14:00:16 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3851658)[0;0m ERROR 12-04 14:00:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 247, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=3851658)[0;0m ERROR 12-04 14:00:16 [core.py:842]     self.model_executor.initialize_from_config(kv_cache_configs)
[1;36m(EngineCore_DP0 pid=3851658)[0;0m ERROR 12-04 14:00:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 115, in initialize_from_config
[1;36m(EngineCore_DP0 pid=3851658)[0;0m ERROR 12-04 14:00:16 [core.py:842]     self.collective_rpc("initialize_from_config", args=(kv_cache_configs,))
[1;36m(EngineCore_DP0 pid=3851658)[0;0m ERROR 12-04 14:00:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=3851658)[0;0m ERROR 12-04 14:00:16 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=3851658)[0;0m ERROR 12-04 14:00:16 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3851658)[0;0m ERROR 12-04 14:00:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=3851658)[0;0m ERROR 12-04 14:00:16 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3851658)[0;0m ERROR 12-04 14:00:16 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3851658)[0;0m ERROR 12-04 14:00:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 319, in initialize_from_config
[1;36m(EngineCore_DP0 pid=3851658)[0;0m ERROR 12-04 14:00:16 [core.py:842]     self.worker.initialize_from_config(kv_cache_config)  # type: ignore
[1;36m(EngineCore_DP0 pid=3851658)[0;0m ERROR 12-04 14:00:16 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3851658)[0;0m ERROR 12-04 14:00:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 404, in initialize_from_config
[1;36m(EngineCore_DP0 pid=3851658)[0;0m ERROR 12-04 14:00:16 [core.py:842]     self.model_runner.initialize_kv_cache(kv_cache_config)
[1;36m(EngineCore_DP0 pid=3851658)[0;0m ERROR 12-04 14:00:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 5007, in initialize_kv_cache
[1;36m(EngineCore_DP0 pid=3851658)[0;0m ERROR 12-04 14:00:16 [core.py:842]     kv_caches = self.initialize_kv_cache_tensors(
[1;36m(EngineCore_DP0 pid=3851658)[0;0m ERROR 12-04 14:00:16 [core.py:842]                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3851658)[0;0m ERROR 12-04 14:00:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4933, in initialize_kv_cache_tensors
[1;36m(EngineCore_DP0 pid=3851658)[0;0m ERROR 12-04 14:00:16 [core.py:842]     kv_cache_raw_tensors = self._allocate_kv_cache_tensors(kv_cache_config)
[1;36m(EngineCore_DP0 pid=3851658)[0;0m ERROR 12-04 14:00:16 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3851658)[0;0m ERROR 12-04 14:00:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4718, in _allocate_kv_cache_tensors
[1;36m(EngineCore_DP0 pid=3851658)[0;0m ERROR 12-04 14:00:16 [core.py:842]     tensor = torch.zeros(
[1;36m(EngineCore_DP0 pid=3851658)[0;0m ERROR 12-04 14:00:16 [core.py:842]              ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3851658)[0;0m ERROR 12-04 14:00:16 [core.py:842] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.16 GiB. GPU 0 has a total capacity of 44.29 GiB of which 5.69 MiB is free. Including non-PyTorch memory, this process has 10.25 GiB memory in use. Process 3851657 has 34.04 GiB memory in use. Of the allocated memory 9.90 GiB is allocated by PyTorch, and 134.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=3851658)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=3851658)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=3851658)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=3851658)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=3851658)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=3851658)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=3851658)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=3851658)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=3851658)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=3851658)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3851658)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3851658)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=3851658)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=3851658)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=3851658)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=3851658)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3851658)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 247, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=3851658)[0;0m     self.model_executor.initialize_from_config(kv_cache_configs)
[1;36m(EngineCore_DP0 pid=3851658)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 115, in initialize_from_config
[1;36m(EngineCore_DP0 pid=3851658)[0;0m     self.collective_rpc("initialize_from_config", args=(kv_cache_configs,))
[1;36m(EngineCore_DP0 pid=3851658)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=3851658)[0;0m     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=3851658)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3851658)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=3851658)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3851658)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3851658)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 319, in initialize_from_config
[1;36m(EngineCore_DP0 pid=3851658)[0;0m     self.worker.initialize_from_config(kv_cache_config)  # type: ignore
[1;36m(EngineCore_DP0 pid=3851658)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3851658)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 404, in initialize_from_config
[1;36m(EngineCore_DP0 pid=3851658)[0;0m     self.model_runner.initialize_kv_cache(kv_cache_config)
[1;36m(EngineCore_DP0 pid=3851658)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 5007, in initialize_kv_cache
[1;36m(EngineCore_DP0 pid=3851658)[0;0m     kv_caches = self.initialize_kv_cache_tensors(
[1;36m(EngineCore_DP0 pid=3851658)[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3851658)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4933, in initialize_kv_cache_tensors
[1;36m(EngineCore_DP0 pid=3851658)[0;0m     kv_cache_raw_tensors = self._allocate_kv_cache_tensors(kv_cache_config)
[1;36m(EngineCore_DP0 pid=3851658)[0;0m                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3851658)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4718, in _allocate_kv_cache_tensors
[1;36m(EngineCore_DP0 pid=3851658)[0;0m     tensor = torch.zeros(
[1;36m(EngineCore_DP0 pid=3851658)[0;0m              ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3851658)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.16 GiB. GPU 0 has a total capacity of 44.29 GiB of which 5.69 MiB is free. Including non-PyTorch memory, this process has 10.25 GiB memory in use. Process 3851657 has 34.04 GiB memory in use. Of the allocated memory 9.90 GiB is allocated by PyTorch, and 134.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=3851657)[0;0m ERROR 12-04 14:00:17 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=3851657)[0;0m ERROR 12-04 14:00:17 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=3851657)[0;0m ERROR 12-04 14:00:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=3851657)[0;0m ERROR 12-04 14:00:17 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3851657)[0;0m ERROR 12-04 14:00:17 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3851657)[0;0m ERROR 12-04 14:00:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=3851657)[0;0m ERROR 12-04 14:00:17 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=3851657)[0;0m ERROR 12-04 14:00:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=3851657)[0;0m ERROR 12-04 14:00:17 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=3851657)[0;0m ERROR 12-04 14:00:17 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3851657)[0;0m ERROR 12-04 14:00:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 247, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=3851657)[0;0m ERROR 12-04 14:00:17 [core.py:842]     self.model_executor.initialize_from_config(kv_cache_configs)
[1;36m(EngineCore_DP0 pid=3851657)[0;0m ERROR 12-04 14:00:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 116, in initialize_from_config
[1;36m(EngineCore_DP0 pid=3851657)[0;0m ERROR 12-04 14:00:17 [core.py:842]     self.collective_rpc("compile_or_warm_up_model")
[1;36m(EngineCore_DP0 pid=3851657)[0;0m ERROR 12-04 14:00:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=3851657)[0;0m ERROR 12-04 14:00:17 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=3851657)[0;0m ERROR 12-04 14:00:17 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3851657)[0;0m ERROR 12-04 14:00:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=3851657)[0;0m ERROR 12-04 14:00:17 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3851657)[0;0m ERROR 12-04 14:00:17 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3851657)[0;0m ERROR 12-04 14:00:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 425, in compile_or_warm_up_model
[1;36m(EngineCore_DP0 pid=3851657)[0;0m ERROR 12-04 14:00:17 [core.py:842]     kernel_warmup(self)
[1;36m(EngineCore_DP0 pid=3851657)[0;0m ERROR 12-04 14:00:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/warmup/kernel_warmup.py", line 68, in kernel_warmup
[1;36m(EngineCore_DP0 pid=3851657)[0;0m ERROR 12-04 14:00:17 [core.py:842]     worker.model_runner._dummy_run(
[1;36m(EngineCore_DP0 pid=3851657)[0;0m ERROR 12-04 14:00:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=3851657)[0;0m ERROR 12-04 14:00:17 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3851657)[0;0m ERROR 12-04 14:00:17 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3851657)[0;0m ERROR 12-04 14:00:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3756, in _dummy_run
[1;36m(EngineCore_DP0 pid=3851657)[0;0m ERROR 12-04 14:00:17 [core.py:842]     attn_metadata, _ = self._build_attention_metadata(
[1;36m(EngineCore_DP0 pid=3851657)[0;0m ERROR 12-04 14:00:17 [core.py:842]                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3851657)[0;0m ERROR 12-04 14:00:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1576, in _build_attention_metadata
[1;36m(EngineCore_DP0 pid=3851657)[0;0m ERROR 12-04 14:00:17 [core.py:842]     attn_metadata_i = builder.build_for_cudagraph_capture(
[1;36m(EngineCore_DP0 pid=3851657)[0;0m ERROR 12-04 14:00:17 [core.py:842]                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3851657)[0;0m ERROR 12-04 14:00:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/attention/backends/utils.py", line 332, in build_for_cudagraph_capture
[1;36m(EngineCore_DP0 pid=3851657)[0;0m ERROR 12-04 14:00:17 [core.py:842]     return self.build(
[1;36m(EngineCore_DP0 pid=3851657)[0;0m ERROR 12-04 14:00:17 [core.py:842]            ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3851657)[0;0m ERROR 12-04 14:00:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/attention/backends/flashinfer.py", line 844, in build
[1;36m(EngineCore_DP0 pid=3851657)[0;0m ERROR 12-04 14:00:17 [core.py:842]     attn_metadata.prefill_wrapper = self._get_prefill_wrapper()
[1;36m(EngineCore_DP0 pid=3851657)[0;0m ERROR 12-04 14:00:17 [core.py:842]                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3851657)[0;0m ERROR 12-04 14:00:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/attention/backends/flashinfer.py", line 598, in _get_prefill_wrapper
[1;36m(EngineCore_DP0 pid=3851657)[0;0m ERROR 12-04 14:00:17 [core.py:842]     self._get_workspace_buffer(), get_kv_cache_layout()
[1;36m(EngineCore_DP0 pid=3851657)[0;0m ERROR 12-04 14:00:17 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3851657)[0;0m ERROR 12-04 14:00:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/attention/backends/flashinfer.py", line 583, in _get_workspace_buffer
[1;36m(EngineCore_DP0 pid=3851657)[0;0m ERROR 12-04 14:00:17 [core.py:842]     self._workspace_buffer = torch.zeros(
[1;36m(EngineCore_DP0 pid=3851657)[0;0m ERROR 12-04 14:00:17 [core.py:842]                              ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3851657)[0;0m ERROR 12-04 14:00:17 [core.py:842] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 394.00 MiB. GPU 0 has a total capacity of 44.29 GiB of which 5.69 MiB is free. Process 3851658 has 10.25 GiB memory in use. Including non-PyTorch memory, this process has 34.04 GiB memory in use. Of the allocated memory 33.69 GiB is allocated by PyTorch, and 127.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=3851657)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=3851657)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=3851657)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=3851657)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=3851657)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=3851657)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=3851657)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=3851657)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=3851657)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=3851657)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3851657)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3851657)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=3851657)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=3851657)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=3851657)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=3851657)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3851657)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 247, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=3851657)[0;0m     self.model_executor.initialize_from_config(kv_cache_configs)
[1;36m(EngineCore_DP0 pid=3851657)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 116, in initialize_from_config
[1;36m(EngineCore_DP0 pid=3851657)[0;0m     self.collective_rpc("compile_or_warm_up_model")
[1;36m(EngineCore_DP0 pid=3851657)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=3851657)[0;0m     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=3851657)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3851657)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=3851657)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3851657)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3851657)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 425, in compile_or_warm_up_model
[1;36m(EngineCore_DP0 pid=3851657)[0;0m     kernel_warmup(self)
[1;36m(EngineCore_DP0 pid=3851657)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/warmup/kernel_warmup.py", line 68, in kernel_warmup
[1;36m(EngineCore_DP0 pid=3851657)[0;0m     worker.model_runner._dummy_run(
[1;36m(EngineCore_DP0 pid=3851657)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=3851657)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3851657)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3851657)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3756, in _dummy_run
[1;36m(EngineCore_DP0 pid=3851657)[0;0m     attn_metadata, _ = self._build_attention_metadata(
[1;36m(EngineCore_DP0 pid=3851657)[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3851657)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1576, in _build_attention_metadata
[1;36m(EngineCore_DP0 pid=3851657)[0;0m     attn_metadata_i = builder.build_for_cudagraph_capture(
[1;36m(EngineCore_DP0 pid=3851657)[0;0m                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3851657)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/attention/backends/utils.py", line 332, in build_for_cudagraph_capture
[1;36m(EngineCore_DP0 pid=3851657)[0;0m     return self.build(
[1;36m(EngineCore_DP0 pid=3851657)[0;0m            ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3851657)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/attention/backends/flashinfer.py", line 844, in build
[1;36m(EngineCore_DP0 pid=3851657)[0;0m     attn_metadata.prefill_wrapper = self._get_prefill_wrapper()
[1;36m(EngineCore_DP0 pid=3851657)[0;0m                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3851657)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/attention/backends/flashinfer.py", line 598, in _get_prefill_wrapper
[1;36m(EngineCore_DP0 pid=3851657)[0;0m     self._get_workspace_buffer(), get_kv_cache_layout()
[1;36m(EngineCore_DP0 pid=3851657)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3851657)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/attention/backends/flashinfer.py", line 583, in _get_workspace_buffer
[1;36m(EngineCore_DP0 pid=3851657)[0;0m     self._workspace_buffer = torch.zeros(
[1;36m(EngineCore_DP0 pid=3851657)[0;0m                              ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3851657)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 394.00 MiB. GPU 0 has a total capacity of 44.29 GiB of which 5.69 MiB is free. Process 3851658 has 10.25 GiB memory in use. Including non-PyTorch memory, this process has 34.04 GiB memory in use. Of the allocated memory 33.69 GiB is allocated by PyTorch, and 127.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W1204 14:00:17.605479509 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank0]:[W1204 14:00:18.331751228 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: WeiboAI/VibeThinker-1.5B (backend=vllm)
[ModelCache] Using max_model_len=131072 for WeiboAI/VibeThinker-1.5B
[ModelCache] Detected 1 GPU(s), 44.3GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 14:00:43 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'WeiboAI/VibeThinker-1.5B'}
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: WeiboAI/VibeThinker-1.5B (backend=vllm)
[ModelCache] Using max_model_len=131072 for WeiboAI/VibeThinker-1.5B
[ModelCache] Detected 1 GPU(s), 44.3GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 14:00:43 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'WeiboAI/VibeThinker-1.5B'}
INFO 12-04 14:00:43 [model.py:631] Resolved architecture: Qwen2ForCausalLM
WARNING 12-04 14:00:43 [model.py:1921] Your device 'Quadro RTX 8000' (with compute capability 7.5) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.
WARNING 12-04 14:00:43 [model.py:1971] Casting torch.bfloat16 to torch.float16.
INFO 12-04 14:00:43 [model.py:1745] Using max model len 131072
INFO 12-04 14:00:43 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
INFO 12-04 14:00:43 [model.py:631] Resolved architecture: Qwen2ForCausalLM
WARNING 12-04 14:00:43 [model.py:1921] Your device 'Quadro RTX 8000' (with compute capability 7.5) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.
WARNING 12-04 14:00:43 [model.py:1971] Casting torch.bfloat16 to torch.float16.
INFO 12-04 14:00:43 [model.py:1745] Using max model len 131072
INFO 12-04 14:00:43 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=3852142)[0;0m INFO 12-04 14:00:58 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='WeiboAI/VibeThinker-1.5B', speculative_config=None, tokenizer='WeiboAI/VibeThinker-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=WeiboAI/VibeThinker-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=3852145)[0;0m INFO 12-04 14:00:58 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='WeiboAI/VibeThinker-1.5B', speculative_config=None, tokenizer='WeiboAI/VibeThinker-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=WeiboAI/VibeThinker-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=3852145)[0;0m ERROR 12-04 14:00:59 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8
[1;36m(EngineCore_DP0 pid=3852142)[0;0m ERROR 12-04 14:00:59 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8
[1;36m(EngineCore_DP0 pid=3852142)[0;0m INFO 12-04 14:01:00 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.7:49159 backend=nccl
[1;36m(EngineCore_DP0 pid=3852145)[0;0m INFO 12-04 14:01:00 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.7:33125 backend=nccl
[W1204 14:01:00.381558064 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-23-2.rc.tch.harvard.edu]:49159 (errno: 97 - Address family not supported by protocol).
[W1204 14:01:00.381998894 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-23-2.rc.tch.harvard.edu]:33125 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=3852145)[0;0m INFO 12-04 14:01:00 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=3852142)[0;0m INFO 12-04 14:01:00 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=3852145)[0;0m INFO 12-04 14:01:01 [gpu_model_runner.py:3259] Starting to load model WeiboAI/VibeThinker-1.5B...
[1;36m(EngineCore_DP0 pid=3852142)[0;0m INFO 12-04 14:01:01 [gpu_model_runner.py:3259] Starting to load model WeiboAI/VibeThinker-1.5B...
[1;36m(EngineCore_DP0 pid=3852145)[0;0m INFO 12-04 14:01:02 [cuda.py:418] Valid backends: ['FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=3852145)[0;0m INFO 12-04 14:01:02 [cuda.py:427] Using FLASHINFER backend.
[1;36m(EngineCore_DP0 pid=3852142)[0;0m INFO 12-04 14:01:02 [cuda.py:418] Valid backends: ['FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=3852142)[0;0m INFO 12-04 14:01:02 [cuda.py:427] Using FLASHINFER backend.
[1;36m(EngineCore_DP0 pid=3852145)[0;0m INFO 12-04 14:01:02 [weight_utils.py:481] No model.safetensors.index.json found in remote.
[1;36m(EngineCore_DP0 pid=3852145)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
[1;36m(EngineCore_DP0 pid=3852142)[0;0m INFO 12-04 14:01:02 [weight_utils.py:481] No model.safetensors.index.json found in remote.
[1;36m(EngineCore_DP0 pid=3852142)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
[1;36m(EngineCore_DP0 pid=3852142)[0;0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:04<00:00,  4.20s/it]
[1;36m(EngineCore_DP0 pid=3852142)[0;0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:04<00:00,  4.20s/it]
[1;36m(EngineCore_DP0 pid=3852142)[0;0m 
[1;36m(EngineCore_DP0 pid=3852142)[0;0m INFO 12-04 14:01:07 [default_loader.py:314] Loading weights took 4.37 seconds
[1;36m(EngineCore_DP0 pid=3852145)[0;0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:04<00:00,  4.63s/it]
[1;36m(EngineCore_DP0 pid=3852145)[0;0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:04<00:00,  4.63s/it]
[1;36m(EngineCore_DP0 pid=3852145)[0;0m 
[1;36m(EngineCore_DP0 pid=3852145)[0;0m INFO 12-04 14:01:07 [default_loader.py:314] Loading weights took 4.77 seconds
[1;36m(EngineCore_DP0 pid=3852142)[0;0m INFO 12-04 14:01:07 [gpu_model_runner.py:3338] Model loading took 2.9110 GiB memory and 5.600893 seconds
[1;36m(EngineCore_DP0 pid=3852145)[0;0m INFO 12-04 14:01:08 [gpu_model_runner.py:3338] Model loading took 2.9110 GiB memory and 5.907341 seconds
[1;36m(EngineCore_DP0 pid=3852145)[0;0m INFO 12-04 14:01:19 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/6847b7a26b/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=3852142)[0;0m INFO 12-04 14:01:19 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/6847b7a26b/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=3852145)[0;0m INFO 12-04 14:01:19 [backends.py:647] Dynamo bytecode transform time: 10.62 s
[1;36m(EngineCore_DP0 pid=3852142)[0;0m INFO 12-04 14:01:19 [backends.py:647] Dynamo bytecode transform time: 10.90 s
[1;36m(EngineCore_DP0 pid=3852142)[0;0m INFO 12-04 14:01:26 [backends.py:210] Directly load the compiled graph(s) for dynamic shape from the cache, took 6.384 s
[1;36m(EngineCore_DP0 pid=3852145)[0;0m INFO 12-04 14:01:26 [backends.py:210] Directly load the compiled graph(s) for dynamic shape from the cache, took 6.385 s
[1;36m(EngineCore_DP0 pid=3852142)[0;0m INFO 12-04 14:01:28 [monitor.py:34] torch.compile takes 17.28 s in total
[1;36m(EngineCore_DP0 pid=3852145)[0;0m INFO 12-04 14:01:28 [monitor.py:34] torch.compile takes 17.01 s in total
[1;36m(EngineCore_DP0 pid=3852145)[0;0m INFO 12-04 14:01:29 [gpu_worker.py:359] Available KV cache memory: 30.58 GiB
[1;36m(EngineCore_DP0 pid=3852142)[0;0m INFO 12-04 14:01:30 [gpu_worker.py:359] Available KV cache memory: 32.42 GiB
[1;36m(EngineCore_DP0 pid=3852145)[0;0m INFO 12-04 14:01:30 [kv_cache_utils.py:1229] GPU KV cache size: 1,145,296 tokens
[1;36m(EngineCore_DP0 pid=3852145)[0;0m INFO 12-04 14:01:30 [kv_cache_utils.py:1234] Maximum concurrency for 131,072 tokens per request: 8.74x
[1;36m(EngineCore_DP0 pid=3852142)[0;0m INFO 12-04 14:01:30 [kv_cache_utils.py:1229] GPU KV cache size: 1,213,904 tokens
[1;36m(EngineCore_DP0 pid=3852142)[0;0m INFO 12-04 14:01:30 [kv_cache_utils.py:1234] Maximum concurrency for 131,072 tokens per request: 9.26x
[1;36m(EngineCore_DP0 pid=3852145)[0;0m INFO 12-04 14:01:30 [kernel_warmup.py:65] Warming up FlashInfer attention.
[1;36m(EngineCore_DP0 pid=3852142)[0;0m ERROR 12-04 14:01:30 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=3852142)[0;0m ERROR 12-04 14:01:30 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=3852142)[0;0m ERROR 12-04 14:01:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=3852142)[0;0m ERROR 12-04 14:01:30 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3852142)[0;0m ERROR 12-04 14:01:30 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852142)[0;0m ERROR 12-04 14:01:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=3852142)[0;0m ERROR 12-04 14:01:30 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=3852142)[0;0m ERROR 12-04 14:01:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=3852142)[0;0m ERROR 12-04 14:01:30 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=3852142)[0;0m ERROR 12-04 14:01:30 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852142)[0;0m ERROR 12-04 14:01:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 247, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=3852142)[0;0m ERROR 12-04 14:01:30 [core.py:842]     self.model_executor.initialize_from_config(kv_cache_configs)
[1;36m(EngineCore_DP0 pid=3852142)[0;0m ERROR 12-04 14:01:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 115, in initialize_from_config
[1;36m(EngineCore_DP0 pid=3852142)[0;0m ERROR 12-04 14:01:30 [core.py:842]     self.collective_rpc("initialize_from_config", args=(kv_cache_configs,))
[1;36m(EngineCore_DP0 pid=3852142)[0;0m ERROR 12-04 14:01:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=3852142)[0;0m ERROR 12-04 14:01:30 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=3852142)[0;0m ERROR 12-04 14:01:30 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852142)[0;0m ERROR 12-04 14:01:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=3852142)[0;0m ERROR 12-04 14:01:30 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3852142)[0;0m ERROR 12-04 14:01:30 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852142)[0;0m ERROR 12-04 14:01:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 319, in initialize_from_config
[1;36m(EngineCore_DP0 pid=3852142)[0;0m ERROR 12-04 14:01:30 [core.py:842]     self.worker.initialize_from_config(kv_cache_config)  # type: ignore
[1;36m(EngineCore_DP0 pid=3852142)[0;0m ERROR 12-04 14:01:30 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852142)[0;0m ERROR 12-04 14:01:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 404, in initialize_from_config
[1;36m(EngineCore_DP0 pid=3852142)[0;0m ERROR 12-04 14:01:30 [core.py:842]     self.model_runner.initialize_kv_cache(kv_cache_config)
[1;36m(EngineCore_DP0 pid=3852142)[0;0m ERROR 12-04 14:01:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 5007, in initialize_kv_cache
[1;36m(EngineCore_DP0 pid=3852142)[0;0m ERROR 12-04 14:01:30 [core.py:842]     kv_caches = self.initialize_kv_cache_tensors(
[1;36m(EngineCore_DP0 pid=3852142)[0;0m ERROR 12-04 14:01:30 [core.py:842]                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852142)[0;0m ERROR 12-04 14:01:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4933, in initialize_kv_cache_tensors
[1;36m(EngineCore_DP0 pid=3852142)[0;0m ERROR 12-04 14:01:30 [core.py:842]     kv_cache_raw_tensors = self._allocate_kv_cache_tensors(kv_cache_config)
[1;36m(EngineCore_DP0 pid=3852142)[0;0m ERROR 12-04 14:01:30 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852142)[0;0m ERROR 12-04 14:01:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4718, in _allocate_kv_cache_tensors
[1;36m(EngineCore_DP0 pid=3852142)[0;0m ERROR 12-04 14:01:30 [core.py:842]     tensor = torch.zeros(
[1;36m(EngineCore_DP0 pid=3852142)[0;0m ERROR 12-04 14:01:30 [core.py:842]              ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852142)[0;0m ERROR 12-04 14:01:30 [core.py:842] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.16 GiB. GPU 0 has a total capacity of 44.29 GiB of which 117.69 MiB is free. Process 3852145 has 33.93 GiB memory in use. Including non-PyTorch memory, this process has 10.25 GiB memory in use. Of the allocated memory 9.91 GiB is allocated by PyTorch, and 127.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=3852142)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=3852142)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=3852142)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=3852142)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=3852142)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=3852142)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=3852142)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=3852142)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=3852142)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=3852142)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3852142)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852142)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=3852142)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=3852142)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=3852142)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=3852142)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852142)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 247, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=3852142)[0;0m     self.model_executor.initialize_from_config(kv_cache_configs)
[1;36m(EngineCore_DP0 pid=3852142)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 115, in initialize_from_config
[1;36m(EngineCore_DP0 pid=3852142)[0;0m     self.collective_rpc("initialize_from_config", args=(kv_cache_configs,))
[1;36m(EngineCore_DP0 pid=3852142)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=3852142)[0;0m     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=3852142)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852142)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=3852142)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3852142)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852142)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 319, in initialize_from_config
[1;36m(EngineCore_DP0 pid=3852142)[0;0m     self.worker.initialize_from_config(kv_cache_config)  # type: ignore
[1;36m(EngineCore_DP0 pid=3852142)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852142)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 404, in initialize_from_config
[1;36m(EngineCore_DP0 pid=3852142)[0;0m     self.model_runner.initialize_kv_cache(kv_cache_config)
[1;36m(EngineCore_DP0 pid=3852142)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 5007, in initialize_kv_cache
[1;36m(EngineCore_DP0 pid=3852142)[0;0m     kv_caches = self.initialize_kv_cache_tensors(
[1;36m(EngineCore_DP0 pid=3852142)[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852142)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4933, in initialize_kv_cache_tensors
[1;36m(EngineCore_DP0 pid=3852142)[0;0m     kv_cache_raw_tensors = self._allocate_kv_cache_tensors(kv_cache_config)
[1;36m(EngineCore_DP0 pid=3852142)[0;0m                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852142)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4718, in _allocate_kv_cache_tensors
[1;36m(EngineCore_DP0 pid=3852142)[0;0m     tensor = torch.zeros(
[1;36m(EngineCore_DP0 pid=3852142)[0;0m              ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852142)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.16 GiB. GPU 0 has a total capacity of 44.29 GiB of which 117.69 MiB is free. Process 3852145 has 33.93 GiB memory in use. Including non-PyTorch memory, this process has 10.25 GiB memory in use. Of the allocated memory 9.91 GiB is allocated by PyTorch, and 127.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=3852145)[0;0m ERROR 12-04 14:01:30 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=3852145)[0;0m ERROR 12-04 14:01:30 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=3852145)[0;0m ERROR 12-04 14:01:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=3852145)[0;0m ERROR 12-04 14:01:30 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3852145)[0;0m ERROR 12-04 14:01:30 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852145)[0;0m ERROR 12-04 14:01:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=3852145)[0;0m ERROR 12-04 14:01:30 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=3852145)[0;0m ERROR 12-04 14:01:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=3852145)[0;0m ERROR 12-04 14:01:30 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=3852145)[0;0m ERROR 12-04 14:01:30 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852145)[0;0m ERROR 12-04 14:01:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 247, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=3852145)[0;0m ERROR 12-04 14:01:30 [core.py:842]     self.model_executor.initialize_from_config(kv_cache_configs)
[1;36m(EngineCore_DP0 pid=3852145)[0;0m ERROR 12-04 14:01:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 116, in initialize_from_config
[1;36m(EngineCore_DP0 pid=3852145)[0;0m ERROR 12-04 14:01:30 [core.py:842]     self.collective_rpc("compile_or_warm_up_model")
[1;36m(EngineCore_DP0 pid=3852145)[0;0m ERROR 12-04 14:01:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=3852145)[0;0m ERROR 12-04 14:01:30 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=3852145)[0;0m ERROR 12-04 14:01:30 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852145)[0;0m ERROR 12-04 14:01:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=3852145)[0;0m ERROR 12-04 14:01:30 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3852145)[0;0m ERROR 12-04 14:01:30 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852145)[0;0m ERROR 12-04 14:01:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 425, in compile_or_warm_up_model
[1;36m(EngineCore_DP0 pid=3852145)[0;0m ERROR 12-04 14:01:30 [core.py:842]     kernel_warmup(self)
[1;36m(EngineCore_DP0 pid=3852145)[0;0m ERROR 12-04 14:01:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/warmup/kernel_warmup.py", line 68, in kernel_warmup
[1;36m(EngineCore_DP0 pid=3852145)[0;0m ERROR 12-04 14:01:30 [core.py:842]     worker.model_runner._dummy_run(
[1;36m(EngineCore_DP0 pid=3852145)[0;0m ERROR 12-04 14:01:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=3852145)[0;0m ERROR 12-04 14:01:30 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3852145)[0;0m ERROR 12-04 14:01:30 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852145)[0;0m ERROR 12-04 14:01:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3756, in _dummy_run
[1;36m(EngineCore_DP0 pid=3852145)[0;0m ERROR 12-04 14:01:30 [core.py:842]     attn_metadata, _ = self._build_attention_metadata(
[1;36m(EngineCore_DP0 pid=3852145)[0;0m ERROR 12-04 14:01:30 [core.py:842]                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852145)[0;0m ERROR 12-04 14:01:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1576, in _build_attention_metadata
[1;36m(EngineCore_DP0 pid=3852145)[0;0m ERROR 12-04 14:01:30 [core.py:842]     attn_metadata_i = builder.build_for_cudagraph_capture(
[1;36m(EngineCore_DP0 pid=3852145)[0;0m ERROR 12-04 14:01:30 [core.py:842]                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852145)[0;0m ERROR 12-04 14:01:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/attention/backends/utils.py", line 332, in build_for_cudagraph_capture
[1;36m(EngineCore_DP0 pid=3852145)[0;0m ERROR 12-04 14:01:30 [core.py:842]     return self.build(
[1;36m(EngineCore_DP0 pid=3852145)[0;0m ERROR 12-04 14:01:30 [core.py:842]            ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852145)[0;0m ERROR 12-04 14:01:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/attention/backends/flashinfer.py", line 844, in build
[1;36m(EngineCore_DP0 pid=3852145)[0;0m ERROR 12-04 14:01:30 [core.py:842]     attn_metadata.prefill_wrapper = self._get_prefill_wrapper()
[1;36m(EngineCore_DP0 pid=3852145)[0;0m ERROR 12-04 14:01:30 [core.py:842]                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852145)[0;0m ERROR 12-04 14:01:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/attention/backends/flashinfer.py", line 598, in _get_prefill_wrapper
[1;36m(EngineCore_DP0 pid=3852145)[0;0m ERROR 12-04 14:01:30 [core.py:842]     self._get_workspace_buffer(), get_kv_cache_layout()
[1;36m(EngineCore_DP0 pid=3852145)[0;0m ERROR 12-04 14:01:30 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852145)[0;0m ERROR 12-04 14:01:30 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/attention/backends/flashinfer.py", line 583, in _get_workspace_buffer
[1;36m(EngineCore_DP0 pid=3852145)[0;0m ERROR 12-04 14:01:30 [core.py:842]     self._workspace_buffer = torch.zeros(
[1;36m(EngineCore_DP0 pid=3852145)[0;0m ERROR 12-04 14:01:30 [core.py:842]                              ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852145)[0;0m ERROR 12-04 14:01:30 [core.py:842] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 394.00 MiB. GPU 0 has a total capacity of 44.29 GiB of which 117.69 MiB is free. Including non-PyTorch memory, this process has 33.93 GiB memory in use. Process 3852142 has 10.25 GiB memory in use. Of the allocated memory 33.54 GiB is allocated by PyTorch, and 171.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=3852145)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=3852145)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=3852145)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=3852145)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=3852145)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=3852145)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=3852145)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=3852145)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=3852145)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=3852145)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3852145)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852145)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=3852145)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=3852145)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=3852145)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=3852145)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852145)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 247, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=3852145)[0;0m     self.model_executor.initialize_from_config(kv_cache_configs)
[1;36m(EngineCore_DP0 pid=3852145)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 116, in initialize_from_config
[1;36m(EngineCore_DP0 pid=3852145)[0;0m     self.collective_rpc("compile_or_warm_up_model")
[1;36m(EngineCore_DP0 pid=3852145)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=3852145)[0;0m     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=3852145)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852145)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=3852145)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3852145)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852145)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 425, in compile_or_warm_up_model
[1;36m(EngineCore_DP0 pid=3852145)[0;0m     kernel_warmup(self)
[1;36m(EngineCore_DP0 pid=3852145)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/warmup/kernel_warmup.py", line 68, in kernel_warmup
[1;36m(EngineCore_DP0 pid=3852145)[0;0m     worker.model_runner._dummy_run(
[1;36m(EngineCore_DP0 pid=3852145)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=3852145)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3852145)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852145)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3756, in _dummy_run
[1;36m(EngineCore_DP0 pid=3852145)[0;0m     attn_metadata, _ = self._build_attention_metadata(
[1;36m(EngineCore_DP0 pid=3852145)[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852145)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1576, in _build_attention_metadata
[1;36m(EngineCore_DP0 pid=3852145)[0;0m     attn_metadata_i = builder.build_for_cudagraph_capture(
[1;36m(EngineCore_DP0 pid=3852145)[0;0m                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852145)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/attention/backends/utils.py", line 332, in build_for_cudagraph_capture
[1;36m(EngineCore_DP0 pid=3852145)[0;0m     return self.build(
[1;36m(EngineCore_DP0 pid=3852145)[0;0m            ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852145)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/attention/backends/flashinfer.py", line 844, in build
[1;36m(EngineCore_DP0 pid=3852145)[0;0m     attn_metadata.prefill_wrapper = self._get_prefill_wrapper()
[1;36m(EngineCore_DP0 pid=3852145)[0;0m                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852145)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/attention/backends/flashinfer.py", line 598, in _get_prefill_wrapper
[1;36m(EngineCore_DP0 pid=3852145)[0;0m     self._get_workspace_buffer(), get_kv_cache_layout()
[1;36m(EngineCore_DP0 pid=3852145)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852145)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/attention/backends/flashinfer.py", line 583, in _get_workspace_buffer
[1;36m(EngineCore_DP0 pid=3852145)[0;0m     self._workspace_buffer = torch.zeros(
[1;36m(EngineCore_DP0 pid=3852145)[0;0m                              ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852145)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 394.00 MiB. GPU 0 has a total capacity of 44.29 GiB of which 117.69 MiB is free. Including non-PyTorch memory, this process has 33.93 GiB memory in use. Process 3852142 has 10.25 GiB memory in use. Of the allocated memory 33.54 GiB is allocated by PyTorch, and 171.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W1204 14:01:31.452220197 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank0]:[W1204 14:01:31.546373408 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: WeiboAI/VibeThinker-1.5B (backend=vllm)
[ModelCache] Using max_model_len=131072 for WeiboAI/VibeThinker-1.5B
[ModelCache] Detected 1 GPU(s), 44.3GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 14:01:53 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'WeiboAI/VibeThinker-1.5B'}
INFO 12-04 14:01:53 [model.py:631] Resolved architecture: Qwen2ForCausalLM
WARNING 12-04 14:01:53 [model.py:1921] Your device 'Quadro RTX 8000' (with compute capability 7.5) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.
WARNING 12-04 14:01:53 [model.py:1971] Casting torch.bfloat16 to torch.float16.
INFO 12-04 14:01:53 [model.py:1745] Using max model len 131072
INFO 12-04 14:01:53 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: WeiboAI/VibeThinker-1.5B (backend=vllm)
[ModelCache] Using max_model_len=131072 for WeiboAI/VibeThinker-1.5B
[ModelCache] Detected 1 GPU(s), 44.3GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 14:01:53 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'WeiboAI/VibeThinker-1.5B'}
INFO 12-04 14:01:53 [model.py:631] Resolved architecture: Qwen2ForCausalLM
WARNING 12-04 14:01:53 [model.py:1921] Your device 'Quadro RTX 8000' (with compute capability 7.5) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.
WARNING 12-04 14:01:53 [model.py:1971] Casting torch.bfloat16 to torch.float16.
INFO 12-04 14:01:53 [model.py:1745] Using max model len 131072
INFO 12-04 14:01:53 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=3852325)[0;0m INFO 12-04 14:02:09 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='WeiboAI/VibeThinker-1.5B', speculative_config=None, tokenizer='WeiboAI/VibeThinker-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=WeiboAI/VibeThinker-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=3852322)[0;0m INFO 12-04 14:02:09 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='WeiboAI/VibeThinker-1.5B', speculative_config=None, tokenizer='WeiboAI/VibeThinker-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=WeiboAI/VibeThinker-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=3852325)[0;0m ERROR 12-04 14:02:10 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8
[1;36m(EngineCore_DP0 pid=3852322)[0;0m ERROR 12-04 14:02:10 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8
[1;36m(EngineCore_DP0 pid=3852322)[0;0m INFO 12-04 14:02:11 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.7:39461 backend=nccl
[1;36m(EngineCore_DP0 pid=3852325)[0;0m INFO 12-04 14:02:11 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.7:51037 backend=nccl
[W1204 14:02:11.126309388 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-23-2.rc.tch.harvard.edu]:39461 (errno: 97 - Address family not supported by protocol).
[W1204 14:02:11.127804993 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-23-2.rc.tch.harvard.edu]:51037 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=3852325)[0;0m INFO 12-04 14:02:11 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=3852322)[0;0m INFO 12-04 14:02:11 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=3852325)[0;0m INFO 12-04 14:02:11 [gpu_model_runner.py:3259] Starting to load model WeiboAI/VibeThinker-1.5B...
[1;36m(EngineCore_DP0 pid=3852322)[0;0m INFO 12-04 14:02:11 [gpu_model_runner.py:3259] Starting to load model WeiboAI/VibeThinker-1.5B...
[1;36m(EngineCore_DP0 pid=3852325)[0;0m INFO 12-04 14:02:12 [cuda.py:418] Valid backends: ['FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=3852325)[0;0m INFO 12-04 14:02:12 [cuda.py:427] Using FLASHINFER backend.
[1;36m(EngineCore_DP0 pid=3852322)[0;0m INFO 12-04 14:02:12 [cuda.py:418] Valid backends: ['FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=3852322)[0;0m INFO 12-04 14:02:12 [cuda.py:427] Using FLASHINFER backend.
[1;36m(EngineCore_DP0 pid=3852325)[0;0m INFO 12-04 14:02:13 [weight_utils.py:481] No model.safetensors.index.json found in remote.
[1;36m(EngineCore_DP0 pid=3852325)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
[1;36m(EngineCore_DP0 pid=3852322)[0;0m INFO 12-04 14:02:13 [weight_utils.py:481] No model.safetensors.index.json found in remote.
[1;36m(EngineCore_DP0 pid=3852322)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
[1;36m(EngineCore_DP0 pid=3852325)[0;0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:04<00:00,  4.80s/it]
[1;36m(EngineCore_DP0 pid=3852325)[0;0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:04<00:00,  4.80s/it]
[1;36m(EngineCore_DP0 pid=3852325)[0;0m 
[1;36m(EngineCore_DP0 pid=3852325)[0;0m INFO 12-04 14:02:18 [default_loader.py:314] Loading weights took 4.94 seconds
[1;36m(EngineCore_DP0 pid=3852322)[0;0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:04<00:00,  4.85s/it]
[1;36m(EngineCore_DP0 pid=3852322)[0;0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:04<00:00,  4.85s/it]
[1;36m(EngineCore_DP0 pid=3852322)[0;0m 
[1;36m(EngineCore_DP0 pid=3852322)[0;0m INFO 12-04 14:02:18 [default_loader.py:314] Loading weights took 5.00 seconds
[1;36m(EngineCore_DP0 pid=3852325)[0;0m INFO 12-04 14:02:19 [gpu_model_runner.py:3338] Model loading took 2.9110 GiB memory and 6.112892 seconds
[1;36m(EngineCore_DP0 pid=3852322)[0;0m INFO 12-04 14:02:19 [gpu_model_runner.py:3338] Model loading took 2.9110 GiB memory and 6.286173 seconds
[1;36m(EngineCore_DP0 pid=3852325)[0;0m INFO 12-04 14:02:31 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/6847b7a26b/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=3852322)[0;0m INFO 12-04 14:02:31 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/6847b7a26b/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=3852322)[0;0m INFO 12-04 14:02:31 [backends.py:647] Dynamo bytecode transform time: 11.55 s
[1;36m(EngineCore_DP0 pid=3852325)[0;0m INFO 12-04 14:02:31 [backends.py:647] Dynamo bytecode transform time: 11.73 s
[1;36m(EngineCore_DP0 pid=3852325)[0;0m INFO 12-04 14:02:37 [backends.py:210] Directly load the compiled graph(s) for dynamic shape from the cache, took 5.989 s
[1;36m(EngineCore_DP0 pid=3852322)[0;0m INFO 12-04 14:02:37 [backends.py:210] Directly load the compiled graph(s) for dynamic shape from the cache, took 5.992 s
[1;36m(EngineCore_DP0 pid=3852322)[0;0m INFO 12-04 14:02:39 [monitor.py:34] torch.compile takes 17.54 s in total
[1;36m(EngineCore_DP0 pid=3852325)[0;0m INFO 12-04 14:02:39 [monitor.py:34] torch.compile takes 17.72 s in total
[1;36m(EngineCore_DP0 pid=3852325)[0;0m INFO 12-04 14:02:41 [gpu_worker.py:359] Available KV cache memory: 30.58 GiB
[1;36m(EngineCore_DP0 pid=3852322)[0;0m INFO 12-04 14:02:41 [gpu_worker.py:359] Available KV cache memory: 32.40 GiB
[1;36m(EngineCore_DP0 pid=3852322)[0;0m INFO 12-04 14:02:42 [kv_cache_utils.py:1229] GPU KV cache size: 1,213,328 tokens
[1;36m(EngineCore_DP0 pid=3852322)[0;0m INFO 12-04 14:02:42 [kv_cache_utils.py:1234] Maximum concurrency for 131,072 tokens per request: 9.26x
[1;36m(EngineCore_DP0 pid=3852325)[0;0m INFO 12-04 14:02:42 [kv_cache_utils.py:1229] GPU KV cache size: 1,145,296 tokens
[1;36m(EngineCore_DP0 pid=3852325)[0;0m INFO 12-04 14:02:42 [kv_cache_utils.py:1234] Maximum concurrency for 131,072 tokens per request: 8.74x
[1;36m(EngineCore_DP0 pid=3852322)[0;0m ERROR 12-04 14:02:42 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=3852322)[0;0m ERROR 12-04 14:02:42 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=3852322)[0;0m ERROR 12-04 14:02:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=3852322)[0;0m ERROR 12-04 14:02:42 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3852322)[0;0m ERROR 12-04 14:02:42 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852322)[0;0m ERROR 12-04 14:02:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=3852322)[0;0m ERROR 12-04 14:02:42 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=3852322)[0;0m ERROR 12-04 14:02:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=3852322)[0;0m ERROR 12-04 14:02:42 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=3852322)[0;0m ERROR 12-04 14:02:42 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852322)[0;0m ERROR 12-04 14:02:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 247, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=3852322)[0;0m ERROR 12-04 14:02:42 [core.py:842]     self.model_executor.initialize_from_config(kv_cache_configs)
[1;36m(EngineCore_DP0 pid=3852322)[0;0m ERROR 12-04 14:02:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 115, in initialize_from_config
[1;36m(EngineCore_DP0 pid=3852322)[0;0m ERROR 12-04 14:02:42 [core.py:842]     self.collective_rpc("initialize_from_config", args=(kv_cache_configs,))
[1;36m(EngineCore_DP0 pid=3852322)[0;0m ERROR 12-04 14:02:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=3852322)[0;0m ERROR 12-04 14:02:42 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=3852322)[0;0m ERROR 12-04 14:02:42 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852322)[0;0m ERROR 12-04 14:02:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=3852322)[0;0m ERROR 12-04 14:02:42 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3852322)[0;0m ERROR 12-04 14:02:42 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852322)[0;0m ERROR 12-04 14:02:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 319, in initialize_from_config
[1;36m(EngineCore_DP0 pid=3852322)[0;0m ERROR 12-04 14:02:42 [core.py:842]     self.worker.initialize_from_config(kv_cache_config)  # type: ignore
[1;36m(EngineCore_DP0 pid=3852322)[0;0m ERROR 12-04 14:02:42 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852322)[0;0m ERROR 12-04 14:02:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 404, in initialize_from_config
[1;36m(EngineCore_DP0 pid=3852322)[0;0m ERROR 12-04 14:02:42 [core.py:842]     self.model_runner.initialize_kv_cache(kv_cache_config)
[1;36m(EngineCore_DP0 pid=3852322)[0;0m ERROR 12-04 14:02:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 5007, in initialize_kv_cache
[1;36m(EngineCore_DP0 pid=3852322)[0;0m ERROR 12-04 14:02:42 [core.py:842]     kv_caches = self.initialize_kv_cache_tensors(
[1;36m(EngineCore_DP0 pid=3852322)[0;0m ERROR 12-04 14:02:42 [core.py:842]                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852322)[0;0m ERROR 12-04 14:02:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4933, in initialize_kv_cache_tensors
[1;36m(EngineCore_DP0 pid=3852322)[0;0m ERROR 12-04 14:02:42 [core.py:842]     kv_cache_raw_tensors = self._allocate_kv_cache_tensors(kv_cache_config)
[1;36m(EngineCore_DP0 pid=3852322)[0;0m ERROR 12-04 14:02:42 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852322)[0;0m ERROR 12-04 14:02:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4718, in _allocate_kv_cache_tensors
[1;36m(EngineCore_DP0 pid=3852322)[0;0m ERROR 12-04 14:02:42 [core.py:842]     tensor = torch.zeros(
[1;36m(EngineCore_DP0 pid=3852322)[0;0m ERROR 12-04 14:02:42 [core.py:842]              ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852322)[0;0m ERROR 12-04 14:02:42 [core.py:842] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.16 GiB. GPU 0 has a total capacity of 44.29 GiB of which 49.69 MiB is free. Process 3852325 has 13.14 GiB memory in use. Including non-PyTorch memory, this process has 31.10 GiB memory in use. Of the allocated memory 30.73 GiB is allocated by PyTorch, and 154.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=3852322)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=3852325)[0;0m ERROR 12-04 14:02:42 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=3852325)[0;0m ERROR 12-04 14:02:42 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=3852325)[0;0m ERROR 12-04 14:02:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=3852325)[0;0m ERROR 12-04 14:02:42 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3852325)[0;0m ERROR 12-04 14:02:42 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852325)[0;0m ERROR 12-04 14:02:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=3852325)[0;0m ERROR 12-04 14:02:42 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=3852325)[0;0m ERROR 12-04 14:02:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=3852325)[0;0m ERROR 12-04 14:02:42 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=3852325)[0;0m ERROR 12-04 14:02:42 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852325)[0;0m ERROR 12-04 14:02:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 247, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=3852325)[0;0m ERROR 12-04 14:02:42 [core.py:842]     self.model_executor.initialize_from_config(kv_cache_configs)
[1;36m(EngineCore_DP0 pid=3852325)[0;0m ERROR 12-04 14:02:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 115, in initialize_from_config
[1;36m(EngineCore_DP0 pid=3852325)[0;0m ERROR 12-04 14:02:42 [core.py:842]     self.collective_rpc("initialize_from_config", args=(kv_cache_configs,))
[1;36m(EngineCore_DP0 pid=3852325)[0;0m ERROR 12-04 14:02:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=3852325)[0;0m ERROR 12-04 14:02:42 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=3852325)[0;0m ERROR 12-04 14:02:42 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852325)[0;0m ERROR 12-04 14:02:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=3852325)[0;0m ERROR 12-04 14:02:42 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3852325)[0;0m ERROR 12-04 14:02:42 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852325)[0;0m ERROR 12-04 14:02:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 319, in initialize_from_config
[1;36m(EngineCore_DP0 pid=3852325)[0;0m ERROR 12-04 14:02:42 [core.py:842]     self.worker.initialize_from_config(kv_cache_config)  # type: ignore
[1;36m(EngineCore_DP0 pid=3852325)[0;0m ERROR 12-04 14:02:42 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852325)[0;0m ERROR 12-04 14:02:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 404, in initialize_from_config
[1;36m(EngineCore_DP0 pid=3852325)[0;0m ERROR 12-04 14:02:42 [core.py:842]     self.model_runner.initialize_kv_cache(kv_cache_config)
[1;36m(EngineCore_DP0 pid=3852325)[0;0m ERROR 12-04 14:02:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 5007, in initialize_kv_cache
[1;36m(EngineCore_DP0 pid=3852325)[0;0m ERROR 12-04 14:02:42 [core.py:842]     kv_caches = self.initialize_kv_cache_tensors(
[1;36m(EngineCore_DP0 pid=3852325)[0;0m ERROR 12-04 14:02:42 [core.py:842]                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852325)[0;0m ERROR 12-04 14:02:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4933, in initialize_kv_cache_tensors
[1;36m(EngineCore_DP0 pid=3852325)[0;0m ERROR 12-04 14:02:42 [core.py:842]     kv_cache_raw_tensors = self._allocate_kv_cache_tensors(kv_cache_config)
[1;36m(EngineCore_DP0 pid=3852325)[0;0m ERROR 12-04 14:02:42 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852325)[0;0m ERROR 12-04 14:02:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4718, in _allocate_kv_cache_tensors
[1;36m(EngineCore_DP0 pid=3852325)[0;0m ERROR 12-04 14:02:42 [core.py:842]     tensor = torch.zeros(
[1;36m(EngineCore_DP0 pid=3852325)[0;0m ERROR 12-04 14:02:42 [core.py:842]              ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852325)[0;0m ERROR 12-04 14:02:42 [core.py:842] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.09 GiB. GPU 0 has a total capacity of 44.29 GiB of which 49.69 MiB is free. Including non-PyTorch memory, this process has 13.14 GiB memory in use. Process 3852322 has 31.10 GiB memory in use. Of the allocated memory 12.79 GiB is allocated by PyTorch, and 141.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=3852325)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=3852322)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=3852322)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=3852322)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=3852322)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=3852322)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=3852322)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=3852322)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=3852322)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=3852322)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3852322)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852322)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=3852322)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=3852322)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=3852322)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=3852322)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852322)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 247, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=3852322)[0;0m     self.model_executor.initialize_from_config(kv_cache_configs)
[1;36m(EngineCore_DP0 pid=3852322)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 115, in initialize_from_config
[1;36m(EngineCore_DP0 pid=3852322)[0;0m     self.collective_rpc("initialize_from_config", args=(kv_cache_configs,))
[1;36m(EngineCore_DP0 pid=3852322)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=3852322)[0;0m     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=3852322)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852325)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=3852322)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=3852322)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3852322)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852322)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 319, in initialize_from_config
[1;36m(EngineCore_DP0 pid=3852322)[0;0m     self.worker.initialize_from_config(kv_cache_config)  # type: ignore
[1;36m(EngineCore_DP0 pid=3852322)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852322)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 404, in initialize_from_config
[1;36m(EngineCore_DP0 pid=3852322)[0;0m     self.model_runner.initialize_kv_cache(kv_cache_config)
[1;36m(EngineCore_DP0 pid=3852322)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 5007, in initialize_kv_cache
[1;36m(EngineCore_DP0 pid=3852322)[0;0m     kv_caches = self.initialize_kv_cache_tensors(
[1;36m(EngineCore_DP0 pid=3852322)[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852322)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4933, in initialize_kv_cache_tensors
[1;36m(EngineCore_DP0 pid=3852322)[0;0m     kv_cache_raw_tensors = self._allocate_kv_cache_tensors(kv_cache_config)
[1;36m(EngineCore_DP0 pid=3852322)[0;0m                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852322)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4718, in _allocate_kv_cache_tensors
[1;36m(EngineCore_DP0 pid=3852322)[0;0m     tensor = torch.zeros(
[1;36m(EngineCore_DP0 pid=3852322)[0;0m              ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852322)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.16 GiB. GPU 0 has a total capacity of 44.29 GiB of which 49.69 MiB is free. Process 3852325 has 13.14 GiB memory in use. Including non-PyTorch memory, this process has 31.10 GiB memory in use. Of the allocated memory 30.73 GiB is allocated by PyTorch, and 154.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=3852325)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=3852325)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=3852325)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=3852325)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=3852325)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=3852325)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=3852325)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=3852325)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3852325)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852325)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=3852325)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=3852325)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=3852325)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=3852325)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852325)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 247, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=3852325)[0;0m     self.model_executor.initialize_from_config(kv_cache_configs)
[1;36m(EngineCore_DP0 pid=3852325)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 115, in initialize_from_config
[1;36m(EngineCore_DP0 pid=3852325)[0;0m     self.collective_rpc("initialize_from_config", args=(kv_cache_configs,))
[1;36m(EngineCore_DP0 pid=3852325)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=3852325)[0;0m     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=3852325)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852325)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=3852325)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3852325)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852325)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 319, in initialize_from_config
[1;36m(EngineCore_DP0 pid=3852325)[0;0m     self.worker.initialize_from_config(kv_cache_config)  # type: ignore
[1;36m(EngineCore_DP0 pid=3852325)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852325)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 404, in initialize_from_config
[1;36m(EngineCore_DP0 pid=3852325)[0;0m     self.model_runner.initialize_kv_cache(kv_cache_config)
[1;36m(EngineCore_DP0 pid=3852325)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 5007, in initialize_kv_cache
[1;36m(EngineCore_DP0 pid=3852325)[0;0m     kv_caches = self.initialize_kv_cache_tensors(
[1;36m(EngineCore_DP0 pid=3852325)[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852325)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4933, in initialize_kv_cache_tensors
[1;36m(EngineCore_DP0 pid=3852325)[0;0m     kv_cache_raw_tensors = self._allocate_kv_cache_tensors(kv_cache_config)
[1;36m(EngineCore_DP0 pid=3852325)[0;0m                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852325)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4718, in _allocate_kv_cache_tensors
[1;36m(EngineCore_DP0 pid=3852325)[0;0m     tensor = torch.zeros(
[1;36m(EngineCore_DP0 pid=3852325)[0;0m              ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852325)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.09 GiB. GPU 0 has a total capacity of 44.29 GiB of which 49.69 MiB is free. Including non-PyTorch memory, this process has 13.14 GiB memory in use. Process 3852322 has 31.10 GiB memory in use. Of the allocated memory 12.79 GiB is allocated by PyTorch, and 141.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W1204 14:02:43.044652753 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank0]:[W1204 14:02:43.045212385 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: WeiboAI/VibeThinker-1.5B (backend=vllm)
[ModelCache] Using max_model_len=131072 for WeiboAI/VibeThinker-1.5B
[ModelCache] Detected 1 GPU(s), 44.3GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 14:03:04 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'WeiboAI/VibeThinker-1.5B'}
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: WeiboAI/VibeThinker-1.5B (backend=vllm)
[ModelCache] Using max_model_len=131072 for WeiboAI/VibeThinker-1.5B
[ModelCache] Detected 1 GPU(s), 44.3GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 14:03:04 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'WeiboAI/VibeThinker-1.5B'}
INFO 12-04 14:03:05 [model.py:631] Resolved architecture: Qwen2ForCausalLM
WARNING 12-04 14:03:05 [model.py:1921] Your device 'Quadro RTX 8000' (with compute capability 7.5) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.
WARNING 12-04 14:03:05 [model.py:1971] Casting torch.bfloat16 to torch.float16.
INFO 12-04 14:03:05 [model.py:1745] Using max model len 131072
INFO 12-04 14:03:05 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
INFO 12-04 14:03:05 [model.py:631] Resolved architecture: Qwen2ForCausalLM
WARNING 12-04 14:03:05 [model.py:1921] Your device 'Quadro RTX 8000' (with compute capability 7.5) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.
WARNING 12-04 14:03:05 [model.py:1971] Casting torch.bfloat16 to torch.float16.
INFO 12-04 14:03:05 [model.py:1745] Using max model len 131072
INFO 12-04 14:03:05 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=3852679)[0;0m INFO 12-04 14:03:20 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='WeiboAI/VibeThinker-1.5B', speculative_config=None, tokenizer='WeiboAI/VibeThinker-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=WeiboAI/VibeThinker-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=3852682)[0;0m INFO 12-04 14:03:20 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='WeiboAI/VibeThinker-1.5B', speculative_config=None, tokenizer='WeiboAI/VibeThinker-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=WeiboAI/VibeThinker-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=3852679)[0;0m ERROR 12-04 14:03:21 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8
[1;36m(EngineCore_DP0 pid=3852682)[0;0m ERROR 12-04 14:03:21 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8
[1;36m(EngineCore_DP0 pid=3852679)[0;0m INFO 12-04 14:03:22 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.7:44527 backend=nccl
[1;36m(EngineCore_DP0 pid=3852682)[0;0m INFO 12-04 14:03:22 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.7:54773 backend=nccl
[W1204 14:03:22.417974266 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-23-2.rc.tch.harvard.edu]:44527 (errno: 97 - Address family not supported by protocol).
[W1204 14:03:22.419044562 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-23-2.rc.tch.harvard.edu]:54773 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=3852682)[0;0m INFO 12-04 14:03:22 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=3852679)[0;0m INFO 12-04 14:03:22 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=3852679)[0;0m INFO 12-04 14:03:23 [gpu_model_runner.py:3259] Starting to load model WeiboAI/VibeThinker-1.5B...
[1;36m(EngineCore_DP0 pid=3852682)[0;0m INFO 12-04 14:03:23 [gpu_model_runner.py:3259] Starting to load model WeiboAI/VibeThinker-1.5B...
[1;36m(EngineCore_DP0 pid=3852679)[0;0m INFO 12-04 14:03:24 [cuda.py:418] Valid backends: ['FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=3852679)[0;0m INFO 12-04 14:03:24 [cuda.py:427] Using FLASHINFER backend.
[1;36m(EngineCore_DP0 pid=3852682)[0;0m INFO 12-04 14:03:24 [cuda.py:418] Valid backends: ['FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=3852682)[0;0m INFO 12-04 14:03:24 [cuda.py:427] Using FLASHINFER backend.
[1;36m(EngineCore_DP0 pid=3852679)[0;0m INFO 12-04 14:03:24 [weight_utils.py:481] No model.safetensors.index.json found in remote.
[1;36m(EngineCore_DP0 pid=3852679)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
[1;36m(EngineCore_DP0 pid=3852682)[0;0m INFO 12-04 14:03:24 [weight_utils.py:481] No model.safetensors.index.json found in remote.
[1;36m(EngineCore_DP0 pid=3852682)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
[1;36m(EngineCore_DP0 pid=3852679)[0;0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:04<00:00,  4.12s/it]
[1;36m(EngineCore_DP0 pid=3852679)[0;0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:04<00:00,  4.12s/it]
[1;36m(EngineCore_DP0 pid=3852679)[0;0m 
[1;36m(EngineCore_DP0 pid=3852682)[0;0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:04<00:00,  4.15s/it]
[1;36m(EngineCore_DP0 pid=3852682)[0;0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:04<00:00,  4.15s/it]
[1;36m(EngineCore_DP0 pid=3852682)[0;0m 
[1;36m(EngineCore_DP0 pid=3852679)[0;0m INFO 12-04 14:03:28 [default_loader.py:314] Loading weights took 4.27 seconds
[1;36m(EngineCore_DP0 pid=3852682)[0;0m INFO 12-04 14:03:29 [default_loader.py:314] Loading weights took 4.31 seconds
[1;36m(EngineCore_DP0 pid=3852679)[0;0m INFO 12-04 14:03:29 [gpu_model_runner.py:3338] Model loading took 2.9110 GiB memory and 5.349610 seconds
[1;36m(EngineCore_DP0 pid=3852682)[0;0m INFO 12-04 14:03:29 [gpu_model_runner.py:3338] Model loading took 2.9110 GiB memory and 5.499486 seconds
[1;36m(EngineCore_DP0 pid=3852682)[0;0m INFO 12-04 14:03:41 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/6847b7a26b/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=3852679)[0;0m INFO 12-04 14:03:41 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/6847b7a26b/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=3852679)[0;0m INFO 12-04 14:03:41 [backends.py:647] Dynamo bytecode transform time: 11.37 s
[1;36m(EngineCore_DP0 pid=3852682)[0;0m INFO 12-04 14:03:41 [backends.py:647] Dynamo bytecode transform time: 11.22 s
[1;36m(EngineCore_DP0 pid=3852682)[0;0m INFO 12-04 14:03:48 [backends.py:210] Directly load the compiled graph(s) for dynamic shape from the cache, took 6.053 s
[1;36m(EngineCore_DP0 pid=3852679)[0;0m INFO 12-04 14:03:48 [backends.py:210] Directly load the compiled graph(s) for dynamic shape from the cache, took 6.055 s
[1;36m(EngineCore_DP0 pid=3852682)[0;0m INFO 12-04 14:03:50 [monitor.py:34] torch.compile takes 17.27 s in total
[1;36m(EngineCore_DP0 pid=3852679)[0;0m INFO 12-04 14:03:50 [monitor.py:34] torch.compile takes 17.42 s in total
[1;36m(EngineCore_DP0 pid=3852679)[0;0m INFO 12-04 14:03:51 [gpu_worker.py:359] Available KV cache memory: 30.58 GiB
[1;36m(EngineCore_DP0 pid=3852682)[0;0m INFO 12-04 14:03:51 [gpu_worker.py:359] Available KV cache memory: 32.40 GiB
[1;36m(EngineCore_DP0 pid=3852679)[0;0m INFO 12-04 14:03:52 [kv_cache_utils.py:1229] GPU KV cache size: 1,145,232 tokens
[1;36m(EngineCore_DP0 pid=3852679)[0;0m INFO 12-04 14:03:52 [kv_cache_utils.py:1234] Maximum concurrency for 131,072 tokens per request: 8.74x
[1;36m(EngineCore_DP0 pid=3852679)[0;0m INFO 12-04 14:03:52 [kernel_warmup.py:65] Warming up FlashInfer attention.
[1;36m(EngineCore_DP0 pid=3852682)[0;0m INFO 12-04 14:03:52 [kv_cache_utils.py:1229] GPU KV cache size: 1,213,184 tokens
[1;36m(EngineCore_DP0 pid=3852682)[0;0m INFO 12-04 14:03:52 [kv_cache_utils.py:1234] Maximum concurrency for 131,072 tokens per request: 9.26x
[1;36m(EngineCore_DP0 pid=3852682)[0;0m ERROR 12-04 14:03:52 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=3852682)[0;0m ERROR 12-04 14:03:52 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=3852682)[0;0m ERROR 12-04 14:03:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=3852682)[0;0m ERROR 12-04 14:03:52 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3852682)[0;0m ERROR 12-04 14:03:52 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852682)[0;0m ERROR 12-04 14:03:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=3852682)[0;0m ERROR 12-04 14:03:52 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=3852682)[0;0m ERROR 12-04 14:03:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=3852682)[0;0m ERROR 12-04 14:03:52 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=3852682)[0;0m ERROR 12-04 14:03:52 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852682)[0;0m ERROR 12-04 14:03:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 247, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=3852682)[0;0m ERROR 12-04 14:03:52 [core.py:842]     self.model_executor.initialize_from_config(kv_cache_configs)
[1;36m(EngineCore_DP0 pid=3852682)[0;0m ERROR 12-04 14:03:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 115, in initialize_from_config
[1;36m(EngineCore_DP0 pid=3852682)[0;0m ERROR 12-04 14:03:52 [core.py:842]     self.collective_rpc("initialize_from_config", args=(kv_cache_configs,))
[1;36m(EngineCore_DP0 pid=3852682)[0;0m ERROR 12-04 14:03:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=3852682)[0;0m ERROR 12-04 14:03:52 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=3852682)[0;0m ERROR 12-04 14:03:52 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852682)[0;0m ERROR 12-04 14:03:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=3852682)[0;0m ERROR 12-04 14:03:52 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3852682)[0;0m ERROR 12-04 14:03:52 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852682)[0;0m ERROR 12-04 14:03:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 319, in initialize_from_config
[1;36m(EngineCore_DP0 pid=3852682)[0;0m ERROR 12-04 14:03:52 [core.py:842]     self.worker.initialize_from_config(kv_cache_config)  # type: ignore
[1;36m(EngineCore_DP0 pid=3852682)[0;0m ERROR 12-04 14:03:52 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852682)[0;0m ERROR 12-04 14:03:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 404, in initialize_from_config
[1;36m(EngineCore_DP0 pid=3852682)[0;0m ERROR 12-04 14:03:52 [core.py:842]     self.model_runner.initialize_kv_cache(kv_cache_config)
[1;36m(EngineCore_DP0 pid=3852682)[0;0m ERROR 12-04 14:03:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 5007, in initialize_kv_cache
[1;36m(EngineCore_DP0 pid=3852682)[0;0m ERROR 12-04 14:03:52 [core.py:842]     kv_caches = self.initialize_kv_cache_tensors(
[1;36m(EngineCore_DP0 pid=3852682)[0;0m ERROR 12-04 14:03:52 [core.py:842]                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852682)[0;0m ERROR 12-04 14:03:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4933, in initialize_kv_cache_tensors
[1;36m(EngineCore_DP0 pid=3852682)[0;0m ERROR 12-04 14:03:52 [core.py:842]     kv_cache_raw_tensors = self._allocate_kv_cache_tensors(kv_cache_config)
[1;36m(EngineCore_DP0 pid=3852682)[0;0m ERROR 12-04 14:03:52 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852682)[0;0m ERROR 12-04 14:03:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4718, in _allocate_kv_cache_tensors
[1;36m(EngineCore_DP0 pid=3852682)[0;0m ERROR 12-04 14:03:52 [core.py:842]     tensor = torch.zeros(
[1;36m(EngineCore_DP0 pid=3852682)[0;0m ERROR 12-04 14:03:52 [core.py:842]              ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852682)[0;0m ERROR 12-04 14:03:52 [core.py:842] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.16 GiB. GPU 0 has a total capacity of 44.29 GiB of which 117.69 MiB is free. Process 3852679 has 33.93 GiB memory in use. Including non-PyTorch memory, this process has 10.25 GiB memory in use. Of the allocated memory 9.90 GiB is allocated by PyTorch, and 135.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=3852682)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=3852682)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=3852682)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=3852682)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=3852682)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=3852682)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=3852682)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=3852682)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=3852682)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=3852682)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3852682)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852682)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=3852682)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=3852682)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=3852682)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=3852682)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852682)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 247, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=3852682)[0;0m     self.model_executor.initialize_from_config(kv_cache_configs)
[1;36m(EngineCore_DP0 pid=3852682)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 115, in initialize_from_config
[1;36m(EngineCore_DP0 pid=3852682)[0;0m     self.collective_rpc("initialize_from_config", args=(kv_cache_configs,))
[1;36m(EngineCore_DP0 pid=3852682)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=3852682)[0;0m     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=3852682)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852682)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=3852682)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3852682)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852682)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 319, in initialize_from_config
[1;36m(EngineCore_DP0 pid=3852682)[0;0m     self.worker.initialize_from_config(kv_cache_config)  # type: ignore
[1;36m(EngineCore_DP0 pid=3852682)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852682)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 404, in initialize_from_config
[1;36m(EngineCore_DP0 pid=3852682)[0;0m     self.model_runner.initialize_kv_cache(kv_cache_config)
[1;36m(EngineCore_DP0 pid=3852682)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 5007, in initialize_kv_cache
[1;36m(EngineCore_DP0 pid=3852682)[0;0m     kv_caches = self.initialize_kv_cache_tensors(
[1;36m(EngineCore_DP0 pid=3852682)[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852682)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4933, in initialize_kv_cache_tensors
[1;36m(EngineCore_DP0 pid=3852682)[0;0m     kv_cache_raw_tensors = self._allocate_kv_cache_tensors(kv_cache_config)
[1;36m(EngineCore_DP0 pid=3852682)[0;0m                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852682)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4718, in _allocate_kv_cache_tensors
[1;36m(EngineCore_DP0 pid=3852682)[0;0m     tensor = torch.zeros(
[1;36m(EngineCore_DP0 pid=3852682)[0;0m              ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852682)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.16 GiB. GPU 0 has a total capacity of 44.29 GiB of which 117.69 MiB is free. Process 3852679 has 33.93 GiB memory in use. Including non-PyTorch memory, this process has 10.25 GiB memory in use. Of the allocated memory 9.90 GiB is allocated by PyTorch, and 135.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=3852679)[0;0m ERROR 12-04 14:03:52 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=3852679)[0;0m ERROR 12-04 14:03:52 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=3852679)[0;0m ERROR 12-04 14:03:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=3852679)[0;0m ERROR 12-04 14:03:52 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3852679)[0;0m ERROR 12-04 14:03:52 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852679)[0;0m ERROR 12-04 14:03:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=3852679)[0;0m ERROR 12-04 14:03:52 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=3852679)[0;0m ERROR 12-04 14:03:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=3852679)[0;0m ERROR 12-04 14:03:52 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=3852679)[0;0m ERROR 12-04 14:03:52 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852679)[0;0m ERROR 12-04 14:03:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 247, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=3852679)[0;0m ERROR 12-04 14:03:52 [core.py:842]     self.model_executor.initialize_from_config(kv_cache_configs)
[1;36m(EngineCore_DP0 pid=3852679)[0;0m ERROR 12-04 14:03:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 116, in initialize_from_config
[1;36m(EngineCore_DP0 pid=3852679)[0;0m ERROR 12-04 14:03:52 [core.py:842]     self.collective_rpc("compile_or_warm_up_model")
[1;36m(EngineCore_DP0 pid=3852679)[0;0m ERROR 12-04 14:03:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=3852679)[0;0m ERROR 12-04 14:03:52 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=3852679)[0;0m ERROR 12-04 14:03:52 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852679)[0;0m ERROR 12-04 14:03:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=3852679)[0;0m ERROR 12-04 14:03:52 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3852679)[0;0m ERROR 12-04 14:03:52 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852679)[0;0m ERROR 12-04 14:03:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 425, in compile_or_warm_up_model
[1;36m(EngineCore_DP0 pid=3852679)[0;0m ERROR 12-04 14:03:52 [core.py:842]     kernel_warmup(self)
[1;36m(EngineCore_DP0 pid=3852679)[0;0m ERROR 12-04 14:03:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/warmup/kernel_warmup.py", line 68, in kernel_warmup
[1;36m(EngineCore_DP0 pid=3852679)[0;0m ERROR 12-04 14:03:52 [core.py:842]     worker.model_runner._dummy_run(
[1;36m(EngineCore_DP0 pid=3852679)[0;0m ERROR 12-04 14:03:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=3852679)[0;0m ERROR 12-04 14:03:52 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3852679)[0;0m ERROR 12-04 14:03:52 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852679)[0;0m ERROR 12-04 14:03:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3756, in _dummy_run
[1;36m(EngineCore_DP0 pid=3852679)[0;0m ERROR 12-04 14:03:52 [core.py:842]     attn_metadata, _ = self._build_attention_metadata(
[1;36m(EngineCore_DP0 pid=3852679)[0;0m ERROR 12-04 14:03:52 [core.py:842]                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852679)[0;0m ERROR 12-04 14:03:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1576, in _build_attention_metadata
[1;36m(EngineCore_DP0 pid=3852679)[0;0m ERROR 12-04 14:03:52 [core.py:842]     attn_metadata_i = builder.build_for_cudagraph_capture(
[1;36m(EngineCore_DP0 pid=3852679)[0;0m ERROR 12-04 14:03:52 [core.py:842]                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852679)[0;0m ERROR 12-04 14:03:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/attention/backends/utils.py", line 332, in build_for_cudagraph_capture
[1;36m(EngineCore_DP0 pid=3852679)[0;0m ERROR 12-04 14:03:52 [core.py:842]     return self.build(
[1;36m(EngineCore_DP0 pid=3852679)[0;0m ERROR 12-04 14:03:52 [core.py:842]            ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852679)[0;0m ERROR 12-04 14:03:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/attention/backends/flashinfer.py", line 844, in build
[1;36m(EngineCore_DP0 pid=3852679)[0;0m ERROR 12-04 14:03:52 [core.py:842]     attn_metadata.prefill_wrapper = self._get_prefill_wrapper()
[1;36m(EngineCore_DP0 pid=3852679)[0;0m ERROR 12-04 14:03:52 [core.py:842]                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852679)[0;0m ERROR 12-04 14:03:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/attention/backends/flashinfer.py", line 598, in _get_prefill_wrapper
[1;36m(EngineCore_DP0 pid=3852679)[0;0m ERROR 12-04 14:03:52 [core.py:842]     self._get_workspace_buffer(), get_kv_cache_layout()
[1;36m(EngineCore_DP0 pid=3852679)[0;0m ERROR 12-04 14:03:52 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852679)[0;0m ERROR 12-04 14:03:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/attention/backends/flashinfer.py", line 583, in _get_workspace_buffer
[1;36m(EngineCore_DP0 pid=3852679)[0;0m ERROR 12-04 14:03:52 [core.py:842]     self._workspace_buffer = torch.zeros(
[1;36m(EngineCore_DP0 pid=3852679)[0;0m ERROR 12-04 14:03:52 [core.py:842]                              ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852679)[0;0m ERROR 12-04 14:03:52 [core.py:842] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 394.00 MiB. GPU 0 has a total capacity of 44.29 GiB of which 117.69 MiB is free. Including non-PyTorch memory, this process has 33.93 GiB memory in use. Process 3852682 has 10.25 GiB memory in use. Of the allocated memory 33.54 GiB is allocated by PyTorch, and 172.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=3852679)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=3852679)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=3852679)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=3852679)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=3852679)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=3852679)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=3852679)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=3852679)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=3852679)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=3852679)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3852679)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852679)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=3852679)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=3852679)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=3852679)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=3852679)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852679)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 247, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=3852679)[0;0m     self.model_executor.initialize_from_config(kv_cache_configs)
[1;36m(EngineCore_DP0 pid=3852679)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 116, in initialize_from_config
[1;36m(EngineCore_DP0 pid=3852679)[0;0m     self.collective_rpc("compile_or_warm_up_model")
[1;36m(EngineCore_DP0 pid=3852679)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=3852679)[0;0m     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=3852679)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852679)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=3852679)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3852679)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852679)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 425, in compile_or_warm_up_model
[1;36m(EngineCore_DP0 pid=3852679)[0;0m     kernel_warmup(self)
[1;36m(EngineCore_DP0 pid=3852679)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/warmup/kernel_warmup.py", line 68, in kernel_warmup
[1;36m(EngineCore_DP0 pid=3852679)[0;0m     worker.model_runner._dummy_run(
[1;36m(EngineCore_DP0 pid=3852679)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=3852679)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3852679)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852679)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3756, in _dummy_run
[1;36m(EngineCore_DP0 pid=3852679)[0;0m     attn_metadata, _ = self._build_attention_metadata(
[1;36m(EngineCore_DP0 pid=3852679)[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852679)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1576, in _build_attention_metadata
[1;36m(EngineCore_DP0 pid=3852679)[0;0m     attn_metadata_i = builder.build_for_cudagraph_capture(
[1;36m(EngineCore_DP0 pid=3852679)[0;0m                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852679)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/attention/backends/utils.py", line 332, in build_for_cudagraph_capture
[1;36m(EngineCore_DP0 pid=3852679)[0;0m     return self.build(
[1;36m(EngineCore_DP0 pid=3852679)[0;0m            ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852679)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/attention/backends/flashinfer.py", line 844, in build
[1;36m(EngineCore_DP0 pid=3852679)[0;0m     attn_metadata.prefill_wrapper = self._get_prefill_wrapper()
[1;36m(EngineCore_DP0 pid=3852679)[0;0m                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852679)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/attention/backends/flashinfer.py", line 598, in _get_prefill_wrapper
[1;36m(EngineCore_DP0 pid=3852679)[0;0m     self._get_workspace_buffer(), get_kv_cache_layout()
[1;36m(EngineCore_DP0 pid=3852679)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852679)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/attention/backends/flashinfer.py", line 583, in _get_workspace_buffer
[1;36m(EngineCore_DP0 pid=3852679)[0;0m     self._workspace_buffer = torch.zeros(
[1;36m(EngineCore_DP0 pid=3852679)[0;0m                              ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852679)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 394.00 MiB. GPU 0 has a total capacity of 44.29 GiB of which 117.69 MiB is free. Including non-PyTorch memory, this process has 33.93 GiB memory in use. Process 3852682 has 10.25 GiB memory in use. Of the allocated memory 33.54 GiB is allocated by PyTorch, and 172.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W1204 14:03:53.409560899 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank0]:[W1204 14:03:53.449748198 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: WeiboAI/VibeThinker-1.5B (backend=vllm)
[ModelCache] Using max_model_len=131072 for WeiboAI/VibeThinker-1.5B
[ModelCache] Detected 1 GPU(s), 44.3GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 14:04:15 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'WeiboAI/VibeThinker-1.5B'}
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: WeiboAI/VibeThinker-1.5B (backend=vllm)
[ModelCache] Using max_model_len=131072 for WeiboAI/VibeThinker-1.5B
[ModelCache] Detected 1 GPU(s), 44.3GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 14:04:15 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'WeiboAI/VibeThinker-1.5B'}
INFO 12-04 14:04:15 [model.py:631] Resolved architecture: Qwen2ForCausalLM
WARNING 12-04 14:04:15 [model.py:1921] Your device 'Quadro RTX 8000' (with compute capability 7.5) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.
WARNING 12-04 14:04:15 [model.py:1971] Casting torch.bfloat16 to torch.float16.
INFO 12-04 14:04:15 [model.py:1745] Using max model len 131072
INFO 12-04 14:04:15 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
INFO 12-04 14:04:15 [model.py:631] Resolved architecture: Qwen2ForCausalLM
WARNING 12-04 14:04:15 [model.py:1921] Your device 'Quadro RTX 8000' (with compute capability 7.5) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.
WARNING 12-04 14:04:15 [model.py:1971] Casting torch.bfloat16 to torch.float16.
INFO 12-04 14:04:15 [model.py:1745] Using max model len 131072
INFO 12-04 14:04:15 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=3852964)[0;0m INFO 12-04 14:04:30 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='WeiboAI/VibeThinker-1.5B', speculative_config=None, tokenizer='WeiboAI/VibeThinker-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=WeiboAI/VibeThinker-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=3852961)[0;0m INFO 12-04 14:04:30 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='WeiboAI/VibeThinker-1.5B', speculative_config=None, tokenizer='WeiboAI/VibeThinker-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=WeiboAI/VibeThinker-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=3852961)[0;0m ERROR 12-04 14:04:31 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8
[1;36m(EngineCore_DP0 pid=3852964)[0;0m ERROR 12-04 14:04:31 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8
[1;36m(EngineCore_DP0 pid=3852961)[0;0m INFO 12-04 14:04:32 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.7:45109 backend=nccl
[1;36m(EngineCore_DP0 pid=3852964)[0;0m INFO 12-04 14:04:32 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.7:51163 backend=nccl
[W1204 14:04:32.522355727 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-23-2.rc.tch.harvard.edu]:45109 (errno: 97 - Address family not supported by protocol).
[W1204 14:04:32.524960868 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-23-2.rc.tch.harvard.edu]:51163 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=3852964)[0;0m INFO 12-04 14:04:32 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=3852961)[0;0m INFO 12-04 14:04:32 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=3852961)[0;0m INFO 12-04 14:04:33 [gpu_model_runner.py:3259] Starting to load model WeiboAI/VibeThinker-1.5B...
[1;36m(EngineCore_DP0 pid=3852964)[0;0m INFO 12-04 14:04:33 [gpu_model_runner.py:3259] Starting to load model WeiboAI/VibeThinker-1.5B...
[1;36m(EngineCore_DP0 pid=3852964)[0;0m INFO 12-04 14:04:34 [cuda.py:418] Valid backends: ['FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=3852964)[0;0m INFO 12-04 14:04:34 [cuda.py:427] Using FLASHINFER backend.
[1;36m(EngineCore_DP0 pid=3852961)[0;0m INFO 12-04 14:04:34 [cuda.py:418] Valid backends: ['FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=3852961)[0;0m INFO 12-04 14:04:34 [cuda.py:427] Using FLASHINFER backend.
[1;36m(EngineCore_DP0 pid=3852964)[0;0m INFO 12-04 14:04:34 [weight_utils.py:481] No model.safetensors.index.json found in remote.
[1;36m(EngineCore_DP0 pid=3852964)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
[1;36m(EngineCore_DP0 pid=3852961)[0;0m INFO 12-04 14:04:34 [weight_utils.py:481] No model.safetensors.index.json found in remote.
[1;36m(EngineCore_DP0 pid=3852961)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
[1;36m(EngineCore_DP0 pid=3852961)[0;0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:04<00:00,  4.12s/it]
[1;36m(EngineCore_DP0 pid=3852961)[0;0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:04<00:00,  4.12s/it]
[1;36m(EngineCore_DP0 pid=3852961)[0;0m 
[1;36m(EngineCore_DP0 pid=3852961)[0;0m INFO 12-04 14:04:39 [default_loader.py:314] Loading weights took 4.26 seconds
[1;36m(EngineCore_DP0 pid=3852964)[0;0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:04<00:00,  4.59s/it]
[1;36m(EngineCore_DP0 pid=3852964)[0;0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:04<00:00,  4.59s/it]
[1;36m(EngineCore_DP0 pid=3852964)[0;0m 
[1;36m(EngineCore_DP0 pid=3852964)[0;0m INFO 12-04 14:04:39 [default_loader.py:314] Loading weights took 4.74 seconds
[1;36m(EngineCore_DP0 pid=3852961)[0;0m INFO 12-04 14:04:39 [gpu_model_runner.py:3338] Model loading took 2.9110 GiB memory and 5.500186 seconds
[1;36m(EngineCore_DP0 pid=3852964)[0;0m INFO 12-04 14:04:40 [gpu_model_runner.py:3338] Model loading took 2.9110 GiB memory and 5.841016 seconds
[1;36m(EngineCore_DP0 pid=3852961)[0;0m INFO 12-04 14:04:51 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/6847b7a26b/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=3852964)[0;0m INFO 12-04 14:04:51 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/6847b7a26b/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=3852961)[0;0m INFO 12-04 14:04:51 [backends.py:647] Dynamo bytecode transform time: 11.32 s
[1;36m(EngineCore_DP0 pid=3852964)[0;0m INFO 12-04 14:04:51 [backends.py:647] Dynamo bytecode transform time: 11.00 s
[1;36m(EngineCore_DP0 pid=3852964)[0;0m INFO 12-04 14:04:58 [backends.py:210] Directly load the compiled graph(s) for dynamic shape from the cache, took 5.971 s
[1;36m(EngineCore_DP0 pid=3852961)[0;0m INFO 12-04 14:04:58 [backends.py:210] Directly load the compiled graph(s) for dynamic shape from the cache, took 5.987 s
[1;36m(EngineCore_DP0 pid=3852964)[0;0m INFO 12-04 14:05:00 [monitor.py:34] torch.compile takes 16.98 s in total
[1;36m(EngineCore_DP0 pid=3852961)[0;0m INFO 12-04 14:05:00 [monitor.py:34] torch.compile takes 17.31 s in total
[1;36m(EngineCore_DP0 pid=3852964)[0;0m INFO 12-04 14:05:01 [gpu_worker.py:359] Available KV cache memory: 30.58 GiB
[1;36m(EngineCore_DP0 pid=3852961)[0;0m INFO 12-04 14:05:01 [gpu_worker.py:359] Available KV cache memory: 32.40 GiB
[1;36m(EngineCore_DP0 pid=3852964)[0;0m INFO 12-04 14:05:02 [kv_cache_utils.py:1229] GPU KV cache size: 1,145,232 tokens
[1;36m(EngineCore_DP0 pid=3852964)[0;0m INFO 12-04 14:05:02 [kv_cache_utils.py:1234] Maximum concurrency for 131,072 tokens per request: 8.74x
[1;36m(EngineCore_DP0 pid=3852961)[0;0m INFO 12-04 14:05:02 [kv_cache_utils.py:1229] GPU KV cache size: 1,213,248 tokens
[1;36m(EngineCore_DP0 pid=3852961)[0;0m INFO 12-04 14:05:02 [kv_cache_utils.py:1234] Maximum concurrency for 131,072 tokens per request: 9.26x
[1;36m(EngineCore_DP0 pid=3852961)[0;0m ERROR 12-04 14:05:02 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=3852961)[0;0m ERROR 12-04 14:05:02 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=3852961)[0;0m ERROR 12-04 14:05:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=3852961)[0;0m ERROR 12-04 14:05:02 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3852961)[0;0m ERROR 12-04 14:05:02 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852961)[0;0m ERROR 12-04 14:05:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=3852961)[0;0m ERROR 12-04 14:05:02 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=3852961)[0;0m ERROR 12-04 14:05:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=3852961)[0;0m ERROR 12-04 14:05:02 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=3852961)[0;0m ERROR 12-04 14:05:02 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852961)[0;0m ERROR 12-04 14:05:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 247, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=3852961)[0;0m ERROR 12-04 14:05:02 [core.py:842]     self.model_executor.initialize_from_config(kv_cache_configs)
[1;36m(EngineCore_DP0 pid=3852961)[0;0m ERROR 12-04 14:05:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 115, in initialize_from_config
[1;36m(EngineCore_DP0 pid=3852961)[0;0m ERROR 12-04 14:05:02 [core.py:842]     self.collective_rpc("initialize_from_config", args=(kv_cache_configs,))
[1;36m(EngineCore_DP0 pid=3852961)[0;0m ERROR 12-04 14:05:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=3852961)[0;0m ERROR 12-04 14:05:02 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=3852961)[0;0m ERROR 12-04 14:05:02 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852961)[0;0m ERROR 12-04 14:05:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=3852961)[0;0m ERROR 12-04 14:05:02 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3852961)[0;0m ERROR 12-04 14:05:02 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852961)[0;0m ERROR 12-04 14:05:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 319, in initialize_from_config
[1;36m(EngineCore_DP0 pid=3852961)[0;0m ERROR 12-04 14:05:02 [core.py:842]     self.worker.initialize_from_config(kv_cache_config)  # type: ignore
[1;36m(EngineCore_DP0 pid=3852961)[0;0m ERROR 12-04 14:05:02 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852961)[0;0m ERROR 12-04 14:05:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 404, in initialize_from_config
[1;36m(EngineCore_DP0 pid=3852961)[0;0m ERROR 12-04 14:05:02 [core.py:842]     self.model_runner.initialize_kv_cache(kv_cache_config)
[1;36m(EngineCore_DP0 pid=3852961)[0;0m ERROR 12-04 14:05:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 5007, in initialize_kv_cache
[1;36m(EngineCore_DP0 pid=3852961)[0;0m ERROR 12-04 14:05:02 [core.py:842]     kv_caches = self.initialize_kv_cache_tensors(
[1;36m(EngineCore_DP0 pid=3852961)[0;0m ERROR 12-04 14:05:02 [core.py:842]                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852961)[0;0m ERROR 12-04 14:05:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4933, in initialize_kv_cache_tensors
[1;36m(EngineCore_DP0 pid=3852961)[0;0m ERROR 12-04 14:05:02 [core.py:842]     kv_cache_raw_tensors = self._allocate_kv_cache_tensors(kv_cache_config)
[1;36m(EngineCore_DP0 pid=3852961)[0;0m ERROR 12-04 14:05:02 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852961)[0;0m ERROR 12-04 14:05:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4718, in _allocate_kv_cache_tensors
[1;36m(EngineCore_DP0 pid=3852961)[0;0m ERROR 12-04 14:05:02 [core.py:842]     tensor = torch.zeros(
[1;36m(EngineCore_DP0 pid=3852961)[0;0m ERROR 12-04 14:05:02 [core.py:842]              ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852961)[0;0m ERROR 12-04 14:05:02 [core.py:842] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.16 GiB. GPU 0 has a total capacity of 44.29 GiB of which 841.69 MiB is free. Process 3852964 has 26.27 GiB memory in use. Including non-PyTorch memory, this process has 17.20 GiB memory in use. Of the allocated memory 16.84 GiB is allocated by PyTorch, and 142.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=3852961)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=3852961)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=3852961)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=3852961)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=3852961)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=3852961)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=3852961)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=3852961)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=3852961)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=3852961)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3852961)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852961)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=3852961)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=3852961)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=3852961)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=3852961)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852961)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 247, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=3852961)[0;0m     self.model_executor.initialize_from_config(kv_cache_configs)
[1;36m(EngineCore_DP0 pid=3852961)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 115, in initialize_from_config
[1;36m(EngineCore_DP0 pid=3852961)[0;0m     self.collective_rpc("initialize_from_config", args=(kv_cache_configs,))
[1;36m(EngineCore_DP0 pid=3852961)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=3852961)[0;0m     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=3852961)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852961)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=3852961)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3852961)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852961)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 319, in initialize_from_config
[1;36m(EngineCore_DP0 pid=3852961)[0;0m     self.worker.initialize_from_config(kv_cache_config)  # type: ignore
[1;36m(EngineCore_DP0 pid=3852961)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852961)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 404, in initialize_from_config
[1;36m(EngineCore_DP0 pid=3852961)[0;0m     self.model_runner.initialize_kv_cache(kv_cache_config)
[1;36m(EngineCore_DP0 pid=3852961)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 5007, in initialize_kv_cache
[1;36m(EngineCore_DP0 pid=3852961)[0;0m     kv_caches = self.initialize_kv_cache_tensors(
[1;36m(EngineCore_DP0 pid=3852961)[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852961)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4933, in initialize_kv_cache_tensors
[1;36m(EngineCore_DP0 pid=3852961)[0;0m     kv_cache_raw_tensors = self._allocate_kv_cache_tensors(kv_cache_config)
[1;36m(EngineCore_DP0 pid=3852961)[0;0m                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852961)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4718, in _allocate_kv_cache_tensors
[1;36m(EngineCore_DP0 pid=3852961)[0;0m     tensor = torch.zeros(
[1;36m(EngineCore_DP0 pid=3852961)[0;0m              ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852961)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.16 GiB. GPU 0 has a total capacity of 44.29 GiB of which 841.69 MiB is free. Process 3852964 has 26.27 GiB memory in use. Including non-PyTorch memory, this process has 17.20 GiB memory in use. Of the allocated memory 16.84 GiB is allocated by PyTorch, and 142.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=3852964)[0;0m ERROR 12-04 14:05:02 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=3852964)[0;0m ERROR 12-04 14:05:02 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=3852964)[0;0m ERROR 12-04 14:05:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=3852964)[0;0m ERROR 12-04 14:05:02 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3852964)[0;0m ERROR 12-04 14:05:02 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852964)[0;0m ERROR 12-04 14:05:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=3852964)[0;0m ERROR 12-04 14:05:02 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=3852964)[0;0m ERROR 12-04 14:05:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=3852964)[0;0m ERROR 12-04 14:05:02 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=3852964)[0;0m ERROR 12-04 14:05:02 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852964)[0;0m ERROR 12-04 14:05:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 247, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=3852964)[0;0m ERROR 12-04 14:05:02 [core.py:842]     self.model_executor.initialize_from_config(kv_cache_configs)
[1;36m(EngineCore_DP0 pid=3852964)[0;0m ERROR 12-04 14:05:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 115, in initialize_from_config
[1;36m(EngineCore_DP0 pid=3852964)[0;0m ERROR 12-04 14:05:02 [core.py:842]     self.collective_rpc("initialize_from_config", args=(kv_cache_configs,))
[1;36m(EngineCore_DP0 pid=3852964)[0;0m ERROR 12-04 14:05:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=3852964)[0;0m ERROR 12-04 14:05:02 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=3852964)[0;0m ERROR 12-04 14:05:02 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852964)[0;0m ERROR 12-04 14:05:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=3852964)[0;0m ERROR 12-04 14:05:02 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3852964)[0;0m ERROR 12-04 14:05:02 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852964)[0;0m ERROR 12-04 14:05:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 319, in initialize_from_config
[1;36m(EngineCore_DP0 pid=3852964)[0;0m ERROR 12-04 14:05:02 [core.py:842]     self.worker.initialize_from_config(kv_cache_config)  # type: ignore
[1;36m(EngineCore_DP0 pid=3852964)[0;0m ERROR 12-04 14:05:02 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852964)[0;0m ERROR 12-04 14:05:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 404, in initialize_from_config
[1;36m(EngineCore_DP0 pid=3852964)[0;0m ERROR 12-04 14:05:02 [core.py:842]     self.model_runner.initialize_kv_cache(kv_cache_config)
[1;36m(EngineCore_DP0 pid=3852964)[0;0m ERROR 12-04 14:05:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 5007, in initialize_kv_cache
[1;36m(EngineCore_DP0 pid=3852964)[0;0m ERROR 12-04 14:05:02 [core.py:842]     kv_caches = self.initialize_kv_cache_tensors(
[1;36m(EngineCore_DP0 pid=3852964)[0;0m ERROR 12-04 14:05:02 [core.py:842]                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852964)[0;0m ERROR 12-04 14:05:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4933, in initialize_kv_cache_tensors
[1;36m(EngineCore_DP0 pid=3852964)[0;0m ERROR 12-04 14:05:02 [core.py:842]     kv_cache_raw_tensors = self._allocate_kv_cache_tensors(kv_cache_config)
[1;36m(EngineCore_DP0 pid=3852964)[0;0m ERROR 12-04 14:05:02 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852964)[0;0m ERROR 12-04 14:05:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4718, in _allocate_kv_cache_tensors
[1;36m(EngineCore_DP0 pid=3852964)[0;0m ERROR 12-04 14:05:02 [core.py:842]     tensor = torch.zeros(
[1;36m(EngineCore_DP0 pid=3852964)[0;0m ERROR 12-04 14:05:02 [core.py:842]              ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852964)[0;0m ERROR 12-04 14:05:02 [core.py:842] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.09 GiB. GPU 0 has a total capacity of 44.29 GiB of which 841.69 MiB is free. Including non-PyTorch memory, this process has 26.27 GiB memory in use. Process 3852961 has 17.20 GiB memory in use. Of the allocated memory 25.89 GiB is allocated by PyTorch, and 161.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=3852964)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=3852964)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=3852964)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=3852964)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=3852964)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=3852964)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=3852964)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=3852964)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=3852964)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=3852964)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3852964)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852964)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=3852964)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=3852964)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=3852964)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=3852964)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852964)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 247, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=3852964)[0;0m     self.model_executor.initialize_from_config(kv_cache_configs)
[1;36m(EngineCore_DP0 pid=3852964)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 115, in initialize_from_config
[1;36m(EngineCore_DP0 pid=3852964)[0;0m     self.collective_rpc("initialize_from_config", args=(kv_cache_configs,))
[1;36m(EngineCore_DP0 pid=3852964)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=3852964)[0;0m     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=3852964)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852964)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=3852964)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3852964)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852964)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 319, in initialize_from_config
[1;36m(EngineCore_DP0 pid=3852964)[0;0m     self.worker.initialize_from_config(kv_cache_config)  # type: ignore
[1;36m(EngineCore_DP0 pid=3852964)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852964)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 404, in initialize_from_config
[1;36m(EngineCore_DP0 pid=3852964)[0;0m     self.model_runner.initialize_kv_cache(kv_cache_config)
[1;36m(EngineCore_DP0 pid=3852964)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 5007, in initialize_kv_cache
[1;36m(EngineCore_DP0 pid=3852964)[0;0m     kv_caches = self.initialize_kv_cache_tensors(
[1;36m(EngineCore_DP0 pid=3852964)[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852964)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4933, in initialize_kv_cache_tensors
[1;36m(EngineCore_DP0 pid=3852964)[0;0m     kv_cache_raw_tensors = self._allocate_kv_cache_tensors(kv_cache_config)
[1;36m(EngineCore_DP0 pid=3852964)[0;0m                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852964)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4718, in _allocate_kv_cache_tensors
[1;36m(EngineCore_DP0 pid=3852964)[0;0m     tensor = torch.zeros(
[1;36m(EngineCore_DP0 pid=3852964)[0;0m              ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3852964)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.09 GiB. GPU 0 has a total capacity of 44.29 GiB of which 841.69 MiB is free. Including non-PyTorch memory, this process has 26.27 GiB memory in use. Process 3852961 has 17.20 GiB memory in use. Of the allocated memory 25.89 GiB is allocated by PyTorch, and 161.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W1204 14:05:03.349680949 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank0]:[W1204 14:05:03.349671413 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: WeiboAI/VibeThinker-1.5B (backend=vllm)
[ModelCache] Using max_model_len=131072 for WeiboAI/VibeThinker-1.5B
[ModelCache] Detected 1 GPU(s), 44.3GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 14:05:25 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'WeiboAI/VibeThinker-1.5B'}
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: WeiboAI/VibeThinker-1.5B (backend=vllm)
[ModelCache] Using max_model_len=131072 for WeiboAI/VibeThinker-1.5B
[ModelCache] Detected 1 GPU(s), 44.3GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 14:05:25 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'WeiboAI/VibeThinker-1.5B'}
INFO 12-04 14:05:25 [model.py:631] Resolved architecture: Qwen2ForCausalLM
WARNING 12-04 14:05:25 [model.py:1921] Your device 'Quadro RTX 8000' (with compute capability 7.5) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.
WARNING 12-04 14:05:25 [model.py:1971] Casting torch.bfloat16 to torch.float16.
INFO 12-04 14:05:25 [model.py:1745] Using max model len 131072
INFO 12-04 14:05:25 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
INFO 12-04 14:05:25 [model.py:631] Resolved architecture: Qwen2ForCausalLM
WARNING 12-04 14:05:25 [model.py:1921] Your device 'Quadro RTX 8000' (with compute capability 7.5) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.
WARNING 12-04 14:05:25 [model.py:1971] Casting torch.bfloat16 to torch.float16.
INFO 12-04 14:05:25 [model.py:1745] Using max model len 131072
INFO 12-04 14:05:25 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=3853127)[0;0m INFO 12-04 14:05:41 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='WeiboAI/VibeThinker-1.5B', speculative_config=None, tokenizer='WeiboAI/VibeThinker-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=WeiboAI/VibeThinker-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=3853128)[0;0m INFO 12-04 14:05:41 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='WeiboAI/VibeThinker-1.5B', speculative_config=None, tokenizer='WeiboAI/VibeThinker-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=WeiboAI/VibeThinker-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=3853127)[0;0m ERROR 12-04 14:05:42 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8
[1;36m(EngineCore_DP0 pid=3853128)[0;0m ERROR 12-04 14:05:42 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8
[1;36m(EngineCore_DP0 pid=3853128)[0;0m INFO 12-04 14:05:42 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.7:41431 backend=nccl
[1;36m(EngineCore_DP0 pid=3853127)[0;0m INFO 12-04 14:05:42 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.7:43423 backend=nccl
[W1204 14:05:42.811089300 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-23-2.rc.tch.harvard.edu]:41431 (errno: 97 - Address family not supported by protocol).
[W1204 14:05:42.814132535 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-23-2.rc.tch.harvard.edu]:43423 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=3853127)[0;0m INFO 12-04 14:05:42 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=3853128)[0;0m INFO 12-04 14:05:42 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=3853127)[0;0m INFO 12-04 14:05:43 [gpu_model_runner.py:3259] Starting to load model WeiboAI/VibeThinker-1.5B...
[1;36m(EngineCore_DP0 pid=3853128)[0;0m INFO 12-04 14:05:43 [gpu_model_runner.py:3259] Starting to load model WeiboAI/VibeThinker-1.5B...
[1;36m(EngineCore_DP0 pid=3853127)[0;0m INFO 12-04 14:05:44 [cuda.py:418] Valid backends: ['FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=3853127)[0;0m INFO 12-04 14:05:44 [cuda.py:427] Using FLASHINFER backend.
[1;36m(EngineCore_DP0 pid=3853128)[0;0m INFO 12-04 14:05:44 [cuda.py:418] Valid backends: ['FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=3853128)[0;0m INFO 12-04 14:05:44 [cuda.py:427] Using FLASHINFER backend.
[1;36m(EngineCore_DP0 pid=3853128)[0;0m INFO 12-04 14:05:45 [weight_utils.py:481] No model.safetensors.index.json found in remote.
[1;36m(EngineCore_DP0 pid=3853128)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
[1;36m(EngineCore_DP0 pid=3853127)[0;0m INFO 12-04 14:05:45 [weight_utils.py:481] No model.safetensors.index.json found in remote.
[1;36m(EngineCore_DP0 pid=3853127)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
[1;36m(EngineCore_DP0 pid=3853128)[0;0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:04<00:00,  4.04s/it]
[1;36m(EngineCore_DP0 pid=3853128)[0;0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:04<00:00,  4.04s/it]
[1;36m(EngineCore_DP0 pid=3853128)[0;0m 
[1;36m(EngineCore_DP0 pid=3853128)[0;0m INFO 12-04 14:05:49 [default_loader.py:314] Loading weights took 4.18 seconds
[1;36m(EngineCore_DP0 pid=3853127)[0;0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:04<00:00,  4.59s/it]
[1;36m(EngineCore_DP0 pid=3853127)[0;0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:04<00:00,  4.59s/it]
[1;36m(EngineCore_DP0 pid=3853127)[0;0m 
[1;36m(EngineCore_DP0 pid=3853127)[0;0m INFO 12-04 14:05:50 [default_loader.py:314] Loading weights took 4.74 seconds
[1;36m(EngineCore_DP0 pid=3853128)[0;0m INFO 12-04 14:05:50 [gpu_model_runner.py:3338] Model loading took 2.9110 GiB memory and 5.348566 seconds
[1;36m(EngineCore_DP0 pid=3853127)[0;0m INFO 12-04 14:05:50 [gpu_model_runner.py:3338] Model loading took 2.9110 GiB memory and 6.070020 seconds
[1;36m(EngineCore_DP0 pid=3853127)[0;0m INFO 12-04 14:06:01 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/6847b7a26b/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=3853128)[0;0m INFO 12-04 14:06:01 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/6847b7a26b/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=3853128)[0;0m INFO 12-04 14:06:01 [backends.py:647] Dynamo bytecode transform time: 11.21 s
[1;36m(EngineCore_DP0 pid=3853127)[0;0m INFO 12-04 14:06:01 [backends.py:647] Dynamo bytecode transform time: 10.55 s
[1;36m(EngineCore_DP0 pid=3853127)[0;0m INFO 12-04 14:06:08 [backends.py:210] Directly load the compiled graph(s) for dynamic shape from the cache, took 6.124 s
[1;36m(EngineCore_DP0 pid=3853128)[0;0m INFO 12-04 14:06:08 [backends.py:210] Directly load the compiled graph(s) for dynamic shape from the cache, took 6.125 s
[1;36m(EngineCore_DP0 pid=3853127)[0;0m INFO 12-04 14:06:10 [monitor.py:34] torch.compile takes 16.67 s in total
[1;36m(EngineCore_DP0 pid=3853128)[0;0m INFO 12-04 14:06:10 [monitor.py:34] torch.compile takes 17.34 s in total
[1;36m(EngineCore_DP0 pid=3853128)[0;0m INFO 12-04 14:06:12 [gpu_worker.py:359] Available KV cache memory: 30.58 GiB
[1;36m(EngineCore_DP0 pid=3853127)[0;0m INFO 12-04 14:06:12 [gpu_worker.py:359] Available KV cache memory: 32.40 GiB
[1;36m(EngineCore_DP0 pid=3853128)[0;0m INFO 12-04 14:06:12 [kv_cache_utils.py:1229] GPU KV cache size: 1,145,296 tokens
[1;36m(EngineCore_DP0 pid=3853128)[0;0m INFO 12-04 14:06:12 [kv_cache_utils.py:1234] Maximum concurrency for 131,072 tokens per request: 8.74x
[1;36m(EngineCore_DP0 pid=3853128)[0;0m INFO 12-04 14:06:12 [kernel_warmup.py:65] Warming up FlashInfer attention.
[1;36m(EngineCore_DP0 pid=3853127)[0;0m INFO 12-04 14:06:12 [kv_cache_utils.py:1229] GPU KV cache size: 1,213,248 tokens
[1;36m(EngineCore_DP0 pid=3853127)[0;0m INFO 12-04 14:06:12 [kv_cache_utils.py:1234] Maximum concurrency for 131,072 tokens per request: 9.26x
[1;36m(EngineCore_DP0 pid=3853127)[0;0m ERROR 12-04 14:06:12 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=3853127)[0;0m ERROR 12-04 14:06:12 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=3853127)[0;0m ERROR 12-04 14:06:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=3853127)[0;0m ERROR 12-04 14:06:12 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3853127)[0;0m ERROR 12-04 14:06:12 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3853127)[0;0m ERROR 12-04 14:06:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=3853127)[0;0m ERROR 12-04 14:06:12 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=3853127)[0;0m ERROR 12-04 14:06:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=3853127)[0;0m ERROR 12-04 14:06:12 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=3853127)[0;0m ERROR 12-04 14:06:12 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3853127)[0;0m ERROR 12-04 14:06:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 247, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=3853127)[0;0m ERROR 12-04 14:06:12 [core.py:842]     self.model_executor.initialize_from_config(kv_cache_configs)
[1;36m(EngineCore_DP0 pid=3853127)[0;0m ERROR 12-04 14:06:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 115, in initialize_from_config
[1;36m(EngineCore_DP0 pid=3853127)[0;0m ERROR 12-04 14:06:12 [core.py:842]     self.collective_rpc("initialize_from_config", args=(kv_cache_configs,))
[1;36m(EngineCore_DP0 pid=3853127)[0;0m ERROR 12-04 14:06:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=3853127)[0;0m ERROR 12-04 14:06:12 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=3853127)[0;0m ERROR 12-04 14:06:12 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3853127)[0;0m ERROR 12-04 14:06:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=3853127)[0;0m ERROR 12-04 14:06:12 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3853127)[0;0m ERROR 12-04 14:06:12 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3853127)[0;0m ERROR 12-04 14:06:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 319, in initialize_from_config
[1;36m(EngineCore_DP0 pid=3853127)[0;0m ERROR 12-04 14:06:12 [core.py:842]     self.worker.initialize_from_config(kv_cache_config)  # type: ignore
[1;36m(EngineCore_DP0 pid=3853127)[0;0m ERROR 12-04 14:06:12 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3853127)[0;0m ERROR 12-04 14:06:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 404, in initialize_from_config
[1;36m(EngineCore_DP0 pid=3853127)[0;0m ERROR 12-04 14:06:12 [core.py:842]     self.model_runner.initialize_kv_cache(kv_cache_config)
[1;36m(EngineCore_DP0 pid=3853127)[0;0m ERROR 12-04 14:06:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 5007, in initialize_kv_cache
[1;36m(EngineCore_DP0 pid=3853127)[0;0m ERROR 12-04 14:06:12 [core.py:842]     kv_caches = self.initialize_kv_cache_tensors(
[1;36m(EngineCore_DP0 pid=3853127)[0;0m ERROR 12-04 14:06:12 [core.py:842]                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3853127)[0;0m ERROR 12-04 14:06:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4933, in initialize_kv_cache_tensors
[1;36m(EngineCore_DP0 pid=3853127)[0;0m ERROR 12-04 14:06:12 [core.py:842]     kv_cache_raw_tensors = self._allocate_kv_cache_tensors(kv_cache_config)
[1;36m(EngineCore_DP0 pid=3853127)[0;0m ERROR 12-04 14:06:12 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3853127)[0;0m ERROR 12-04 14:06:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4718, in _allocate_kv_cache_tensors
[1;36m(EngineCore_DP0 pid=3853127)[0;0m ERROR 12-04 14:06:12 [core.py:842]     tensor = torch.zeros(
[1;36m(EngineCore_DP0 pid=3853127)[0;0m ERROR 12-04 14:06:12 [core.py:842]              ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3853127)[0;0m ERROR 12-04 14:06:12 [core.py:842] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.16 GiB. GPU 0 has a total capacity of 44.29 GiB of which 117.69 MiB is free. Including non-PyTorch memory, this process has 10.25 GiB memory in use. Process 3853128 has 33.93 GiB memory in use. Of the allocated memory 9.90 GiB is allocated by PyTorch, and 134.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=3853127)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=3853127)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=3853127)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=3853127)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=3853127)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=3853127)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=3853127)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=3853127)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=3853127)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=3853127)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3853127)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3853127)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=3853127)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=3853127)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=3853127)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=3853127)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3853127)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 247, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=3853127)[0;0m     self.model_executor.initialize_from_config(kv_cache_configs)
[1;36m(EngineCore_DP0 pid=3853127)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 115, in initialize_from_config
[1;36m(EngineCore_DP0 pid=3853127)[0;0m     self.collective_rpc("initialize_from_config", args=(kv_cache_configs,))
[1;36m(EngineCore_DP0 pid=3853127)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=3853127)[0;0m     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=3853127)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3853127)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=3853127)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3853127)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3853127)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 319, in initialize_from_config
[1;36m(EngineCore_DP0 pid=3853127)[0;0m     self.worker.initialize_from_config(kv_cache_config)  # type: ignore
[1;36m(EngineCore_DP0 pid=3853127)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3853127)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 404, in initialize_from_config
[1;36m(EngineCore_DP0 pid=3853127)[0;0m     self.model_runner.initialize_kv_cache(kv_cache_config)
[1;36m(EngineCore_DP0 pid=3853127)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 5007, in initialize_kv_cache
[1;36m(EngineCore_DP0 pid=3853127)[0;0m     kv_caches = self.initialize_kv_cache_tensors(
[1;36m(EngineCore_DP0 pid=3853127)[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3853127)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4933, in initialize_kv_cache_tensors
[1;36m(EngineCore_DP0 pid=3853127)[0;0m     kv_cache_raw_tensors = self._allocate_kv_cache_tensors(kv_cache_config)
[1;36m(EngineCore_DP0 pid=3853127)[0;0m                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3853127)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4718, in _allocate_kv_cache_tensors
[1;36m(EngineCore_DP0 pid=3853127)[0;0m     tensor = torch.zeros(
[1;36m(EngineCore_DP0 pid=3853127)[0;0m              ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3853127)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.16 GiB. GPU 0 has a total capacity of 44.29 GiB of which 117.69 MiB is free. Including non-PyTorch memory, this process has 10.25 GiB memory in use. Process 3853128 has 33.93 GiB memory in use. Of the allocated memory 9.90 GiB is allocated by PyTorch, and 134.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=3853128)[0;0m ERROR 12-04 14:06:12 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=3853128)[0;0m ERROR 12-04 14:06:12 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=3853128)[0;0m ERROR 12-04 14:06:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=3853128)[0;0m ERROR 12-04 14:06:12 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3853128)[0;0m ERROR 12-04 14:06:12 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3853128)[0;0m ERROR 12-04 14:06:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=3853128)[0;0m ERROR 12-04 14:06:12 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=3853128)[0;0m ERROR 12-04 14:06:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=3853128)[0;0m ERROR 12-04 14:06:12 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=3853128)[0;0m ERROR 12-04 14:06:12 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3853128)[0;0m ERROR 12-04 14:06:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 247, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=3853128)[0;0m ERROR 12-04 14:06:12 [core.py:842]     self.model_executor.initialize_from_config(kv_cache_configs)
[1;36m(EngineCore_DP0 pid=3853128)[0;0m ERROR 12-04 14:06:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 116, in initialize_from_config
[1;36m(EngineCore_DP0 pid=3853128)[0;0m ERROR 12-04 14:06:12 [core.py:842]     self.collective_rpc("compile_or_warm_up_model")
[1;36m(EngineCore_DP0 pid=3853128)[0;0m ERROR 12-04 14:06:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=3853128)[0;0m ERROR 12-04 14:06:12 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=3853128)[0;0m ERROR 12-04 14:06:12 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3853128)[0;0m ERROR 12-04 14:06:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=3853128)[0;0m ERROR 12-04 14:06:12 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3853128)[0;0m ERROR 12-04 14:06:12 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3853128)[0;0m ERROR 12-04 14:06:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 425, in compile_or_warm_up_model
[1;36m(EngineCore_DP0 pid=3853128)[0;0m ERROR 12-04 14:06:12 [core.py:842]     kernel_warmup(self)
[1;36m(EngineCore_DP0 pid=3853128)[0;0m ERROR 12-04 14:06:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/warmup/kernel_warmup.py", line 68, in kernel_warmup
[1;36m(EngineCore_DP0 pid=3853128)[0;0m ERROR 12-04 14:06:12 [core.py:842]     worker.model_runner._dummy_run(
[1;36m(EngineCore_DP0 pid=3853128)[0;0m ERROR 12-04 14:06:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=3853128)[0;0m ERROR 12-04 14:06:12 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3853128)[0;0m ERROR 12-04 14:06:12 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3853128)[0;0m ERROR 12-04 14:06:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3756, in _dummy_run
[1;36m(EngineCore_DP0 pid=3853128)[0;0m ERROR 12-04 14:06:12 [core.py:842]     attn_metadata, _ = self._build_attention_metadata(
[1;36m(EngineCore_DP0 pid=3853128)[0;0m ERROR 12-04 14:06:12 [core.py:842]                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3853128)[0;0m ERROR 12-04 14:06:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1576, in _build_attention_metadata
[1;36m(EngineCore_DP0 pid=3853128)[0;0m ERROR 12-04 14:06:12 [core.py:842]     attn_metadata_i = builder.build_for_cudagraph_capture(
[1;36m(EngineCore_DP0 pid=3853128)[0;0m ERROR 12-04 14:06:12 [core.py:842]                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3853128)[0;0m ERROR 12-04 14:06:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/attention/backends/utils.py", line 332, in build_for_cudagraph_capture
[1;36m(EngineCore_DP0 pid=3853128)[0;0m ERROR 12-04 14:06:12 [core.py:842]     return self.build(
[1;36m(EngineCore_DP0 pid=3853128)[0;0m ERROR 12-04 14:06:12 [core.py:842]            ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3853128)[0;0m ERROR 12-04 14:06:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/attention/backends/flashinfer.py", line 844, in build
[1;36m(EngineCore_DP0 pid=3853128)[0;0m ERROR 12-04 14:06:12 [core.py:842]     attn_metadata.prefill_wrapper = self._get_prefill_wrapper()
[1;36m(EngineCore_DP0 pid=3853128)[0;0m ERROR 12-04 14:06:12 [core.py:842]                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3853128)[0;0m ERROR 12-04 14:06:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/attention/backends/flashinfer.py", line 598, in _get_prefill_wrapper
[1;36m(EngineCore_DP0 pid=3853128)[0;0m ERROR 12-04 14:06:12 [core.py:842]     self._get_workspace_buffer(), get_kv_cache_layout()
[1;36m(EngineCore_DP0 pid=3853128)[0;0m ERROR 12-04 14:06:12 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3853128)[0;0m ERROR 12-04 14:06:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/attention/backends/flashinfer.py", line 583, in _get_workspace_buffer
[1;36m(EngineCore_DP0 pid=3853128)[0;0m ERROR 12-04 14:06:12 [core.py:842]     self._workspace_buffer = torch.zeros(
[1;36m(EngineCore_DP0 pid=3853128)[0;0m ERROR 12-04 14:06:12 [core.py:842]                              ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3853128)[0;0m ERROR 12-04 14:06:12 [core.py:842] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 394.00 MiB. GPU 0 has a total capacity of 44.29 GiB of which 117.69 MiB is free. Process 3853127 has 10.25 GiB memory in use. Including non-PyTorch memory, this process has 33.93 GiB memory in use. Of the allocated memory 33.54 GiB is allocated by PyTorch, and 171.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=3853128)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=3853128)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=3853128)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=3853128)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=3853128)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=3853128)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=3853128)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=3853128)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=3853128)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=3853128)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3853128)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3853128)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=3853128)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=3853128)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=3853128)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=3853128)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3853128)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 247, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=3853128)[0;0m     self.model_executor.initialize_from_config(kv_cache_configs)
[1;36m(EngineCore_DP0 pid=3853128)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 116, in initialize_from_config
[1;36m(EngineCore_DP0 pid=3853128)[0;0m     self.collective_rpc("compile_or_warm_up_model")
[1;36m(EngineCore_DP0 pid=3853128)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=3853128)[0;0m     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=3853128)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3853128)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=3853128)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3853128)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3853128)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 425, in compile_or_warm_up_model
[1;36m(EngineCore_DP0 pid=3853128)[0;0m     kernel_warmup(self)
[1;36m(EngineCore_DP0 pid=3853128)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/warmup/kernel_warmup.py", line 68, in kernel_warmup
[1;36m(EngineCore_DP0 pid=3853128)[0;0m     worker.model_runner._dummy_run(
[1;36m(EngineCore_DP0 pid=3853128)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=3853128)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3853128)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3853128)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3756, in _dummy_run
[1;36m(EngineCore_DP0 pid=3853128)[0;0m     attn_metadata, _ = self._build_attention_metadata(
[1;36m(EngineCore_DP0 pid=3853128)[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3853128)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1576, in _build_attention_metadata
[1;36m(EngineCore_DP0 pid=3853128)[0;0m     attn_metadata_i = builder.build_for_cudagraph_capture(
[1;36m(EngineCore_DP0 pid=3853128)[0;0m                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3853128)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/attention/backends/utils.py", line 332, in build_for_cudagraph_capture
[1;36m(EngineCore_DP0 pid=3853128)[0;0m     return self.build(
[1;36m(EngineCore_DP0 pid=3853128)[0;0m            ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3853128)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/attention/backends/flashinfer.py", line 844, in build
[1;36m(EngineCore_DP0 pid=3853128)[0;0m     attn_metadata.prefill_wrapper = self._get_prefill_wrapper()
[1;36m(EngineCore_DP0 pid=3853128)[0;0m                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3853128)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/attention/backends/flashinfer.py", line 598, in _get_prefill_wrapper
[1;36m(EngineCore_DP0 pid=3853128)[0;0m     self._get_workspace_buffer(), get_kv_cache_layout()
[1;36m(EngineCore_DP0 pid=3853128)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3853128)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/attention/backends/flashinfer.py", line 583, in _get_workspace_buffer
[1;36m(EngineCore_DP0 pid=3853128)[0;0m     self._workspace_buffer = torch.zeros(
[1;36m(EngineCore_DP0 pid=3853128)[0;0m                              ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3853128)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 394.00 MiB. GPU 0 has a total capacity of 44.29 GiB of which 117.69 MiB is free. Process 3853127 has 10.25 GiB memory in use. Including non-PyTorch memory, this process has 33.93 GiB memory in use. Of the allocated memory 33.54 GiB is allocated by PyTorch, and 171.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W1204 14:06:13.790433562 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank0]:[W1204 14:06:13.843809696 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: WeiboAI/VibeThinker-1.5B (backend=vllm)
[ModelCache] Using max_model_len=131072 for WeiboAI/VibeThinker-1.5B
[ModelCache] Detected 1 GPU(s), 44.3GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 14:06:35 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'WeiboAI/VibeThinker-1.5B'}
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: WeiboAI/VibeThinker-1.5B (backend=vllm)
[ModelCache] Using max_model_len=131072 for WeiboAI/VibeThinker-1.5B
[ModelCache] Detected 1 GPU(s), 44.3GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 14:06:35 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'WeiboAI/VibeThinker-1.5B'}
INFO 12-04 14:06:36 [model.py:631] Resolved architecture: Qwen2ForCausalLM
WARNING 12-04 14:06:36 [model.py:1921] Your device 'Quadro RTX 8000' (with compute capability 7.5) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.
WARNING 12-04 14:06:36 [model.py:1971] Casting torch.bfloat16 to torch.float16.
INFO 12-04 14:06:36 [model.py:1745] Using max model len 131072
INFO 12-04 14:06:36 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
INFO 12-04 14:06:36 [model.py:631] Resolved architecture: Qwen2ForCausalLM
WARNING 12-04 14:06:36 [model.py:1921] Your device 'Quadro RTX 8000' (with compute capability 7.5) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.
WARNING 12-04 14:06:36 [model.py:1971] Casting torch.bfloat16 to torch.float16.
INFO 12-04 14:06:36 [model.py:1745] Using max model len 131072
INFO 12-04 14:06:36 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=3853315)[0;0m INFO 12-04 14:06:51 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='WeiboAI/VibeThinker-1.5B', speculative_config=None, tokenizer='WeiboAI/VibeThinker-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=WeiboAI/VibeThinker-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=3853318)[0;0m INFO 12-04 14:06:51 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='WeiboAI/VibeThinker-1.5B', speculative_config=None, tokenizer='WeiboAI/VibeThinker-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=WeiboAI/VibeThinker-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=3853315)[0;0m ERROR 12-04 14:06:52 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8
[1;36m(EngineCore_DP0 pid=3853318)[0;0m ERROR 12-04 14:06:52 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8
[1;36m(EngineCore_DP0 pid=3853315)[0;0m INFO 12-04 14:06:52 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.7:44805 backend=nccl
[1;36m(EngineCore_DP0 pid=3853318)[0;0m INFO 12-04 14:06:52 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.7:42763 backend=nccl
[W1204 14:06:52.841815621 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-23-2.rc.tch.harvard.edu]:44805 (errno: 97 - Address family not supported by protocol).
[W1204 14:06:52.845314276 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-23-2.rc.tch.harvard.edu]:42763 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=3853315)[0;0m INFO 12-04 14:06:53 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=3853318)[0;0m INFO 12-04 14:06:53 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=3853315)[0;0m INFO 12-04 14:06:53 [gpu_model_runner.py:3259] Starting to load model WeiboAI/VibeThinker-1.5B...
[1;36m(EngineCore_DP0 pid=3853318)[0;0m INFO 12-04 14:06:53 [gpu_model_runner.py:3259] Starting to load model WeiboAI/VibeThinker-1.5B...
[1;36m(EngineCore_DP0 pid=3853315)[0;0m INFO 12-04 14:06:54 [cuda.py:418] Valid backends: ['FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=3853315)[0;0m INFO 12-04 14:06:54 [cuda.py:427] Using FLASHINFER backend.
[1;36m(EngineCore_DP0 pid=3853318)[0;0m INFO 12-04 14:06:54 [cuda.py:418] Valid backends: ['FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=3853318)[0;0m INFO 12-04 14:06:54 [cuda.py:427] Using FLASHINFER backend.
[1;36m(EngineCore_DP0 pid=3853318)[0;0m INFO 12-04 14:06:55 [weight_utils.py:481] No model.safetensors.index.json found in remote.
[1;36m(EngineCore_DP0 pid=3853318)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
[1;36m(EngineCore_DP0 pid=3853315)[0;0m INFO 12-04 14:06:55 [weight_utils.py:481] No model.safetensors.index.json found in remote.
[1;36m(EngineCore_DP0 pid=3853315)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
[1;36m(EngineCore_DP0 pid=3853318)[0;0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.99s/it]
[1;36m(EngineCore_DP0 pid=3853318)[0;0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.99s/it]
[1;36m(EngineCore_DP0 pid=3853318)[0;0m 
[1;36m(EngineCore_DP0 pid=3853318)[0;0m INFO 12-04 14:06:59 [default_loader.py:314] Loading weights took 4.13 seconds
[1;36m(EngineCore_DP0 pid=3853315)[0;0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:04<00:00,  4.03s/it]
[1;36m(EngineCore_DP0 pid=3853315)[0;0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:04<00:00,  4.03s/it]
[1;36m(EngineCore_DP0 pid=3853315)[0;0m 
[1;36m(EngineCore_DP0 pid=3853315)[0;0m INFO 12-04 14:06:59 [default_loader.py:314] Loading weights took 4.18 seconds
[1;36m(EngineCore_DP0 pid=3853318)[0;0m INFO 12-04 14:07:00 [gpu_model_runner.py:3338] Model loading took 2.9110 GiB memory and 5.417458 seconds
[1;36m(EngineCore_DP0 pid=3853315)[0;0m INFO 12-04 14:07:00 [gpu_model_runner.py:3338] Model loading took 2.9110 GiB memory and 5.649703 seconds
[1;36m(EngineCore_DP0 pid=3853315)[0;0m INFO 12-04 14:07:11 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/6847b7a26b/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=3853318)[0;0m INFO 12-04 14:07:11 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/6847b7a26b/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=3853315)[0;0m INFO 12-04 14:07:11 [backends.py:647] Dynamo bytecode transform time: 11.11 s
[1;36m(EngineCore_DP0 pid=3853318)[0;0m INFO 12-04 14:07:11 [backends.py:647] Dynamo bytecode transform time: 11.33 s
[1;36m(EngineCore_DP0 pid=3853315)[0;0m INFO 12-04 14:07:18 [backends.py:210] Directly load the compiled graph(s) for dynamic shape from the cache, took 6.067 s
[1;36m(EngineCore_DP0 pid=3853318)[0;0m INFO 12-04 14:07:18 [backends.py:210] Directly load the compiled graph(s) for dynamic shape from the cache, took 6.071 s
[1;36m(EngineCore_DP0 pid=3853318)[0;0m INFO 12-04 14:07:20 [monitor.py:34] torch.compile takes 17.41 s in total
[1;36m(EngineCore_DP0 pid=3853315)[0;0m INFO 12-04 14:07:20 [monitor.py:34] torch.compile takes 17.18 s in total
[1;36m(EngineCore_DP0 pid=3853318)[0;0m INFO 12-04 14:07:22 [gpu_worker.py:359] Available KV cache memory: 30.96 GiB
[1;36m(EngineCore_DP0 pid=3853315)[0;0m INFO 12-04 14:07:22 [gpu_worker.py:359] Available KV cache memory: 32.40 GiB
[1;36m(EngineCore_DP0 pid=3853318)[0;0m INFO 12-04 14:07:22 [kv_cache_utils.py:1229] GPU KV cache size: 1,159,264 tokens
[1;36m(EngineCore_DP0 pid=3853318)[0;0m INFO 12-04 14:07:22 [kv_cache_utils.py:1234] Maximum concurrency for 131,072 tokens per request: 8.84x
[1;36m(EngineCore_DP0 pid=3853315)[0;0m INFO 12-04 14:07:22 [kv_cache_utils.py:1229] GPU KV cache size: 1,213,184 tokens
[1;36m(EngineCore_DP0 pid=3853315)[0;0m INFO 12-04 14:07:22 [kv_cache_utils.py:1234] Maximum concurrency for 131,072 tokens per request: 9.26x
[1;36m(EngineCore_DP0 pid=3853318)[0;0m INFO 12-04 14:07:22 [kernel_warmup.py:65] Warming up FlashInfer attention.
[1;36m(EngineCore_DP0 pid=3853315)[0;0m ERROR 12-04 14:07:22 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=3853315)[0;0m ERROR 12-04 14:07:22 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=3853315)[0;0m ERROR 12-04 14:07:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=3853315)[0;0m ERROR 12-04 14:07:22 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3853315)[0;0m ERROR 12-04 14:07:22 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3853315)[0;0m ERROR 12-04 14:07:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=3853315)[0;0m ERROR 12-04 14:07:22 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=3853315)[0;0m ERROR 12-04 14:07:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=3853315)[0;0m ERROR 12-04 14:07:22 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=3853315)[0;0m ERROR 12-04 14:07:22 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3853315)[0;0m ERROR 12-04 14:07:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 247, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=3853315)[0;0m ERROR 12-04 14:07:22 [core.py:842]     self.model_executor.initialize_from_config(kv_cache_configs)
[1;36m(EngineCore_DP0 pid=3853315)[0;0m ERROR 12-04 14:07:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 115, in initialize_from_config
[1;36m(EngineCore_DP0 pid=3853315)[0;0m ERROR 12-04 14:07:22 [core.py:842]     self.collective_rpc("initialize_from_config", args=(kv_cache_configs,))
[1;36m(EngineCore_DP0 pid=3853315)[0;0m ERROR 12-04 14:07:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=3853315)[0;0m ERROR 12-04 14:07:22 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=3853315)[0;0m ERROR 12-04 14:07:22 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3853315)[0;0m ERROR 12-04 14:07:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=3853315)[0;0m ERROR 12-04 14:07:22 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3853315)[0;0m ERROR 12-04 14:07:22 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3853315)[0;0m ERROR 12-04 14:07:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 319, in initialize_from_config
[1;36m(EngineCore_DP0 pid=3853315)[0;0m ERROR 12-04 14:07:22 [core.py:842]     self.worker.initialize_from_config(kv_cache_config)  # type: ignore
[1;36m(EngineCore_DP0 pid=3853315)[0;0m ERROR 12-04 14:07:22 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3853315)[0;0m ERROR 12-04 14:07:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 404, in initialize_from_config
[1;36m(EngineCore_DP0 pid=3853315)[0;0m ERROR 12-04 14:07:22 [core.py:842]     self.model_runner.initialize_kv_cache(kv_cache_config)
[1;36m(EngineCore_DP0 pid=3853315)[0;0m ERROR 12-04 14:07:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 5007, in initialize_kv_cache
[1;36m(EngineCore_DP0 pid=3853315)[0;0m ERROR 12-04 14:07:22 [core.py:842]     kv_caches = self.initialize_kv_cache_tensors(
[1;36m(EngineCore_DP0 pid=3853315)[0;0m ERROR 12-04 14:07:22 [core.py:842]                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3853315)[0;0m ERROR 12-04 14:07:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4933, in initialize_kv_cache_tensors
[1;36m(EngineCore_DP0 pid=3853315)[0;0m ERROR 12-04 14:07:22 [core.py:842]     kv_cache_raw_tensors = self._allocate_kv_cache_tensors(kv_cache_config)
[1;36m(EngineCore_DP0 pid=3853315)[0;0m ERROR 12-04 14:07:22 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3853315)[0;0m ERROR 12-04 14:07:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4718, in _allocate_kv_cache_tensors
[1;36m(EngineCore_DP0 pid=3853315)[0;0m ERROR 12-04 14:07:22 [core.py:842]     tensor = torch.zeros(
[1;36m(EngineCore_DP0 pid=3853315)[0;0m ERROR 12-04 14:07:22 [core.py:842]              ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3853315)[0;0m ERROR 12-04 14:07:22 [core.py:842] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.16 GiB. GPU 0 has a total capacity of 44.29 GiB of which 911.69 MiB is free. Process 3853318 has 34.31 GiB memory in use. Including non-PyTorch memory, this process has 9.09 GiB memory in use. Of the allocated memory 8.74 GiB is allocated by PyTorch, and 134.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=3853315)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=3853315)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=3853315)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=3853315)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=3853315)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=3853315)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=3853315)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=3853315)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=3853315)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=3853315)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3853315)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3853315)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=3853315)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=3853315)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=3853315)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=3853315)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3853315)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 247, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=3853315)[0;0m     self.model_executor.initialize_from_config(kv_cache_configs)
[1;36m(EngineCore_DP0 pid=3853315)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 115, in initialize_from_config
[1;36m(EngineCore_DP0 pid=3853315)[0;0m     self.collective_rpc("initialize_from_config", args=(kv_cache_configs,))
[1;36m(EngineCore_DP0 pid=3853315)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=3853315)[0;0m     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=3853315)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3853315)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=3853315)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3853315)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3853315)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 319, in initialize_from_config
[1;36m(EngineCore_DP0 pid=3853315)[0;0m     self.worker.initialize_from_config(kv_cache_config)  # type: ignore
[1;36m(EngineCore_DP0 pid=3853315)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3853315)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 404, in initialize_from_config
[1;36m(EngineCore_DP0 pid=3853315)[0;0m     self.model_runner.initialize_kv_cache(kv_cache_config)
[1;36m(EngineCore_DP0 pid=3853315)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 5007, in initialize_kv_cache
[1;36m(EngineCore_DP0 pid=3853315)[0;0m     kv_caches = self.initialize_kv_cache_tensors(
[1;36m(EngineCore_DP0 pid=3853315)[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3853315)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4933, in initialize_kv_cache_tensors
[1;36m(EngineCore_DP0 pid=3853315)[0;0m     kv_cache_raw_tensors = self._allocate_kv_cache_tensors(kv_cache_config)
[1;36m(EngineCore_DP0 pid=3853315)[0;0m                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3853315)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4718, in _allocate_kv_cache_tensors
[1;36m(EngineCore_DP0 pid=3853315)[0;0m     tensor = torch.zeros(
[1;36m(EngineCore_DP0 pid=3853315)[0;0m              ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3853315)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.16 GiB. GPU 0 has a total capacity of 44.29 GiB of which 911.69 MiB is free. Process 3853318 has 34.31 GiB memory in use. Including non-PyTorch memory, this process has 9.09 GiB memory in use. Of the allocated memory 8.74 GiB is allocated by PyTorch, and 134.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W1204 14:07:23.818049523 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[1;36m(EngineCore_DP0 pid=3853318)[0;0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/51 [00:00<?, ?it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   4%|▍         | 2/51 [00:00<00:02, 19.19it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|▊         | 4/51 [00:00<00:02, 19.20it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  12%|█▏        | 6/51 [00:00<00:02, 18.41it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|█▌        | 8/51 [00:00<00:02, 18.66it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|█▉        | 10/51 [00:00<00:02, 18.72it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  24%|██▎       | 12/51 [00:00<00:02, 18.64it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|██▋       | 14/51 [00:00<00:01, 18.65it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███▏      | 16/51 [00:00<00:01, 18.82it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  35%|███▌      | 18/51 [00:00<00:01, 18.71it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  39%|███▉      | 20/51 [00:01<00:01, 18.46it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 22/51 [00:01<00:01, 18.61it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 24/51 [00:01<00:01, 18.68it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|█████     | 26/51 [00:01<00:01, 18.83it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▍    | 28/51 [00:01<00:01, 18.70it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  59%|█████▉    | 30/51 [00:01<00:01, 18.90it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 32/51 [00:01<00:01, 18.37it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 34/51 [00:01<00:01, 16.08it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 37/51 [00:02<00:00, 18.19it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  78%|███████▊  | 40/51 [00:02<00:00, 19.79it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  84%|████████▍ | 43/51 [00:02<00:00, 20.94it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  90%|█████████ | 46/51 [00:02<00:00, 21.81it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  96%|█████████▌| 49/51 [00:02<00:00, 22.57it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:02<00:00, 18.75it/s]
[1;36m(EngineCore_DP0 pid=3853318)[0;0m Capturing CUDA graphs (decode, FULL):   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graphs (decode, FULL):   9%|▊         | 3/35 [00:00<00:01, 20.78it/s]Capturing CUDA graphs (decode, FULL):  17%|█▋        | 6/35 [00:00<00:01, 20.70it/s]Capturing CUDA graphs (decode, FULL):  26%|██▌       | 9/35 [00:00<00:01, 20.86it/s]Capturing CUDA graphs (decode, FULL):  34%|███▍      | 12/35 [00:00<00:01, 20.60it/s]Capturing CUDA graphs (decode, FULL):  43%|████▎     | 15/35 [00:00<00:00, 20.57it/s]Capturing CUDA graphs (decode, FULL):  51%|█████▏    | 18/35 [00:00<00:00, 20.72it/s]Capturing CUDA graphs (decode, FULL):  60%|██████    | 21/35 [00:01<00:00, 20.80it/s]Capturing CUDA graphs (decode, FULL):  69%|██████▊   | 24/35 [00:01<00:00, 20.96it/s]Capturing CUDA graphs (decode, FULL):  77%|███████▋  | 27/35 [00:01<00:00, 21.01it/s]Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 30/35 [00:01<00:00, 21.09it/s]Capturing CUDA graphs (decode, FULL):  94%|█████████▍| 33/35 [00:01<00:00, 21.28it/s]Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:01<00:00, 21.01it/s]
[1;36m(EngineCore_DP0 pid=3853318)[0;0m INFO 12-04 14:07:28 [gpu_model_runner.py:4244] Graph capturing finished in 5 secs, took -8.29 GiB
[1;36m(EngineCore_DP0 pid=3853318)[0;0m INFO 12-04 14:07:28 [core.py:250] init engine (profile, create kv cache, warmup model) took 28.44 seconds
INFO 12-04 14:07:30 [llm.py:352] Supported tasks: ['generate']
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 252.56it/s]
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: WeiboAI/VibeThinker-1.5B (backend=vllm)
[ModelCache] Using max_model_len=131072 for WeiboAI/VibeThinker-1.5B
[ModelCache] Detected 1 GPU(s), 44.3GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 14:07:45 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'WeiboAI/VibeThinker-1.5B'}
INFO 12-04 14:07:46 [model.py:631] Resolved architecture: Qwen2ForCausalLM
WARNING 12-04 14:07:46 [model.py:1921] Your device 'Quadro RTX 8000' (with compute capability 7.5) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.
WARNING 12-04 14:07:46 [model.py:1971] Casting torch.bfloat16 to torch.float16.
INFO 12-04 14:07:46 [model.py:1745] Using max model len 131072
INFO 12-04 14:07:46 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:30<00:00, 30.71s/it, est. speed input: 4.62 toks/s, output: 103.70 toks/s]Processed prompts: 100%|██████████| 1/1 [00:30<00:00, 30.71s/it, est. speed input: 4.62 toks/s, output: 103.70 toks/s]Processed prompts: 100%|██████████| 1/1 [00:30<00:00, 30.71s/it, est. speed input: 4.62 toks/s, output: 103.70 toks/s]
Agent 1 response: Jared starts with a typing speed of 47 WPM. After some lessons, his speed increases to 52 WPM. He th...

--- Problem 1/20, Round 1, Agent 2/7 ---
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 647.67it/s]
[1;36m(EngineCore_DP0 pid=3853694)[0;0m INFO 12-04 14:08:03 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='WeiboAI/VibeThinker-1.5B', speculative_config=None, tokenizer='WeiboAI/VibeThinker-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=WeiboAI/VibeThinker-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=3853694)[0;0m ERROR 12-04 14:08:04 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8
[1;36m(EngineCore_DP0 pid=3853694)[0;0m INFO 12-04 14:08:05 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.7:39085 backend=nccl
[W1204 14:08:05.914599709 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-23-2.rc.tch.harvard.edu]:39085 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=3853694)[0;0m INFO 12-04 14:08:05 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=3853694)[0;0m ERROR 12-04 14:08:05 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=3853694)[0;0m ERROR 12-04 14:08:05 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=3853694)[0;0m ERROR 12-04 14:08:05 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=3853694)[0;0m ERROR 12-04 14:08:05 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3853694)[0;0m ERROR 12-04 14:08:05 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3853694)[0;0m ERROR 12-04 14:08:05 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=3853694)[0;0m ERROR 12-04 14:08:05 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=3853694)[0;0m ERROR 12-04 14:08:05 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=3853694)[0;0m ERROR 12-04 14:08:05 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=3853694)[0;0m ERROR 12-04 14:08:05 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3853694)[0;0m ERROR 12-04 14:08:05 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=3853694)[0;0m ERROR 12-04 14:08:05 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=3853694)[0;0m ERROR 12-04 14:08:05 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=3853694)[0;0m ERROR 12-04 14:08:05 [core.py:842]     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=3853694)[0;0m ERROR 12-04 14:08:05 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=3853694)[0;0m ERROR 12-04 14:08:05 [core.py:842]     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=3853694)[0;0m ERROR 12-04 14:08:05 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3853694)[0;0m ERROR 12-04 14:08:05 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=3853694)[0;0m ERROR 12-04 14:08:05 [core.py:842]     raise ValueError(
[1;36m(EngineCore_DP0 pid=3853694)[0;0m ERROR 12-04 14:08:05 [core.py:842] ValueError: Free memory on device (7.14/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[1;36m(EngineCore_DP0 pid=3853694)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=3853694)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=3853694)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=3853694)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=3853694)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=3853694)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=3853694)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=3853694)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=3853694)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=3853694)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3853694)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3853694)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=3853694)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=3853694)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=3853694)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=3853694)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3853694)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=3853694)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=3853694)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=3853694)[0;0m     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=3853694)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=3853694)[0;0m     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=3853694)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3853694)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=3853694)[0;0m     raise ValueError(
[1;36m(EngineCore_DP0 pid=3853694)[0;0m ValueError: Free memory on device (7.14/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W1204 14:08:06.197178146 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: WeiboAI/VibeThinker-1.5B (backend=vllm)
[ModelCache] Using max_model_len=131072 for WeiboAI/VibeThinker-1.5B
[ModelCache] Detected 1 GPU(s), 44.3GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 14:08:27 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'WeiboAI/VibeThinker-1.5B'}
INFO 12-04 14:08:27 [model.py:631] Resolved architecture: Qwen2ForCausalLM
WARNING 12-04 14:08:27 [model.py:1921] Your device 'Quadro RTX 8000' (with compute capability 7.5) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.
WARNING 12-04 14:08:27 [model.py:1971] Casting torch.bfloat16 to torch.float16.
INFO 12-04 14:08:27 [model.py:1745] Using max model len 131072
INFO 12-04 14:08:27 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:27<00:00, 27.69s/it, est. speed input: 5.24 toks/s, output: 104.94 toks/s]Processed prompts: 100%|██████████| 1/1 [00:27<00:00, 27.69s/it, est. speed input: 5.24 toks/s, output: 104.94 toks/s]Processed prompts: 100%|██████████| 1/1 [00:27<00:00, 27.69s/it, est. speed input: 5.24 toks/s, output: 104.94 toks/s]
Agent 2 response: Jared starts with an initial typing speed of 47 WPM. After some lessons, his speed increases to 52 W...

--- Problem 1/20, Round 1, Agent 3/7 ---
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 685.23it/s]
[1;36m(EngineCore_DP0 pid=3853752)[0;0m INFO 12-04 14:08:42 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='WeiboAI/VibeThinker-1.5B', speculative_config=None, tokenizer='WeiboAI/VibeThinker-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=WeiboAI/VibeThinker-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:13<00:00, 13.65s/it, est. speed input: 10.63 toks/s, output: 100.99 toks/s]Processed prompts: 100%|██████████| 1/1 [00:13<00:00, 13.65s/it, est. speed input: 10.63 toks/s, output: 100.99 toks/s]Processed prompts: 100%|██████████| 1/1 [00:13<00:00, 13.65s/it, est. speed input: 10.63 toks/s, output: 100.99 toks/s]
Agent 3 response: The three measurements of Jared's typing speed are 47 WPM, 52 WPM, and 57 WPM (calculated as 52 + 5)...

--- Problem 1/20, Round 1, Agent 4/7 ---
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 616.72it/s]
[1;36m(EngineCore_DP0 pid=3853752)[0;0m ERROR 12-04 14:08:43 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8
[1;36m(EngineCore_DP0 pid=3853752)[0;0m INFO 12-04 14:08:43 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.7:43201 backend=nccl
[W1204 14:08:43.853945941 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-23-2.rc.tch.harvard.edu]:43201 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=3853752)[0;0m INFO 12-04 14:08:44 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=3853752)[0;0m ERROR 12-04 14:08:44 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=3853752)[0;0m ERROR 12-04 14:08:44 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=3853752)[0;0m ERROR 12-04 14:08:44 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=3853752)[0;0m ERROR 12-04 14:08:44 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3853752)[0;0m ERROR 12-04 14:08:44 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3853752)[0;0m ERROR 12-04 14:08:44 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=3853752)[0;0m ERROR 12-04 14:08:44 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=3853752)[0;0m ERROR 12-04 14:08:44 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=3853752)[0;0m ERROR 12-04 14:08:44 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=3853752)[0;0m ERROR 12-04 14:08:44 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3853752)[0;0m ERROR 12-04 14:08:44 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=3853752)[0;0m ERROR 12-04 14:08:44 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=3853752)[0;0m ERROR 12-04 14:08:44 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=3853752)[0;0m ERROR 12-04 14:08:44 [core.py:842]     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=3853752)[0;0m ERROR 12-04 14:08:44 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=3853752)[0;0m ERROR 12-04 14:08:44 [core.py:842]     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=3853752)[0;0m ERROR 12-04 14:08:44 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3853752)[0;0m ERROR 12-04 14:08:44 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=3853752)[0;0m ERROR 12-04 14:08:44 [core.py:842]     raise ValueError(
[1;36m(EngineCore_DP0 pid=3853752)[0;0m ERROR 12-04 14:08:44 [core.py:842] ValueError: Free memory on device (7.14/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[1;36m(EngineCore_DP0 pid=3853752)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=3853752)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=3853752)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=3853752)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=3853752)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=3853752)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=3853752)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=3853752)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=3853752)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=3853752)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3853752)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3853752)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=3853752)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=3853752)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=3853752)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=3853752)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3853752)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=3853752)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=3853752)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=3853752)[0;0m     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=3853752)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=3853752)[0;0m     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=3853752)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3853752)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=3853752)[0;0m     raise ValueError(
[1;36m(EngineCore_DP0 pid=3853752)[0;0m ValueError: Free memory on device (7.14/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W1204 14:08:45.151751261 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: WeiboAI/VibeThinker-1.5B (backend=vllm)
[ModelCache] Using max_model_len=131072 for WeiboAI/VibeThinker-1.5B
[ModelCache] Detected 1 GPU(s), 44.3GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 14:09:06 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'WeiboAI/VibeThinker-1.5B'}
INFO 12-04 14:09:06 [model.py:631] Resolved architecture: Qwen2ForCausalLM
WARNING 12-04 14:09:06 [model.py:1921] Your device 'Quadro RTX 8000' (with compute capability 7.5) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.
WARNING 12-04 14:09:06 [model.py:1971] Casting torch.bfloat16 to torch.float16.
INFO 12-04 14:09:06 [model.py:1745] Using max model len 131072
INFO 12-04 14:09:06 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:29<00:00, 29.30s/it, est. speed input: 4.98 toks/s, output: 104.43 toks/s]Processed prompts: 100%|██████████| 1/1 [00:29<00:00, 29.30s/it, est. speed input: 4.98 toks/s, output: 104.43 toks/s]Processed prompts: 100%|██████████| 1/1 [00:29<00:00, 29.30s/it, est. speed input: 4.98 toks/s, output: 104.43 toks/s]
Agent 4 response: Jared starts with a typing speed of 47 WPM. After some lessons, his speed increases to 52 WPM. If he...

--- Problem 1/20, Round 1, Agent 5/7 ---
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 621.10it/s]
[1;36m(EngineCore_DP0 pid=3853812)[0;0m INFO 12-04 14:09:21 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='WeiboAI/VibeThinker-1.5B', speculative_config=None, tokenizer='WeiboAI/VibeThinker-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=WeiboAI/VibeThinker-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=3853812)[0;0m ERROR 12-04 14:09:22 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8
[1;36m(EngineCore_DP0 pid=3853812)[0;0m INFO 12-04 14:09:23 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.7:34937 backend=nccl
[W1204 14:09:23.216136684 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-23-2.rc.tch.harvard.edu]:34937 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=3853812)[0;0m INFO 12-04 14:09:23 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=3853812)[0;0m ERROR 12-04 14:09:23 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=3853812)[0;0m ERROR 12-04 14:09:23 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=3853812)[0;0m ERROR 12-04 14:09:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=3853812)[0;0m ERROR 12-04 14:09:23 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3853812)[0;0m ERROR 12-04 14:09:23 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3853812)[0;0m ERROR 12-04 14:09:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=3853812)[0;0m ERROR 12-04 14:09:23 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=3853812)[0;0m ERROR 12-04 14:09:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=3853812)[0;0m ERROR 12-04 14:09:23 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=3853812)[0;0m ERROR 12-04 14:09:23 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3853812)[0;0m ERROR 12-04 14:09:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=3853812)[0;0m ERROR 12-04 14:09:23 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=3853812)[0;0m ERROR 12-04 14:09:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=3853812)[0;0m ERROR 12-04 14:09:23 [core.py:842]     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=3853812)[0;0m ERROR 12-04 14:09:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=3853812)[0;0m ERROR 12-04 14:09:23 [core.py:842]     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=3853812)[0;0m ERROR 12-04 14:09:23 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3853812)[0;0m ERROR 12-04 14:09:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=3853812)[0;0m ERROR 12-04 14:09:23 [core.py:842]     raise ValueError(
[1;36m(EngineCore_DP0 pid=3853812)[0;0m ERROR 12-04 14:09:23 [core.py:842] ValueError: Free memory on device (7.14/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[1;36m(EngineCore_DP0 pid=3853812)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=3853812)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=3853812)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=3853812)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=3853812)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=3853812)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=3853812)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=3853812)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=3853812)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=3853812)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3853812)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3853812)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=3853812)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=3853812)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=3853812)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=3853812)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3853812)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=3853812)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=3853812)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=3853812)[0;0m     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=3853812)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=3853812)[0;0m     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=3853812)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3853812)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=3853812)[0;0m     raise ValueError(
[1;36m(EngineCore_DP0 pid=3853812)[0;0m ValueError: Free memory on device (7.14/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W1204 14:09:24.465635269 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:18<00:00, 18.41s/it, est. speed input: 7.98 toks/s, output: 102.26 toks/s]Processed prompts: 100%|██████████| 1/1 [00:18<00:00, 18.41s/it, est. speed input: 7.98 toks/s, output: 102.26 toks/s]Processed prompts: 100%|██████████| 1/1 [00:18<00:00, 18.41s/it, est. speed input: 7.98 toks/s, output: 102.26 toks/s]
Agent 5 response: Jared starts with an initial typing speed of 47 WPM. After some lessons, his typing speed increases ...

--- Problem 1/20, Round 1, Agent 6/7 ---
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 942.33it/s]
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: WeiboAI/VibeThinker-1.5B (backend=vllm)
[ModelCache] Using max_model_len=131072 for WeiboAI/VibeThinker-1.5B
[ModelCache] Detected 1 GPU(s), 44.3GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 14:09:45 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'WeiboAI/VibeThinker-1.5B'}
INFO 12-04 14:09:46 [model.py:631] Resolved architecture: Qwen2ForCausalLM
WARNING 12-04 14:09:46 [model.py:1921] Your device 'Quadro RTX 8000' (with compute capability 7.5) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.
WARNING 12-04 14:09:46 [model.py:1971] Casting torch.bfloat16 to torch.float16.
INFO 12-04 14:09:46 [model.py:1745] Using max model len 131072
INFO 12-04 14:09:46 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:19<00:00, 19.96s/it, est. speed input: 7.26 toks/s, output: 105.46 toks/s]Processed prompts: 100%|██████████| 1/1 [00:19<00:00, 19.96s/it, est. speed input: 7.26 toks/s, output: 105.46 toks/s]Processed prompts: 100%|██████████| 1/1 [00:19<00:00, 19.96s/it, est. speed input: 7.26 toks/s, output: 105.46 toks/s]
Agent 6 response: Jared starts with an initial typing speed of 47 WPM. After some lessons, his speed increases to 52 W...

--- Problem 1/20, Round 1, Agent 7/7 ---
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 674.33it/s]
[1;36m(EngineCore_DP0 pid=3853972)[0;0m INFO 12-04 14:10:00 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='WeiboAI/VibeThinker-1.5B', speculative_config=None, tokenizer='WeiboAI/VibeThinker-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=WeiboAI/VibeThinker-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=3853972)[0;0m ERROR 12-04 14:10:00 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8
[1;36m(EngineCore_DP0 pid=3853972)[0;0m INFO 12-04 14:10:01 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.7:59107 backend=nccl
[W1204 14:10:01.479325661 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-23-2.rc.tch.harvard.edu]:59107 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=3853972)[0;0m INFO 12-04 14:10:01 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=3853972)[0;0m ERROR 12-04 14:10:01 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=3853972)[0;0m ERROR 12-04 14:10:01 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=3853972)[0;0m ERROR 12-04 14:10:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=3853972)[0;0m ERROR 12-04 14:10:01 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3853972)[0;0m ERROR 12-04 14:10:01 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3853972)[0;0m ERROR 12-04 14:10:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=3853972)[0;0m ERROR 12-04 14:10:01 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=3853972)[0;0m ERROR 12-04 14:10:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=3853972)[0;0m ERROR 12-04 14:10:01 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=3853972)[0;0m ERROR 12-04 14:10:01 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3853972)[0;0m ERROR 12-04 14:10:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=3853972)[0;0m ERROR 12-04 14:10:01 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=3853972)[0;0m ERROR 12-04 14:10:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=3853972)[0;0m ERROR 12-04 14:10:01 [core.py:842]     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=3853972)[0;0m ERROR 12-04 14:10:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=3853972)[0;0m ERROR 12-04 14:10:01 [core.py:842]     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=3853972)[0;0m ERROR 12-04 14:10:01 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3853972)[0;0m ERROR 12-04 14:10:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=3853972)[0;0m ERROR 12-04 14:10:01 [core.py:842]     raise ValueError(
[1;36m(EngineCore_DP0 pid=3853972)[0;0m ERROR 12-04 14:10:01 [core.py:842] ValueError: Free memory on device (7.14/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[1;36m(EngineCore_DP0 pid=3853972)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=3853972)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=3853972)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=3853972)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=3853972)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=3853972)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=3853972)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=3853972)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=3853972)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=3853972)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3853972)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3853972)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=3853972)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=3853972)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=3853972)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=3853972)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3853972)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=3853972)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=3853972)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=3853972)[0;0m     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=3853972)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=3853972)[0;0m     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=3853972)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3853972)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=3853972)[0;0m     raise ValueError(
[1;36m(EngineCore_DP0 pid=3853972)[0;0m ValueError: Free memory on device (7.14/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W1204 14:10:02.755179297 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:25<00:00, 25.50s/it, est. speed input: 5.84 toks/s, output: 103.14 toks/s]Processed prompts: 100%|██████████| 1/1 [00:25<00:00, 25.50s/it, est. speed input: 5.84 toks/s, output: 103.14 toks/s]Processed prompts: 100%|██████████| 1/1 [00:25<00:00, 25.50s/it, est. speed input: 5.84 toks/s, output: 103.14 toks/s]
Agent 7 response: Jared's initial typing speed is 47 WPM. After some lessons, his speed increases to 52 WPM. He then i...

--- Problem 1/20, Round 2, Agent 1/7 ---
Agent 1 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 201.03it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:03<00:00,  3.06s/it, est. speed input: 452.65 toks/s, output: 104.18 toks/s]Processed prompts: 100%|██████████| 1/1 [00:03<00:00,  3.06s/it, est. speed input: 452.65 toks/s, output: 104.18 toks/s]Processed prompts: 100%|██████████| 1/1 [00:03<00:00,  3.06s/it, est. speed input: 452.65 toks/s, output: 104.18 toks/s]
Agent 1 response: Jared's initial typing speed is 47 WPM. After some lessons, it increases to 52 WPM. Increasing his t...

--- Problem 1/20, Round 2, Agent 2/7 ---
Agent 2 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 227.48it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.98s/it, est. speed input: 466.37 toks/s, output: 104.08 toks/s]Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.98s/it, est. speed input: 466.37 toks/s, output: 104.08 toks/s]Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.98s/it, est. speed input: 466.37 toks/s, output: 104.08 toks/s]
Agent 2 response: Jared starts with a typing speed of 47 WPM. After some lessons, his speed increases to 52 WPM. Conti...

--- Problem 1/20, Round 2, Agent 3/7 ---
Agent 3 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 241.68it/s]
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: WeiboAI/VibeThinker-1.5B (backend=vllm)
[ModelCache] Using max_model_len=131072 for WeiboAI/VibeThinker-1.5B
[ModelCache] Detected 1 GPU(s), 44.3GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 14:10:24 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'WeiboAI/VibeThinker-1.5B'}
INFO 12-04 14:10:24 [model.py:631] Resolved architecture: Qwen2ForCausalLM
WARNING 12-04 14:10:24 [model.py:1921] Your device 'Quadro RTX 8000' (with compute capability 7.5) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.
WARNING 12-04 14:10:24 [model.py:1971] Casting torch.bfloat16 to torch.float16.
INFO 12-04 14:10:24 [model.py:1745] Using max model len 131072
INFO 12-04 14:10:24 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.89s/it, est. speed input: 480.06 toks/s, output: 102.65 toks/s]Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.89s/it, est. speed input: 480.06 toks/s, output: 102.65 toks/s]Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.89s/it, est. speed input: 480.06 toks/s, output: 102.65 toks/s]
Agent 3 response: The three measurements of Jared's typing speed are 47 WPM, 52 WPM, and 57 WPM (calculated as 52 + 5)...

--- Problem 1/20, Round 2, Agent 4/7 ---
Agent 4 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 161.97it/s]
[1;36m(EngineCore_DP0 pid=3854043)[0;0m INFO 12-04 14:10:39 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='WeiboAI/VibeThinker-1.5B', speculative_config=None, tokenizer='WeiboAI/VibeThinker-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=WeiboAI/VibeThinker-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=3854043)[0;0m ERROR 12-04 14:10:40 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:15<00:00, 15.37s/it, est. speed input: 90.41 toks/s, output: 99.84 toks/s]Processed prompts: 100%|██████████| 1/1 [00:15<00:00, 15.37s/it, est. speed input: 90.41 toks/s, output: 99.84 toks/s]Processed prompts: 100%|██████████| 1/1 [00:15<00:00, 15.37s/it, est. speed input: 90.41 toks/s, output: 99.84 toks/s]
Agent 4 response: Jared's initial typing speed is 47 WPM. After some lessons, it increases to 52 WPM. Increasing by an...

--- Problem 1/20, Round 2, Agent 5/7 ---
Agent 5 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 188.01it/s]
[1;36m(EngineCore_DP0 pid=3854043)[0;0m INFO 12-04 14:10:41 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.7:38319 backend=nccl
[W1204 14:10:41.010572538 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-23-2.rc.tch.harvard.edu]:38319 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=3854043)[0;0m INFO 12-04 14:10:41 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=3854043)[0;0m ERROR 12-04 14:10:41 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=3854043)[0;0m ERROR 12-04 14:10:41 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=3854043)[0;0m ERROR 12-04 14:10:41 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=3854043)[0;0m ERROR 12-04 14:10:41 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3854043)[0;0m ERROR 12-04 14:10:41 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3854043)[0;0m ERROR 12-04 14:10:41 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=3854043)[0;0m ERROR 12-04 14:10:41 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=3854043)[0;0m ERROR 12-04 14:10:41 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=3854043)[0;0m ERROR 12-04 14:10:41 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=3854043)[0;0m ERROR 12-04 14:10:41 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3854043)[0;0m ERROR 12-04 14:10:41 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=3854043)[0;0m ERROR 12-04 14:10:41 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=3854043)[0;0m ERROR 12-04 14:10:41 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=3854043)[0;0m ERROR 12-04 14:10:41 [core.py:842]     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=3854043)[0;0m ERROR 12-04 14:10:41 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=3854043)[0;0m ERROR 12-04 14:10:41 [core.py:842]     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=3854043)[0;0m ERROR 12-04 14:10:41 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3854043)[0;0m ERROR 12-04 14:10:41 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=3854043)[0;0m ERROR 12-04 14:10:41 [core.py:842]     raise ValueError(
[1;36m(EngineCore_DP0 pid=3854043)[0;0m ERROR 12-04 14:10:41 [core.py:842] ValueError: Free memory on device (7.14/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[1;36m(EngineCore_DP0 pid=3854043)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=3854043)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=3854043)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=3854043)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=3854043)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=3854043)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=3854043)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=3854043)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=3854043)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=3854043)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3854043)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3854043)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=3854043)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=3854043)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=3854043)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=3854043)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3854043)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=3854043)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=3854043)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=3854043)[0;0m     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=3854043)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=3854043)[0;0m     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=3854043)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3854043)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=3854043)[0;0m     raise ValueError(
[1;36m(EngineCore_DP0 pid=3854043)[0;0m ValueError: Free memory on device (7.14/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W1204 14:10:42.308318912 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:03<00:00,  3.74s/it, est. speed input: 371.75 toks/s, output: 98.88 toks/s]Processed prompts: 100%|██████████| 1/1 [00:03<00:00,  3.74s/it, est. speed input: 371.75 toks/s, output: 98.88 toks/s]Processed prompts: 100%|██████████| 1/1 [00:03<00:00,  3.74s/it, est. speed input: 371.75 toks/s, output: 98.88 toks/s]
Agent 5 response: Jared's initial typing speed is 47 WPM. After some lessons, it increases to 52 WPM. Continuing to in...

--- Problem 1/20, Round 2, Agent 6/7 ---
Agent 6 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 221.22it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:03<00:00,  3.08s/it, est. speed input: 450.94 toks/s, output: 104.54 toks/s]Processed prompts: 100%|██████████| 1/1 [00:03<00:00,  3.08s/it, est. speed input: 450.94 toks/s, output: 104.54 toks/s]Processed prompts: 100%|██████████| 1/1 [00:03<00:00,  3.08s/it, est. speed input: 450.94 toks/s, output: 104.54 toks/s]
Agent 6 response: Jared's initial typing speed is 47 WPM. After lessons, it increases to 52 WPM, which is an increase ...

--- Problem 1/20, Round 2, Agent 7/7 ---
Agent 7 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 226.90it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:06<00:00,  6.10s/it, est. speed input: 228.26 toks/s, output: 105.53 toks/s]Processed prompts: 100%|██████████| 1/1 [00:06<00:00,  6.10s/it, est. speed input: 228.26 toks/s, output: 105.53 toks/s]Processed prompts: 100%|██████████| 1/1 [00:06<00:00,  6.10s/it, est. speed input: 228.26 toks/s, output: 105.53 toks/s]
Agent 7 response: Jared's typing speed starts at 47 WPM. After some lessons, it increases to 52 WPM. Increasing it by ...

--- Problem 1/20, Round 3, Agent 1/7 ---
Agent 1 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 135.44it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:06<00:00,  6.60s/it, est. speed input: 382.85 toks/s, output: 105.14 toks/s]Processed prompts: 100%|██████████| 1/1 [00:06<00:00,  6.60s/it, est. speed input: 382.85 toks/s, output: 105.14 toks/s]Processed prompts: 100%|██████████| 1/1 [00:06<00:00,  6.60s/it, est. speed input: 382.85 toks/s, output: 105.14 toks/s]
Agent 1 response: Jared's initial typing speed is 47 WPM. After some lessons, his speed increases to 52 WPM. Continuin...

--- Problem 1/20, Round 3, Agent 2/7 ---
Agent 2 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 148.88it/s]
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: WeiboAI/VibeThinker-1.5B (backend=vllm)
[ModelCache] Using max_model_len=131072 for WeiboAI/VibeThinker-1.5B
[ModelCache] Detected 1 GPU(s), 44.3GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 14:11:03 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'WeiboAI/VibeThinker-1.5B'}
INFO 12-04 14:11:04 [model.py:631] Resolved architecture: Qwen2ForCausalLM
WARNING 12-04 14:11:04 [model.py:1921] Your device 'Quadro RTX 8000' (with compute capability 7.5) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.
WARNING 12-04 14:11:04 [model.py:1971] Casting torch.bfloat16 to torch.float16.
INFO 12-04 14:11:04 [model.py:1745] Using max model len 131072
INFO 12-04 14:11:04 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.01s/it, est. speed input: 504.57 toks/s, output: 103.71 toks/s]Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.01s/it, est. speed input: 504.57 toks/s, output: 103.71 toks/s]Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.01s/it, est. speed input: 504.57 toks/s, output: 103.71 toks/s]
Agent 2 response: Jared's initial typing speed is 47 WPM. After some lessons, it increases to 52 WPM. Continuing to in...

--- Problem 1/20, Round 3, Agent 3/7 ---
Agent 3 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 87.85it/s]
[1;36m(EngineCore_DP0 pid=3854201)[0;0m INFO 12-04 14:11:18 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='WeiboAI/VibeThinker-1.5B', speculative_config=None, tokenizer='WeiboAI/VibeThinker-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=WeiboAI/VibeThinker-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=3854201)[0;0m ERROR 12-04 14:11:19 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8
[1;36m(EngineCore_DP0 pid=3854201)[0;0m INFO 12-04 14:11:19 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.7:58329 backend=nccl
[W1204 14:11:19.750512238 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-23-2.rc.tch.harvard.edu]:58329 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=3854201)[0;0m INFO 12-04 14:11:19 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=3854201)[0;0m ERROR 12-04 14:11:20 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=3854201)[0;0m ERROR 12-04 14:11:20 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=3854201)[0;0m ERROR 12-04 14:11:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=3854201)[0;0m ERROR 12-04 14:11:20 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3854201)[0;0m ERROR 12-04 14:11:20 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3854201)[0;0m ERROR 12-04 14:11:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=3854201)[0;0m ERROR 12-04 14:11:20 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=3854201)[0;0m ERROR 12-04 14:11:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=3854201)[0;0m ERROR 12-04 14:11:20 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=3854201)[0;0m ERROR 12-04 14:11:20 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3854201)[0;0m ERROR 12-04 14:11:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=3854201)[0;0m ERROR 12-04 14:11:20 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=3854201)[0;0m ERROR 12-04 14:11:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=3854201)[0;0m ERROR 12-04 14:11:20 [core.py:842]     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=3854201)[0;0m ERROR 12-04 14:11:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=3854201)[0;0m ERROR 12-04 14:11:20 [core.py:842]     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=3854201)[0;0m ERROR 12-04 14:11:20 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3854201)[0;0m ERROR 12-04 14:11:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=3854201)[0;0m ERROR 12-04 14:11:20 [core.py:842]     raise ValueError(
[1;36m(EngineCore_DP0 pid=3854201)[0;0m ERROR 12-04 14:11:20 [core.py:842] ValueError: Free memory on device (7.14/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[1;36m(EngineCore_DP0 pid=3854201)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=3854201)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=3854201)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=3854201)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=3854201)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=3854201)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=3854201)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=3854201)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=3854201)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=3854201)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3854201)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3854201)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=3854201)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=3854201)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=3854201)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=3854201)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3854201)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=3854201)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=3854201)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=3854201)[0;0m     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=3854201)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=3854201)[0;0m     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=3854201)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3854201)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=3854201)[0;0m     raise ValueError(
[1;36m(EngineCore_DP0 pid=3854201)[0;0m ValueError: Free memory on device (7.14/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W1204 14:11:21.024048161 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:32<00:00, 32.70s/it, est. speed input: 77.36 toks/s, output: 102.01 toks/s]Processed prompts: 100%|██████████| 1/1 [00:32<00:00, 32.70s/it, est. speed input: 77.36 toks/s, output: 102.01 toks/s]Processed prompts: 100%|██████████| 1/1 [00:32<00:00, 32.70s/it, est. speed input: 77.36 toks/s, output: 102.01 toks/s]
Agent 3 response: Jared's typing speed starts at 47 WPM. After some lessons, his speed increases to 52 WPM. Continuing...

--- Problem 1/20, Round 3, Agent 4/7 ---
Agent 4 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 126.65it/s]
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: WeiboAI/VibeThinker-1.5B (backend=vllm)
[ModelCache] Using max_model_len=131072 for WeiboAI/VibeThinker-1.5B
[ModelCache] Detected 1 GPU(s), 44.3GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 14:11:42 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'WeiboAI/VibeThinker-1.5B'}
INFO 12-04 14:11:42 [model.py:631] Resolved architecture: Qwen2ForCausalLM
WARNING 12-04 14:11:42 [model.py:1921] Your device 'Quadro RTX 8000' (with compute capability 7.5) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.
WARNING 12-04 14:11:42 [model.py:1971] Casting torch.bfloat16 to torch.float16.
INFO 12-04 14:11:42 [model.py:1745] Using max model len 131072
INFO 12-04 14:11:42 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:13<00:00, 13.27s/it, est. speed input: 190.77 toks/s, output: 101.83 toks/s]Processed prompts: 100%|██████████| 1/1 [00:13<00:00, 13.27s/it, est. speed input: 190.77 toks/s, output: 101.83 toks/s]Processed prompts: 100%|██████████| 1/1 [00:13<00:00, 13.27s/it, est. speed input: 190.77 toks/s, output: 101.83 toks/s]
Agent 4 response: Jared's initial typing speed is 47 WPM. After some lessons, his speed increases to 52 WPM. Increasin...

--- Problem 1/20, Round 3, Agent 5/7 ---
Agent 5 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 86.55it/s]
[1;36m(EngineCore_DP0 pid=3854265)[0;0m INFO 12-04 14:11:58 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='WeiboAI/VibeThinker-1.5B', speculative_config=None, tokenizer='WeiboAI/VibeThinker-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=WeiboAI/VibeThinker-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=3854265)[0;0m ERROR 12-04 14:11:59 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8
[1;36m(EngineCore_DP0 pid=3854265)[0;0m INFO 12-04 14:11:59 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.7:52833 backend=nccl
[W1204 14:11:59.752445198 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-23-2.rc.tch.harvard.edu]:52833 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=3854265)[0;0m INFO 12-04 14:11:59 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=3854265)[0;0m ERROR 12-04 14:12:00 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=3854265)[0;0m ERROR 12-04 14:12:00 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=3854265)[0;0m ERROR 12-04 14:12:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=3854265)[0;0m ERROR 12-04 14:12:00 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3854265)[0;0m ERROR 12-04 14:12:00 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3854265)[0;0m ERROR 12-04 14:12:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=3854265)[0;0m ERROR 12-04 14:12:00 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=3854265)[0;0m ERROR 12-04 14:12:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=3854265)[0;0m ERROR 12-04 14:12:00 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=3854265)[0;0m ERROR 12-04 14:12:00 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3854265)[0;0m ERROR 12-04 14:12:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=3854265)[0;0m ERROR 12-04 14:12:00 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=3854265)[0;0m ERROR 12-04 14:12:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=3854265)[0;0m ERROR 12-04 14:12:00 [core.py:842]     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=3854265)[0;0m ERROR 12-04 14:12:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=3854265)[0;0m ERROR 12-04 14:12:00 [core.py:842]     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=3854265)[0;0m ERROR 12-04 14:12:00 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3854265)[0;0m ERROR 12-04 14:12:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=3854265)[0;0m ERROR 12-04 14:12:00 [core.py:842]     raise ValueError(
[1;36m(EngineCore_DP0 pid=3854265)[0;0m ERROR 12-04 14:12:00 [core.py:842] ValueError: Free memory on device (7.14/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[1;36m(EngineCore_DP0 pid=3854265)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=3854265)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=3854265)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=3854265)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=3854265)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=3854265)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=3854265)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=3854265)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=3854265)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=3854265)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3854265)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3854265)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=3854265)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=3854265)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=3854265)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=3854265)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3854265)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=3854265)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=3854265)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=3854265)[0;0m     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=3854265)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=3854265)[0;0m     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=3854265)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3854265)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=3854265)[0;0m     raise ValueError(
[1;36m(EngineCore_DP0 pid=3854265)[0;0m ValueError: Free memory on device (7.14/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W1204 14:12:01.033033373 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:15<00:00, 15.85s/it, est. speed input: 159.72 toks/s, output: 101.43 toks/s]Processed prompts: 100%|██████████| 1/1 [00:15<00:00, 15.85s/it, est. speed input: 159.72 toks/s, output: 101.43 toks/s]Processed prompts: 100%|██████████| 1/1 [00:15<00:00, 15.85s/it, est. speed input: 159.72 toks/s, output: 101.43 toks/s]
Agent 5 response: Jared starts with an initial typing speed of 47 WPM. After some lessons, his typing speed increases ...

--- Problem 1/20, Round 3, Agent 6/7 ---
Agent 6 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 139.25it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:06<00:00,  6.25s/it, est. speed input: 404.60 toks/s, output: 105.07 toks/s]Processed prompts: 100%|██████████| 1/1 [00:06<00:00,  6.25s/it, est. speed input: 404.60 toks/s, output: 105.07 toks/s]Processed prompts: 100%|██████████| 1/1 [00:06<00:00,  6.25s/it, est. speed input: 404.60 toks/s, output: 105.07 toks/s]
Agent 6 response: Jared's initial typing speed is 47 WPM. After some lessons, it increases to 52 WPM, which is an incr...

--- Problem 1/20, Round 3, Agent 7/7 ---
Agent 7 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 130.57it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.30s/it, est. speed input: 347.19 toks/s, output: 104.81 toks/s]Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.30s/it, est. speed input: 347.19 toks/s, output: 104.81 toks/s]Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.30s/it, est. speed input: 347.19 toks/s, output: 104.81 toks/s]
Agent 7 response: Jared's typing speed starts at 47 WPM. After some lessons, it increases to 52 WPM. Increasing the sp...

Running accuracy: 1.000 ± 0.000

--- Problem 2/20, Round 1, Agent 1/7 ---
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 766.36it/s]
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: WeiboAI/VibeThinker-1.5B (backend=vllm)
[ModelCache] Using max_model_len=131072 for WeiboAI/VibeThinker-1.5B
[ModelCache] Detected 1 GPU(s), 44.3GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 14:12:22 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'WeiboAI/VibeThinker-1.5B'}
INFO 12-04 14:12:22 [model.py:631] Resolved architecture: Qwen2ForCausalLM
WARNING 12-04 14:12:22 [model.py:1921] Your device 'Quadro RTX 8000' (with compute capability 7.5) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.
WARNING 12-04 14:12:22 [model.py:1971] Casting torch.bfloat16 to torch.float16.
INFO 12-04 14:12:22 [model.py:1745] Using max model len 131072
INFO 12-04 14:12:22 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=3854353)[0;0m INFO 12-04 14:12:37 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='WeiboAI/VibeThinker-1.5B', speculative_config=None, tokenizer='WeiboAI/VibeThinker-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=WeiboAI/VibeThinker-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=3854353)[0;0m ERROR 12-04 14:12:38 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8
[1;36m(EngineCore_DP0 pid=3854353)[0;0m INFO 12-04 14:12:38 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.7:59225 backend=nccl
[W1204 14:12:38.896116948 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-23-2.rc.tch.harvard.edu]:59225 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=3854353)[0;0m INFO 12-04 14:12:39 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=3854353)[0;0m ERROR 12-04 14:12:39 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=3854353)[0;0m ERROR 12-04 14:12:39 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=3854353)[0;0m ERROR 12-04 14:12:39 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=3854353)[0;0m ERROR 12-04 14:12:39 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3854353)[0;0m ERROR 12-04 14:12:39 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3854353)[0;0m ERROR 12-04 14:12:39 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=3854353)[0;0m ERROR 12-04 14:12:39 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=3854353)[0;0m ERROR 12-04 14:12:39 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=3854353)[0;0m ERROR 12-04 14:12:39 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=3854353)[0;0m ERROR 12-04 14:12:39 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3854353)[0;0m ERROR 12-04 14:12:39 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=3854353)[0;0m ERROR 12-04 14:12:39 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=3854353)[0;0m ERROR 12-04 14:12:39 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=3854353)[0;0m ERROR 12-04 14:12:39 [core.py:842]     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=3854353)[0;0m ERROR 12-04 14:12:39 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=3854353)[0;0m ERROR 12-04 14:12:39 [core.py:842]     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=3854353)[0;0m ERROR 12-04 14:12:39 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3854353)[0;0m ERROR 12-04 14:12:39 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=3854353)[0;0m ERROR 12-04 14:12:39 [core.py:842]     raise ValueError(
[1;36m(EngineCore_DP0 pid=3854353)[0;0m ERROR 12-04 14:12:39 [core.py:842] ValueError: Free memory on device (7.14/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[1;36m(EngineCore_DP0 pid=3854353)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=3854353)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=3854353)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=3854353)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=3854353)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=3854353)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=3854353)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=3854353)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=3854353)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=3854353)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3854353)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3854353)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=3854353)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=3854353)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=3854353)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=3854353)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3854353)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=3854353)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=3854353)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=3854353)[0;0m     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=3854353)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=3854353)[0;0m     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=3854353)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3854353)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=3854353)[0;0m     raise ValueError(
[1;36m(EngineCore_DP0 pid=3854353)[0;0m ValueError: Free memory on device (7.14/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W1204 14:12:40.175372266 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: WeiboAI/VibeThinker-1.5B (backend=vllm)
[ModelCache] Using max_model_len=131072 for WeiboAI/VibeThinker-1.5B
[ModelCache] Detected 1 GPU(s), 44.3GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 14:13:01 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'WeiboAI/VibeThinker-1.5B'}
INFO 12-04 14:13:01 [model.py:631] Resolved architecture: Qwen2ForCausalLM
WARNING 12-04 14:13:01 [model.py:1921] Your device 'Quadro RTX 8000' (with compute capability 7.5) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.
WARNING 12-04 14:13:01 [model.py:1971] Casting torch.bfloat16 to torch.float16.
INFO 12-04 14:13:01 [model.py:1745] Using max model len 131072
INFO 12-04 14:13:01 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:51<00:00, 51.79s/it, est. speed input: 2.20 toks/s, output: 102.46 toks/s]Processed prompts: 100%|██████████| 1/1 [00:51<00:00, 51.79s/it, est. speed input: 2.20 toks/s, output: 102.46 toks/s]Processed prompts: 100%|██████████| 1/1 [00:51<00:00, 51.79s/it, est. speed input: 2.20 toks/s, output: 102.46 toks/s]
Agent 1 response: Jordan's two children each require 5 diaper changes per day, resulting in a total of \(2 \times 5 = ...

--- Problem 2/20, Round 1, Agent 2/7 ---
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 743.41it/s]
[1;36m(EngineCore_DP0 pid=3854495)[0;0m INFO 12-04 14:13:16 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='WeiboAI/VibeThinker-1.5B', speculative_config=None, tokenizer='WeiboAI/VibeThinker-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=WeiboAI/VibeThinker-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=3854495)[0;0m ERROR 12-04 14:13:17 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8
[1;36m(EngineCore_DP0 pid=3854495)[0;0m INFO 12-04 14:13:18 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.7:51341 backend=nccl
[W1204 14:13:18.153807459 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-23-2.rc.tch.harvard.edu]:51341 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=3854495)[0;0m INFO 12-04 14:13:18 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=3854495)[0;0m ERROR 12-04 14:13:18 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=3854495)[0;0m ERROR 12-04 14:13:18 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=3854495)[0;0m ERROR 12-04 14:13:18 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=3854495)[0;0m ERROR 12-04 14:13:18 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3854495)[0;0m ERROR 12-04 14:13:18 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3854495)[0;0m ERROR 12-04 14:13:18 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=3854495)[0;0m ERROR 12-04 14:13:18 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=3854495)[0;0m ERROR 12-04 14:13:18 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=3854495)[0;0m ERROR 12-04 14:13:18 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=3854495)[0;0m ERROR 12-04 14:13:18 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3854495)[0;0m ERROR 12-04 14:13:18 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=3854495)[0;0m ERROR 12-04 14:13:18 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=3854495)[0;0m ERROR 12-04 14:13:18 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=3854495)[0;0m ERROR 12-04 14:13:18 [core.py:842]     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=3854495)[0;0m ERROR 12-04 14:13:18 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=3854495)[0;0m ERROR 12-04 14:13:18 [core.py:842]     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=3854495)[0;0m ERROR 12-04 14:13:18 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3854495)[0;0m ERROR 12-04 14:13:18 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=3854495)[0;0m ERROR 12-04 14:13:18 [core.py:842]     raise ValueError(
[1;36m(EngineCore_DP0 pid=3854495)[0;0m ERROR 12-04 14:13:18 [core.py:842] ValueError: Free memory on device (7.14/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[1;36m(EngineCore_DP0 pid=3854495)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=3854495)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=3854495)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=3854495)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=3854495)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=3854495)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=3854495)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=3854495)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=3854495)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=3854495)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3854495)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3854495)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=3854495)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=3854495)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=3854495)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=3854495)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3854495)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=3854495)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=3854495)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=3854495)[0;0m     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=3854495)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=3854495)[0;0m     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=3854495)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3854495)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=3854495)[0;0m     raise ValueError(
[1;36m(EngineCore_DP0 pid=3854495)[0;0m ValueError: Free memory on device (7.14/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W1204 14:13:19.428187024 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: WeiboAI/VibeThinker-1.5B (backend=vllm)
[ModelCache] Using max_model_len=131072 for WeiboAI/VibeThinker-1.5B
[ModelCache] Detected 1 GPU(s), 44.3GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 14:13:40 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'WeiboAI/VibeThinker-1.5B'}
INFO 12-04 14:13:41 [model.py:631] Resolved architecture: Qwen2ForCausalLM
WARNING 12-04 14:13:41 [model.py:1921] Your device 'Quadro RTX 8000' (with compute capability 7.5) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.
WARNING 12-04 14:13:41 [model.py:1971] Casting torch.bfloat16 to torch.float16.
INFO 12-04 14:13:41 [model.py:1745] Using max model len 131072
INFO 12-04 14:13:41 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=3854556)[0;0m INFO 12-04 14:13:55 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='WeiboAI/VibeThinker-1.5B', speculative_config=None, tokenizer='WeiboAI/VibeThinker-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=WeiboAI/VibeThinker-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=3854556)[0;0m ERROR 12-04 14:13:56 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8
[1;36m(EngineCore_DP0 pid=3854556)[0;0m INFO 12-04 14:13:57 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.7:34877 backend=nccl
[W1204 14:13:57.123826586 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-23-2.rc.tch.harvard.edu]:34877 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=3854556)[0;0m INFO 12-04 14:13:57 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=3854556)[0;0m ERROR 12-04 14:13:57 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=3854556)[0;0m ERROR 12-04 14:13:57 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=3854556)[0;0m ERROR 12-04 14:13:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=3854556)[0;0m ERROR 12-04 14:13:57 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3854556)[0;0m ERROR 12-04 14:13:57 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3854556)[0;0m ERROR 12-04 14:13:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=3854556)[0;0m ERROR 12-04 14:13:57 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=3854556)[0;0m ERROR 12-04 14:13:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=3854556)[0;0m ERROR 12-04 14:13:57 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=3854556)[0;0m ERROR 12-04 14:13:57 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3854556)[0;0m ERROR 12-04 14:13:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=3854556)[0;0m ERROR 12-04 14:13:57 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=3854556)[0;0m ERROR 12-04 14:13:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=3854556)[0;0m ERROR 12-04 14:13:57 [core.py:842]     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=3854556)[0;0m ERROR 12-04 14:13:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=3854556)[0;0m ERROR 12-04 14:13:57 [core.py:842]     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=3854556)[0;0m ERROR 12-04 14:13:57 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3854556)[0;0m ERROR 12-04 14:13:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=3854556)[0;0m ERROR 12-04 14:13:57 [core.py:842]     raise ValueError(
[1;36m(EngineCore_DP0 pid=3854556)[0;0m ERROR 12-04 14:13:57 [core.py:842] ValueError: Free memory on device (7.14/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[1;36m(EngineCore_DP0 pid=3854556)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=3854556)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=3854556)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=3854556)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=3854556)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=3854556)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=3854556)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=3854556)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=3854556)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=3854556)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3854556)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3854556)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=3854556)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=3854556)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=3854556)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=3854556)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3854556)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=3854556)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=3854556)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=3854556)[0;0m     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=3854556)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=3854556)[0;0m     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=3854556)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3854556)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=3854556)[0;0m     raise ValueError(
[1;36m(EngineCore_DP0 pid=3854556)[0;0m ValueError: Free memory on device (7.14/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W1204 14:13:58.413140161 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:55<00:00, 55.62s/it, est. speed input: 2.10 toks/s, output: 102.87 toks/s]Processed prompts: 100%|██████████| 1/1 [00:55<00:00, 55.62s/it, est. speed input: 2.10 toks/s, output: 102.87 toks/s]Processed prompts: 100%|██████████| 1/1 [00:55<00:00, 55.62s/it, est. speed input: 2.10 toks/s, output: 102.87 toks/s]
Agent 2 response: Jordan's two children each require 5 diaper changes per day, resulting in a total of \(2 \times 5 = ...

--- Problem 2/20, Round 1, Agent 3/7 ---
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1013.36it/s]
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: WeiboAI/VibeThinker-1.5B (backend=vllm)
[ModelCache] Using max_model_len=131072 for WeiboAI/VibeThinker-1.5B
[ModelCache] Detected 1 GPU(s), 44.3GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 14:14:19 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'WeiboAI/VibeThinker-1.5B'}
INFO 12-04 14:14:20 [model.py:631] Resolved architecture: Qwen2ForCausalLM
WARNING 12-04 14:14:20 [model.py:1921] Your device 'Quadro RTX 8000' (with compute capability 7.5) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.
WARNING 12-04 14:14:20 [model.py:1971] Casting torch.bfloat16 to torch.float16.
INFO 12-04 14:14:20 [model.py:1745] Using max model len 131072
INFO 12-04 14:14:20 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=3854728)[0;0m INFO 12-04 14:14:35 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='WeiboAI/VibeThinker-1.5B', speculative_config=None, tokenizer='WeiboAI/VibeThinker-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=WeiboAI/VibeThinker-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=3854728)[0;0m ERROR 12-04 14:14:36 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8
[1;36m(EngineCore_DP0 pid=3854728)[0;0m INFO 12-04 14:14:37 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.7:50617 backend=nccl
[W1204 14:14:37.344484172 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-23-2.rc.tch.harvard.edu]:50617 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=3854728)[0;0m INFO 12-04 14:14:37 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=3854728)[0;0m ERROR 12-04 14:14:37 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=3854728)[0;0m ERROR 12-04 14:14:37 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=3854728)[0;0m ERROR 12-04 14:14:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=3854728)[0;0m ERROR 12-04 14:14:37 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3854728)[0;0m ERROR 12-04 14:14:37 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3854728)[0;0m ERROR 12-04 14:14:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=3854728)[0;0m ERROR 12-04 14:14:37 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=3854728)[0;0m ERROR 12-04 14:14:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=3854728)[0;0m ERROR 12-04 14:14:37 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=3854728)[0;0m ERROR 12-04 14:14:37 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3854728)[0;0m ERROR 12-04 14:14:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=3854728)[0;0m ERROR 12-04 14:14:37 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=3854728)[0;0m ERROR 12-04 14:14:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=3854728)[0;0m ERROR 12-04 14:14:37 [core.py:842]     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=3854728)[0;0m ERROR 12-04 14:14:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=3854728)[0;0m ERROR 12-04 14:14:37 [core.py:842]     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=3854728)[0;0m ERROR 12-04 14:14:37 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3854728)[0;0m ERROR 12-04 14:14:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=3854728)[0;0m ERROR 12-04 14:14:37 [core.py:842]     raise ValueError(
[1;36m(EngineCore_DP0 pid=3854728)[0;0m ERROR 12-04 14:14:37 [core.py:842] ValueError: Free memory on device (7.14/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[1;36m(EngineCore_DP0 pid=3854728)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=3854728)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=3854728)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=3854728)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=3854728)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=3854728)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=3854728)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=3854728)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=3854728)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=3854728)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3854728)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3854728)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=3854728)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=3854728)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=3854728)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=3854728)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3854728)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=3854728)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=3854728)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=3854728)[0;0m     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=3854728)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=3854728)[0;0m     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=3854728)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3854728)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=3854728)[0;0m     raise ValueError(
[1;36m(EngineCore_DP0 pid=3854728)[0;0m ValueError: Free memory on device (7.14/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W1204 14:14:38.635803498 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:49<00:00, 49.43s/it, est. speed input: 2.37 toks/s, output: 103.35 toks/s]Processed prompts: 100%|██████████| 1/1 [00:49<00:00, 49.43s/it, est. speed input: 2.37 toks/s, output: 103.35 toks/s]Processed prompts: 100%|██████████| 1/1 [00:49<00:00, 49.43s/it, est. speed input: 2.37 toks/s, output: 103.35 toks/s]
Agent 3 response: Jordan's two children each require 5 diaper changes per day, leading to a total of \(2 \times 5 = 10...

--- Problem 2/20, Round 1, Agent 4/7 ---
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 860.55it/s]
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: WeiboAI/VibeThinker-1.5B (backend=vllm)
[ModelCache] Using max_model_len=131072 for WeiboAI/VibeThinker-1.5B
[ModelCache] Detected 1 GPU(s), 44.3GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 14:15:00 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'WeiboAI/VibeThinker-1.5B'}
INFO 12-04 14:15:00 [model.py:631] Resolved architecture: Qwen2ForCausalLM
WARNING 12-04 14:15:00 [model.py:1921] Your device 'Quadro RTX 8000' (with compute capability 7.5) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.
WARNING 12-04 14:15:00 [model.py:1971] Casting torch.bfloat16 to torch.float16.
INFO 12-04 14:15:00 [model.py:1745] Using max model len 131072
INFO 12-04 14:15:00 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=3854799)[0;0m INFO 12-04 14:15:14 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='WeiboAI/VibeThinker-1.5B', speculative_config=None, tokenizer='WeiboAI/VibeThinker-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=WeiboAI/VibeThinker-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=3854799)[0;0m ERROR 12-04 14:15:15 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8
[1;36m(EngineCore_DP0 pid=3854799)[0;0m INFO 12-04 14:15:16 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.7:32799 backend=nccl
[W1204 14:15:16.317070771 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-23-2.rc.tch.harvard.edu]:32799 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=3854799)[0;0m INFO 12-04 14:15:16 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=3854799)[0;0m ERROR 12-04 14:15:16 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=3854799)[0;0m ERROR 12-04 14:15:16 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=3854799)[0;0m ERROR 12-04 14:15:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=3854799)[0;0m ERROR 12-04 14:15:16 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3854799)[0;0m ERROR 12-04 14:15:16 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3854799)[0;0m ERROR 12-04 14:15:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=3854799)[0;0m ERROR 12-04 14:15:16 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=3854799)[0;0m ERROR 12-04 14:15:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=3854799)[0;0m ERROR 12-04 14:15:16 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=3854799)[0;0m ERROR 12-04 14:15:16 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3854799)[0;0m ERROR 12-04 14:15:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=3854799)[0;0m ERROR 12-04 14:15:16 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=3854799)[0;0m ERROR 12-04 14:15:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=3854799)[0;0m ERROR 12-04 14:15:16 [core.py:842]     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=3854799)[0;0m ERROR 12-04 14:15:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=3854799)[0;0m ERROR 12-04 14:15:16 [core.py:842]     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=3854799)[0;0m ERROR 12-04 14:15:16 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3854799)[0;0m ERROR 12-04 14:15:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=3854799)[0;0m ERROR 12-04 14:15:16 [core.py:842]     raise ValueError(
[1;36m(EngineCore_DP0 pid=3854799)[0;0m ERROR 12-04 14:15:16 [core.py:842] ValueError: Free memory on device (7.14/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[1;36m(EngineCore_DP0 pid=3854799)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=3854799)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=3854799)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=3854799)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=3854799)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=3854799)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=3854799)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=3854799)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=3854799)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=3854799)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3854799)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3854799)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=3854799)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=3854799)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=3854799)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=3854799)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3854799)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=3854799)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=3854799)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=3854799)[0;0m     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=3854799)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=3854799)[0;0m     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=3854799)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3854799)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=3854799)[0;0m     raise ValueError(
[1;36m(EngineCore_DP0 pid=3854799)[0;0m ValueError: Free memory on device (7.14/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W1204 14:15:17.604338831 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:40<00:00, 40.66s/it, est. speed input: 2.90 toks/s, output: 103.44 toks/s]Processed prompts: 100%|██████████| 1/1 [00:40<00:00, 40.66s/it, est. speed input: 2.90 toks/s, output: 103.44 toks/s]Processed prompts: 100%|██████████| 1/1 [00:40<00:00, 40.66s/it, est. speed input: 2.90 toks/s, output: 103.44 toks/s]
Agent 4 response: Jordan's two children each require 5 diaper changes per day, resulting in a total of \(2 \times 5 = ...

--- Problem 2/20, Round 1, Agent 5/7 ---
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1038.97it/s]
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: WeiboAI/VibeThinker-1.5B (backend=vllm)
[ModelCache] Using max_model_len=131072 for WeiboAI/VibeThinker-1.5B
[ModelCache] Detected 1 GPU(s), 44.3GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 14:15:39 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'WeiboAI/VibeThinker-1.5B'}
INFO 12-04 14:15:39 [model.py:631] Resolved architecture: Qwen2ForCausalLM
WARNING 12-04 14:15:39 [model.py:1921] Your device 'Quadro RTX 8000' (with compute capability 7.5) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.
WARNING 12-04 14:15:39 [model.py:1971] Casting torch.bfloat16 to torch.float16.
INFO 12-04 14:15:39 [model.py:1745] Using max model len 131072
INFO 12-04 14:15:39 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=3854959)[0;0m INFO 12-04 14:15:54 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='WeiboAI/VibeThinker-1.5B', speculative_config=None, tokenizer='WeiboAI/VibeThinker-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=WeiboAI/VibeThinker-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=3854959)[0;0m ERROR 12-04 14:15:55 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8
[1;36m(EngineCore_DP0 pid=3854959)[0;0m INFO 12-04 14:15:56 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.7:52945 backend=nccl
[W1204 14:15:56.476321786 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-23-2.rc.tch.harvard.edu]:52945 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=3854959)[0;0m INFO 12-04 14:15:56 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=3854959)[0;0m ERROR 12-04 14:15:56 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=3854959)[0;0m ERROR 12-04 14:15:56 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=3854959)[0;0m ERROR 12-04 14:15:56 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=3854959)[0;0m ERROR 12-04 14:15:56 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3854959)[0;0m ERROR 12-04 14:15:56 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3854959)[0;0m ERROR 12-04 14:15:56 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=3854959)[0;0m ERROR 12-04 14:15:56 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=3854959)[0;0m ERROR 12-04 14:15:56 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=3854959)[0;0m ERROR 12-04 14:15:56 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=3854959)[0;0m ERROR 12-04 14:15:56 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3854959)[0;0m ERROR 12-04 14:15:56 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=3854959)[0;0m ERROR 12-04 14:15:56 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=3854959)[0;0m ERROR 12-04 14:15:56 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=3854959)[0;0m ERROR 12-04 14:15:56 [core.py:842]     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=3854959)[0;0m ERROR 12-04 14:15:56 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=3854959)[0;0m ERROR 12-04 14:15:56 [core.py:842]     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=3854959)[0;0m ERROR 12-04 14:15:56 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3854959)[0;0m ERROR 12-04 14:15:56 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=3854959)[0;0m ERROR 12-04 14:15:56 [core.py:842]     raise ValueError(
[1;36m(EngineCore_DP0 pid=3854959)[0;0m ERROR 12-04 14:15:56 [core.py:842] ValueError: Free memory on device (7.14/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[1;36m(EngineCore_DP0 pid=3854959)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=3854959)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=3854959)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=3854959)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=3854959)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=3854959)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=3854959)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=3854959)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=3854959)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=3854959)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3854959)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3854959)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=3854959)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=3854959)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=3854959)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=3854959)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3854959)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=3854959)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=3854959)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=3854959)[0;0m     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=3854959)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=3854959)[0;0m     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=3854959)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3854959)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=3854959)[0;0m     raise ValueError(
[1;36m(EngineCore_DP0 pid=3854959)[0;0m ValueError: Free memory on device (7.14/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W1204 14:15:57.748230008 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:37<00:00, 37.95s/it, est. speed input: 3.14 toks/s, output: 103.11 toks/s]Processed prompts: 100%|██████████| 1/1 [00:37<00:00, 37.95s/it, est. speed input: 3.14 toks/s, output: 103.11 toks/s]Processed prompts: 100%|██████████| 1/1 [00:37<00:00, 37.95s/it, est. speed input: 3.14 toks/s, output: 103.11 toks/s]
Agent 5 response: Jordan's two children each require 5 diaper changes per day, resulting in a total of \(2 \times 5 = ...

--- Problem 2/20, Round 1, Agent 6/7 ---
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 796.49it/s]
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: WeiboAI/VibeThinker-1.5B (backend=vllm)
[ModelCache] Using max_model_len=131072 for WeiboAI/VibeThinker-1.5B
[ModelCache] Detected 1 GPU(s), 44.3GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 14:16:19 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'WeiboAI/VibeThinker-1.5B'}
INFO 12-04 14:16:19 [model.py:631] Resolved architecture: Qwen2ForCausalLM
WARNING 12-04 14:16:19 [model.py:1921] Your device 'Quadro RTX 8000' (with compute capability 7.5) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.
WARNING 12-04 14:16:19 [model.py:1971] Casting torch.bfloat16 to torch.float16.
INFO 12-04 14:16:19 [model.py:1745] Using max model len 131072
INFO 12-04 14:16:19 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=3855017)[0;0m INFO 12-04 14:16:34 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='WeiboAI/VibeThinker-1.5B', speculative_config=None, tokenizer='WeiboAI/VibeThinker-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=WeiboAI/VibeThinker-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=3855017)[0;0m ERROR 12-04 14:16:35 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8
[1;36m(EngineCore_DP0 pid=3855017)[0;0m INFO 12-04 14:16:36 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.7:41095 backend=nccl
[W1204 14:16:36.020684094 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-23-2.rc.tch.harvard.edu]:41095 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=3855017)[0;0m INFO 12-04 14:16:36 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=3855017)[0;0m ERROR 12-04 14:16:36 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=3855017)[0;0m ERROR 12-04 14:16:36 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=3855017)[0;0m ERROR 12-04 14:16:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=3855017)[0;0m ERROR 12-04 14:16:36 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3855017)[0;0m ERROR 12-04 14:16:36 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3855017)[0;0m ERROR 12-04 14:16:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=3855017)[0;0m ERROR 12-04 14:16:36 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=3855017)[0;0m ERROR 12-04 14:16:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=3855017)[0;0m ERROR 12-04 14:16:36 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=3855017)[0;0m ERROR 12-04 14:16:36 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3855017)[0;0m ERROR 12-04 14:16:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=3855017)[0;0m ERROR 12-04 14:16:36 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=3855017)[0;0m ERROR 12-04 14:16:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=3855017)[0;0m ERROR 12-04 14:16:36 [core.py:842]     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=3855017)[0;0m ERROR 12-04 14:16:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=3855017)[0;0m ERROR 12-04 14:16:36 [core.py:842]     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=3855017)[0;0m ERROR 12-04 14:16:36 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3855017)[0;0m ERROR 12-04 14:16:36 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=3855017)[0;0m ERROR 12-04 14:16:36 [core.py:842]     raise ValueError(
[1;36m(EngineCore_DP0 pid=3855017)[0;0m ERROR 12-04 14:16:36 [core.py:842] ValueError: Free memory on device (7.14/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[1;36m(EngineCore_DP0 pid=3855017)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=3855017)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=3855017)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=3855017)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=3855017)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=3855017)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=3855017)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=3855017)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=3855017)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=3855017)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3855017)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3855017)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=3855017)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=3855017)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=3855017)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=3855017)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3855017)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=3855017)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=3855017)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=3855017)[0;0m     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=3855017)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=3855017)[0;0m     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=3855017)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3855017)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=3855017)[0;0m     raise ValueError(
[1;36m(EngineCore_DP0 pid=3855017)[0;0m ValueError: Free memory on device (7.14/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W1204 14:16:37.299406664 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: WeiboAI/VibeThinker-1.5B (backend=vllm)
[ModelCache] Using max_model_len=131072 for WeiboAI/VibeThinker-1.5B
[ModelCache] Detected 1 GPU(s), 44.3GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 14:16:58 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'WeiboAI/VibeThinker-1.5B'}
INFO 12-04 14:16:59 [model.py:631] Resolved architecture: Qwen2ForCausalLM
WARNING 12-04 14:16:59 [model.py:1921] Your device 'Quadro RTX 8000' (with compute capability 7.5) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.
WARNING 12-04 14:16:59 [model.py:1971] Casting torch.bfloat16 to torch.float16.
INFO 12-04 14:16:59 [model.py:1745] Using max model len 131072
INFO 12-04 14:16:59 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=3855078)[0;0m INFO 12-04 14:17:14 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='WeiboAI/VibeThinker-1.5B', speculative_config=None, tokenizer='WeiboAI/VibeThinker-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=WeiboAI/VibeThinker-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=3855078)[0;0m ERROR 12-04 14:17:15 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8
[1;36m(EngineCore_DP0 pid=3855078)[0;0m INFO 12-04 14:17:16 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.7:58089 backend=nccl
[W1204 14:17:16.027412013 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-23-2.rc.tch.harvard.edu]:58089 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=3855078)[0;0m INFO 12-04 14:17:16 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=3855078)[0;0m ERROR 12-04 14:17:16 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=3855078)[0;0m ERROR 12-04 14:17:16 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=3855078)[0;0m ERROR 12-04 14:17:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=3855078)[0;0m ERROR 12-04 14:17:16 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3855078)[0;0m ERROR 12-04 14:17:16 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3855078)[0;0m ERROR 12-04 14:17:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=3855078)[0;0m ERROR 12-04 14:17:16 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=3855078)[0;0m ERROR 12-04 14:17:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=3855078)[0;0m ERROR 12-04 14:17:16 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=3855078)[0;0m ERROR 12-04 14:17:16 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3855078)[0;0m ERROR 12-04 14:17:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=3855078)[0;0m ERROR 12-04 14:17:16 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=3855078)[0;0m ERROR 12-04 14:17:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=3855078)[0;0m ERROR 12-04 14:17:16 [core.py:842]     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=3855078)[0;0m ERROR 12-04 14:17:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=3855078)[0;0m ERROR 12-04 14:17:16 [core.py:842]     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=3855078)[0;0m ERROR 12-04 14:17:16 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3855078)[0;0m ERROR 12-04 14:17:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=3855078)[0;0m ERROR 12-04 14:17:16 [core.py:842]     raise ValueError(
[1;36m(EngineCore_DP0 pid=3855078)[0;0m ERROR 12-04 14:17:16 [core.py:842] ValueError: Free memory on device (7.14/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[1;36m(EngineCore_DP0 pid=3855078)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=3855078)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=3855078)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=3855078)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=3855078)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=3855078)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=3855078)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=3855078)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=3855078)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=3855078)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3855078)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3855078)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=3855078)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=3855078)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=3855078)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=3855078)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3855078)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=3855078)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=3855078)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=3855078)[0;0m     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=3855078)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=3855078)[0;0m     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=3855078)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3855078)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=3855078)[0;0m     raise ValueError(
[1;36m(EngineCore_DP0 pid=3855078)[0;0m ValueError: Free memory on device (7.14/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W1204 14:17:17.310593586 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [01:10<00:00, 70.48s/it, est. speed input: 1.66 toks/s, output: 102.10 toks/s]Processed prompts: 100%|██████████| 1/1 [01:10<00:00, 70.48s/it, est. speed input: 1.66 toks/s, output: 102.10 toks/s]Processed prompts: 100%|██████████| 1/1 [01:10<00:00, 70.48s/it, est. speed input: 1.66 toks/s, output: 102.10 toks/s]
Agent 6 response: Jordan's two children each require 5 diaper changes per day, totaling \(2 \times 5 = 10\) changes. T...

--- Problem 2/20, Round 1, Agent 7/7 ---
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 952.60it/s]
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: WeiboAI/VibeThinker-1.5B (backend=vllm)
[ModelCache] Using max_model_len=131072 for WeiboAI/VibeThinker-1.5B
[ModelCache] Detected 1 GPU(s), 44.3GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 14:17:38 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'WeiboAI/VibeThinker-1.5B'}
INFO 12-04 14:17:39 [model.py:631] Resolved architecture: Qwen2ForCausalLM
WARNING 12-04 14:17:39 [model.py:1921] Your device 'Quadro RTX 8000' (with compute capability 7.5) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.
WARNING 12-04 14:17:39 [model.py:1971] Casting torch.bfloat16 to torch.float16.
INFO 12-04 14:17:39 [model.py:1745] Using max model len 131072
INFO 12-04 14:17:39 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=3855248)[0;0m INFO 12-04 14:17:53 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='WeiboAI/VibeThinker-1.5B', speculative_config=None, tokenizer='WeiboAI/VibeThinker-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=WeiboAI/VibeThinker-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=3855248)[0;0m ERROR 12-04 14:17:54 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8
[1;36m(EngineCore_DP0 pid=3855248)[0;0m INFO 12-04 14:17:55 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.7:43343 backend=nccl
[W1204 14:17:55.292017268 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-23-2.rc.tch.harvard.edu]:43343 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=3855248)[0;0m INFO 12-04 14:17:55 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=3855248)[0;0m ERROR 12-04 14:17:55 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=3855248)[0;0m ERROR 12-04 14:17:55 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=3855248)[0;0m ERROR 12-04 14:17:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=3855248)[0;0m ERROR 12-04 14:17:55 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3855248)[0;0m ERROR 12-04 14:17:55 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3855248)[0;0m ERROR 12-04 14:17:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=3855248)[0;0m ERROR 12-04 14:17:55 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=3855248)[0;0m ERROR 12-04 14:17:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=3855248)[0;0m ERROR 12-04 14:17:55 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=3855248)[0;0m ERROR 12-04 14:17:55 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3855248)[0;0m ERROR 12-04 14:17:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=3855248)[0;0m ERROR 12-04 14:17:55 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=3855248)[0;0m ERROR 12-04 14:17:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=3855248)[0;0m ERROR 12-04 14:17:55 [core.py:842]     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=3855248)[0;0m ERROR 12-04 14:17:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=3855248)[0;0m ERROR 12-04 14:17:55 [core.py:842]     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=3855248)[0;0m ERROR 12-04 14:17:55 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3855248)[0;0m ERROR 12-04 14:17:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=3855248)[0;0m ERROR 12-04 14:17:55 [core.py:842]     raise ValueError(
[1;36m(EngineCore_DP0 pid=3855248)[0;0m ERROR 12-04 14:17:55 [core.py:842] ValueError: Free memory on device (7.14/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[1;36m(EngineCore_DP0 pid=3855248)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=3855248)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=3855248)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=3855248)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=3855248)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=3855248)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=3855248)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=3855248)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=3855248)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=3855248)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3855248)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3855248)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=3855248)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=3855248)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=3855248)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=3855248)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3855248)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=3855248)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=3855248)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=3855248)[0;0m     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=3855248)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=3855248)[0;0m     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=3855248)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3855248)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=3855248)[0;0m     raise ValueError(
[1;36m(EngineCore_DP0 pid=3855248)[0;0m ValueError: Free memory on device (7.14/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W1204 14:17:56.584040533 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: WeiboAI/VibeThinker-1.5B (backend=vllm)
[ModelCache] Using max_model_len=131072 for WeiboAI/VibeThinker-1.5B
[ModelCache] Detected 1 GPU(s), 44.3GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 14:18:18 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'WeiboAI/VibeThinker-1.5B'}
INFO 12-04 14:18:18 [model.py:631] Resolved architecture: Qwen2ForCausalLM
WARNING 12-04 14:18:18 [model.py:1921] Your device 'Quadro RTX 8000' (with compute capability 7.5) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.
WARNING 12-04 14:18:18 [model.py:1971] Casting torch.bfloat16 to torch.float16.
INFO 12-04 14:18:18 [model.py:1745] Using max model len 131072
INFO 12-04 14:18:18 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [01:03<00:00, 63.65s/it, est. speed input: 1.90 toks/s, output: 102.72 toks/s]Processed prompts: 100%|██████████| 1/1 [01:03<00:00, 63.65s/it, est. speed input: 1.90 toks/s, output: 102.72 toks/s]Processed prompts: 100%|██████████| 1/1 [01:03<00:00, 63.65s/it, est. speed input: 1.90 toks/s, output: 102.72 toks/s]
Agent 7 response: Jordan's two children each require 5 diaper changes per day, leading to a total of \(2 \times 5 = 10...

--- Problem 2/20, Round 2, Agent 1/7 ---
Agent 1 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 124.86it/s]
[1;36m(EngineCore_DP0 pid=3855405)[0;0m INFO 12-04 14:18:33 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='WeiboAI/VibeThinker-1.5B', speculative_config=None, tokenizer='WeiboAI/VibeThinker-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=WeiboAI/VibeThinker-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=3855405)[0;0m ERROR 12-04 14:18:34 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8
[1;36m(EngineCore_DP0 pid=3855405)[0;0m INFO 12-04 14:18:35 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.7:33895 backend=nccl
[W1204 14:18:35.096182819 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-23-2.rc.tch.harvard.edu]:33895 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=3855405)[0;0m INFO 12-04 14:18:35 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=3855405)[0;0m ERROR 12-04 14:18:35 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=3855405)[0;0m ERROR 12-04 14:18:35 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=3855405)[0;0m ERROR 12-04 14:18:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=3855405)[0;0m ERROR 12-04 14:18:35 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3855405)[0;0m ERROR 12-04 14:18:35 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3855405)[0;0m ERROR 12-04 14:18:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=3855405)[0;0m ERROR 12-04 14:18:35 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=3855405)[0;0m ERROR 12-04 14:18:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=3855405)[0;0m ERROR 12-04 14:18:35 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=3855405)[0;0m ERROR 12-04 14:18:35 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3855405)[0;0m ERROR 12-04 14:18:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=3855405)[0;0m ERROR 12-04 14:18:35 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=3855405)[0;0m ERROR 12-04 14:18:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=3855405)[0;0m ERROR 12-04 14:18:35 [core.py:842]     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=3855405)[0;0m ERROR 12-04 14:18:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=3855405)[0;0m ERROR 12-04 14:18:35 [core.py:842]     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=3855405)[0;0m ERROR 12-04 14:18:35 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3855405)[0;0m ERROR 12-04 14:18:35 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=3855405)[0;0m ERROR 12-04 14:18:35 [core.py:842]     raise ValueError(
[1;36m(EngineCore_DP0 pid=3855405)[0;0m ERROR 12-04 14:18:35 [core.py:842] ValueError: Free memory on device (7.14/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[1;36m(EngineCore_DP0 pid=3855405)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=3855405)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=3855405)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=3855405)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=3855405)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=3855405)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=3855405)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=3855405)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=3855405)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=3855405)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3855405)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3855405)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=3855405)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=3855405)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=3855405)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=3855405)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3855405)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=3855405)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=3855405)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=3855405)[0;0m     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=3855405)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=3855405)[0;0m     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=3855405)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3855405)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=3855405)[0;0m     raise ValueError(
[1;36m(EngineCore_DP0 pid=3855405)[0;0m ValueError: Free memory on device (7.14/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W1204 14:18:36.407815588 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: WeiboAI/VibeThinker-1.5B (backend=vllm)
[ModelCache] Using max_model_len=131072 for WeiboAI/VibeThinker-1.5B
[ModelCache] Detected 1 GPU(s), 44.3GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 14:18:57 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'WeiboAI/VibeThinker-1.5B'}
INFO 12-04 14:18:58 [model.py:631] Resolved architecture: Qwen2ForCausalLM
WARNING 12-04 14:18:58 [model.py:1921] Your device 'Quadro RTX 8000' (with compute capability 7.5) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.
WARNING 12-04 14:18:58 [model.py:1971] Casting torch.bfloat16 to torch.float16.
INFO 12-04 14:18:58 [model.py:1745] Using max model len 131072
INFO 12-04 14:18:58 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:35<00:00, 35.13s/it, est. speed input: 31.60 toks/s, output: 103.08 toks/s]Processed prompts: 100%|██████████| 1/1 [00:35<00:00, 35.13s/it, est. speed input: 31.60 toks/s, output: 103.08 toks/s]Processed prompts: 100%|██████████| 1/1 [00:35<00:00, 35.13s/it, est. speed input: 31.60 toks/s, output: 103.08 toks/s]
Agent 1 response: Jordan's two children each require 5 diaper changes per day. Therefore, the total number of diaper c...

--- Problem 2/20, Round 2, Agent 2/7 ---
Agent 2 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 196.53it/s]
[1;36m(EngineCore_DP0 pid=3855481)[0;0m INFO 12-04 14:19:13 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='WeiboAI/VibeThinker-1.5B', speculative_config=None, tokenizer='WeiboAI/VibeThinker-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=WeiboAI/VibeThinker-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=3855481)[0;0m ERROR 12-04 14:19:14 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8
[1;36m(EngineCore_DP0 pid=3855481)[0;0m INFO 12-04 14:19:14 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.7:40769 backend=nccl
[W1204 14:19:14.769177485 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-23-2.rc.tch.harvard.edu]:40769 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=3855481)[0;0m INFO 12-04 14:19:14 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=3855481)[0;0m ERROR 12-04 14:19:15 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=3855481)[0;0m ERROR 12-04 14:19:15 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=3855481)[0;0m ERROR 12-04 14:19:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=3855481)[0;0m ERROR 12-04 14:19:15 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3855481)[0;0m ERROR 12-04 14:19:15 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3855481)[0;0m ERROR 12-04 14:19:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=3855481)[0;0m ERROR 12-04 14:19:15 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=3855481)[0;0m ERROR 12-04 14:19:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=3855481)[0;0m ERROR 12-04 14:19:15 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=3855481)[0;0m ERROR 12-04 14:19:15 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3855481)[0;0m ERROR 12-04 14:19:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=3855481)[0;0m ERROR 12-04 14:19:15 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=3855481)[0;0m ERROR 12-04 14:19:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=3855481)[0;0m ERROR 12-04 14:19:15 [core.py:842]     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=3855481)[0;0m ERROR 12-04 14:19:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=3855481)[0;0m ERROR 12-04 14:19:15 [core.py:842]     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=3855481)[0;0m ERROR 12-04 14:19:15 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3855481)[0;0m ERROR 12-04 14:19:15 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=3855481)[0;0m ERROR 12-04 14:19:15 [core.py:842]     raise ValueError(
[1;36m(EngineCore_DP0 pid=3855481)[0;0m ERROR 12-04 14:19:15 [core.py:842] ValueError: Free memory on device (7.14/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[1;36m(EngineCore_DP0 pid=3855481)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=3855481)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=3855481)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=3855481)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=3855481)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=3855481)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=3855481)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=3855481)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=3855481)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=3855481)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3855481)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3855481)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=3855481)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=3855481)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=3855481)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=3855481)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3855481)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=3855481)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=3855481)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=3855481)[0;0m     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=3855481)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=3855481)[0;0m     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=3855481)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3855481)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=3855481)[0;0m     raise ValueError(
[1;36m(EngineCore_DP0 pid=3855481)[0;0m ValueError: Free memory on device (7.14/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W1204 14:19:16.022701176 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: WeiboAI/VibeThinker-1.5B (backend=vllm)
[ModelCache] Using max_model_len=131072 for WeiboAI/VibeThinker-1.5B
[ModelCache] Detected 1 GPU(s), 44.3GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 14:19:37 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'WeiboAI/VibeThinker-1.5B'}
INFO 12-04 14:19:37 [model.py:631] Resolved architecture: Qwen2ForCausalLM
WARNING 12-04 14:19:37 [model.py:1921] Your device 'Quadro RTX 8000' (with compute capability 7.5) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.
WARNING 12-04 14:19:37 [model.py:1971] Casting torch.bfloat16 to torch.float16.
INFO 12-04 14:19:37 [model.py:1745] Using max model len 131072
INFO 12-04 14:19:37 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=3855545)[0;0m INFO 12-04 14:19:52 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='WeiboAI/VibeThinker-1.5B', speculative_config=None, tokenizer='WeiboAI/VibeThinker-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=WeiboAI/VibeThinker-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=3855545)[0;0m ERROR 12-04 14:19:53 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8
[1;36m(EngineCore_DP0 pid=3855545)[0;0m INFO 12-04 14:19:54 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.7:57331 backend=nccl
[W1204 14:19:54.312975357 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-23-2.rc.tch.harvard.edu]:57331 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=3855545)[0;0m INFO 12-04 14:19:54 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=3855545)[0;0m ERROR 12-04 14:19:54 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=3855545)[0;0m ERROR 12-04 14:19:54 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=3855545)[0;0m ERROR 12-04 14:19:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=3855545)[0;0m ERROR 12-04 14:19:54 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3855545)[0;0m ERROR 12-04 14:19:54 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3855545)[0;0m ERROR 12-04 14:19:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=3855545)[0;0m ERROR 12-04 14:19:54 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=3855545)[0;0m ERROR 12-04 14:19:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=3855545)[0;0m ERROR 12-04 14:19:54 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=3855545)[0;0m ERROR 12-04 14:19:54 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3855545)[0;0m ERROR 12-04 14:19:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=3855545)[0;0m ERROR 12-04 14:19:54 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=3855545)[0;0m ERROR 12-04 14:19:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=3855545)[0;0m ERROR 12-04 14:19:54 [core.py:842]     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=3855545)[0;0m ERROR 12-04 14:19:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=3855545)[0;0m ERROR 12-04 14:19:54 [core.py:842]     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=3855545)[0;0m ERROR 12-04 14:19:54 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3855545)[0;0m ERROR 12-04 14:19:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=3855545)[0;0m ERROR 12-04 14:19:54 [core.py:842]     raise ValueError(
[1;36m(EngineCore_DP0 pid=3855545)[0;0m ERROR 12-04 14:19:54 [core.py:842] ValueError: Free memory on device (7.14/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[1;36m(EngineCore_DP0 pid=3855545)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=3855545)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=3855545)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=3855545)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=3855545)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=3855545)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=3855545)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=3855545)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=3855545)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=3855545)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3855545)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3855545)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=3855545)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=3855545)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=3855545)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=3855545)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3855545)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=3855545)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=3855545)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=3855545)[0;0m     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=3855545)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=3855545)[0;0m     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=3855545)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3855545)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=3855545)[0;0m     raise ValueError(
[1;36m(EngineCore_DP0 pid=3855545)[0;0m ValueError: Free memory on device (7.14/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W1204 14:19:55.574209861 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:57<00:00, 57.63s/it, est. speed input: 19.31 toks/s, output: 102.15 toks/s]Processed prompts: 100%|██████████| 1/1 [00:57<00:00, 57.63s/it, est. speed input: 19.31 toks/s, output: 102.15 toks/s]Processed prompts: 100%|██████████| 1/1 [00:57<00:00, 57.63s/it, est. speed input: 19.31 toks/s, output: 102.15 toks/s]
Agent 2 response: The problem states that each of Jordan's two children requires 5 diaper changes per day. Therefore, ...

--- Problem 2/20, Round 2, Agent 3/7 ---
Agent 3 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 236.71it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.52s/it, est. speed input: 441.37 toks/s, output: 105.09 toks/s]Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.52s/it, est. speed input: 441.37 toks/s, output: 105.09 toks/s]Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.52s/it, est. speed input: 441.37 toks/s, output: 105.09 toks/s]
Agent 3 response: Jordan's two children each require 5 diaper changes per day, resulting in a total of \(2 \times 5 = ...

--- Problem 2/20, Round 2, Agent 4/7 ---
Agent 4 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 263.78it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:04<00:00,  4.40s/it, est. speed input: 253.11 toks/s, output: 105.65 toks/s]Processed prompts: 100%|██████████| 1/1 [00:04<00:00,  4.40s/it, est. speed input: 253.11 toks/s, output: 105.65 toks/s]Processed prompts: 100%|██████████| 1/1 [00:04<00:00,  4.40s/it, est. speed input: 253.11 toks/s, output: 105.65 toks/s]
Agent 4 response: Jordan's two children each require 5 diaper changes per day, resulting in a total of \(2 \times 5 = ...

--- Problem 2/20, Round 2, Agent 5/7 ---
Agent 5 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 256.80it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.37s/it, est. speed input: 470.16 toks/s, output: 104.99 toks/s]Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.37s/it, est. speed input: 470.16 toks/s, output: 104.99 toks/s]Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.37s/it, est. speed input: 470.16 toks/s, output: 104.99 toks/s]
Agent 5 response: Jordan's two children each require 5 diaper changes per day, resulting in a total of \(2 \times 5 = ...

--- Problem 2/20, Round 2, Agent 6/7 ---
Agent 6 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 259.56it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.44s/it, est. speed input: 457.00 toks/s, output: 104.70 toks/s]Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.44s/it, est. speed input: 457.00 toks/s, output: 104.70 toks/s]Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.44s/it, est. speed input: 457.00 toks/s, output: 104.70 toks/s]
Agent 6 response: Jordan's two children each require 5 diaper changes per day, totaling \(2 \times 5 = 10\) changes pe...

--- Problem 2/20, Round 2, Agent 7/7 ---
Agent 7 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 238.33it/s]
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: WeiboAI/VibeThinker-1.5B (backend=vllm)
[ModelCache] Using max_model_len=131072 for WeiboAI/VibeThinker-1.5B
[ModelCache] Detected 1 GPU(s), 44.3GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 14:20:17 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'WeiboAI/VibeThinker-1.5B'}
INFO 12-04 14:20:17 [model.py:631] Resolved architecture: Qwen2ForCausalLM
WARNING 12-04 14:20:17 [model.py:1921] Your device 'Quadro RTX 8000' (with compute capability 7.5) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.
WARNING 12-04 14:20:17 [model.py:1971] Casting torch.bfloat16 to torch.float16.
INFO 12-04 14:20:17 [model.py:1745] Using max model len 131072
INFO 12-04 14:20:17 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.99s/it, est. speed input: 373.83 toks/s, output: 104.75 toks/s]Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.99s/it, est. speed input: 373.83 toks/s, output: 104.75 toks/s]Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.99s/it, est. speed input: 373.83 toks/s, output: 104.75 toks/s]
Agent 7 response: Jordan's two children each require 5 diaper changes per day, resulting in a total of \(2 \times 5 = ...

--- Problem 2/20, Round 3, Agent 1/7 ---
Agent 1 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 91.23it/s]
[1;36m(EngineCore_DP0 pid=3855602)[0;0m INFO 12-04 14:20:31 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='WeiboAI/VibeThinker-1.5B', speculative_config=None, tokenizer='WeiboAI/VibeThinker-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=WeiboAI/VibeThinker-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=3855602)[0;0m ERROR 12-04 14:20:32 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8
[1;36m(EngineCore_DP0 pid=3855602)[0;0m INFO 12-04 14:20:32 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.7:44177 backend=nccl
[W1204 14:20:32.628728896 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-23-2.rc.tch.harvard.edu]:44177 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=3855602)[0;0m INFO 12-04 14:20:32 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=3855602)[0;0m ERROR 12-04 14:20:33 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=3855602)[0;0m ERROR 12-04 14:20:33 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=3855602)[0;0m ERROR 12-04 14:20:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=3855602)[0;0m ERROR 12-04 14:20:33 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3855602)[0;0m ERROR 12-04 14:20:33 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3855602)[0;0m ERROR 12-04 14:20:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=3855602)[0;0m ERROR 12-04 14:20:33 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=3855602)[0;0m ERROR 12-04 14:20:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=3855602)[0;0m ERROR 12-04 14:20:33 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=3855602)[0;0m ERROR 12-04 14:20:33 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3855602)[0;0m ERROR 12-04 14:20:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=3855602)[0;0m ERROR 12-04 14:20:33 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=3855602)[0;0m ERROR 12-04 14:20:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=3855602)[0;0m ERROR 12-04 14:20:33 [core.py:842]     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=3855602)[0;0m ERROR 12-04 14:20:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=3855602)[0;0m ERROR 12-04 14:20:33 [core.py:842]     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=3855602)[0;0m ERROR 12-04 14:20:33 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3855602)[0;0m ERROR 12-04 14:20:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=3855602)[0;0m ERROR 12-04 14:20:33 [core.py:842]     raise ValueError(
[1;36m(EngineCore_DP0 pid=3855602)[0;0m ERROR 12-04 14:20:33 [core.py:842] ValueError: Free memory on device (7.14/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[1;36m(EngineCore_DP0 pid=3855602)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=3855602)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=3855602)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=3855602)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=3855602)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=3855602)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=3855602)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=3855602)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=3855602)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=3855602)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3855602)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3855602)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=3855602)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=3855602)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=3855602)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=3855602)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3855602)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=3855602)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=3855602)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=3855602)[0;0m     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=3855602)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=3855602)[0;0m     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=3855602)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3855602)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=3855602)[0;0m     raise ValueError(
[1;36m(EngineCore_DP0 pid=3855602)[0;0m ValueError: Free memory on device (7.14/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:16<00:00, 16.39s/it, est. speed input: 127.82 toks/s, output: 99.81 toks/s]Processed prompts: 100%|██████████| 1/1 [00:16<00:00, 16.39s/it, est. speed input: 127.82 toks/s, output: 99.81 toks/s]Processed prompts: 100%|██████████| 1/1 [00:16<00:00, 16.39s/it, est. speed input: 127.82 toks/s, output: 99.81 toks/s]
Agent 1 response: The total number of diaper changes required per day for the two children is calculated as follows: e...

--- Problem 2/20, Round 3, Agent 2/7 ---
Agent 2 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 84.86it/s]
[rank0]:[W1204 14:20:34.900634315 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: WeiboAI/VibeThinker-1.5B (backend=vllm)
[ModelCache] Using max_model_len=131072 for WeiboAI/VibeThinker-1.5B
[ModelCache] Detected 1 GPU(s), 44.3GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 14:20:55 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'WeiboAI/VibeThinker-1.5B'}
INFO 12-04 14:20:55 [model.py:631] Resolved architecture: Qwen2ForCausalLM
WARNING 12-04 14:20:55 [model.py:1921] Your device 'Quadro RTX 8000' (with compute capability 7.5) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.
WARNING 12-04 14:20:55 [model.py:1971] Casting torch.bfloat16 to torch.float16.
INFO 12-04 14:20:55 [model.py:1745] Using max model len 131072
INFO 12-04 14:20:55 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:23<00:00, 23.33s/it, est. speed input: 89.93 toks/s, output: 104.81 toks/s]Processed prompts: 100%|██████████| 1/1 [00:23<00:00, 23.33s/it, est. speed input: 89.93 toks/s, output: 104.81 toks/s]Processed prompts: 100%|██████████| 1/1 [00:23<00:00, 23.33s/it, est. speed input: 89.93 toks/s, output: 104.81 toks/s]
Agent 2 response: The total number of diaper changes required per day for both children is calculated as:  
\(2 \times...

--- Problem 2/20, Round 3, Agent 3/7 ---
Agent 3 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 86.98it/s]
[1;36m(EngineCore_DP0 pid=3855662)[0;0m INFO 12-04 14:21:11 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='WeiboAI/VibeThinker-1.5B', speculative_config=None, tokenizer='WeiboAI/VibeThinker-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=WeiboAI/VibeThinker-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=3855662)[0;0m ERROR 12-04 14:21:12 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8
[1;36m(EngineCore_DP0 pid=3855662)[0;0m INFO 12-04 14:21:12 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.7:48903 backend=nccl
[W1204 14:21:12.672271072 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-23-2.rc.tch.harvard.edu]:48903 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=3855662)[0;0m INFO 12-04 14:21:12 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=3855662)[0;0m ERROR 12-04 14:21:13 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=3855662)[0;0m ERROR 12-04 14:21:13 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=3855662)[0;0m ERROR 12-04 14:21:13 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=3855662)[0;0m ERROR 12-04 14:21:13 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3855662)[0;0m ERROR 12-04 14:21:13 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3855662)[0;0m ERROR 12-04 14:21:13 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=3855662)[0;0m ERROR 12-04 14:21:13 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=3855662)[0;0m ERROR 12-04 14:21:13 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=3855662)[0;0m ERROR 12-04 14:21:13 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=3855662)[0;0m ERROR 12-04 14:21:13 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3855662)[0;0m ERROR 12-04 14:21:13 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=3855662)[0;0m ERROR 12-04 14:21:13 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=3855662)[0;0m ERROR 12-04 14:21:13 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=3855662)[0;0m ERROR 12-04 14:21:13 [core.py:842]     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=3855662)[0;0m ERROR 12-04 14:21:13 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=3855662)[0;0m ERROR 12-04 14:21:13 [core.py:842]     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=3855662)[0;0m ERROR 12-04 14:21:13 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3855662)[0;0m ERROR 12-04 14:21:13 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=3855662)[0;0m ERROR 12-04 14:21:13 [core.py:842]     raise ValueError(
[1;36m(EngineCore_DP0 pid=3855662)[0;0m ERROR 12-04 14:21:13 [core.py:842] ValueError: Free memory on device (7.14/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[1;36m(EngineCore_DP0 pid=3855662)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=3855662)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=3855662)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=3855662)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=3855662)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=3855662)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=3855662)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=3855662)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=3855662)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=3855662)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3855662)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3855662)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=3855662)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=3855662)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=3855662)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=3855662)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3855662)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=3855662)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=3855662)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=3855662)[0;0m     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=3855662)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=3855662)[0;0m     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=3855662)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3855662)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=3855662)[0;0m     raise ValueError(
[1;36m(EngineCore_DP0 pid=3855662)[0;0m ValueError: Free memory on device (7.14/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W1204 14:21:14.984289045 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:18<00:00, 18.14s/it, est. speed input: 115.66 toks/s, output: 99.17 toks/s]Processed prompts: 100%|██████████| 1/1 [00:18<00:00, 18.14s/it, est. speed input: 115.66 toks/s, output: 99.17 toks/s]Processed prompts: 100%|██████████| 1/1 [00:18<00:00, 18.14s/it, est. speed input: 115.66 toks/s, output: 99.17 toks/s]
Agent 3 response: Jordan's two children each require 5 diaper changes per day, resulting in a total of \(2 \times 5 = ...

--- Problem 2/20, Round 3, Agent 4/7 ---
Agent 4 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 100.53it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:17<00:00, 17.69s/it, est. speed input: 118.68 toks/s, output: 105.40 toks/s]Processed prompts: 100%|██████████| 1/1 [00:17<00:00, 17.69s/it, est. speed input: 118.68 toks/s, output: 105.40 toks/s]Processed prompts: 100%|██████████| 1/1 [00:17<00:00, 17.69s/it, est. speed input: 118.68 toks/s, output: 105.40 toks/s]
Agent 4 response: Jordan's two children each require 5 diaper changes per day, resulting in a total of \(2 \times 5 = ...

--- Problem 2/20, Round 3, Agent 5/7 ---
Agent 5 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 132.75it/s]
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: WeiboAI/VibeThinker-1.5B (backend=vllm)
[ModelCache] Using max_model_len=131072 for WeiboAI/VibeThinker-1.5B
[ModelCache] Detected 1 GPU(s), 44.3GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 14:21:35 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'WeiboAI/VibeThinker-1.5B'}
INFO 12-04 14:21:35 [model.py:631] Resolved architecture: Qwen2ForCausalLM
WARNING 12-04 14:21:35 [model.py:1921] Your device 'Quadro RTX 8000' (with compute capability 7.5) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.
WARNING 12-04 14:21:35 [model.py:1971] Casting torch.bfloat16 to torch.float16.
INFO 12-04 14:21:35 [model.py:1745] Using max model len 131072
INFO 12-04 14:21:35 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.65s/it, est. speed input: 793.78 toks/s, output: 103.95 toks/s]Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.65s/it, est. speed input: 793.78 toks/s, output: 103.95 toks/s]Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.65s/it, est. speed input: 793.78 toks/s, output: 103.95 toks/s]
Agent 5 response: Jordan's two children each require 5 diaper changes per day, leading to a total of \(2 \times 5 = 10...

--- Problem 2/20, Round 3, Agent 6/7 ---
Agent 6 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 148.18it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.59s/it, est. speed input: 811.37 toks/s, output: 98.62 toks/s]Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.59s/it, est. speed input: 811.37 toks/s, output: 98.62 toks/s]Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.59s/it, est. speed input: 811.37 toks/s, output: 98.62 toks/s]
Agent 6 response: Jordan's two children each require 5 diaper changes per day, resulting in a total of \(2 \times 5 = ...

--- Problem 2/20, Round 3, Agent 7/7 ---
Agent 7 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 109.72it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.70s/it, est. speed input: 779.13 toks/s, output: 98.60 toks/s]Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.70s/it, est. speed input: 779.13 toks/s, output: 98.60 toks/s]Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.70s/it, est. speed input: 779.13 toks/s, output: 98.60 toks/s]
Agent 7 response: Jordan's two children each require 5 diaper changes per day, resulting in a total of \(2 \times 5 = ...

Running accuracy: 1.000 ± 0.000

--- Problem 3/20, Round 1, Agent 1/7 ---
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 659.07it/s]
[1;36m(EngineCore_DP0 pid=3855837)[0;0m INFO 12-04 14:21:50 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='WeiboAI/VibeThinker-1.5B', speculative_config=None, tokenizer='WeiboAI/VibeThinker-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=WeiboAI/VibeThinker-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=3855837)[0;0m ERROR 12-04 14:21:51 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8
[1;36m(EngineCore_DP0 pid=3855837)[0;0m INFO 12-04 14:21:51 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.7:33593 backend=nccl
[W1204 14:21:51.825989212 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-23-2.rc.tch.harvard.edu]:33593 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=3855837)[0;0m INFO 12-04 14:21:51 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=3855837)[0;0m ERROR 12-04 14:21:52 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=3855837)[0;0m ERROR 12-04 14:21:52 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=3855837)[0;0m ERROR 12-04 14:21:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=3855837)[0;0m ERROR 12-04 14:21:52 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3855837)[0;0m ERROR 12-04 14:21:52 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3855837)[0;0m ERROR 12-04 14:21:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=3855837)[0;0m ERROR 12-04 14:21:52 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=3855837)[0;0m ERROR 12-04 14:21:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=3855837)[0;0m ERROR 12-04 14:21:52 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=3855837)[0;0m ERROR 12-04 14:21:52 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3855837)[0;0m ERROR 12-04 14:21:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=3855837)[0;0m ERROR 12-04 14:21:52 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=3855837)[0;0m ERROR 12-04 14:21:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=3855837)[0;0m ERROR 12-04 14:21:52 [core.py:842]     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=3855837)[0;0m ERROR 12-04 14:21:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=3855837)[0;0m ERROR 12-04 14:21:52 [core.py:842]     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=3855837)[0;0m ERROR 12-04 14:21:52 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3855837)[0;0m ERROR 12-04 14:21:52 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=3855837)[0;0m ERROR 12-04 14:21:52 [core.py:842]     raise ValueError(
[1;36m(EngineCore_DP0 pid=3855837)[0;0m ERROR 12-04 14:21:52 [core.py:842] ValueError: Free memory on device (7.14/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[1;36m(EngineCore_DP0 pid=3855837)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=3855837)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=3855837)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=3855837)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=3855837)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=3855837)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=3855837)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=3855837)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=3855837)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=3855837)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3855837)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3855837)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=3855837)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=3855837)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=3855837)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=3855837)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3855837)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=3855837)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=3855837)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=3855837)[0;0m     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=3855837)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=3855837)[0;0m     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=3855837)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3855837)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=3855837)[0;0m     raise ValueError(
[1;36m(EngineCore_DP0 pid=3855837)[0;0m ValueError: Free memory on device (7.14/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W1204 14:21:53.100494182 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:25<00:00, 25.82s/it, est. speed input: 5.85 toks/s, output: 103.36 toks/s]Processed prompts: 100%|██████████| 1/1 [00:25<00:00, 25.82s/it, est. speed input: 5.85 toks/s, output: 103.36 toks/s]Processed prompts: 100%|██████████| 1/1 [00:25<00:00, 25.82s/it, est. speed input: 5.85 toks/s, output: 103.36 toks/s]
Agent 1 response: The bridge has a weight limit of 5000 pounds. The combined weight of the driver and an empty truck i...

--- Problem 3/20, Round 1, Agent 2/7 ---
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 951.09it/s]
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: WeiboAI/VibeThinker-1.5B (backend=vllm)
[ModelCache] Using max_model_len=131072 for WeiboAI/VibeThinker-1.5B
[ModelCache] Detected 1 GPU(s), 44.3GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 14:22:14 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'WeiboAI/VibeThinker-1.5B'}
INFO 12-04 14:22:14 [model.py:631] Resolved architecture: Qwen2ForCausalLM
WARNING 12-04 14:22:14 [model.py:1921] Your device 'Quadro RTX 8000' (with compute capability 7.5) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.
WARNING 12-04 14:22:14 [model.py:1971] Casting torch.bfloat16 to torch.float16.
INFO 12-04 14:22:14 [model.py:1745] Using max model len 131072
INFO 12-04 14:22:14 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:17<00:00, 17.86s/it, est. speed input: 8.62 toks/s, output: 103.19 toks/s]Processed prompts: 100%|██████████| 1/1 [00:17<00:00, 17.86s/it, est. speed input: 8.62 toks/s, output: 103.19 toks/s]Processed prompts: 100%|██████████| 1/1 [00:17<00:00, 17.86s/it, est. speed input: 8.62 toks/s, output: 103.19 toks/s]
Agent 2 response: The bridge's weight limit is 5000 pounds. The combined weight of the driver and the empty truck is 3...

--- Problem 3/20, Round 1, Agent 3/7 ---
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 826.46it/s]
[1;36m(EngineCore_DP0 pid=3855994)[0;0m INFO 12-04 14:22:29 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='WeiboAI/VibeThinker-1.5B', speculative_config=None, tokenizer='WeiboAI/VibeThinker-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=WeiboAI/VibeThinker-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=3855994)[0;0m ERROR 12-04 14:22:30 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8
[1;36m(EngineCore_DP0 pid=3855994)[0;0m INFO 12-04 14:22:31 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.7:46811 backend=nccl
[W1204 14:22:31.086295605 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-23-2.rc.tch.harvard.edu]:46811 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=3855994)[0;0m INFO 12-04 14:22:31 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=3855994)[0;0m ERROR 12-04 14:22:31 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=3855994)[0;0m ERROR 12-04 14:22:31 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=3855994)[0;0m ERROR 12-04 14:22:31 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=3855994)[0;0m ERROR 12-04 14:22:31 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3855994)[0;0m ERROR 12-04 14:22:31 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3855994)[0;0m ERROR 12-04 14:22:31 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=3855994)[0;0m ERROR 12-04 14:22:31 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=3855994)[0;0m ERROR 12-04 14:22:31 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=3855994)[0;0m ERROR 12-04 14:22:31 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=3855994)[0;0m ERROR 12-04 14:22:31 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3855994)[0;0m ERROR 12-04 14:22:31 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=3855994)[0;0m ERROR 12-04 14:22:31 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=3855994)[0;0m ERROR 12-04 14:22:31 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=3855994)[0;0m ERROR 12-04 14:22:31 [core.py:842]     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=3855994)[0;0m ERROR 12-04 14:22:31 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=3855994)[0;0m ERROR 12-04 14:22:31 [core.py:842]     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=3855994)[0;0m ERROR 12-04 14:22:31 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3855994)[0;0m ERROR 12-04 14:22:31 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=3855994)[0;0m ERROR 12-04 14:22:31 [core.py:842]     raise ValueError(
[1;36m(EngineCore_DP0 pid=3855994)[0;0m ERROR 12-04 14:22:31 [core.py:842] ValueError: Free memory on device (7.14/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[1;36m(EngineCore_DP0 pid=3855994)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=3855994)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=3855994)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=3855994)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=3855994)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=3855994)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=3855994)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=3855994)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=3855994)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=3855994)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3855994)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3855994)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=3855994)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=3855994)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=3855994)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=3855994)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3855994)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=3855994)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=3855994)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=3855994)[0;0m     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=3855994)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=3855994)[0;0m     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=3855994)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3855994)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=3855994)[0;0m     raise ValueError(
[1;36m(EngineCore_DP0 pid=3855994)[0;0m ValueError: Free memory on device (7.14/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W1204 14:22:32.357961873 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: WeiboAI/VibeThinker-1.5B (backend=vllm)
[ModelCache] Using max_model_len=131072 for WeiboAI/VibeThinker-1.5B
[ModelCache] Detected 1 GPU(s), 44.3GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 14:22:53 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'WeiboAI/VibeThinker-1.5B'}
INFO 12-04 14:22:54 [model.py:631] Resolved architecture: Qwen2ForCausalLM
WARNING 12-04 14:22:54 [model.py:1921] Your device 'Quadro RTX 8000' (with compute capability 7.5) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.
WARNING 12-04 14:22:54 [model.py:1971] Casting torch.bfloat16 to torch.float16.
INFO 12-04 14:22:54 [model.py:1745] Using max model len 131072
INFO 12-04 14:22:54 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:32<00:00, 32.71s/it, est. speed input: 4.71 toks/s, output: 104.13 toks/s]Processed prompts: 100%|██████████| 1/1 [00:32<00:00, 32.71s/it, est. speed input: 4.71 toks/s, output: 104.13 toks/s]Processed prompts: 100%|██████████| 1/1 [00:32<00:00, 32.71s/it, est. speed input: 4.71 toks/s, output: 104.13 toks/s]
Agent 3 response: The wooden bridge has a weight limit of 5000 pounds. The combined weight of the driver and the empty...

--- Problem 3/20, Round 1, Agent 4/7 ---
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 525.73it/s]
[1;36m(EngineCore_DP0 pid=3856148)[0;0m INFO 12-04 14:23:08 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='WeiboAI/VibeThinker-1.5B', speculative_config=None, tokenizer='WeiboAI/VibeThinker-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=WeiboAI/VibeThinker-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=3856148)[0;0m ERROR 12-04 14:23:09 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8
[1;36m(EngineCore_DP0 pid=3856148)[0;0m INFO 12-04 14:23:09 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.7:52253 backend=nccl
[W1204 14:23:09.755465599 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-23-2.rc.tch.harvard.edu]:52253 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=3856148)[0;0m INFO 12-04 14:23:09 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=3856148)[0;0m ERROR 12-04 14:23:10 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=3856148)[0;0m ERROR 12-04 14:23:10 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=3856148)[0;0m ERROR 12-04 14:23:10 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=3856148)[0;0m ERROR 12-04 14:23:10 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3856148)[0;0m ERROR 12-04 14:23:10 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3856148)[0;0m ERROR 12-04 14:23:10 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=3856148)[0;0m ERROR 12-04 14:23:10 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=3856148)[0;0m ERROR 12-04 14:23:10 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=3856148)[0;0m ERROR 12-04 14:23:10 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=3856148)[0;0m ERROR 12-04 14:23:10 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3856148)[0;0m ERROR 12-04 14:23:10 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=3856148)[0;0m ERROR 12-04 14:23:10 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=3856148)[0;0m ERROR 12-04 14:23:10 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=3856148)[0;0m ERROR 12-04 14:23:10 [core.py:842]     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=3856148)[0;0m ERROR 12-04 14:23:10 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=3856148)[0;0m ERROR 12-04 14:23:10 [core.py:842]     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=3856148)[0;0m ERROR 12-04 14:23:10 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3856148)[0;0m ERROR 12-04 14:23:10 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=3856148)[0;0m ERROR 12-04 14:23:10 [core.py:842]     raise ValueError(
[1;36m(EngineCore_DP0 pid=3856148)[0;0m ERROR 12-04 14:23:10 [core.py:842] ValueError: Free memory on device (7.14/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[1;36m(EngineCore_DP0 pid=3856148)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=3856148)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=3856148)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=3856148)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=3856148)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=3856148)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=3856148)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=3856148)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=3856148)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=3856148)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3856148)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3856148)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=3856148)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=3856148)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=3856148)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=3856148)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3856148)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=3856148)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=3856148)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=3856148)[0;0m     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=3856148)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=3856148)[0;0m     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=3856148)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3856148)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=3856148)[0;0m     raise ValueError(
[1;36m(EngineCore_DP0 pid=3856148)[0;0m ValueError: Free memory on device (7.14/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W1204 14:23:11.023700470 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:26<00:00, 26.74s/it, est. speed input: 5.80 toks/s, output: 103.02 toks/s]Processed prompts: 100%|██████████| 1/1 [00:26<00:00, 26.74s/it, est. speed input: 5.80 toks/s, output: 103.02 toks/s]Processed prompts: 100%|██████████| 1/1 [00:26<00:00, 26.74s/it, est. speed input: 5.80 toks/s, output: 103.02 toks/s]
Agent 4 response: The bridge's weight limit is 5000 pounds. The combined weight of the driver and the empty truck is 3...

--- Problem 3/20, Round 1, Agent 5/7 ---
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 854.41it/s]
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: WeiboAI/VibeThinker-1.5B (backend=vllm)
[ModelCache] Using max_model_len=131072 for WeiboAI/VibeThinker-1.5B
[ModelCache] Detected 1 GPU(s), 44.3GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 14:23:32 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'WeiboAI/VibeThinker-1.5B'}
INFO 12-04 14:23:32 [model.py:631] Resolved architecture: Qwen2ForCausalLM
WARNING 12-04 14:23:32 [model.py:1921] Your device 'Quadro RTX 8000' (with compute capability 7.5) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.
WARNING 12-04 14:23:32 [model.py:1971] Casting torch.bfloat16 to torch.float16.
INFO 12-04 14:23:32 [model.py:1745] Using max model len 131072
INFO 12-04 14:23:32 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:18<00:00, 18.27s/it, est. speed input: 8.54 toks/s, output: 103.44 toks/s]Processed prompts: 100%|██████████| 1/1 [00:18<00:00, 18.27s/it, est. speed input: 8.54 toks/s, output: 103.44 toks/s]Processed prompts: 100%|██████████| 1/1 [00:18<00:00, 18.27s/it, est. speed input: 8.54 toks/s, output: 103.44 toks/s]
Agent 5 response: The combined weight of the driver and the empty truck is 3755 pounds. When loading boxes onto the tr...

--- Problem 3/20, Round 1, Agent 6/7 ---
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 669.16it/s]
[1;36m(EngineCore_DP0 pid=3856205)[0;0m INFO 12-04 14:23:47 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='WeiboAI/VibeThinker-1.5B', speculative_config=None, tokenizer='WeiboAI/VibeThinker-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=WeiboAI/VibeThinker-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=3856205)[0;0m ERROR 12-04 14:23:48 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8
[1;36m(EngineCore_DP0 pid=3856205)[0;0m INFO 12-04 14:23:49 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.7:34277 backend=nccl
[W1204 14:23:49.106522993 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-23-2.rc.tch.harvard.edu]:34277 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=3856205)[0;0m INFO 12-04 14:23:49 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=3856205)[0;0m ERROR 12-04 14:23:49 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=3856205)[0;0m ERROR 12-04 14:23:49 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=3856205)[0;0m ERROR 12-04 14:23:49 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=3856205)[0;0m ERROR 12-04 14:23:49 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3856205)[0;0m ERROR 12-04 14:23:49 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3856205)[0;0m ERROR 12-04 14:23:49 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=3856205)[0;0m ERROR 12-04 14:23:49 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=3856205)[0;0m ERROR 12-04 14:23:49 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=3856205)[0;0m ERROR 12-04 14:23:49 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=3856205)[0;0m ERROR 12-04 14:23:49 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3856205)[0;0m ERROR 12-04 14:23:49 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=3856205)[0;0m ERROR 12-04 14:23:49 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=3856205)[0;0m ERROR 12-04 14:23:49 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=3856205)[0;0m ERROR 12-04 14:23:49 [core.py:842]     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=3856205)[0;0m ERROR 12-04 14:23:49 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=3856205)[0;0m ERROR 12-04 14:23:49 [core.py:842]     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=3856205)[0;0m ERROR 12-04 14:23:49 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3856205)[0;0m ERROR 12-04 14:23:49 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=3856205)[0;0m ERROR 12-04 14:23:49 [core.py:842]     raise ValueError(
[1;36m(EngineCore_DP0 pid=3856205)[0;0m ERROR 12-04 14:23:49 [core.py:842] ValueError: Free memory on device (7.14/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[1;36m(EngineCore_DP0 pid=3856205)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=3856205)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=3856205)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=3856205)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=3856205)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=3856205)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=3856205)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=3856205)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=3856205)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=3856205)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3856205)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3856205)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=3856205)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=3856205)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=3856205)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=3856205)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3856205)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=3856205)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=3856205)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=3856205)[0;0m     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=3856205)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=3856205)[0;0m     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=3856205)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3856205)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=3856205)[0;0m     raise ValueError(
[1;36m(EngineCore_DP0 pid=3856205)[0;0m ValueError: Free memory on device (7.14/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W1204 14:23:50.377383874 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: WeiboAI/VibeThinker-1.5B (backend=vllm)
[ModelCache] Using max_model_len=131072 for WeiboAI/VibeThinker-1.5B
[ModelCache] Detected 1 GPU(s), 44.3GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 14:24:11 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'WeiboAI/VibeThinker-1.5B'}
INFO 12-04 14:24:12 [model.py:631] Resolved architecture: Qwen2ForCausalLM
WARNING 12-04 14:24:12 [model.py:1921] Your device 'Quadro RTX 8000' (with compute capability 7.5) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.
WARNING 12-04 14:24:12 [model.py:1971] Casting torch.bfloat16 to torch.float16.
INFO 12-04 14:24:12 [model.py:1745] Using max model len 131072
INFO 12-04 14:24:12 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:32<00:00, 32.40s/it, est. speed input: 4.75 toks/s, output: 104.10 toks/s]Processed prompts: 100%|██████████| 1/1 [00:32<00:00, 32.40s/it, est. speed input: 4.75 toks/s, output: 104.10 toks/s]Processed prompts: 100%|██████████| 1/1 [00:32<00:00, 32.40s/it, est. speed input: 4.75 toks/s, output: 104.10 toks/s]
Agent 6 response: The bridge's weight limit is 5000 pounds. The combined weight of the driver and the empty truck is 3...

--- Problem 3/20, Round 1, Agent 7/7 ---
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 693.16it/s]
[1;36m(EngineCore_DP0 pid=3856266)[0;0m INFO 12-04 14:24:26 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='WeiboAI/VibeThinker-1.5B', speculative_config=None, tokenizer='WeiboAI/VibeThinker-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=WeiboAI/VibeThinker-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=3856266)[0;0m ERROR 12-04 14:24:27 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8
[1;36m(EngineCore_DP0 pid=3856266)[0;0m INFO 12-04 14:24:28 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.7:54365 backend=nccl
[W1204 14:24:28.083569574 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-23-2.rc.tch.harvard.edu]:54365 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=3856266)[0;0m INFO 12-04 14:24:28 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=3856266)[0;0m ERROR 12-04 14:24:28 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=3856266)[0;0m ERROR 12-04 14:24:28 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=3856266)[0;0m ERROR 12-04 14:24:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=3856266)[0;0m ERROR 12-04 14:24:28 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3856266)[0;0m ERROR 12-04 14:24:28 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3856266)[0;0m ERROR 12-04 14:24:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=3856266)[0;0m ERROR 12-04 14:24:28 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=3856266)[0;0m ERROR 12-04 14:24:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=3856266)[0;0m ERROR 12-04 14:24:28 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=3856266)[0;0m ERROR 12-04 14:24:28 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3856266)[0;0m ERROR 12-04 14:24:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=3856266)[0;0m ERROR 12-04 14:24:28 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=3856266)[0;0m ERROR 12-04 14:24:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=3856266)[0;0m ERROR 12-04 14:24:28 [core.py:842]     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=3856266)[0;0m ERROR 12-04 14:24:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=3856266)[0;0m ERROR 12-04 14:24:28 [core.py:842]     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=3856266)[0;0m ERROR 12-04 14:24:28 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3856266)[0;0m ERROR 12-04 14:24:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=3856266)[0;0m ERROR 12-04 14:24:28 [core.py:842]     raise ValueError(
[1;36m(EngineCore_DP0 pid=3856266)[0;0m ERROR 12-04 14:24:28 [core.py:842] ValueError: Free memory on device (7.14/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[1;36m(EngineCore_DP0 pid=3856266)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=3856266)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=3856266)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=3856266)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=3856266)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=3856266)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=3856266)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=3856266)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=3856266)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=3856266)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3856266)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3856266)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=3856266)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=3856266)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=3856266)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=3856266)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3856266)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=3856266)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=3856266)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=3856266)[0;0m     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=3856266)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=3856266)[0;0m     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=3856266)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3856266)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=3856266)[0;0m     raise ValueError(
[1;36m(EngineCore_DP0 pid=3856266)[0;0m ValueError: Free memory on device (7.14/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W1204 14:24:29.412634537 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:27<00:00, 27.83s/it, est. speed input: 5.68 toks/s, output: 102.47 toks/s]Processed prompts: 100%|██████████| 1/1 [00:27<00:00, 27.83s/it, est. speed input: 5.68 toks/s, output: 102.47 toks/s]Processed prompts: 100%|██████████| 1/1 [00:27<00:00, 27.83s/it, est. speed input: 5.68 toks/s, output: 102.47 toks/s]
Agent 7 response: The combined weight of the driver and the empty truck is 3755 pounds. The bridge's weight limit is 5...

--- Problem 3/20, Round 2, Agent 1/7 ---
Agent 1 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 153.38it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:04<00:00,  4.48s/it, est. speed input: 383.15 toks/s, output: 104.37 toks/s]Processed prompts: 100%|██████████| 1/1 [00:04<00:00,  4.48s/it, est. speed input: 383.15 toks/s, output: 104.37 toks/s]Processed prompts: 100%|██████████| 1/1 [00:04<00:00,  4.48s/it, est. speed input: 383.15 toks/s, output: 104.37 toks/s]
Agent 1 response: The bridge's weight limit is 5000 pounds. The combined weight of the driver and the empty truck is 3...

--- Problem 3/20, Round 2, Agent 2/7 ---
Agent 2 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 179.37it/s]
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: WeiboAI/VibeThinker-1.5B (backend=vllm)
[ModelCache] Using max_model_len=131072 for WeiboAI/VibeThinker-1.5B
[ModelCache] Detected 1 GPU(s), 44.3GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 14:24:50 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'WeiboAI/VibeThinker-1.5B'}
INFO 12-04 14:24:51 [model.py:631] Resolved architecture: Qwen2ForCausalLM
WARNING 12-04 14:24:51 [model.py:1921] Your device 'Quadro RTX 8000' (with compute capability 7.5) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.
WARNING 12-04 14:24:51 [model.py:1971] Casting torch.bfloat16 to torch.float16.
INFO 12-04 14:24:51 [model.py:1745] Using max model len 131072
INFO 12-04 14:24:51 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:11<00:00, 11.45s/it, est. speed input: 150.24 toks/s, output: 101.18 toks/s]Processed prompts: 100%|██████████| 1/1 [00:11<00:00, 11.45s/it, est. speed input: 150.24 toks/s, output: 101.18 toks/s]Processed prompts: 100%|██████████| 1/1 [00:11<00:00, 11.46s/it, est. speed input: 150.24 toks/s, output: 101.18 toks/s]
Agent 2 response: The bridge's weight limit is 5000 pounds. The combined weight of the driver and empty truck is 3755 ...

--- Problem 3/20, Round 2, Agent 3/7 ---
Agent 3 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 115.06it/s]
[1;36m(EngineCore_DP0 pid=3856449)[0;0m INFO 12-04 14:25:06 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='WeiboAI/VibeThinker-1.5B', speculative_config=None, tokenizer='WeiboAI/VibeThinker-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=WeiboAI/VibeThinker-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=3856449)[0;0m ERROR 12-04 14:25:07 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8
[1;36m(EngineCore_DP0 pid=3856449)[0;0m INFO 12-04 14:25:07 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.7:48963 backend=nccl
[W1204 14:25:07.716090724 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-23-2.rc.tch.harvard.edu]:48963 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=3856449)[0;0m INFO 12-04 14:25:07 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=3856449)[0;0m ERROR 12-04 14:25:08 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=3856449)[0;0m ERROR 12-04 14:25:08 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=3856449)[0;0m ERROR 12-04 14:25:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=3856449)[0;0m ERROR 12-04 14:25:08 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3856449)[0;0m ERROR 12-04 14:25:08 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3856449)[0;0m ERROR 12-04 14:25:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=3856449)[0;0m ERROR 12-04 14:25:08 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=3856449)[0;0m ERROR 12-04 14:25:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=3856449)[0;0m ERROR 12-04 14:25:08 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=3856449)[0;0m ERROR 12-04 14:25:08 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3856449)[0;0m ERROR 12-04 14:25:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=3856449)[0;0m ERROR 12-04 14:25:08 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=3856449)[0;0m ERROR 12-04 14:25:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=3856449)[0;0m ERROR 12-04 14:25:08 [core.py:842]     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=3856449)[0;0m ERROR 12-04 14:25:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=3856449)[0;0m ERROR 12-04 14:25:08 [core.py:842]     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=3856449)[0;0m ERROR 12-04 14:25:08 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3856449)[0;0m ERROR 12-04 14:25:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=3856449)[0;0m ERROR 12-04 14:25:08 [core.py:842]     raise ValueError(
[1;36m(EngineCore_DP0 pid=3856449)[0;0m ERROR 12-04 14:25:08 [core.py:842] ValueError: Free memory on device (7.14/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[1;36m(EngineCore_DP0 pid=3856449)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=3856449)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=3856449)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=3856449)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=3856449)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=3856449)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=3856449)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=3856449)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=3856449)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=3856449)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3856449)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3856449)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=3856449)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=3856449)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=3856449)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=3856449)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3856449)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=3856449)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=3856449)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=3856449)[0;0m     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=3856449)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=3856449)[0;0m     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=3856449)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3856449)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=3856449)[0;0m     raise ValueError(
[1;36m(EngineCore_DP0 pid=3856449)[0;0m ValueError: Free memory on device (7.14/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W1204 14:25:09.995295253 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:24<00:00, 24.97s/it, est. speed input: 68.91 toks/s, output: 102.66 toks/s]Processed prompts: 100%|██████████| 1/1 [00:24<00:00, 24.97s/it, est. speed input: 68.91 toks/s, output: 102.66 toks/s]Processed prompts: 100%|██████████| 1/1 [00:24<00:00, 24.98s/it, est. speed input: 68.91 toks/s, output: 102.66 toks/s]
Agent 3 response: The bridge has a weight limit of 5000 pounds. The combined weight of the driver and the empty truck ...

--- Problem 3/20, Round 2, Agent 4/7 ---
Agent 4 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 182.77it/s]
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: WeiboAI/VibeThinker-1.5B (backend=vllm)
[ModelCache] Using max_model_len=131072 for WeiboAI/VibeThinker-1.5B
[ModelCache] Detected 1 GPU(s), 44.3GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 14:25:30 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'WeiboAI/VibeThinker-1.5B'}
INFO 12-04 14:25:30 [model.py:631] Resolved architecture: Qwen2ForCausalLM
WARNING 12-04 14:25:30 [model.py:1921] Your device 'Quadro RTX 8000' (with compute capability 7.5) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.
WARNING 12-04 14:25:30 [model.py:1971] Casting torch.bfloat16 to torch.float16.
INFO 12-04 14:25:30 [model.py:1745] Using max model len 131072
INFO 12-04 14:25:30 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:15<00:00, 15.13s/it, est. speed input: 113.79 toks/s, output: 102.36 toks/s]Processed prompts: 100%|██████████| 1/1 [00:15<00:00, 15.13s/it, est. speed input: 113.79 toks/s, output: 102.36 toks/s]Processed prompts: 100%|██████████| 1/1 [00:15<00:00, 15.13s/it, est. speed input: 113.79 toks/s, output: 102.36 toks/s]
Agent 4 response: The bridge has a weight limit of 5000 pounds. The combined weight of the driver and an empty truck i...

--- Problem 3/20, Round 2, Agent 5/7 ---
Agent 5 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 116.79it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:03<00:00,  3.98s/it, est. speed input: 433.29 toks/s, output: 98.58 toks/s]Processed prompts: 100%|██████████| 1/1 [00:03<00:00,  3.98s/it, est. speed input: 433.29 toks/s, output: 98.58 toks/s]Processed prompts: 100%|██████████| 1/1 [00:03<00:00,  3.98s/it, est. speed input: 433.29 toks/s, output: 98.58 toks/s]
Agent 5 response: The bridge's weight limit is 5000 pounds. The combined weight of the driver and the empty truck is 3...

--- Problem 3/20, Round 2, Agent 6/7 ---
Agent 6 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 124.70it/s]
[1;36m(EngineCore_DP0 pid=3856511)[0;0m INFO 12-04 14:25:44 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='WeiboAI/VibeThinker-1.5B', speculative_config=None, tokenizer='WeiboAI/VibeThinker-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=WeiboAI/VibeThinker-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=3856511)[0;0m ERROR 12-04 14:25:45 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8
[1;36m(EngineCore_DP0 pid=3856511)[0;0m INFO 12-04 14:25:46 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.7:53703 backend=nccl
[W1204 14:25:46.063687016 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-23-2.rc.tch.harvard.edu]:53703 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=3856511)[0;0m INFO 12-04 14:25:46 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=3856511)[0;0m ERROR 12-04 14:25:46 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=3856511)[0;0m ERROR 12-04 14:25:46 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=3856511)[0;0m ERROR 12-04 14:25:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=3856511)[0;0m ERROR 12-04 14:25:46 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3856511)[0;0m ERROR 12-04 14:25:46 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3856511)[0;0m ERROR 12-04 14:25:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=3856511)[0;0m ERROR 12-04 14:25:46 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=3856511)[0;0m ERROR 12-04 14:25:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=3856511)[0;0m ERROR 12-04 14:25:46 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=3856511)[0;0m ERROR 12-04 14:25:46 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3856511)[0;0m ERROR 12-04 14:25:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=3856511)[0;0m ERROR 12-04 14:25:46 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=3856511)[0;0m ERROR 12-04 14:25:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=3856511)[0;0m ERROR 12-04 14:25:46 [core.py:842]     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=3856511)[0;0m ERROR 12-04 14:25:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=3856511)[0;0m ERROR 12-04 14:25:46 [core.py:842]     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=3856511)[0;0m ERROR 12-04 14:25:46 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3856511)[0;0m ERROR 12-04 14:25:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=3856511)[0;0m ERROR 12-04 14:25:46 [core.py:842]     raise ValueError(
[1;36m(EngineCore_DP0 pid=3856511)[0;0m ERROR 12-04 14:25:46 [core.py:842] ValueError: Free memory on device (7.14/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[1;36m(EngineCore_DP0 pid=3856511)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=3856511)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=3856511)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=3856511)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=3856511)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=3856511)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=3856511)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=3856511)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=3856511)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=3856511)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3856511)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3856511)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=3856511)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=3856511)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=3856511)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=3856511)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3856511)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=3856511)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=3856511)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=3856511)[0;0m     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=3856511)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=3856511)[0;0m     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=3856511)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3856511)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=3856511)[0;0m     raise ValueError(
[1;36m(EngineCore_DP0 pid=3856511)[0;0m ValueError: Free memory on device (7.14/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W1204 14:25:47.342593347 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:15<00:00, 15.94s/it, est. speed input: 107.97 toks/s, output: 103.26 toks/s]Processed prompts: 100%|██████████| 1/1 [00:15<00:00, 15.94s/it, est. speed input: 107.97 toks/s, output: 103.26 toks/s]Processed prompts: 100%|██████████| 1/1 [00:15<00:00, 15.94s/it, est. speed input: 107.97 toks/s, output: 103.26 toks/s]
Agent 6 response: The bridge has a weight limit of 5000 pounds. The combined weight of the driver and the empty truck ...

--- Problem 3/20, Round 2, Agent 7/7 ---
Agent 7 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 174.84it/s]
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: WeiboAI/VibeThinker-1.5B (backend=vllm)
[ModelCache] Using max_model_len=131072 for WeiboAI/VibeThinker-1.5B
[ModelCache] Detected 1 GPU(s), 44.3GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 14:26:08 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'WeiboAI/VibeThinker-1.5B'}
INFO 12-04 14:26:09 [model.py:631] Resolved architecture: Qwen2ForCausalLM
WARNING 12-04 14:26:09 [model.py:1921] Your device 'Quadro RTX 8000' (with compute capability 7.5) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.
WARNING 12-04 14:26:09 [model.py:1971] Casting torch.bfloat16 to torch.float16.
INFO 12-04 14:26:09 [model.py:1745] Using max model len 131072
INFO 12-04 14:26:09 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:14<00:00, 14.51s/it, est. speed input: 118.91 toks/s, output: 103.96 toks/s]Processed prompts: 100%|██████████| 1/1 [00:14<00:00, 14.51s/it, est. speed input: 118.91 toks/s, output: 103.96 toks/s]Processed prompts: 100%|██████████| 1/1 [00:14<00:00, 14.51s/it, est. speed input: 118.91 toks/s, output: 103.96 toks/s]
Agent 7 response: The bridge has a weight limit of 5000 pounds. The combined weight of the driver and the empty truck ...

--- Problem 3/20, Round 3, Agent 1/7 ---
Agent 1 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 53.56it/s]
[1;36m(EngineCore_DP0 pid=3856572)[0;0m INFO 12-04 14:26:23 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='WeiboAI/VibeThinker-1.5B', speculative_config=None, tokenizer='WeiboAI/VibeThinker-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=WeiboAI/VibeThinker-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=3856572)[0;0m ERROR 12-04 14:26:24 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8
[1;36m(EngineCore_DP0 pid=3856572)[0;0m INFO 12-04 14:26:25 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.7:33917 backend=nccl
[W1204 14:26:25.395290114 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-23-2.rc.tch.harvard.edu]:33917 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=3856572)[0;0m INFO 12-04 14:26:25 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=3856572)[0;0m ERROR 12-04 14:26:25 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=3856572)[0;0m ERROR 12-04 14:26:25 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=3856572)[0;0m ERROR 12-04 14:26:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=3856572)[0;0m ERROR 12-04 14:26:25 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3856572)[0;0m ERROR 12-04 14:26:25 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3856572)[0;0m ERROR 12-04 14:26:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=3856572)[0;0m ERROR 12-04 14:26:25 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=3856572)[0;0m ERROR 12-04 14:26:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=3856572)[0;0m ERROR 12-04 14:26:25 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=3856572)[0;0m ERROR 12-04 14:26:25 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3856572)[0;0m ERROR 12-04 14:26:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=3856572)[0;0m ERROR 12-04 14:26:25 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=3856572)[0;0m ERROR 12-04 14:26:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=3856572)[0;0m ERROR 12-04 14:26:25 [core.py:842]     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=3856572)[0;0m ERROR 12-04 14:26:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=3856572)[0;0m ERROR 12-04 14:26:25 [core.py:842]     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=3856572)[0;0m ERROR 12-04 14:26:25 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3856572)[0;0m ERROR 12-04 14:26:25 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=3856572)[0;0m ERROR 12-04 14:26:25 [core.py:842]     raise ValueError(
[1;36m(EngineCore_DP0 pid=3856572)[0;0m ERROR 12-04 14:26:25 [core.py:842] ValueError: Free memory on device (7.14/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[1;36m(EngineCore_DP0 pid=3856572)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=3856572)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=3856572)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=3856572)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=3856572)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=3856572)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=3856572)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=3856572)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=3856572)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=3856572)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3856572)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3856572)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=3856572)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=3856572)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=3856572)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=3856572)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3856572)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=3856572)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=3856572)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=3856572)[0;0m     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=3856572)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=3856572)[0;0m     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=3856572)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3856572)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=3856572)[0;0m     raise ValueError(
[1;36m(EngineCore_DP0 pid=3856572)[0;0m ValueError: Free memory on device (7.14/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W1204 14:26:26.678316453 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:17<00:00, 17.47s/it, est. speed input: 196.26 toks/s, output: 99.90 toks/s]Processed prompts: 100%|██████████| 1/1 [00:17<00:00, 17.47s/it, est. speed input: 196.26 toks/s, output: 99.90 toks/s]Processed prompts: 100%|██████████| 1/1 [00:17<00:00, 17.47s/it, est. speed input: 196.26 toks/s, output: 99.90 toks/s]
Agent 1 response: The bridge has a weight limit of 5000 pounds. The combined weight of the driver and the empty truck ...

--- Problem 3/20, Round 3, Agent 2/7 ---
Agent 2 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 87.21it/s]
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: WeiboAI/VibeThinker-1.5B (backend=vllm)
[ModelCache] Using max_model_len=131072 for WeiboAI/VibeThinker-1.5B
[ModelCache] Detected 1 GPU(s), 44.3GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 14:26:48 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'WeiboAI/VibeThinker-1.5B'}
INFO 12-04 14:26:48 [model.py:631] Resolved architecture: Qwen2ForCausalLM
WARNING 12-04 14:26:48 [model.py:1921] Your device 'Quadro RTX 8000' (with compute capability 7.5) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.
WARNING 12-04 14:26:48 [model.py:1971] Casting torch.bfloat16 to torch.float16.
INFO 12-04 14:26:48 [model.py:1745] Using max model len 131072
INFO 12-04 14:26:48 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:19<00:00, 19.87s/it, est. speed input: 172.68 toks/s, output: 103.83 toks/s]Processed prompts: 100%|██████████| 1/1 [00:19<00:00, 19.87s/it, est. speed input: 172.68 toks/s, output: 103.83 toks/s]Processed prompts: 100%|██████████| 1/1 [00:19<00:00, 19.87s/it, est. speed input: 172.68 toks/s, output: 103.83 toks/s]
Agent 2 response: The bridge has a weight limit of 5000 pounds. The combined weight of the driver and the empty truck ...

--- Problem 3/20, Round 3, Agent 3/7 ---
Agent 3 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 55.65it/s]
[1;36m(EngineCore_DP0 pid=3856716)[0;0m INFO 12-04 14:27:02 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='WeiboAI/VibeThinker-1.5B', speculative_config=None, tokenizer='WeiboAI/VibeThinker-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=WeiboAI/VibeThinker-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=3856716)[0;0m ERROR 12-04 14:27:03 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8
[1;36m(EngineCore_DP0 pid=3856716)[0;0m INFO 12-04 14:27:04 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.7:37555 backend=nccl
[W1204 14:27:04.960316126 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-23-2.rc.tch.harvard.edu]:37555 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=3856716)[0;0m INFO 12-04 14:27:04 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=3856716)[0;0m ERROR 12-04 14:27:04 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=3856716)[0;0m ERROR 12-04 14:27:04 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=3856716)[0;0m ERROR 12-04 14:27:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=3856716)[0;0m ERROR 12-04 14:27:04 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3856716)[0;0m ERROR 12-04 14:27:04 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3856716)[0;0m ERROR 12-04 14:27:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=3856716)[0;0m ERROR 12-04 14:27:04 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=3856716)[0;0m ERROR 12-04 14:27:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=3856716)[0;0m ERROR 12-04 14:27:04 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=3856716)[0;0m ERROR 12-04 14:27:04 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3856716)[0;0m ERROR 12-04 14:27:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=3856716)[0;0m ERROR 12-04 14:27:04 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=3856716)[0;0m ERROR 12-04 14:27:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=3856716)[0;0m ERROR 12-04 14:27:04 [core.py:842]     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=3856716)[0;0m ERROR 12-04 14:27:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=3856716)[0;0m ERROR 12-04 14:27:04 [core.py:842]     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=3856716)[0;0m ERROR 12-04 14:27:04 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3856716)[0;0m ERROR 12-04 14:27:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=3856716)[0;0m ERROR 12-04 14:27:04 [core.py:842]     raise ValueError(
[1;36m(EngineCore_DP0 pid=3856716)[0;0m ERROR 12-04 14:27:04 [core.py:842] ValueError: Free memory on device (7.14/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[1;36m(EngineCore_DP0 pid=3856716)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=3856716)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=3856716)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=3856716)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=3856716)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=3856716)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=3856716)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=3856716)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=3856716)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=3856716)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3856716)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3856716)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=3856716)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=3856716)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=3856716)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=3856716)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3856716)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=3856716)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=3856716)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=3856716)[0;0m     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=3856716)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=3856716)[0;0m     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=3856716)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3856716)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=3856716)[0;0m     raise ValueError(
[1;36m(EngineCore_DP0 pid=3856716)[0;0m ValueError: Free memory on device (7.14/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W1204 14:27:05.248566326 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:15<00:00, 15.74s/it, est. speed input: 218.00 toks/s, output: 98.48 toks/s]Processed prompts: 100%|██████████| 1/1 [00:15<00:00, 15.74s/it, est. speed input: 218.00 toks/s, output: 98.48 toks/s]Processed prompts: 100%|██████████| 1/1 [00:15<00:00, 15.74s/it, est. speed input: 218.00 toks/s, output: 98.48 toks/s]
Agent 3 response: The bridge has a weight limit of 5000 pounds. The combined weight of the driver and the empty truck ...

--- Problem 3/20, Round 3, Agent 4/7 ---
Agent 4 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 61.62it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:13<00:00, 13.94s/it, est. speed input: 246.15 toks/s, output: 104.43 toks/s]Processed prompts: 100%|██████████| 1/1 [00:13<00:00, 13.94s/it, est. speed input: 246.15 toks/s, output: 104.43 toks/s]Processed prompts: 100%|██████████| 1/1 [00:13<00:00, 13.94s/it, est. speed input: 246.15 toks/s, output: 104.43 toks/s]
Agent 4 response: The bridge has a weight limit of 5000 pounds. The combined weight of the driver and the empty truck ...

--- Problem 3/20, Round 3, Agent 5/7 ---
Agent 5 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 95.55it/s]
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: WeiboAI/VibeThinker-1.5B (backend=vllm)
[ModelCache] Using max_model_len=131072 for WeiboAI/VibeThinker-1.5B
[ModelCache] Detected 1 GPU(s), 44.3GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 14:27:26 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'WeiboAI/VibeThinker-1.5B'}
INFO 12-04 14:27:27 [model.py:631] Resolved architecture: Qwen2ForCausalLM
WARNING 12-04 14:27:27 [model.py:1921] Your device 'Quadro RTX 8000' (with compute capability 7.5) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.
WARNING 12-04 14:27:27 [model.py:1971] Casting torch.bfloat16 to torch.float16.
INFO 12-04 14:27:27 [model.py:1745] Using max model len 131072
INFO 12-04 14:27:27 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:19<00:00, 19.52s/it, est. speed input: 175.88 toks/s, output: 101.03 toks/s]Processed prompts: 100%|██████████| 1/1 [00:19<00:00, 19.52s/it, est. speed input: 175.88 toks/s, output: 101.03 toks/s]Processed prompts: 100%|██████████| 1/1 [00:19<00:00, 19.52s/it, est. speed input: 175.88 toks/s, output: 101.03 toks/s]
Agent 5 response: The bridge has a weight limit of 5000 pounds. The combined weight of the driver and the empty truck ...

--- Problem 3/20, Round 3, Agent 6/7 ---
Agent 6 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 67.20it/s]
[1;36m(EngineCore_DP0 pid=3857679)[0;0m INFO 12-04 14:27:41 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='WeiboAI/VibeThinker-1.5B', speculative_config=None, tokenizer='WeiboAI/VibeThinker-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=WeiboAI/VibeThinker-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=3857679)[0;0m ERROR 12-04 14:27:42 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8
[1;36m(EngineCore_DP0 pid=3857679)[0;0m INFO 12-04 14:27:43 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.7:48795 backend=nccl
[W1204 14:27:43.486734385 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-23-2.rc.tch.harvard.edu]:48795 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=3857679)[0;0m INFO 12-04 14:27:43 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=3857679)[0;0m ERROR 12-04 14:27:43 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=3857679)[0;0m ERROR 12-04 14:27:43 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=3857679)[0;0m ERROR 12-04 14:27:43 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=3857679)[0;0m ERROR 12-04 14:27:43 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3857679)[0;0m ERROR 12-04 14:27:43 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3857679)[0;0m ERROR 12-04 14:27:43 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=3857679)[0;0m ERROR 12-04 14:27:43 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=3857679)[0;0m ERROR 12-04 14:27:43 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=3857679)[0;0m ERROR 12-04 14:27:43 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=3857679)[0;0m ERROR 12-04 14:27:43 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3857679)[0;0m ERROR 12-04 14:27:43 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=3857679)[0;0m ERROR 12-04 14:27:43 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=3857679)[0;0m ERROR 12-04 14:27:43 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=3857679)[0;0m ERROR 12-04 14:27:43 [core.py:842]     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=3857679)[0;0m ERROR 12-04 14:27:43 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=3857679)[0;0m ERROR 12-04 14:27:43 [core.py:842]     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=3857679)[0;0m ERROR 12-04 14:27:43 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3857679)[0;0m ERROR 12-04 14:27:43 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=3857679)[0;0m ERROR 12-04 14:27:43 [core.py:842]     raise ValueError(
[1;36m(EngineCore_DP0 pid=3857679)[0;0m ERROR 12-04 14:27:43 [core.py:842] ValueError: Free memory on device (7.14/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[1;36m(EngineCore_DP0 pid=3857679)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=3857679)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=3857679)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=3857679)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=3857679)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=3857679)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=3857679)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=3857679)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=3857679)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=3857679)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3857679)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3857679)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=3857679)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=3857679)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=3857679)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=3857679)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3857679)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=3857679)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=3857679)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=3857679)[0;0m     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=3857679)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=3857679)[0;0m     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=3857679)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3857679)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=3857679)[0;0m     raise ValueError(
[1;36m(EngineCore_DP0 pid=3857679)[0;0m ValueError: Free memory on device (7.14/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W1204 14:27:44.760183006 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:10<00:00, 10.73s/it, est. speed input: 319.81 toks/s, output: 100.76 toks/s]Processed prompts: 100%|██████████| 1/1 [00:10<00:00, 10.73s/it, est. speed input: 319.81 toks/s, output: 100.76 toks/s]Processed prompts: 100%|██████████| 1/1 [00:10<00:00, 10.73s/it, est. speed input: 319.81 toks/s, output: 100.76 toks/s]
Agent 6 response: The bridge has a weight limit of 5000 pounds. The combined weight of the driver and the empty truck ...

--- Problem 3/20, Round 3, Agent 7/7 ---
Agent 7 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 85.96it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:15<00:00, 15.23s/it, est. speed input: 225.58 toks/s, output: 103.83 toks/s]Processed prompts: 100%|██████████| 1/1 [00:15<00:00, 15.23s/it, est. speed input: 225.58 toks/s, output: 103.83 toks/s]Processed prompts: 100%|██████████| 1/1 [00:15<00:00, 15.23s/it, est. speed input: 225.58 toks/s, output: 103.83 toks/s]
Agent 7 response: The bridge has a weight limit of 5000 pounds. The combined weight of the driver and the empty truck ...

Running accuracy: 1.000 ± 0.000

--- Problem 4/20, Round 1, Agent 1/7 ---
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 969.78it/s]
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: WeiboAI/VibeThinker-1.5B (backend=vllm)
[ModelCache] Using max_model_len=131072 for WeiboAI/VibeThinker-1.5B
[ModelCache] Detected 1 GPU(s), 44.3GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 14:28:06 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'WeiboAI/VibeThinker-1.5B'}
INFO 12-04 14:28:06 [model.py:631] Resolved architecture: Qwen2ForCausalLM
WARNING 12-04 14:28:06 [model.py:1921] Your device 'Quadro RTX 8000' (with compute capability 7.5) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.
WARNING 12-04 14:28:06 [model.py:1971] Casting torch.bfloat16 to torch.float16.
INFO 12-04 14:28:06 [model.py:1745] Using max model len 131072
INFO 12-04 14:28:06 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=3857939)[0;0m INFO 12-04 14:28:20 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='WeiboAI/VibeThinker-1.5B', speculative_config=None, tokenizer='WeiboAI/VibeThinker-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=WeiboAI/VibeThinker-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=3857939)[0;0m ERROR 12-04 14:28:21 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8
[1;36m(EngineCore_DP0 pid=3857939)[0;0m INFO 12-04 14:28:22 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.7:43273 backend=nccl
[W1204 14:28:22.930240842 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-23-2.rc.tch.harvard.edu]:43273 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=3857939)[0;0m INFO 12-04 14:28:22 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=3857939)[0;0m ERROR 12-04 14:28:22 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=3857939)[0;0m ERROR 12-04 14:28:22 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=3857939)[0;0m ERROR 12-04 14:28:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=3857939)[0;0m ERROR 12-04 14:28:22 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3857939)[0;0m ERROR 12-04 14:28:22 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3857939)[0;0m ERROR 12-04 14:28:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=3857939)[0;0m ERROR 12-04 14:28:22 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=3857939)[0;0m ERROR 12-04 14:28:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=3857939)[0;0m ERROR 12-04 14:28:22 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=3857939)[0;0m ERROR 12-04 14:28:22 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3857939)[0;0m ERROR 12-04 14:28:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=3857939)[0;0m ERROR 12-04 14:28:22 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=3857939)[0;0m ERROR 12-04 14:28:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=3857939)[0;0m ERROR 12-04 14:28:22 [core.py:842]     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=3857939)[0;0m ERROR 12-04 14:28:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=3857939)[0;0m ERROR 12-04 14:28:22 [core.py:842]     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=3857939)[0;0m ERROR 12-04 14:28:22 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3857939)[0;0m ERROR 12-04 14:28:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=3857939)[0;0m ERROR 12-04 14:28:22 [core.py:842]     raise ValueError(
[1;36m(EngineCore_DP0 pid=3857939)[0;0m ERROR 12-04 14:28:22 [core.py:842] ValueError: Free memory on device (7.14/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[1;36m(EngineCore_DP0 pid=3857939)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=3857939)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=3857939)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=3857939)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=3857939)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=3857939)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=3857939)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=3857939)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=3857939)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=3857939)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3857939)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3857939)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=3857939)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=3857939)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=3857939)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=3857939)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3857939)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=3857939)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=3857939)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=3857939)[0;0m     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=3857939)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=3857939)[0;0m     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=3857939)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3857939)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=3857939)[0;0m     raise ValueError(
[1;36m(EngineCore_DP0 pid=3857939)[0;0m ValueError: Free memory on device (7.14/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W1204 14:28:23.209104664 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:35<00:00, 35.70s/it, est. speed input: 3.56 toks/s, output: 102.93 toks/s]Processed prompts: 100%|██████████| 1/1 [00:35<00:00, 35.70s/it, est. speed input: 3.56 toks/s, output: 102.93 toks/s]Processed prompts: 100%|██████████| 1/1 [00:35<00:00, 35.70s/it, est. speed input: 3.56 toks/s, output: 102.93 toks/s]
Agent 1 response: Tim starts with 7 blue shoe boxes and 9 red shoe boxes. He uses 3 blue shoe boxes, leaving \(7 - 3 =...

--- Problem 4/20, Round 1, Agent 2/7 ---
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 993.44it/s]
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: WeiboAI/VibeThinker-1.5B (backend=vllm)
[ModelCache] Using max_model_len=131072 for WeiboAI/VibeThinker-1.5B
[ModelCache] Detected 1 GPU(s), 44.3GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 14:28:44 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'WeiboAI/VibeThinker-1.5B'}
INFO 12-04 14:28:44 [model.py:631] Resolved architecture: Qwen2ForCausalLM
WARNING 12-04 14:28:44 [model.py:1921] Your device 'Quadro RTX 8000' (with compute capability 7.5) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.
WARNING 12-04 14:28:44 [model.py:1971] Casting torch.bfloat16 to torch.float16.
INFO 12-04 14:28:44 [model.py:1745] Using max model len 131072
INFO 12-04 14:28:44 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=3858098)[0;0m INFO 12-04 14:28:59 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='WeiboAI/VibeThinker-1.5B', speculative_config=None, tokenizer='WeiboAI/VibeThinker-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=WeiboAI/VibeThinker-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=3858098)[0;0m ERROR 12-04 14:29:00 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8
[1;36m(EngineCore_DP0 pid=3858098)[0;0m INFO 12-04 14:29:01 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.7:37365 backend=nccl
[W1204 14:29:01.281571564 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-23-2.rc.tch.harvard.edu]:37365 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=3858098)[0;0m INFO 12-04 14:29:01 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=3858098)[0;0m ERROR 12-04 14:29:01 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=3858098)[0;0m ERROR 12-04 14:29:01 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=3858098)[0;0m ERROR 12-04 14:29:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=3858098)[0;0m ERROR 12-04 14:29:01 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3858098)[0;0m ERROR 12-04 14:29:01 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3858098)[0;0m ERROR 12-04 14:29:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=3858098)[0;0m ERROR 12-04 14:29:01 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=3858098)[0;0m ERROR 12-04 14:29:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=3858098)[0;0m ERROR 12-04 14:29:01 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=3858098)[0;0m ERROR 12-04 14:29:01 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3858098)[0;0m ERROR 12-04 14:29:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=3858098)[0;0m ERROR 12-04 14:29:01 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=3858098)[0;0m ERROR 12-04 14:29:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=3858098)[0;0m ERROR 12-04 14:29:01 [core.py:842]     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=3858098)[0;0m ERROR 12-04 14:29:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=3858098)[0;0m ERROR 12-04 14:29:01 [core.py:842]     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=3858098)[0;0m ERROR 12-04 14:29:01 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3858098)[0;0m ERROR 12-04 14:29:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=3858098)[0;0m ERROR 12-04 14:29:01 [core.py:842]     raise ValueError(
[1;36m(EngineCore_DP0 pid=3858098)[0;0m ERROR 12-04 14:29:01 [core.py:842] ValueError: Free memory on device (7.14/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[1;36m(EngineCore_DP0 pid=3858098)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=3858098)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=3858098)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=3858098)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=3858098)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=3858098)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=3858098)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=3858098)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=3858098)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=3858098)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3858098)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3858098)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=3858098)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=3858098)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=3858098)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=3858098)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3858098)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=3858098)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=3858098)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=3858098)[0;0m     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=3858098)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=3858098)[0;0m     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=3858098)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3858098)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=3858098)[0;0m     raise ValueError(
[1;36m(EngineCore_DP0 pid=3858098)[0;0m ValueError: Free memory on device (7.14/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W1204 14:29:02.561273251 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:25<00:00, 25.40s/it, est. speed input: 5.12 toks/s, output: 101.98 toks/s]Processed prompts: 100%|██████████| 1/1 [00:25<00:00, 25.40s/it, est. speed input: 5.12 toks/s, output: 101.98 toks/s]Processed prompts: 100%|██████████| 1/1 [00:25<00:00, 25.40s/it, est. speed input: 5.12 toks/s, output: 101.98 toks/s]
Agent 2 response: Tim starts with 7 blue shoe boxes and 9 red shoe boxes. He uses 3 blue shoe boxes, leaving \(7 - 3 =...

--- Problem 4/20, Round 1, Agent 3/7 ---
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 835.19it/s]
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: WeiboAI/VibeThinker-1.5B (backend=vllm)
[ModelCache] Using max_model_len=131072 for WeiboAI/VibeThinker-1.5B
[ModelCache] Detected 1 GPU(s), 44.3GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 14:29:24 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'WeiboAI/VibeThinker-1.5B'}
INFO 12-04 14:29:24 [model.py:631] Resolved architecture: Qwen2ForCausalLM
WARNING 12-04 14:29:24 [model.py:1921] Your device 'Quadro RTX 8000' (with compute capability 7.5) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.
WARNING 12-04 14:29:24 [model.py:1971] Casting torch.bfloat16 to torch.float16.
INFO 12-04 14:29:24 [model.py:1745] Using max model len 131072
INFO 12-04 14:29:24 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:25<00:00, 25.88s/it, est. speed input: 5.02 toks/s, output: 104.46 toks/s]Processed prompts: 100%|██████████| 1/1 [00:25<00:00, 25.88s/it, est. speed input: 5.02 toks/s, output: 104.46 toks/s]Processed prompts: 100%|██████████| 1/1 [00:25<00:00, 25.88s/it, est. speed input: 5.02 toks/s, output: 104.46 toks/s]
Agent 3 response: Tim starts with 7 blue shoe boxes and 9 red shoe boxes. After using 3 blue shoe boxes, the number of...

--- Problem 4/20, Round 1, Agent 4/7 ---
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 778.74it/s]
[1;36m(EngineCore_DP0 pid=3858248)[0;0m INFO 12-04 14:29:38 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='WeiboAI/VibeThinker-1.5B', speculative_config=None, tokenizer='WeiboAI/VibeThinker-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=WeiboAI/VibeThinker-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=3858248)[0;0m ERROR 12-04 14:29:39 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8
[1;36m(EngineCore_DP0 pid=3858248)[0;0m INFO 12-04 14:29:40 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.7:60819 backend=nccl
[W1204 14:29:40.059189085 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-23-2.rc.tch.harvard.edu]:60819 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=3858248)[0;0m INFO 12-04 14:29:40 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=3858248)[0;0m ERROR 12-04 14:29:40 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=3858248)[0;0m ERROR 12-04 14:29:40 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=3858248)[0;0m ERROR 12-04 14:29:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=3858248)[0;0m ERROR 12-04 14:29:40 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3858248)[0;0m ERROR 12-04 14:29:40 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3858248)[0;0m ERROR 12-04 14:29:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=3858248)[0;0m ERROR 12-04 14:29:40 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=3858248)[0;0m ERROR 12-04 14:29:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=3858248)[0;0m ERROR 12-04 14:29:40 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=3858248)[0;0m ERROR 12-04 14:29:40 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3858248)[0;0m ERROR 12-04 14:29:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=3858248)[0;0m ERROR 12-04 14:29:40 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=3858248)[0;0m ERROR 12-04 14:29:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=3858248)[0;0m ERROR 12-04 14:29:40 [core.py:842]     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=3858248)[0;0m ERROR 12-04 14:29:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=3858248)[0;0m ERROR 12-04 14:29:40 [core.py:842]     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=3858248)[0;0m ERROR 12-04 14:29:40 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3858248)[0;0m ERROR 12-04 14:29:40 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=3858248)[0;0m ERROR 12-04 14:29:40 [core.py:842]     raise ValueError(
[1;36m(EngineCore_DP0 pid=3858248)[0;0m ERROR 12-04 14:29:40 [core.py:842] ValueError: Free memory on device (7.14/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[1;36m(EngineCore_DP0 pid=3858248)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=3858248)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=3858248)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=3858248)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=3858248)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=3858248)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=3858248)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=3858248)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=3858248)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=3858248)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3858248)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3858248)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=3858248)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=3858248)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=3858248)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=3858248)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3858248)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=3858248)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=3858248)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=3858248)[0;0m     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=3858248)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=3858248)[0;0m     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=3858248)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3858248)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=3858248)[0;0m     raise ValueError(
[1;36m(EngineCore_DP0 pid=3858248)[0;0m ValueError: Free memory on device (7.14/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W1204 14:29:41.320445541 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:26<00:00, 26.34s/it, est. speed input: 4.97 toks/s, output: 104.16 toks/s]Processed prompts: 100%|██████████| 1/1 [00:26<00:00, 26.34s/it, est. speed input: 4.97 toks/s, output: 104.16 toks/s]Processed prompts: 100%|██████████| 1/1 [00:26<00:00, 26.34s/it, est. speed input: 4.97 toks/s, output: 104.16 toks/s]
Agent 4 response: Tim starts with 7 blue shoe boxes and 9 red shoe boxes. He uses 3 blue shoe boxes, leaving \(7 - 3 =...

--- Problem 4/20, Round 1, Agent 5/7 ---
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 997.22it/s]
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: WeiboAI/VibeThinker-1.5B (backend=vllm)
[ModelCache] Using max_model_len=131072 for WeiboAI/VibeThinker-1.5B
[ModelCache] Detected 1 GPU(s), 44.3GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 14:30:02 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'WeiboAI/VibeThinker-1.5B'}
INFO 12-04 14:30:03 [model.py:631] Resolved architecture: Qwen2ForCausalLM
WARNING 12-04 14:30:03 [model.py:1921] Your device 'Quadro RTX 8000' (with compute capability 7.5) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.
WARNING 12-04 14:30:03 [model.py:1971] Casting torch.bfloat16 to torch.float16.
INFO 12-04 14:30:03 [model.py:1745] Using max model len 131072
INFO 12-04 14:30:03 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=3858308)[0;0m INFO 12-04 14:30:18 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='WeiboAI/VibeThinker-1.5B', speculative_config=None, tokenizer='WeiboAI/VibeThinker-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=WeiboAI/VibeThinker-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=3858308)[0;0m ERROR 12-04 14:30:19 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8
[1;36m(EngineCore_DP0 pid=3858308)[0;0m INFO 12-04 14:30:20 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.7:37323 backend=nccl
[W1204 14:30:20.411674909 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-23-2.rc.tch.harvard.edu]:37323 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=3858308)[0;0m INFO 12-04 14:30:20 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=3858308)[0;0m ERROR 12-04 14:30:20 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=3858308)[0;0m ERROR 12-04 14:30:20 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=3858308)[0;0m ERROR 12-04 14:30:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=3858308)[0;0m ERROR 12-04 14:30:20 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3858308)[0;0m ERROR 12-04 14:30:20 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3858308)[0;0m ERROR 12-04 14:30:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=3858308)[0;0m ERROR 12-04 14:30:20 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=3858308)[0;0m ERROR 12-04 14:30:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=3858308)[0;0m ERROR 12-04 14:30:20 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=3858308)[0;0m ERROR 12-04 14:30:20 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3858308)[0;0m ERROR 12-04 14:30:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=3858308)[0;0m ERROR 12-04 14:30:20 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=3858308)[0;0m ERROR 12-04 14:30:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=3858308)[0;0m ERROR 12-04 14:30:20 [core.py:842]     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=3858308)[0;0m ERROR 12-04 14:30:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=3858308)[0;0m ERROR 12-04 14:30:20 [core.py:842]     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=3858308)[0;0m ERROR 12-04 14:30:20 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3858308)[0;0m ERROR 12-04 14:30:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=3858308)[0;0m ERROR 12-04 14:30:20 [core.py:842]     raise ValueError(
[1;36m(EngineCore_DP0 pid=3858308)[0;0m ERROR 12-04 14:30:20 [core.py:842] ValueError: Free memory on device (7.14/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[1;36m(EngineCore_DP0 pid=3858308)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=3858308)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=3858308)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=3858308)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=3858308)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=3858308)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=3858308)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=3858308)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=3858308)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=3858308)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3858308)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3858308)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=3858308)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=3858308)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=3858308)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=3858308)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3858308)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=3858308)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=3858308)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=3858308)[0;0m     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=3858308)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=3858308)[0;0m     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=3858308)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3858308)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=3858308)[0;0m     raise ValueError(
[1;36m(EngineCore_DP0 pid=3858308)[0;0m ValueError: Free memory on device (7.14/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W1204 14:30:21.690098180 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:27<00:00, 27.12s/it, est. speed input: 4.87 toks/s, output: 102.14 toks/s]Processed prompts: 100%|██████████| 1/1 [00:27<00:00, 27.12s/it, est. speed input: 4.87 toks/s, output: 102.14 toks/s]Processed prompts: 100%|██████████| 1/1 [00:27<00:00, 27.12s/it, est. speed input: 4.87 toks/s, output: 102.14 toks/s]
Agent 5 response: Tim starts with 7 blue shoe boxes and 9 red shoe boxes. 

For the blue shoe boxes:
- He uses 3, so t...

--- Problem 4/20, Round 1, Agent 6/7 ---
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1006.31it/s]
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: WeiboAI/VibeThinker-1.5B (backend=vllm)
[ModelCache] Using max_model_len=131072 for WeiboAI/VibeThinker-1.5B
[ModelCache] Detected 1 GPU(s), 44.3GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 14:30:43 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'WeiboAI/VibeThinker-1.5B'}
INFO 12-04 14:30:43 [model.py:631] Resolved architecture: Qwen2ForCausalLM
WARNING 12-04 14:30:43 [model.py:1921] Your device 'Quadro RTX 8000' (with compute capability 7.5) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.
WARNING 12-04 14:30:43 [model.py:1971] Casting torch.bfloat16 to torch.float16.
INFO 12-04 14:30:43 [model.py:1745] Using max model len 131072
INFO 12-04 14:30:43 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:28<00:00, 28.69s/it, est. speed input: 4.53 toks/s, output: 104.21 toks/s]Processed prompts: 100%|██████████| 1/1 [00:28<00:00, 28.69s/it, est. speed input: 4.53 toks/s, output: 104.21 toks/s]Processed prompts: 100%|██████████| 1/1 [00:28<00:00, 28.69s/it, est. speed input: 4.53 toks/s, output: 104.21 toks/s]
Agent 6 response: Tim starts with 7 blue shoe boxes and 9 red shoe boxes, giving a total of 16 shoe boxes.  
He uses 3...

--- Problem 4/20, Round 1, Agent 7/7 ---
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 757.09it/s]
[1;36m(EngineCore_DP0 pid=3858371)[0;0m INFO 12-04 14:30:58 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='WeiboAI/VibeThinker-1.5B', speculative_config=None, tokenizer='WeiboAI/VibeThinker-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=WeiboAI/VibeThinker-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=3858371)[0;0m ERROR 12-04 14:30:59 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8
[1;36m(EngineCore_DP0 pid=3858371)[0;0m INFO 12-04 14:31:00 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.7:58529 backend=nccl
[W1204 14:31:00.359386650 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-23-2.rc.tch.harvard.edu]:58529 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=3858371)[0;0m INFO 12-04 14:31:00 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=3858371)[0;0m ERROR 12-04 14:31:00 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=3858371)[0;0m ERROR 12-04 14:31:00 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=3858371)[0;0m ERROR 12-04 14:31:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=3858371)[0;0m ERROR 12-04 14:31:00 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3858371)[0;0m ERROR 12-04 14:31:00 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3858371)[0;0m ERROR 12-04 14:31:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=3858371)[0;0m ERROR 12-04 14:31:00 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=3858371)[0;0m ERROR 12-04 14:31:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=3858371)[0;0m ERROR 12-04 14:31:00 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=3858371)[0;0m ERROR 12-04 14:31:00 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3858371)[0;0m ERROR 12-04 14:31:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=3858371)[0;0m ERROR 12-04 14:31:00 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=3858371)[0;0m ERROR 12-04 14:31:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=3858371)[0;0m ERROR 12-04 14:31:00 [core.py:842]     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=3858371)[0;0m ERROR 12-04 14:31:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=3858371)[0;0m ERROR 12-04 14:31:00 [core.py:842]     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=3858371)[0;0m ERROR 12-04 14:31:00 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3858371)[0;0m ERROR 12-04 14:31:00 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=3858371)[0;0m ERROR 12-04 14:31:00 [core.py:842]     raise ValueError(
[1;36m(EngineCore_DP0 pid=3858371)[0;0m ERROR 12-04 14:31:00 [core.py:842] ValueError: Free memory on device (7.14/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[1;36m(EngineCore_DP0 pid=3858371)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=3858371)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=3858371)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=3858371)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=3858371)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=3858371)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=3858371)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=3858371)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=3858371)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=3858371)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3858371)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3858371)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=3858371)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=3858371)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=3858371)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=3858371)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3858371)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=3858371)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=3858371)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=3858371)[0;0m     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=3858371)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=3858371)[0;0m     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=3858371)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3858371)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=3858371)[0;0m     raise ValueError(
[1;36m(EngineCore_DP0 pid=3858371)[0;0m ValueError: Free memory on device (7.14/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W1204 14:31:01.653570007 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: WeiboAI/VibeThinker-1.5B (backend=vllm)
[ModelCache] Using max_model_len=131072 for WeiboAI/VibeThinker-1.5B
[ModelCache] Detected 1 GPU(s), 44.3GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 14:31:23 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'WeiboAI/VibeThinker-1.5B'}
INFO 12-04 14:31:23 [model.py:631] Resolved architecture: Qwen2ForCausalLM
WARNING 12-04 14:31:23 [model.py:1921] Your device 'Quadro RTX 8000' (with compute capability 7.5) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.
WARNING 12-04 14:31:23 [model.py:1971] Casting torch.bfloat16 to torch.float16.
INFO 12-04 14:31:23 [model.py:1745] Using max model len 131072
INFO 12-04 14:31:23 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=3858456)[0;0m INFO 12-04 14:31:39 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='WeiboAI/VibeThinker-1.5B', speculative_config=None, tokenizer='WeiboAI/VibeThinker-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=WeiboAI/VibeThinker-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=3858456)[0;0m ERROR 12-04 14:31:40 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8
[1;36m(EngineCore_DP0 pid=3858456)[0;0m INFO 12-04 14:31:41 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.7:43187 backend=nccl
[W1204 14:31:41.102796060 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-23-2.rc.tch.harvard.edu]:43187 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=3858456)[0;0m INFO 12-04 14:31:41 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=3858456)[0;0m ERROR 12-04 14:31:41 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=3858456)[0;0m ERROR 12-04 14:31:41 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=3858456)[0;0m ERROR 12-04 14:31:41 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=3858456)[0;0m ERROR 12-04 14:31:41 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3858456)[0;0m ERROR 12-04 14:31:41 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3858456)[0;0m ERROR 12-04 14:31:41 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=3858456)[0;0m ERROR 12-04 14:31:41 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=3858456)[0;0m ERROR 12-04 14:31:41 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=3858456)[0;0m ERROR 12-04 14:31:41 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=3858456)[0;0m ERROR 12-04 14:31:41 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3858456)[0;0m ERROR 12-04 14:31:41 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=3858456)[0;0m ERROR 12-04 14:31:41 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=3858456)[0;0m ERROR 12-04 14:31:41 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=3858456)[0;0m ERROR 12-04 14:31:41 [core.py:842]     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=3858456)[0;0m ERROR 12-04 14:31:41 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=3858456)[0;0m ERROR 12-04 14:31:41 [core.py:842]     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=3858456)[0;0m ERROR 12-04 14:31:41 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3858456)[0;0m ERROR 12-04 14:31:41 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=3858456)[0;0m ERROR 12-04 14:31:41 [core.py:842]     raise ValueError(
[1;36m(EngineCore_DP0 pid=3858456)[0;0m ERROR 12-04 14:31:41 [core.py:842] ValueError: Free memory on device (7.14/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[1;36m(EngineCore_DP0 pid=3858456)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=3858456)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=3858456)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=3858456)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=3858456)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=3858456)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=3858456)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=3858456)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=3858456)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=3858456)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3858456)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3858456)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=3858456)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=3858456)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=3858456)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=3858456)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3858456)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=3858456)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=3858456)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=3858456)[0;0m     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=3858456)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=3858456)[0;0m     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=3858456)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3858456)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=3858456)[0;0m     raise ValueError(
[1;36m(EngineCore_DP0 pid=3858456)[0;0m ValueError: Free memory on device (7.14/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W1204 14:31:42.390869959 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [01:08<00:00, 68.50s/it, est. speed input: 1.96 toks/s, output: 102.57 toks/s]Processed prompts: 100%|██████████| 1/1 [01:08<00:00, 68.50s/it, est. speed input: 1.96 toks/s, output: 102.57 toks/s]Processed prompts: 100%|██████████| 1/1 [01:08<00:00, 68.50s/it, est. speed input: 1.96 toks/s, output: 102.57 toks/s]
Agent 7 response: Tim starts with 7 blue shoe boxes and 9 red shoe boxes. He uses 3 blue shoe boxes, leaving \(7 - 3 =...

--- Problem 4/20, Round 2, Agent 1/7 ---
Agent 1 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 231.81it/s]
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: WeiboAI/VibeThinker-1.5B (backend=vllm)
[ModelCache] Using max_model_len=131072 for WeiboAI/VibeThinker-1.5B
[ModelCache] Detected 1 GPU(s), 44.3GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 14:32:03 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'WeiboAI/VibeThinker-1.5B'}
INFO 12-04 14:32:04 [model.py:631] Resolved architecture: Qwen2ForCausalLM
WARNING 12-04 14:32:04 [model.py:1921] Your device 'Quadro RTX 8000' (with compute capability 7.5) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.
WARNING 12-04 14:32:04 [model.py:1971] Casting torch.bfloat16 to torch.float16.
INFO 12-04 14:32:04 [model.py:1745] Using max model len 131072
INFO 12-04 14:32:04 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:11<00:00, 11.18s/it, est. speed input: 107.97 toks/s, output: 100.46 toks/s]Processed prompts: 100%|██████████| 1/1 [00:11<00:00, 11.18s/it, est. speed input: 107.97 toks/s, output: 100.46 toks/s]Processed prompts: 100%|██████████| 1/1 [00:11<00:00, 11.18s/it, est. speed input: 107.97 toks/s, output: 100.46 toks/s]
Agent 1 response: Tim starts with 7 blue shoe boxes and 9 red shoe boxes.  

He uses 3 blue shoe boxes, leaving \(7 - ...

--- Problem 4/20, Round 2, Agent 2/7 ---
Agent 2 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 154.59it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.36s/it, est. speed input: 513.59 toks/s, output: 99.32 toks/s]Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.36s/it, est. speed input: 513.59 toks/s, output: 99.32 toks/s]Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.36s/it, est. speed input: 513.59 toks/s, output: 99.32 toks/s]
Agent 2 response: Tim starts with 7 blue shoe boxes and 9 red shoe boxes.  
He uses 3 blue shoe boxes, leaving \(7 - 3...

--- Problem 4/20, Round 2, Agent 3/7 ---
Agent 3 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 148.56it/s]
[1;36m(EngineCore_DP0 pid=3858620)[0;0m INFO 12-04 14:32:19 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='WeiboAI/VibeThinker-1.5B', speculative_config=None, tokenizer='WeiboAI/VibeThinker-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=WeiboAI/VibeThinker-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=3858620)[0;0m ERROR 12-04 14:32:20 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8
[1;36m(EngineCore_DP0 pid=3858620)[0;0m INFO 12-04 14:32:21 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.7:60623 backend=nccl
[W1204 14:32:21.160066785 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-23-2.rc.tch.harvard.edu]:60623 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=3858620)[0;0m INFO 12-04 14:32:21 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=3858620)[0;0m ERROR 12-04 14:32:21 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=3858620)[0;0m ERROR 12-04 14:32:21 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=3858620)[0;0m ERROR 12-04 14:32:21 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=3858620)[0;0m ERROR 12-04 14:32:21 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3858620)[0;0m ERROR 12-04 14:32:21 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3858620)[0;0m ERROR 12-04 14:32:21 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=3858620)[0;0m ERROR 12-04 14:32:21 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=3858620)[0;0m ERROR 12-04 14:32:21 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=3858620)[0;0m ERROR 12-04 14:32:21 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=3858620)[0;0m ERROR 12-04 14:32:21 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3858620)[0;0m ERROR 12-04 14:32:21 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=3858620)[0;0m ERROR 12-04 14:32:21 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=3858620)[0;0m ERROR 12-04 14:32:21 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=3858620)[0;0m ERROR 12-04 14:32:21 [core.py:842]     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=3858620)[0;0m ERROR 12-04 14:32:21 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=3858620)[0;0m ERROR 12-04 14:32:21 [core.py:842]     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=3858620)[0;0m ERROR 12-04 14:32:21 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3858620)[0;0m ERROR 12-04 14:32:21 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=3858620)[0;0m ERROR 12-04 14:32:21 [core.py:842]     raise ValueError(
[1;36m(EngineCore_DP0 pid=3858620)[0;0m ERROR 12-04 14:32:21 [core.py:842] ValueError: Free memory on device (7.14/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[1;36m(EngineCore_DP0 pid=3858620)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=3858620)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=3858620)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=3858620)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=3858620)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=3858620)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=3858620)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=3858620)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=3858620)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=3858620)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3858620)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3858620)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=3858620)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=3858620)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=3858620)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=3858620)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3858620)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=3858620)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=3858620)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=3858620)[0;0m     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=3858620)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=3858620)[0;0m     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=3858620)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3858620)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=3858620)[0;0m     raise ValueError(
[1;36m(EngineCore_DP0 pid=3858620)[0;0m ValueError: Free memory on device (7.14/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W1204 14:32:22.439107953 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:06<00:00,  6.53s/it, est. speed input: 185.21 toks/s, output: 99.64 toks/s]Processed prompts: 100%|██████████| 1/1 [00:06<00:00,  6.53s/it, est. speed input: 185.21 toks/s, output: 99.64 toks/s]Processed prompts: 100%|██████████| 1/1 [00:06<00:00,  6.53s/it, est. speed input: 185.21 toks/s, output: 99.64 toks/s]
Agent 3 response: Tim starts with 7 blue shoe boxes and 9 red shoe boxes. After using 3 blue shoe boxes, the remaining...

--- Problem 4/20, Round 2, Agent 4/7 ---
Agent 4 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 159.44it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:06<00:00,  6.02s/it, est. speed input: 201.17 toks/s, output: 105.48 toks/s]Processed prompts: 100%|██████████| 1/1 [00:06<00:00,  6.02s/it, est. speed input: 201.17 toks/s, output: 105.48 toks/s]Processed prompts: 100%|██████████| 1/1 [00:06<00:00,  6.02s/it, est. speed input: 201.17 toks/s, output: 105.48 toks/s]
Agent 4 response: Tim starts with 7 blue shoe boxes and 9 red shoe boxes. After using 3 blue shoe boxes, the remaining...

--- Problem 4/20, Round 2, Agent 5/7 ---
Agent 5 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 243.11it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:08<00:00,  8.18s/it, est. speed input: 148.25 toks/s, output: 105.81 toks/s]Processed prompts: 100%|██████████| 1/1 [00:08<00:00,  8.18s/it, est. speed input: 148.25 toks/s, output: 105.81 toks/s]Processed prompts: 100%|██████████| 1/1 [00:08<00:00,  8.18s/it, est. speed input: 148.25 toks/s, output: 105.81 toks/s]
Agent 5 response: Tim starts with 7 blue shoe boxes and 9 red shoe boxes. After using 3 blue shoe boxes, the remaining...

--- Problem 4/20, Round 2, Agent 6/7 ---
Agent 6 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 229.55it/s]
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: WeiboAI/VibeThinker-1.5B (backend=vllm)
[ModelCache] Using max_model_len=131072 for WeiboAI/VibeThinker-1.5B
[ModelCache] Detected 1 GPU(s), 44.3GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 14:32:43 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'WeiboAI/VibeThinker-1.5B'}
INFO 12-04 14:32:44 [model.py:631] Resolved architecture: Qwen2ForCausalLM
WARNING 12-04 14:32:44 [model.py:1921] Your device 'Quadro RTX 8000' (with compute capability 7.5) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.
WARNING 12-04 14:32:44 [model.py:1971] Casting torch.bfloat16 to torch.float16.
INFO 12-04 14:32:44 [model.py:1745] Using max model len 131072
INFO 12-04 14:32:44 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:10<00:00, 10.92s/it, est. speed input: 110.76 toks/s, output: 103.43 toks/s]Processed prompts: 100%|██████████| 1/1 [00:10<00:00, 10.92s/it, est. speed input: 110.76 toks/s, output: 103.43 toks/s]Processed prompts: 100%|██████████| 1/1 [00:10<00:00, 10.93s/it, est. speed input: 110.76 toks/s, output: 103.43 toks/s]
Agent 6 response: Tim starts with 7 blue shoe boxes and 9 red shoe boxes.  
After using 3 blue shoe boxes, the number ...

--- Problem 4/20, Round 2, Agent 7/7 ---
Agent 7 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 174.89it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.48s/it, est. speed input: 489.49 toks/s, output: 98.78 toks/s]Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.48s/it, est. speed input: 489.49 toks/s, output: 98.78 toks/s]Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.48s/it, est. speed input: 489.49 toks/s, output: 98.78 toks/s]
Agent 7 response: Tim starts with 7 blue shoe boxes and 9 red shoe boxes.  
After using 3 blue shoe boxes, the remaini...

--- Problem 4/20, Round 3, Agent 1/7 ---
Agent 1 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 102.99it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.45s/it, est. speed input: 402.85 toks/s, output: 100.21 toks/s]Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.45s/it, est. speed input: 402.85 toks/s, output: 100.21 toks/s]Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.45s/it, est. speed input: 402.85 toks/s, output: 100.21 toks/s]
Agent 1 response: Tim starts with 7 blue shoe boxes and 9 red shoe boxes.  
After using 3 blue shoe boxes, the remaini...

--- Problem 4/20, Round 3, Agent 2/7 ---
Agent 2 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 113.47it/s]
[1;36m(EngineCore_DP0 pid=3858794)[0;0m INFO 12-04 14:33:01 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='WeiboAI/VibeThinker-1.5B', speculative_config=None, tokenizer='WeiboAI/VibeThinker-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=WeiboAI/VibeThinker-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=3858794)[0;0m ERROR 12-04 14:33:02 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8
[1;36m(EngineCore_DP0 pid=3858794)[0;0m INFO 12-04 14:33:03 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.7:57385 backend=nccl
[W1204 14:33:03.406325439 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-23-2.rc.tch.harvard.edu]:57385 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=3858794)[0;0m INFO 12-04 14:33:03 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=3858794)[0;0m ERROR 12-04 14:33:03 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=3858794)[0;0m ERROR 12-04 14:33:03 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=3858794)[0;0m ERROR 12-04 14:33:03 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=3858794)[0;0m ERROR 12-04 14:33:03 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3858794)[0;0m ERROR 12-04 14:33:03 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3858794)[0;0m ERROR 12-04 14:33:03 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=3858794)[0;0m ERROR 12-04 14:33:03 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=3858794)[0;0m ERROR 12-04 14:33:03 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=3858794)[0;0m ERROR 12-04 14:33:03 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=3858794)[0;0m ERROR 12-04 14:33:03 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3858794)[0;0m ERROR 12-04 14:33:03 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=3858794)[0;0m ERROR 12-04 14:33:03 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=3858794)[0;0m ERROR 12-04 14:33:03 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=3858794)[0;0m ERROR 12-04 14:33:03 [core.py:842]     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=3858794)[0;0m ERROR 12-04 14:33:03 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=3858794)[0;0m ERROR 12-04 14:33:03 [core.py:842]     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=3858794)[0;0m ERROR 12-04 14:33:03 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3858794)[0;0m ERROR 12-04 14:33:03 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=3858794)[0;0m ERROR 12-04 14:33:03 [core.py:842]     raise ValueError(
[1;36m(EngineCore_DP0 pid=3858794)[0;0m ERROR 12-04 14:33:03 [core.py:842] ValueError: Free memory on device (7.14/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[1;36m(EngineCore_DP0 pid=3858794)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=3858794)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=3858794)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=3858794)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=3858794)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=3858794)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=3858794)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=3858794)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=3858794)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=3858794)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3858794)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3858794)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=3858794)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=3858794)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=3858794)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=3858794)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3858794)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=3858794)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=3858794)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=3858794)[0;0m     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=3858794)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=3858794)[0;0m     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=3858794)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3858794)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=3858794)[0;0m     raise ValueError(
[1;36m(EngineCore_DP0 pid=3858794)[0;0m ValueError: Free memory on device (7.14/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W1204 14:33:04.693314720 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:11<00:00, 11.02s/it, est. speed input: 199.44 toks/s, output: 100.53 toks/s]Processed prompts: 100%|██████████| 1/1 [00:11<00:00, 11.02s/it, est. speed input: 199.44 toks/s, output: 100.53 toks/s]Processed prompts: 100%|██████████| 1/1 [00:11<00:00, 11.02s/it, est. speed input: 199.44 toks/s, output: 100.53 toks/s]
Agent 2 response: Tim starts with 7 blue shoe boxes and 9 red shoe boxes.  
He uses 3 blue shoe boxes, leaving \(7 - 3...

--- Problem 4/20, Round 3, Agent 3/7 ---
Agent 3 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 144.71it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:06<00:00,  6.65s/it, est. speed input: 330.46 toks/s, output: 105.09 toks/s]Processed prompts: 100%|██████████| 1/1 [00:06<00:00,  6.65s/it, est. speed input: 330.46 toks/s, output: 105.09 toks/s]Processed prompts: 100%|██████████| 1/1 [00:06<00:00,  6.65s/it, est. speed input: 330.46 toks/s, output: 105.09 toks/s]
Agent 3 response: Tim starts with 7 blue shoe boxes and 9 red shoe boxes.  
After using 3 blue shoe boxes, the remaini...

--- Problem 4/20, Round 3, Agent 4/7 ---
Agent 4 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 148.67it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.04s/it, est. speed input: 436.71 toks/s, output: 104.86 toks/s]Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.04s/it, est. speed input: 436.71 toks/s, output: 104.86 toks/s]Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.04s/it, est. speed input: 436.71 toks/s, output: 104.86 toks/s]
Agent 4 response: Tim starts with 7 blue shoe boxes and uses 3, leaving \(7 - 3 = 4\) blue shoe boxes. He has 9 red sh...

--- Problem 4/20, Round 3, Agent 5/7 ---
Agent 5 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 146.54it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:06<00:00,  6.46s/it, est. speed input: 340.40 toks/s, output: 105.21 toks/s]Processed prompts: 100%|██████████| 1/1 [00:06<00:00,  6.46s/it, est. speed input: 340.40 toks/s, output: 105.21 toks/s]Processed prompts: 100%|██████████| 1/1 [00:06<00:00,  6.46s/it, est. speed input: 340.40 toks/s, output: 105.21 toks/s]
Agent 5 response: Tim starts with 7 blue shoe boxes and 9 red shoe boxes.  
After using 3 blue shoe boxes, the remaini...

--- Problem 4/20, Round 3, Agent 6/7 ---
Agent 6 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 144.17it/s]
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: WeiboAI/VibeThinker-1.5B (backend=vllm)
[ModelCache] Using max_model_len=131072 for WeiboAI/VibeThinker-1.5B
[ModelCache] Detected 1 GPU(s), 44.3GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 14:33:26 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'WeiboAI/VibeThinker-1.5B'}
INFO 12-04 14:33:26 [model.py:631] Resolved architecture: Qwen2ForCausalLM
WARNING 12-04 14:33:26 [model.py:1921] Your device 'Quadro RTX 8000' (with compute capability 7.5) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.
WARNING 12-04 14:33:26 [model.py:1971] Casting torch.bfloat16 to torch.float16.
INFO 12-04 14:33:26 [model.py:1745] Using max model len 131072
INFO 12-04 14:33:26 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.32s/it, est. speed input: 300.43 toks/s, output: 99.50 toks/s]Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.32s/it, est. speed input: 300.43 toks/s, output: 99.50 toks/s]Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.32s/it, est. speed input: 300.43 toks/s, output: 99.50 toks/s]
Agent 6 response: Tim starts with 7 blue shoe boxes and 9 red shoe boxes.  
After using 3 blue shoe boxes, the remaini...

--- Problem 4/20, Round 3, Agent 7/7 ---
Agent 7 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 86.18it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:03<00:00,  3.09s/it, est. speed input: 713.69 toks/s, output: 98.53 toks/s]Processed prompts: 100%|██████████| 1/1 [00:03<00:00,  3.09s/it, est. speed input: 713.69 toks/s, output: 98.53 toks/s]Processed prompts: 100%|██████████| 1/1 [00:03<00:00,  3.09s/it, est. speed input: 713.69 toks/s, output: 98.53 toks/s]
Agent 7 response: Tim starts with 7 blue shoe boxes and 9 red shoe boxes.  
He uses 3 blue shoe boxes, leaving \(7 - 3...

Running accuracy: 1.000 ± 0.000

--- Problem 5/20, Round 1, Agent 1/7 ---
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 709.82it/s]
[1;36m(EngineCore_DP0 pid=3858852)[0;0m INFO 12-04 14:33:40 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='WeiboAI/VibeThinker-1.5B', speculative_config=None, tokenizer='WeiboAI/VibeThinker-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=WeiboAI/VibeThinker-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=3858852)[0;0m ERROR 12-04 14:33:41 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8
[1;36m(EngineCore_DP0 pid=3858852)[0;0m INFO 12-04 14:33:42 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.7:37807 backend=nccl
[W1204 14:33:42.314214213 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-23-2.rc.tch.harvard.edu]:37807 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=3858852)[0;0m INFO 12-04 14:33:42 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=3858852)[0;0m ERROR 12-04 14:33:42 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=3858852)[0;0m ERROR 12-04 14:33:42 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=3858852)[0;0m ERROR 12-04 14:33:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=3858852)[0;0m ERROR 12-04 14:33:42 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3858852)[0;0m ERROR 12-04 14:33:42 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3858852)[0;0m ERROR 12-04 14:33:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=3858852)[0;0m ERROR 12-04 14:33:42 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=3858852)[0;0m ERROR 12-04 14:33:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=3858852)[0;0m ERROR 12-04 14:33:42 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=3858852)[0;0m ERROR 12-04 14:33:42 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3858852)[0;0m ERROR 12-04 14:33:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=3858852)[0;0m ERROR 12-04 14:33:42 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=3858852)[0;0m ERROR 12-04 14:33:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=3858852)[0;0m ERROR 12-04 14:33:42 [core.py:842]     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=3858852)[0;0m ERROR 12-04 14:33:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=3858852)[0;0m ERROR 12-04 14:33:42 [core.py:842]     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=3858852)[0;0m ERROR 12-04 14:33:42 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3858852)[0;0m ERROR 12-04 14:33:42 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=3858852)[0;0m ERROR 12-04 14:33:42 [core.py:842]     raise ValueError(
[1;36m(EngineCore_DP0 pid=3858852)[0;0m ERROR 12-04 14:33:42 [core.py:842] ValueError: Free memory on device (7.14/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[1;36m(EngineCore_DP0 pid=3858852)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=3858852)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=3858852)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=3858852)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=3858852)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=3858852)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=3858852)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=3858852)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=3858852)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=3858852)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3858852)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3858852)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=3858852)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=3858852)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=3858852)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=3858852)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3858852)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=3858852)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=3858852)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=3858852)[0;0m     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=3858852)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=3858852)[0;0m     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=3858852)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3858852)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=3858852)[0;0m     raise ValueError(
[1;36m(EngineCore_DP0 pid=3858852)[0;0m ValueError: Free memory on device (7.14/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W1204 14:33:43.601606251 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:16<00:00, 16.70s/it, est. speed input: 7.19 toks/s, output: 103.31 toks/s]Processed prompts: 100%|██████████| 1/1 [00:16<00:00, 16.70s/it, est. speed input: 7.19 toks/s, output: 103.31 toks/s]Processed prompts: 100%|██████████| 1/1 [00:16<00:00, 16.70s/it, est. speed input: 7.19 toks/s, output: 103.31 toks/s]
Agent 1 response: Given that there were 20 helmets, we can determine the number of robots and footballs using the rela...

--- Problem 5/20, Round 1, Agent 2/7 ---
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1015.82it/s]
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: WeiboAI/VibeThinker-1.5B (backend=vllm)
[ModelCache] Using max_model_len=131072 for WeiboAI/VibeThinker-1.5B
[ModelCache] Detected 1 GPU(s), 44.3GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 14:34:05 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'WeiboAI/VibeThinker-1.5B'}
INFO 12-04 14:34:05 [model.py:631] Resolved architecture: Qwen2ForCausalLM
WARNING 12-04 14:34:05 [model.py:1921] Your device 'Quadro RTX 8000' (with compute capability 7.5) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.
WARNING 12-04 14:34:05 [model.py:1971] Casting torch.bfloat16 to torch.float16.
INFO 12-04 14:34:05 [model.py:1745] Using max model len 131072
INFO 12-04 14:34:05 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:25<00:00, 25.94s/it, est. speed input: 4.74 toks/s, output: 103.15 toks/s]Processed prompts: 100%|██████████| 1/1 [00:25<00:00, 25.94s/it, est. speed input: 4.74 toks/s, output: 103.15 toks/s]Processed prompts: 100%|██████████| 1/1 [00:25<00:00, 25.94s/it, est. speed input: 4.74 toks/s, output: 103.15 toks/s]
Agent 2 response: Dominick saw 20 helmets. The number of helmets is half the number of footballs, so the number of foo...

--- Problem 5/20, Round 1, Agent 3/7 ---
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 816.33it/s]
[1;36m(EngineCore_DP0 pid=3858915)[0;0m INFO 12-04 14:34:20 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='WeiboAI/VibeThinker-1.5B', speculative_config=None, tokenizer='WeiboAI/VibeThinker-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=WeiboAI/VibeThinker-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=3858915)[0;0m ERROR 12-04 14:34:21 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8
[1;36m(EngineCore_DP0 pid=3858915)[0;0m INFO 12-04 14:34:21 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.7:37559 backend=nccl
[W1204 14:34:21.875753662 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-23-2.rc.tch.harvard.edu]:37559 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=3858915)[0;0m INFO 12-04 14:34:22 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=3858915)[0;0m ERROR 12-04 14:34:22 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=3858915)[0;0m ERROR 12-04 14:34:22 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=3858915)[0;0m ERROR 12-04 14:34:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=3858915)[0;0m ERROR 12-04 14:34:22 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3858915)[0;0m ERROR 12-04 14:34:22 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3858915)[0;0m ERROR 12-04 14:34:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=3858915)[0;0m ERROR 12-04 14:34:22 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=3858915)[0;0m ERROR 12-04 14:34:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=3858915)[0;0m ERROR 12-04 14:34:22 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=3858915)[0;0m ERROR 12-04 14:34:22 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3858915)[0;0m ERROR 12-04 14:34:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=3858915)[0;0m ERROR 12-04 14:34:22 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=3858915)[0;0m ERROR 12-04 14:34:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=3858915)[0;0m ERROR 12-04 14:34:22 [core.py:842]     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=3858915)[0;0m ERROR 12-04 14:34:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=3858915)[0;0m ERROR 12-04 14:34:22 [core.py:842]     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=3858915)[0;0m ERROR 12-04 14:34:22 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3858915)[0;0m ERROR 12-04 14:34:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=3858915)[0;0m ERROR 12-04 14:34:22 [core.py:842]     raise ValueError(
[1;36m(EngineCore_DP0 pid=3858915)[0;0m ERROR 12-04 14:34:22 [core.py:842] ValueError: Free memory on device (7.14/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[1;36m(EngineCore_DP0 pid=3858915)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=3858915)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=3858915)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=3858915)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=3858915)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=3858915)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=3858915)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=3858915)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=3858915)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=3858915)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3858915)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3858915)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=3858915)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=3858915)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=3858915)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=3858915)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3858915)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=3858915)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=3858915)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=3858915)[0;0m     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=3858915)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=3858915)[0;0m     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=3858915)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3858915)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=3858915)[0;0m     raise ValueError(
[1;36m(EngineCore_DP0 pid=3858915)[0;0m ValueError: Free memory on device (7.14/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W1204 14:34:23.161296880 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:23<00:00, 23.41s/it, est. speed input: 5.25 toks/s, output: 104.76 toks/s]Processed prompts: 100%|██████████| 1/1 [00:23<00:00, 23.41s/it, est. speed input: 5.25 toks/s, output: 104.76 toks/s]Processed prompts: 100%|██████████| 1/1 [00:23<00:00, 23.42s/it, est. speed input: 5.25 toks/s, output: 104.76 toks/s]
Agent 3 response: Dominick saw 20 helmets. Since the number of robots is half the number of helmets, there are \( \fra...

--- Problem 5/20, Round 1, Agent 4/7 ---
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 999.60it/s]
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: WeiboAI/VibeThinker-1.5B (backend=vllm)
[ModelCache] Using max_model_len=131072 for WeiboAI/VibeThinker-1.5B
[ModelCache] Detected 1 GPU(s), 44.3GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 14:34:44 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'WeiboAI/VibeThinker-1.5B'}
INFO 12-04 14:34:44 [model.py:631] Resolved architecture: Qwen2ForCausalLM
WARNING 12-04 14:34:44 [model.py:1921] Your device 'Quadro RTX 8000' (with compute capability 7.5) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.
WARNING 12-04 14:34:44 [model.py:1971] Casting torch.bfloat16 to torch.float16.
INFO 12-04 14:34:44 [model.py:1745] Using max model len 131072
INFO 12-04 14:34:44 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.34s/it, est. speed input: 23.24 toks/s, output: 102.90 toks/s]Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.34s/it, est. speed input: 23.24 toks/s, output: 102.90 toks/s]Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.34s/it, est. speed input: 23.24 toks/s, output: 102.90 toks/s]
Agent 4 response: Dominick saw 20 helmets. The number of robots is half the number of helmets, so there are \(20 \div ...

--- Problem 5/20, Round 1, Agent 5/7 ---
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 771.72it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:11<00:00, 11.47s/it, est. speed input: 10.90 toks/s, output: 100.03 toks/s]Processed prompts: 100%|██████████| 1/1 [00:11<00:00, 11.47s/it, est. speed input: 10.90 toks/s, output: 100.03 toks/s]Processed prompts: 100%|██████████| 1/1 [00:11<00:00, 11.47s/it, est. speed input: 10.90 toks/s, output: 100.03 toks/s]
Agent 5 response: Dominick saw 20 helmets. Since the number of helmets is half as many as the footballs, the number of...

--- Problem 5/20, Round 1, Agent 6/7 ---
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 782.08it/s]
[1;36m(EngineCore_DP0 pid=3859079)[0;0m INFO 12-04 14:34:59 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='WeiboAI/VibeThinker-1.5B', speculative_config=None, tokenizer='WeiboAI/VibeThinker-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=WeiboAI/VibeThinker-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=3859079)[0;0m ERROR 12-04 14:35:00 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8
[1;36m(EngineCore_DP0 pid=3859079)[0;0m INFO 12-04 14:35:01 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.7:35645 backend=nccl
[W1204 14:35:01.307233904 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-23-2.rc.tch.harvard.edu]:35645 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=3859079)[0;0m INFO 12-04 14:35:01 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=3859079)[0;0m ERROR 12-04 14:35:01 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=3859079)[0;0m ERROR 12-04 14:35:01 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=3859079)[0;0m ERROR 12-04 14:35:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=3859079)[0;0m ERROR 12-04 14:35:01 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3859079)[0;0m ERROR 12-04 14:35:01 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3859079)[0;0m ERROR 12-04 14:35:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=3859079)[0;0m ERROR 12-04 14:35:01 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=3859079)[0;0m ERROR 12-04 14:35:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=3859079)[0;0m ERROR 12-04 14:35:01 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=3859079)[0;0m ERROR 12-04 14:35:01 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3859079)[0;0m ERROR 12-04 14:35:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=3859079)[0;0m ERROR 12-04 14:35:01 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=3859079)[0;0m ERROR 12-04 14:35:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=3859079)[0;0m ERROR 12-04 14:35:01 [core.py:842]     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=3859079)[0;0m ERROR 12-04 14:35:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=3859079)[0;0m ERROR 12-04 14:35:01 [core.py:842]     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=3859079)[0;0m ERROR 12-04 14:35:01 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3859079)[0;0m ERROR 12-04 14:35:01 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=3859079)[0;0m ERROR 12-04 14:35:01 [core.py:842]     raise ValueError(
[1;36m(EngineCore_DP0 pid=3859079)[0;0m ERROR 12-04 14:35:01 [core.py:842] ValueError: Free memory on device (7.14/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[1;36m(EngineCore_DP0 pid=3859079)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=3859079)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=3859079)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=3859079)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=3859079)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=3859079)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=3859079)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=3859079)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=3859079)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=3859079)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3859079)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3859079)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=3859079)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=3859079)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=3859079)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=3859079)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3859079)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=3859079)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=3859079)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=3859079)[0;0m     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=3859079)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=3859079)[0;0m     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=3859079)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3859079)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=3859079)[0;0m     raise ValueError(
[1;36m(EngineCore_DP0 pid=3859079)[0;0m ValueError: Free memory on device (7.14/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W1204 14:35:02.597626263 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:12<00:00, 12.75s/it, est. speed input: 9.65 toks/s, output: 103.92 toks/s]Processed prompts: 100%|██████████| 1/1 [00:12<00:00, 12.75s/it, est. speed input: 9.65 toks/s, output: 103.92 toks/s]Processed prompts: 100%|██████████| 1/1 [00:12<00:00, 12.75s/it, est. speed input: 9.65 toks/s, output: 103.92 toks/s]
Agent 6 response: Dominick saw three types of items in the changing room: robots, helmets, and footballs. The problem ...

--- Problem 5/20, Round 1, Agent 7/7 ---
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1009.70it/s]
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: WeiboAI/VibeThinker-1.5B (backend=vllm)
[ModelCache] Using max_model_len=131072 for WeiboAI/VibeThinker-1.5B
[ModelCache] Detected 1 GPU(s), 44.3GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 14:35:24 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'WeiboAI/VibeThinker-1.5B'}
INFO 12-04 14:35:24 [model.py:631] Resolved architecture: Qwen2ForCausalLM
WARNING 12-04 14:35:24 [model.py:1921] Your device 'Quadro RTX 8000' (with compute capability 7.5) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.
WARNING 12-04 14:35:24 [model.py:1971] Casting torch.bfloat16 to torch.float16.
INFO 12-04 14:35:24 [model.py:1745] Using max model len 131072
INFO 12-04 14:35:24 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:13<00:00, 13.01s/it, est. speed input: 9.76 toks/s, output: 106.32 toks/s]Processed prompts: 100%|██████████| 1/1 [00:13<00:00, 13.01s/it, est. speed input: 9.76 toks/s, output: 106.32 toks/s]Processed prompts: 100%|██████████| 1/1 [00:13<00:00, 13.01s/it, est. speed input: 9.76 toks/s, output: 106.32 toks/s]
Agent 7 response: Given that there were 20 helmets, we can determine the number of robots and footballs using the rela...

--- Problem 5/20, Round 2, Agent 1/7 ---
Agent 1 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 138.24it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:03<00:00,  3.38s/it, est. speed input: 422.92 toks/s, output: 98.42 toks/s]Processed prompts: 100%|██████████| 1/1 [00:03<00:00,  3.38s/it, est. speed input: 422.92 toks/s, output: 98.42 toks/s]Processed prompts: 100%|██████████| 1/1 [00:03<00:00,  3.38s/it, est. speed input: 422.92 toks/s, output: 98.42 toks/s]
Agent 1 response: Given there were 20 helmets, we determine the number of robots and footballs using the relationships...

--- Problem 5/20, Round 2, Agent 2/7 ---
Agent 2 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 156.60it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.89s/it, est. speed input: 495.89 toks/s, output: 98.55 toks/s]Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.89s/it, est. speed input: 495.89 toks/s, output: 98.55 toks/s]Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.89s/it, est. speed input: 495.89 toks/s, output: 98.55 toks/s]
Agent 2 response: Dominick saw 20 helmets. The number of robots is half the number of helmets, so there are \( \frac{2...

--- Problem 5/20, Round 2, Agent 3/7 ---
Agent 3 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 145.64it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.54s/it, est. speed input: 564.94 toks/s, output: 98.09 toks/s]Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.54s/it, est. speed input: 564.94 toks/s, output: 98.09 toks/s]Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.54s/it, est. speed input: 564.94 toks/s, output: 98.09 toks/s]
Agent 3 response: Dominick saw 20 helmets.  
The number of robots is half the number of helmets: \( 20 \div 2 = 10 \) ...

--- Problem 5/20, Round 2, Agent 4/7 ---
Agent 4 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 172.36it/s]
[1;36m(EngineCore_DP0 pid=3859246)[0;0m INFO 12-04 14:35:39 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='WeiboAI/VibeThinker-1.5B', speculative_config=None, tokenizer='WeiboAI/VibeThinker-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=WeiboAI/VibeThinker-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=3859246)[0;0m ERROR 12-04 14:35:40 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8
[1;36m(EngineCore_DP0 pid=3859246)[0;0m INFO 12-04 14:35:41 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.7:44923 backend=nccl
[W1204 14:35:41.463754672 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-23-2.rc.tch.harvard.edu]:44923 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=3859246)[0;0m INFO 12-04 14:35:41 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=3859246)[0;0m ERROR 12-04 14:35:41 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=3859246)[0;0m ERROR 12-04 14:35:41 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=3859246)[0;0m ERROR 12-04 14:35:41 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=3859246)[0;0m ERROR 12-04 14:35:41 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3859246)[0;0m ERROR 12-04 14:35:41 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3859246)[0;0m ERROR 12-04 14:35:41 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=3859246)[0;0m ERROR 12-04 14:35:41 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=3859246)[0;0m ERROR 12-04 14:35:41 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=3859246)[0;0m ERROR 12-04 14:35:41 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=3859246)[0;0m ERROR 12-04 14:35:41 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3859246)[0;0m ERROR 12-04 14:35:41 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=3859246)[0;0m ERROR 12-04 14:35:41 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=3859246)[0;0m ERROR 12-04 14:35:41 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=3859246)[0;0m ERROR 12-04 14:35:41 [core.py:842]     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=3859246)[0;0m ERROR 12-04 14:35:41 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=3859246)[0;0m ERROR 12-04 14:35:41 [core.py:842]     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=3859246)[0;0m ERROR 12-04 14:35:41 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3859246)[0;0m ERROR 12-04 14:35:41 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=3859246)[0;0m ERROR 12-04 14:35:41 [core.py:842]     raise ValueError(
[1;36m(EngineCore_DP0 pid=3859246)[0;0m ERROR 12-04 14:35:41 [core.py:842] ValueError: Free memory on device (7.14/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[1;36m(EngineCore_DP0 pid=3859246)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=3859246)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=3859246)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=3859246)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=3859246)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=3859246)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=3859246)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=3859246)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=3859246)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=3859246)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3859246)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3859246)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=3859246)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=3859246)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=3859246)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=3859246)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3859246)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=3859246)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=3859246)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=3859246)[0;0m     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=3859246)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=3859246)[0;0m     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=3859246)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3859246)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=3859246)[0;0m     raise ValueError(
[1;36m(EngineCore_DP0 pid=3859246)[0;0m ValueError: Free memory on device (7.14/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W1204 14:35:42.742244939 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:11<00:00, 11.33s/it, est. speed input: 126.63 toks/s, output: 100.33 toks/s]Processed prompts: 100%|██████████| 1/1 [00:11<00:00, 11.33s/it, est. speed input: 126.63 toks/s, output: 100.33 toks/s]Processed prompts: 100%|██████████| 1/1 [00:11<00:00, 11.33s/it, est. speed input: 126.63 toks/s, output: 100.33 toks/s]
Agent 4 response: Given that there were 20 helmets, and the number of robots is half as many as the helmets, the numbe...

--- Problem 5/20, Round 2, Agent 5/7 ---
Agent 5 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 195.77it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.40s/it, est. speed input: 193.95 toks/s, output: 105.35 toks/s]Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.40s/it, est. speed input: 193.95 toks/s, output: 105.35 toks/s]Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.40s/it, est. speed input: 193.95 toks/s, output: 105.35 toks/s]
Agent 5 response: Dominick saw 20 helmets. Since the number of robots is half as many as the helmets, the number of ro...

--- Problem 5/20, Round 2, Agent 6/7 ---
Agent 6 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 211.37it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.78s/it, est. speed input: 515.94 toks/s, output: 104.34 toks/s]Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.78s/it, est. speed input: 515.94 toks/s, output: 104.34 toks/s]Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.78s/it, est. speed input: 515.94 toks/s, output: 104.34 toks/s]
Agent 6 response: Given that there were 20 helmets:
- The number of robots is half the number of helmets: \( \frac{20}...

--- Problem 5/20, Round 2, Agent 7/7 ---
Agent 7 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 217.70it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.93s/it, est. speed input: 490.27 toks/s, output: 104.33 toks/s]Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.93s/it, est. speed input: 490.27 toks/s, output: 104.33 toks/s]Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.93s/it, est. speed input: 490.27 toks/s, output: 104.33 toks/s]
Agent 7 response: Given that there are 20 helmets (H = 20):
- The number of robots (R) is half the number of helmets: ...

--- Problem 5/20, Round 3, Agent 1/7 ---
Agent 1 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 126.20it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.69s/it, est. speed input: 942.05 toks/s, output: 103.80 toks/s]Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.69s/it, est. speed input: 942.05 toks/s, output: 103.80 toks/s]Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.69s/it, est. speed input: 942.05 toks/s, output: 103.80 toks/s]
Agent 1 response: Dominick saw 20 helmets. The number of robots is half as many as the helmets, so there are \( \frac{...

--- Problem 5/20, Round 3, Agent 2/7 ---
Agent 2 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 133.34it/s]
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: WeiboAI/VibeThinker-1.5B (backend=vllm)
[ModelCache] Using max_model_len=131072 for WeiboAI/VibeThinker-1.5B
[ModelCache] Detected 1 GPU(s), 44.3GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 14:36:04 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'WeiboAI/VibeThinker-1.5B'}
INFO 12-04 14:36:04 [model.py:631] Resolved architecture: Qwen2ForCausalLM
WARNING 12-04 14:36:04 [model.py:1921] Your device 'Quadro RTX 8000' (with compute capability 7.5) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.
WARNING 12-04 14:36:04 [model.py:1971] Casting torch.bfloat16 to torch.float16.
INFO 12-04 14:36:04 [model.py:1745] Using max model len 131072
INFO 12-04 14:36:04 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.16s/it, est. speed input: 491.44 toks/s, output: 102.75 toks/s]Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.16s/it, est. speed input: 491.44 toks/s, output: 102.75 toks/s]Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.16s/it, est. speed input: 491.44 toks/s, output: 102.75 toks/s]
Agent 2 response: Dominick saw 20 helmets.  

The number of robots is half the number of helmets, so:  
\[
\text{Robot...

--- Problem 5/20, Round 3, Agent 3/7 ---
Agent 3 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 79.39it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.72s/it, est. speed input: 328.17 toks/s, output: 99.29 toks/s]Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.72s/it, est. speed input: 328.17 toks/s, output: 99.29 toks/s]Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.73s/it, est. speed input: 328.17 toks/s, output: 99.29 toks/s]
Agent 3 response: Dominick saw 20 helmets.  
The number of robots is half the number of helmets, so there are \( \frac...

--- Problem 5/20, Round 3, Agent 4/7 ---
Agent 4 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 93.25it/s]
[1;36m(EngineCore_DP0 pid=3859308)[0;0m INFO 12-04 14:36:18 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='WeiboAI/VibeThinker-1.5B', speculative_config=None, tokenizer='WeiboAI/VibeThinker-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=WeiboAI/VibeThinker-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=3859308)[0;0m ERROR 12-04 14:36:19 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8
[1;36m(EngineCore_DP0 pid=3859308)[0;0m INFO 12-04 14:36:20 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.7:60601 backend=nccl
[W1204 14:36:20.059422836 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-23-2.rc.tch.harvard.edu]:60601 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=3859308)[0;0m INFO 12-04 14:36:20 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:06<00:00,  6.29s/it, est. speed input: 403.10 toks/s, output: 99.18 toks/s]Processed prompts: 100%|██████████| 1/1 [00:06<00:00,  6.29s/it, est. speed input: 403.10 toks/s, output: 99.18 toks/s]Processed prompts: 100%|██████████| 1/1 [00:06<00:00,  6.29s/it, est. speed input: 403.10 toks/s, output: 99.18 toks/s]
Agent 4 response: Dominick saw 20 helmets. The number of robots is half the number of helmets, so there are \( \frac{2...

--- Problem 5/20, Round 3, Agent 5/7 ---
Agent 5 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 67.57it/s]
[1;36m(EngineCore_DP0 pid=3859308)[0;0m ERROR 12-04 14:36:20 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=3859308)[0;0m ERROR 12-04 14:36:20 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=3859308)[0;0m ERROR 12-04 14:36:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=3859308)[0;0m ERROR 12-04 14:36:20 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3859308)[0;0m ERROR 12-04 14:36:20 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3859308)[0;0m ERROR 12-04 14:36:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=3859308)[0;0m ERROR 12-04 14:36:20 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=3859308)[0;0m ERROR 12-04 14:36:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=3859308)[0;0m ERROR 12-04 14:36:20 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=3859308)[0;0m ERROR 12-04 14:36:20 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3859308)[0;0m ERROR 12-04 14:36:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=3859308)[0;0m ERROR 12-04 14:36:20 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=3859308)[0;0m ERROR 12-04 14:36:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=3859308)[0;0m ERROR 12-04 14:36:20 [core.py:842]     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=3859308)[0;0m ERROR 12-04 14:36:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=3859308)[0;0m ERROR 12-04 14:36:20 [core.py:842]     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=3859308)[0;0m ERROR 12-04 14:36:20 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3859308)[0;0m ERROR 12-04 14:36:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=3859308)[0;0m ERROR 12-04 14:36:20 [core.py:842]     raise ValueError(
[1;36m(EngineCore_DP0 pid=3859308)[0;0m ERROR 12-04 14:36:20 [core.py:842] ValueError: Free memory on device (7.14/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[1;36m(EngineCore_DP0 pid=3859308)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=3859308)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=3859308)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=3859308)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=3859308)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=3859308)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=3859308)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=3859308)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=3859308)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=3859308)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3859308)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3859308)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=3859308)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=3859308)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=3859308)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=3859308)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3859308)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=3859308)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=3859308)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=3859308)[0;0m     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=3859308)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=3859308)[0;0m     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=3859308)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3859308)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=3859308)[0;0m     raise ValueError(
[1;36m(EngineCore_DP0 pid=3859308)[0;0m ValueError: Free memory on device (7.14/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W1204 14:36:21.331593253 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:08<00:00,  8.79s/it, est. speed input: 288.65 toks/s, output: 103.31 toks/s]Processed prompts: 100%|██████████| 1/1 [00:08<00:00,  8.79s/it, est. speed input: 288.65 toks/s, output: 103.31 toks/s]Processed prompts: 100%|██████████| 1/1 [00:08<00:00,  8.79s/it, est. speed input: 288.65 toks/s, output: 103.31 toks/s]
Agent 5 response: Dominick saw 20 helmets. The number of robots is half as many as the helmets, so the number of robot...

--- Problem 5/20, Round 3, Agent 6/7 ---
Agent 6 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 131.38it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.80s/it, est. speed input: 325.00 toks/s, output: 105.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.80s/it, est. speed input: 325.00 toks/s, output: 105.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.80s/it, est. speed input: 325.00 toks/s, output: 105.00 toks/s]
Agent 6 response: Given there were 20 helmets, the number of robots is half the number of helmets:  
\[
\text{Robots} ...

--- Problem 5/20, Round 3, Agent 7/7 ---
Agent 7 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 123.99it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:03<00:00,  3.18s/it, est. speed input: 798.23 toks/s, output: 104.06 toks/s]Processed prompts: 100%|██████████| 1/1 [00:03<00:00,  3.18s/it, est. speed input: 798.23 toks/s, output: 104.06 toks/s]Processed prompts: 100%|██████████| 1/1 [00:03<00:00,  3.18s/it, est. speed input: 798.23 toks/s, output: 104.06 toks/s]
Agent 7 response: Given that there were 20 helmets (H = 20):

1. **Robots (R)**: Half as many as helmets, so \( R = \f...

Running accuracy: 1.000 ± 0.000

--- Problem 6/20, Round 1, Agent 1/7 ---
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 868.39it/s]
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: WeiboAI/VibeThinker-1.5B (backend=vllm)
[ModelCache] Using max_model_len=131072 for WeiboAI/VibeThinker-1.5B
[ModelCache] Detected 1 GPU(s), 44.3GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 14:36:42 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'WeiboAI/VibeThinker-1.5B'}
INFO 12-04 14:36:43 [model.py:631] Resolved architecture: Qwen2ForCausalLM
WARNING 12-04 14:36:43 [model.py:1921] Your device 'Quadro RTX 8000' (with compute capability 7.5) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.
WARNING 12-04 14:36:43 [model.py:1971] Casting torch.bfloat16 to torch.float16.
INFO 12-04 14:36:43 [model.py:1745] Using max model len 131072
INFO 12-04 14:36:43 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=3859371)[0;0m INFO 12-04 14:36:57 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='WeiboAI/VibeThinker-1.5B', speculative_config=None, tokenizer='WeiboAI/VibeThinker-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=WeiboAI/VibeThinker-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=3859371)[0;0m ERROR 12-04 14:36:58 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8
[1;36m(EngineCore_DP0 pid=3859371)[0;0m INFO 12-04 14:36:59 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.7:56325 backend=nccl
[W1204 14:36:59.207557427 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-23-2.rc.tch.harvard.edu]:56325 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=3859371)[0;0m INFO 12-04 14:36:59 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=3859371)[0;0m ERROR 12-04 14:36:59 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=3859371)[0;0m ERROR 12-04 14:36:59 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=3859371)[0;0m ERROR 12-04 14:36:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=3859371)[0;0m ERROR 12-04 14:36:59 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3859371)[0;0m ERROR 12-04 14:36:59 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3859371)[0;0m ERROR 12-04 14:36:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=3859371)[0;0m ERROR 12-04 14:36:59 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=3859371)[0;0m ERROR 12-04 14:36:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=3859371)[0;0m ERROR 12-04 14:36:59 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=3859371)[0;0m ERROR 12-04 14:36:59 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3859371)[0;0m ERROR 12-04 14:36:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=3859371)[0;0m ERROR 12-04 14:36:59 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=3859371)[0;0m ERROR 12-04 14:36:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=3859371)[0;0m ERROR 12-04 14:36:59 [core.py:842]     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=3859371)[0;0m ERROR 12-04 14:36:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=3859371)[0;0m ERROR 12-04 14:36:59 [core.py:842]     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=3859371)[0;0m ERROR 12-04 14:36:59 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3859371)[0;0m ERROR 12-04 14:36:59 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=3859371)[0;0m ERROR 12-04 14:36:59 [core.py:842]     raise ValueError(
[1;36m(EngineCore_DP0 pid=3859371)[0;0m ERROR 12-04 14:36:59 [core.py:842] ValueError: Free memory on device (7.14/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[1;36m(EngineCore_DP0 pid=3859371)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=3859371)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=3859371)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=3859371)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=3859371)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=3859371)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=3859371)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=3859371)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=3859371)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=3859371)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3859371)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3859371)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=3859371)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=3859371)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=3859371)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=3859371)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3859371)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=3859371)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=3859371)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=3859371)[0;0m     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=3859371)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=3859371)[0;0m     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=3859371)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3859371)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=3859371)[0;0m     raise ValueError(
[1;36m(EngineCore_DP0 pid=3859371)[0;0m ValueError: Free memory on device (7.14/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W1204 14:37:00.467135027 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:23<00:00, 23.05s/it, est. speed input: 5.99 toks/s, output: 101.36 toks/s]Processed prompts: 100%|██████████| 1/1 [00:23<00:00, 23.05s/it, est. speed input: 5.99 toks/s, output: 101.36 toks/s]Processed prompts: 100%|██████████| 1/1 [00:23<00:00, 23.05s/it, est. speed input: 5.99 toks/s, output: 101.36 toks/s]
Agent 1 response: The school auditorium has 4 rows with 18 seats each, totaling \(4 \times 18 = 72\) seats. One-fourth...

--- Problem 6/20, Round 1, Agent 2/7 ---
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 948.51it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:13<00:00, 13.50s/it, est. speed input: 10.45 toks/s, output: 106.62 toks/s]Processed prompts: 100%|██████████| 1/1 [00:13<00:00, 13.50s/it, est. speed input: 10.45 toks/s, output: 106.62 toks/s]Processed prompts: 100%|██████████| 1/1 [00:13<00:00, 13.50s/it, est. speed input: 10.45 toks/s, output: 106.62 toks/s]
Agent 2 response: The school auditorium has 4 rows of 18 seats each, totaling \(4 \times 18 = 72\) seats. 

One-fourth...

--- Problem 6/20, Round 1, Agent 3/7 ---
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 931.86it/s]
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: WeiboAI/VibeThinker-1.5B (backend=vllm)
[ModelCache] Using max_model_len=131072 for WeiboAI/VibeThinker-1.5B
[ModelCache] Detected 1 GPU(s), 44.3GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 14:37:21 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'WeiboAI/VibeThinker-1.5B'}
INFO 12-04 14:37:22 [model.py:631] Resolved architecture: Qwen2ForCausalLM
WARNING 12-04 14:37:22 [model.py:1921] Your device 'Quadro RTX 8000' (with compute capability 7.5) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.
WARNING 12-04 14:37:22 [model.py:1971] Casting torch.bfloat16 to torch.float16.
INFO 12-04 14:37:22 [model.py:1745] Using max model len 131072
INFO 12-04 14:37:22 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:17<00:00, 17.56s/it, est. speed input: 8.03 toks/s, output: 102.10 toks/s]Processed prompts: 100%|██████████| 1/1 [00:17<00:00, 17.56s/it, est. speed input: 8.03 toks/s, output: 102.10 toks/s]Processed prompts: 100%|██████████| 1/1 [00:17<00:00, 17.56s/it, est. speed input: 8.03 toks/s, output: 102.10 toks/s]
Agent 3 response: The school auditorium has 4 rows of seats with 18 seats in each row. The total number of seats is ca...

--- Problem 6/20, Round 1, Agent 4/7 ---
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 771.86it/s]
[1;36m(EngineCore_DP0 pid=3859449)[0;0m INFO 12-04 14:37:36 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='WeiboAI/VibeThinker-1.5B', speculative_config=None, tokenizer='WeiboAI/VibeThinker-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=WeiboAI/VibeThinker-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=3859449)[0;0m ERROR 12-04 14:37:37 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8
[1;36m(EngineCore_DP0 pid=3859449)[0;0m INFO 12-04 14:37:38 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.7:52143 backend=nccl
[W1204 14:37:38.950796284 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-23-2.rc.tch.harvard.edu]:52143 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=3859449)[0;0m INFO 12-04 14:37:38 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=3859449)[0;0m ERROR 12-04 14:37:38 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=3859449)[0;0m ERROR 12-04 14:37:38 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=3859449)[0;0m ERROR 12-04 14:37:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=3859449)[0;0m ERROR 12-04 14:37:38 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3859449)[0;0m ERROR 12-04 14:37:38 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3859449)[0;0m ERROR 12-04 14:37:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=3859449)[0;0m ERROR 12-04 14:37:38 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=3859449)[0;0m ERROR 12-04 14:37:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=3859449)[0;0m ERROR 12-04 14:37:38 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=3859449)[0;0m ERROR 12-04 14:37:38 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3859449)[0;0m ERROR 12-04 14:37:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=3859449)[0;0m ERROR 12-04 14:37:38 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=3859449)[0;0m ERROR 12-04 14:37:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=3859449)[0;0m ERROR 12-04 14:37:38 [core.py:842]     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=3859449)[0;0m ERROR 12-04 14:37:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=3859449)[0;0m ERROR 12-04 14:37:38 [core.py:842]     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=3859449)[0;0m ERROR 12-04 14:37:38 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3859449)[0;0m ERROR 12-04 14:37:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=3859449)[0;0m ERROR 12-04 14:37:38 [core.py:842]     raise ValueError(
[1;36m(EngineCore_DP0 pid=3859449)[0;0m ERROR 12-04 14:37:38 [core.py:842] ValueError: Free memory on device (7.14/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[1;36m(EngineCore_DP0 pid=3859449)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=3859449)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=3859449)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=3859449)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=3859449)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=3859449)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=3859449)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=3859449)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=3859449)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=3859449)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3859449)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3859449)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=3859449)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=3859449)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=3859449)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=3859449)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3859449)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=3859449)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=3859449)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=3859449)[0;0m     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=3859449)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=3859449)[0;0m     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=3859449)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3859449)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=3859449)[0;0m     raise ValueError(
[1;36m(EngineCore_DP0 pid=3859449)[0;0m ValueError: Free memory on device (7.14/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W1204 14:37:39.233790205 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:13<00:00, 13.81s/it, est. speed input: 10.28 toks/s, output: 103.34 toks/s]Processed prompts: 100%|██████████| 1/1 [00:13<00:00, 13.81s/it, est. speed input: 10.28 toks/s, output: 103.34 toks/s]Processed prompts: 100%|██████████| 1/1 [00:13<00:00, 13.81s/it, est. speed input: 10.28 toks/s, output: 103.34 toks/s]
Agent 4 response: The school auditorium has 4 rows with 18 seats each, totaling \(4 \times 18 = 72\) seats. One-fourth...

--- Problem 6/20, Round 1, Agent 5/7 ---
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 973.16it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:06<00:00,  6.81s/it, est. speed input: 21.01 toks/s, output: 106.66 toks/s]Processed prompts: 100%|██████████| 1/1 [00:06<00:00,  6.81s/it, est. speed input: 21.01 toks/s, output: 106.66 toks/s]Processed prompts: 100%|██████████| 1/1 [00:06<00:00,  6.81s/it, est. speed input: 21.01 toks/s, output: 106.66 toks/s]
Agent 5 response: The total number of seats in the auditorium is calculated by multiplying the number of rows by the s...

--- Problem 6/20, Round 1, Agent 6/7 ---
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 926.51it/s]
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: WeiboAI/VibeThinker-1.5B (backend=vllm)
[ModelCache] Using max_model_len=131072 for WeiboAI/VibeThinker-1.5B
[ModelCache] Detected 1 GPU(s), 44.3GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 14:38:00 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'WeiboAI/VibeThinker-1.5B'}
INFO 12-04 14:38:00 [model.py:631] Resolved architecture: Qwen2ForCausalLM
WARNING 12-04 14:38:00 [model.py:1921] Your device 'Quadro RTX 8000' (with compute capability 7.5) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.
WARNING 12-04 14:38:00 [model.py:1971] Casting torch.bfloat16 to torch.float16.
INFO 12-04 14:38:00 [model.py:1745] Using max model len 131072
INFO 12-04 14:38:00 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:12<00:00, 12.87s/it, est. speed input: 10.95 toks/s, output: 103.39 toks/s]Processed prompts: 100%|██████████| 1/1 [00:12<00:00, 12.87s/it, est. speed input: 10.95 toks/s, output: 103.39 toks/s]Processed prompts: 100%|██████████| 1/1 [00:12<00:00, 12.87s/it, est. speed input: 10.95 toks/s, output: 103.39 toks/s]
Agent 6 response: The school auditorium has 4 rows with 18 seats each, totaling \(4 \times 18 = 72\) seats. One-fourth...

--- Problem 6/20, Round 1, Agent 7/7 ---
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 700.80it/s]
[1;36m(EngineCore_DP0 pid=3859602)[0;0m INFO 12-04 14:38:15 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='WeiboAI/VibeThinker-1.5B', speculative_config=None, tokenizer='WeiboAI/VibeThinker-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=WeiboAI/VibeThinker-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=3859602)[0;0m ERROR 12-04 14:38:16 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8
slurmstepd-gpu-23-2: error: *** STEP 12735856.0 ON gpu-23-2 CANCELLED AT 2025-12-04T14:38:17 ***
