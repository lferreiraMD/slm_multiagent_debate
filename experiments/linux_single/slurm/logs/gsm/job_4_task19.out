Using persona diversity with 3 different personas
============================================================
GSM Task - Multiagent Debate
============================================================
Model: WeiboAI/VibeThinker-1.5B
Persona diversity mode:
  Agent 1: a stand-up comedian who evaluates suggestions based on their...
  Agent 2: a drone swarm commander who views the task as parallel proce...
  Agent 3: a hermetic alchemist who seeks to transmute the problem into...
Agents: 3
Rounds: 3
Problems: 20
Dataset: /home/ch269957/projects/slm_multiagent_debate/data/gsm8k/test.jsonl
Generation params: {'temperature': 1.0, 'max_tokens': None, 'top_p': 1.0, 'n': 1}
============================================================

--- Problem 1/20, Round 1, Agent 1/3 ---
[ModelCache] Loading model: WeiboAI/VibeThinker-1.5B (backend=vllm)
[ModelCache] Using max_model_len=131072 for WeiboAI/VibeThinker-1.5B
[ModelCache] Detected 1 GPU(s), 44.3GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 13:59:18 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'WeiboAI/VibeThinker-1.5B'}
Using persona diversity with 3 different personas
============================================================
GSM Task - Multiagent Debate
============================================================
Model: WeiboAI/VibeThinker-1.5B
Persona diversity mode:
  Agent 1: a stand-up comedian who evaluates suggestions based on their...
  Agent 2: a drone swarm commander who views the task as parallel proce...
  Agent 3: a hermetic alchemist who seeks to transmute the problem into...
Agents: 3
Rounds: 3
Problems: 20
Dataset: /home/ch269957/projects/slm_multiagent_debate/data/gsm8k/test.jsonl
Generation params: {'temperature': 1.0, 'max_tokens': None, 'top_p': 1.0, 'n': 1}
============================================================

--- Problem 1/20, Round 1, Agent 1/3 ---
[ModelCache] Loading model: WeiboAI/VibeThinker-1.5B (backend=vllm)
[ModelCache] Using max_model_len=131072 for WeiboAI/VibeThinker-1.5B
[ModelCache] Detected 1 GPU(s), 44.3GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 13:59:18 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'WeiboAI/VibeThinker-1.5B'}
INFO 12-04 13:59:18 [model.py:631] Resolved architecture: Qwen2ForCausalLM
WARNING 12-04 13:59:18 [model.py:1921] Your device 'Quadro RTX 8000' (with compute capability 7.5) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.
WARNING 12-04 13:59:18 [model.py:1971] Casting torch.bfloat16 to torch.float16.
INFO 12-04 13:59:18 [model.py:1745] Using max model len 131072
INFO 12-04 13:59:18 [model.py:631] Resolved architecture: Qwen2ForCausalLM
WARNING 12-04 13:59:18 [model.py:1921] Your device 'Quadro RTX 8000' (with compute capability 7.5) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.
WARNING 12-04 13:59:18 [model.py:1971] Casting torch.bfloat16 to torch.float16.
INFO 12-04 13:59:18 [model.py:1745] Using max model len 131072
INFO 12-04 13:59:20 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
INFO 12-04 13:59:20 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4004390)[0;0m INFO 12-04 13:59:40 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='WeiboAI/VibeThinker-1.5B', speculative_config=None, tokenizer='WeiboAI/VibeThinker-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=WeiboAI/VibeThinker-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4004389)[0;0m INFO 12-04 13:59:40 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='WeiboAI/VibeThinker-1.5B', speculative_config=None, tokenizer='WeiboAI/VibeThinker-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=WeiboAI/VibeThinker-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4004390)[0;0m ERROR 12-04 13:59:42 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8
[1;36m(EngineCore_DP0 pid=4004389)[0;0m ERROR 12-04 13:59:42 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8
[1;36m(EngineCore_DP0 pid=4004389)[0;0m INFO 12-04 13:59:45 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.8:54119 backend=nccl
[1;36m(EngineCore_DP0 pid=4004390)[0;0m INFO 12-04 13:59:45 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.8:40573 backend=nccl
[W1204 13:59:45.966063940 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-23-0.rc.tch.harvard.edu]:54119 (errno: 97 - Address family not supported by protocol).
[W1204 13:59:45.966533588 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-23-0.rc.tch.harvard.edu]:40573 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4004390)[0;0m INFO 12-04 13:59:45 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4004389)[0;0m INFO 12-04 13:59:45 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4004390)[0;0m INFO 12-04 13:59:45 [gpu_model_runner.py:3259] Starting to load model WeiboAI/VibeThinker-1.5B...
[1;36m(EngineCore_DP0 pid=4004389)[0;0m INFO 12-04 13:59:45 [gpu_model_runner.py:3259] Starting to load model WeiboAI/VibeThinker-1.5B...
[1;36m(EngineCore_DP0 pid=4004390)[0;0m INFO 12-04 13:59:47 [cuda.py:418] Valid backends: ['FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4004390)[0;0m INFO 12-04 13:59:47 [cuda.py:427] Using FLASHINFER backend.
[1;36m(EngineCore_DP0 pid=4004389)[0;0m INFO 12-04 13:59:47 [cuda.py:418] Valid backends: ['FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4004389)[0;0m INFO 12-04 13:59:47 [cuda.py:427] Using FLASHINFER backend.
[1;36m(EngineCore_DP0 pid=4004390)[0;0m INFO 12-04 13:59:48 [weight_utils.py:481] No model.safetensors.index.json found in remote.
[1;36m(EngineCore_DP0 pid=4004390)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
[1;36m(EngineCore_DP0 pid=4004389)[0;0m INFO 12-04 13:59:48 [weight_utils.py:481] No model.safetensors.index.json found in remote.
[1;36m(EngineCore_DP0 pid=4004389)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
[1;36m(EngineCore_DP0 pid=4004390)[0;0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:43<00:00, 43.71s/it]
[1;36m(EngineCore_DP0 pid=4004389)[0;0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:43<00:00, 43.53s/it]
[1;36m(EngineCore_DP0 pid=4004390)[0;0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:43<00:00, 43.71s/it]
[1;36m(EngineCore_DP0 pid=4004390)[0;0m 
[1;36m(EngineCore_DP0 pid=4004389)[0;0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:43<00:00, 43.53s/it]
[1;36m(EngineCore_DP0 pid=4004389)[0;0m 
[1;36m(EngineCore_DP0 pid=4004390)[0;0m INFO 12-04 14:00:32 [default_loader.py:314] Loading weights took 43.84 seconds
[1;36m(EngineCore_DP0 pid=4004389)[0;0m INFO 12-04 14:00:32 [default_loader.py:314] Loading weights took 43.79 seconds
[1;36m(EngineCore_DP0 pid=4004390)[0;0m INFO 12-04 14:00:32 [gpu_model_runner.py:3338] Model loading took 2.9110 GiB memory and 46.236249 seconds
[1;36m(EngineCore_DP0 pid=4004389)[0;0m INFO 12-04 14:00:33 [gpu_model_runner.py:3338] Model loading took 2.9110 GiB memory and 46.359053 seconds
[1;36m(EngineCore_DP0 pid=4004390)[0;0m INFO 12-04 14:00:56 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/6847b7a26b/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4004389)[0;0m INFO 12-04 14:00:56 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/6847b7a26b/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4004389)[0;0m INFO 12-04 14:00:56 [backends.py:647] Dynamo bytecode transform time: 22.67 s
[1;36m(EngineCore_DP0 pid=4004390)[0;0m INFO 12-04 14:00:56 [backends.py:647] Dynamo bytecode transform time: 22.78 s
[1;36m(EngineCore_DP0 pid=4004389)[0;0m [rank0]:W1204 14:00:56.712000 4004389 site-packages/torch/_inductor/triton_bundler.py:396] [0/0] Directory /tmp/torchinductor_ch269957/triton/0/tmp.6a182d29-3896-43b8-9026-e53865171d1b is not empty - skipping!
[1;36m(EngineCore_DP0 pid=4004389)[0;0m [rank0]:W1204 14:00:56.714000 4004389 site-packages/torch/_inductor/triton_bundler.py:396] [0/0] Directory /tmp/torchinductor_ch269957/triton/0/tmp.0cf740a2-8289-4672-9785-9869e2cb4461 is not empty - skipping!
[1;36m(EngineCore_DP0 pid=4004389)[0;0m [rank0]:W1204 14:00:56.716000 4004389 site-packages/torch/_inductor/triton_bundler.py:396] [0/0] Directory /tmp/torchinductor_ch269957/triton/0/tmp.dfeaa70d-02b4-4768-9858-01b5d487af0e is not empty - skipping!
[1;36m(EngineCore_DP0 pid=4004389)[0;0m [rank0]:W1204 14:00:56.718000 4004389 site-packages/torch/_inductor/triton_bundler.py:396] [0/0] Directory /tmp/torchinductor_ch269957/triton/0/tmp.206273d1-8d8b-4c16-93a9-5ec17e13b064 is not empty - skipping!
[1;36m(EngineCore_DP0 pid=4004389)[0;0m INFO 12-04 14:01:03 [backends.py:210] Directly load the compiled graph(s) for dynamic shape from the cache, took 6.703 s
[1;36m(EngineCore_DP0 pid=4004390)[0;0m INFO 12-04 14:01:03 [backends.py:210] Directly load the compiled graph(s) for dynamic shape from the cache, took 6.704 s
[1;36m(EngineCore_DP0 pid=4004390)[0;0m INFO 12-04 14:01:05 [monitor.py:34] torch.compile takes 29.48 s in total
[1;36m(EngineCore_DP0 pid=4004389)[0;0m INFO 12-04 14:01:05 [monitor.py:34] torch.compile takes 29.37 s in total
[1;36m(EngineCore_DP0 pid=4004390)[0;0m INFO 12-04 14:01:07 [gpu_worker.py:359] Available KV cache memory: 30.58 GiB
[1;36m(EngineCore_DP0 pid=4004389)[0;0m INFO 12-04 14:01:07 [gpu_worker.py:359] Available KV cache memory: 32.40 GiB
[1;36m(EngineCore_DP0 pid=4004390)[0;0m INFO 12-04 14:01:08 [kv_cache_utils.py:1229] GPU KV cache size: 1,145,232 tokens
[1;36m(EngineCore_DP0 pid=4004390)[0;0m INFO 12-04 14:01:08 [kv_cache_utils.py:1234] Maximum concurrency for 131,072 tokens per request: 8.74x
[1;36m(EngineCore_DP0 pid=4004390)[0;0m INFO 12-04 14:01:08 [kernel_warmup.py:65] Warming up FlashInfer attention.
[1;36m(EngineCore_DP0 pid=4004389)[0;0m INFO 12-04 14:01:08 [kv_cache_utils.py:1229] GPU KV cache size: 1,213,184 tokens
[1;36m(EngineCore_DP0 pid=4004389)[0;0m INFO 12-04 14:01:08 [kv_cache_utils.py:1234] Maximum concurrency for 131,072 tokens per request: 9.26x
[1;36m(EngineCore_DP0 pid=4004389)[0;0m ERROR 12-04 14:01:08 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4004389)[0;0m ERROR 12-04 14:01:08 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4004389)[0;0m ERROR 12-04 14:01:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4004389)[0;0m ERROR 12-04 14:01:08 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4004389)[0;0m ERROR 12-04 14:01:08 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4004389)[0;0m ERROR 12-04 14:01:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4004389)[0;0m ERROR 12-04 14:01:08 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4004389)[0;0m ERROR 12-04 14:01:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4004389)[0;0m ERROR 12-04 14:01:08 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4004389)[0;0m ERROR 12-04 14:01:08 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4004389)[0;0m ERROR 12-04 14:01:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 247, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4004389)[0;0m ERROR 12-04 14:01:08 [core.py:842]     self.model_executor.initialize_from_config(kv_cache_configs)
[1;36m(EngineCore_DP0 pid=4004389)[0;0m ERROR 12-04 14:01:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 115, in initialize_from_config
[1;36m(EngineCore_DP0 pid=4004389)[0;0m ERROR 12-04 14:01:08 [core.py:842]     self.collective_rpc("initialize_from_config", args=(kv_cache_configs,))
[1;36m(EngineCore_DP0 pid=4004389)[0;0m ERROR 12-04 14:01:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4004389)[0;0m ERROR 12-04 14:01:08 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4004389)[0;0m ERROR 12-04 14:01:08 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4004389)[0;0m ERROR 12-04 14:01:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4004389)[0;0m ERROR 12-04 14:01:08 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4004389)[0;0m ERROR 12-04 14:01:08 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4004389)[0;0m ERROR 12-04 14:01:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 319, in initialize_from_config
[1;36m(EngineCore_DP0 pid=4004389)[0;0m ERROR 12-04 14:01:08 [core.py:842]     self.worker.initialize_from_config(kv_cache_config)  # type: ignore
[1;36m(EngineCore_DP0 pid=4004389)[0;0m ERROR 12-04 14:01:08 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4004389)[0;0m ERROR 12-04 14:01:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 404, in initialize_from_config
[1;36m(EngineCore_DP0 pid=4004389)[0;0m ERROR 12-04 14:01:08 [core.py:842]     self.model_runner.initialize_kv_cache(kv_cache_config)
[1;36m(EngineCore_DP0 pid=4004389)[0;0m ERROR 12-04 14:01:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 5007, in initialize_kv_cache
[1;36m(EngineCore_DP0 pid=4004389)[0;0m ERROR 12-04 14:01:08 [core.py:842]     kv_caches = self.initialize_kv_cache_tensors(
[1;36m(EngineCore_DP0 pid=4004389)[0;0m ERROR 12-04 14:01:08 [core.py:842]                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4004389)[0;0m ERROR 12-04 14:01:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4933, in initialize_kv_cache_tensors
[1;36m(EngineCore_DP0 pid=4004389)[0;0m ERROR 12-04 14:01:08 [core.py:842]     kv_cache_raw_tensors = self._allocate_kv_cache_tensors(kv_cache_config)
[1;36m(EngineCore_DP0 pid=4004389)[0;0m ERROR 12-04 14:01:08 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4004389)[0;0m ERROR 12-04 14:01:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4718, in _allocate_kv_cache_tensors
[1;36m(EngineCore_DP0 pid=4004389)[0;0m ERROR 12-04 14:01:08 [core.py:842]     tensor = torch.zeros(
[1;36m(EngineCore_DP0 pid=4004389)[0;0m ERROR 12-04 14:01:08 [core.py:842]              ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4004389)[0;0m ERROR 12-04 14:01:08 [core.py:842] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.16 GiB. GPU 0 has a total capacity of 44.29 GiB of which 117.69 MiB is free. Process 4004390 has 33.93 GiB memory in use. Including non-PyTorch memory, this process has 10.25 GiB memory in use. Of the allocated memory 9.90 GiB is allocated by PyTorch, and 135.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=4004389)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=4004389)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4004389)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=4004389)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=4004389)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=4004389)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=4004389)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=4004389)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=4004389)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4004389)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4004389)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4004389)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4004389)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=4004389)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4004389)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4004389)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4004389)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 247, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4004389)[0;0m     self.model_executor.initialize_from_config(kv_cache_configs)
[1;36m(EngineCore_DP0 pid=4004389)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 115, in initialize_from_config
[1;36m(EngineCore_DP0 pid=4004389)[0;0m     self.collective_rpc("initialize_from_config", args=(kv_cache_configs,))
[1;36m(EngineCore_DP0 pid=4004389)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4004389)[0;0m     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4004389)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4004389)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4004389)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4004389)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4004389)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 319, in initialize_from_config
[1;36m(EngineCore_DP0 pid=4004389)[0;0m     self.worker.initialize_from_config(kv_cache_config)  # type: ignore
[1;36m(EngineCore_DP0 pid=4004389)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4004389)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 404, in initialize_from_config
[1;36m(EngineCore_DP0 pid=4004389)[0;0m     self.model_runner.initialize_kv_cache(kv_cache_config)
[1;36m(EngineCore_DP0 pid=4004389)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 5007, in initialize_kv_cache
[1;36m(EngineCore_DP0 pid=4004389)[0;0m     kv_caches = self.initialize_kv_cache_tensors(
[1;36m(EngineCore_DP0 pid=4004389)[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4004389)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4933, in initialize_kv_cache_tensors
[1;36m(EngineCore_DP0 pid=4004389)[0;0m     kv_cache_raw_tensors = self._allocate_kv_cache_tensors(kv_cache_config)
[1;36m(EngineCore_DP0 pid=4004389)[0;0m                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4004389)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4718, in _allocate_kv_cache_tensors
[1;36m(EngineCore_DP0 pid=4004389)[0;0m     tensor = torch.zeros(
[1;36m(EngineCore_DP0 pid=4004389)[0;0m              ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4004389)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.16 GiB. GPU 0 has a total capacity of 44.29 GiB of which 117.69 MiB is free. Process 4004390 has 33.93 GiB memory in use. Including non-PyTorch memory, this process has 10.25 GiB memory in use. Of the allocated memory 9.90 GiB is allocated by PyTorch, and 135.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=4004390)[0;0m ERROR 12-04 14:01:08 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4004390)[0;0m ERROR 12-04 14:01:08 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4004390)[0;0m ERROR 12-04 14:01:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4004390)[0;0m ERROR 12-04 14:01:08 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4004390)[0;0m ERROR 12-04 14:01:08 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4004390)[0;0m ERROR 12-04 14:01:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4004390)[0;0m ERROR 12-04 14:01:08 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4004390)[0;0m ERROR 12-04 14:01:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4004390)[0;0m ERROR 12-04 14:01:08 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4004390)[0;0m ERROR 12-04 14:01:08 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4004390)[0;0m ERROR 12-04 14:01:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 247, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4004390)[0;0m ERROR 12-04 14:01:08 [core.py:842]     self.model_executor.initialize_from_config(kv_cache_configs)
[1;36m(EngineCore_DP0 pid=4004390)[0;0m ERROR 12-04 14:01:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 116, in initialize_from_config
[1;36m(EngineCore_DP0 pid=4004390)[0;0m ERROR 12-04 14:01:08 [core.py:842]     self.collective_rpc("compile_or_warm_up_model")
[1;36m(EngineCore_DP0 pid=4004390)[0;0m ERROR 12-04 14:01:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4004390)[0;0m ERROR 12-04 14:01:08 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4004390)[0;0m ERROR 12-04 14:01:08 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4004390)[0;0m ERROR 12-04 14:01:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4004390)[0;0m ERROR 12-04 14:01:08 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4004390)[0;0m ERROR 12-04 14:01:08 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4004390)[0;0m ERROR 12-04 14:01:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 425, in compile_or_warm_up_model
[1;36m(EngineCore_DP0 pid=4004390)[0;0m ERROR 12-04 14:01:08 [core.py:842]     kernel_warmup(self)
[1;36m(EngineCore_DP0 pid=4004390)[0;0m ERROR 12-04 14:01:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/warmup/kernel_warmup.py", line 68, in kernel_warmup
[1;36m(EngineCore_DP0 pid=4004390)[0;0m ERROR 12-04 14:01:08 [core.py:842]     worker.model_runner._dummy_run(
[1;36m(EngineCore_DP0 pid=4004390)[0;0m ERROR 12-04 14:01:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4004390)[0;0m ERROR 12-04 14:01:08 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4004390)[0;0m ERROR 12-04 14:01:08 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4004390)[0;0m ERROR 12-04 14:01:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3756, in _dummy_run
[1;36m(EngineCore_DP0 pid=4004390)[0;0m ERROR 12-04 14:01:08 [core.py:842]     attn_metadata, _ = self._build_attention_metadata(
[1;36m(EngineCore_DP0 pid=4004390)[0;0m ERROR 12-04 14:01:08 [core.py:842]                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4004390)[0;0m ERROR 12-04 14:01:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1576, in _build_attention_metadata
[1;36m(EngineCore_DP0 pid=4004390)[0;0m ERROR 12-04 14:01:08 [core.py:842]     attn_metadata_i = builder.build_for_cudagraph_capture(
[1;36m(EngineCore_DP0 pid=4004390)[0;0m ERROR 12-04 14:01:08 [core.py:842]                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4004390)[0;0m ERROR 12-04 14:01:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/attention/backends/utils.py", line 332, in build_for_cudagraph_capture
[1;36m(EngineCore_DP0 pid=4004390)[0;0m ERROR 12-04 14:01:08 [core.py:842]     return self.build(
[1;36m(EngineCore_DP0 pid=4004390)[0;0m ERROR 12-04 14:01:08 [core.py:842]            ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4004390)[0;0m ERROR 12-04 14:01:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/attention/backends/flashinfer.py", line 844, in build
[1;36m(EngineCore_DP0 pid=4004390)[0;0m ERROR 12-04 14:01:08 [core.py:842]     attn_metadata.prefill_wrapper = self._get_prefill_wrapper()
[1;36m(EngineCore_DP0 pid=4004390)[0;0m ERROR 12-04 14:01:08 [core.py:842]                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4004390)[0;0m ERROR 12-04 14:01:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/attention/backends/flashinfer.py", line 598, in _get_prefill_wrapper
[1;36m(EngineCore_DP0 pid=4004390)[0;0m ERROR 12-04 14:01:08 [core.py:842]     self._get_workspace_buffer(), get_kv_cache_layout()
[1;36m(EngineCore_DP0 pid=4004390)[0;0m ERROR 12-04 14:01:08 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4004390)[0;0m ERROR 12-04 14:01:08 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/attention/backends/flashinfer.py", line 583, in _get_workspace_buffer
[1;36m(EngineCore_DP0 pid=4004390)[0;0m ERROR 12-04 14:01:08 [core.py:842]     self._workspace_buffer = torch.zeros(
[1;36m(EngineCore_DP0 pid=4004390)[0;0m ERROR 12-04 14:01:08 [core.py:842]                              ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4004390)[0;0m ERROR 12-04 14:01:08 [core.py:842] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 394.00 MiB. GPU 0 has a total capacity of 44.29 GiB of which 117.69 MiB is free. Including non-PyTorch memory, this process has 33.93 GiB memory in use. Process 4004389 has 10.25 GiB memory in use. Of the allocated memory 33.54 GiB is allocated by PyTorch, and 172.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=4004390)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=4004390)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4004390)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=4004390)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=4004390)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=4004390)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=4004390)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=4004390)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=4004390)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4004390)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4004390)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4004390)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4004390)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=4004390)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4004390)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4004390)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4004390)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 247, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4004390)[0;0m     self.model_executor.initialize_from_config(kv_cache_configs)
[1;36m(EngineCore_DP0 pid=4004390)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 116, in initialize_from_config
[1;36m(EngineCore_DP0 pid=4004390)[0;0m     self.collective_rpc("compile_or_warm_up_model")
[1;36m(EngineCore_DP0 pid=4004390)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4004390)[0;0m     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4004390)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4004390)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4004390)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4004390)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4004390)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 425, in compile_or_warm_up_model
[1;36m(EngineCore_DP0 pid=4004390)[0;0m     kernel_warmup(self)
[1;36m(EngineCore_DP0 pid=4004390)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/warmup/kernel_warmup.py", line 68, in kernel_warmup
[1;36m(EngineCore_DP0 pid=4004390)[0;0m     worker.model_runner._dummy_run(
[1;36m(EngineCore_DP0 pid=4004390)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4004390)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4004390)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4004390)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3756, in _dummy_run
[1;36m(EngineCore_DP0 pid=4004390)[0;0m     attn_metadata, _ = self._build_attention_metadata(
[1;36m(EngineCore_DP0 pid=4004390)[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4004390)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1576, in _build_attention_metadata
[1;36m(EngineCore_DP0 pid=4004390)[0;0m     attn_metadata_i = builder.build_for_cudagraph_capture(
[1;36m(EngineCore_DP0 pid=4004390)[0;0m                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4004390)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/attention/backends/utils.py", line 332, in build_for_cudagraph_capture
[1;36m(EngineCore_DP0 pid=4004390)[0;0m     return self.build(
[1;36m(EngineCore_DP0 pid=4004390)[0;0m            ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4004390)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/attention/backends/flashinfer.py", line 844, in build
[1;36m(EngineCore_DP0 pid=4004390)[0;0m     attn_metadata.prefill_wrapper = self._get_prefill_wrapper()
[1;36m(EngineCore_DP0 pid=4004390)[0;0m                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4004390)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/attention/backends/flashinfer.py", line 598, in _get_prefill_wrapper
[1;36m(EngineCore_DP0 pid=4004390)[0;0m     self._get_workspace_buffer(), get_kv_cache_layout()
[1;36m(EngineCore_DP0 pid=4004390)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4004390)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/attention/backends/flashinfer.py", line 583, in _get_workspace_buffer
[1;36m(EngineCore_DP0 pid=4004390)[0;0m     self._workspace_buffer = torch.zeros(
[1;36m(EngineCore_DP0 pid=4004390)[0;0m                              ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4004390)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 394.00 MiB. GPU 0 has a total capacity of 44.29 GiB of which 117.69 MiB is free. Including non-PyTorch memory, this process has 33.93 GiB memory in use. Process 4004389 has 10.25 GiB memory in use. Of the allocated memory 33.54 GiB is allocated by PyTorch, and 172.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W1204 14:01:08.876340305 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank0]:[W1204 14:01:09.487666264 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: WeiboAI/VibeThinker-1.5B (backend=vllm)
[ModelCache] Using max_model_len=131072 for WeiboAI/VibeThinker-1.5B
[ModelCache] Detected 1 GPU(s), 44.3GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 14:01:37 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'WeiboAI/VibeThinker-1.5B'}
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: WeiboAI/VibeThinker-1.5B (backend=vllm)
[ModelCache] Using max_model_len=131072 for WeiboAI/VibeThinker-1.5B
[ModelCache] Detected 1 GPU(s), 44.3GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 14:01:37 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'WeiboAI/VibeThinker-1.5B'}
INFO 12-04 14:01:37 [model.py:631] Resolved architecture: Qwen2ForCausalLM
WARNING 12-04 14:01:37 [model.py:1921] Your device 'Quadro RTX 8000' (with compute capability 7.5) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.
WARNING 12-04 14:01:37 [model.py:1971] Casting torch.bfloat16 to torch.float16.
INFO 12-04 14:01:37 [model.py:1745] Using max model len 131072
INFO 12-04 14:01:37 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
INFO 12-04 14:01:37 [model.py:631] Resolved architecture: Qwen2ForCausalLM
WARNING 12-04 14:01:37 [model.py:1921] Your device 'Quadro RTX 8000' (with compute capability 7.5) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.
WARNING 12-04 14:01:37 [model.py:1971] Casting torch.bfloat16 to torch.float16.
INFO 12-04 14:01:37 [model.py:1745] Using max model len 131072
INFO 12-04 14:01:37 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4004600)[0;0m INFO 12-04 14:02:09 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='WeiboAI/VibeThinker-1.5B', speculative_config=None, tokenizer='WeiboAI/VibeThinker-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=WeiboAI/VibeThinker-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4004700)[0;0m INFO 12-04 14:02:09 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='WeiboAI/VibeThinker-1.5B', speculative_config=None, tokenizer='WeiboAI/VibeThinker-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=WeiboAI/VibeThinker-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4004600)[0;0m ERROR 12-04 14:02:12 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8
[1;36m(EngineCore_DP0 pid=4004700)[0;0m ERROR 12-04 14:02:12 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8
[1;36m(EngineCore_DP0 pid=4004700)[0;0m INFO 12-04 14:02:13 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.8:60599 backend=nccl
[1;36m(EngineCore_DP0 pid=4004600)[0;0m INFO 12-04 14:02:13 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.8:60421 backend=nccl
[W1204 14:02:13.644411644 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-23-0.rc.tch.harvard.edu]:60599 (errno: 97 - Address family not supported by protocol).
[W1204 14:02:13.645375116 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-23-0.rc.tch.harvard.edu]:60421 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4004700)[0;0m INFO 12-04 14:02:13 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4004600)[0;0m INFO 12-04 14:02:13 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4004600)[0;0m INFO 12-04 14:02:14 [gpu_model_runner.py:3259] Starting to load model WeiboAI/VibeThinker-1.5B...
[1;36m(EngineCore_DP0 pid=4004700)[0;0m INFO 12-04 14:02:14 [gpu_model_runner.py:3259] Starting to load model WeiboAI/VibeThinker-1.5B...
[1;36m(EngineCore_DP0 pid=4004600)[0;0m INFO 12-04 14:02:15 [cuda.py:418] Valid backends: ['FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4004600)[0;0m INFO 12-04 14:02:15 [cuda.py:427] Using FLASHINFER backend.
[1;36m(EngineCore_DP0 pid=4004700)[0;0m INFO 12-04 14:02:15 [cuda.py:418] Valid backends: ['FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4004700)[0;0m INFO 12-04 14:02:15 [cuda.py:427] Using FLASHINFER backend.
[1;36m(EngineCore_DP0 pid=4004700)[0;0m INFO 12-04 14:02:16 [weight_utils.py:481] No model.safetensors.index.json found in remote.
[1;36m(EngineCore_DP0 pid=4004700)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
[1;36m(EngineCore_DP0 pid=4004600)[0;0m INFO 12-04 14:02:16 [weight_utils.py:481] No model.safetensors.index.json found in remote.
[1;36m(EngineCore_DP0 pid=4004600)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
[1;36m(EngineCore_DP0 pid=4004700)[0;0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.77s/it]
[1;36m(EngineCore_DP0 pid=4004700)[0;0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.77s/it]
[1;36m(EngineCore_DP0 pid=4004700)[0;0m 
[1;36m(EngineCore_DP0 pid=4004600)[0;0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.73s/it]
[1;36m(EngineCore_DP0 pid=4004600)[0;0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.73s/it]
[1;36m(EngineCore_DP0 pid=4004600)[0;0m 
[1;36m(EngineCore_DP0 pid=4004700)[0;0m INFO 12-04 14:02:20 [default_loader.py:314] Loading weights took 3.91 seconds
[1;36m(EngineCore_DP0 pid=4004600)[0;0m INFO 12-04 14:02:20 [default_loader.py:314] Loading weights took 3.86 seconds
[1;36m(EngineCore_DP0 pid=4004700)[0;0m INFO 12-04 14:02:20 [gpu_model_runner.py:3338] Model loading took 2.9110 GiB memory and 5.370311 seconds
[1;36m(EngineCore_DP0 pid=4004600)[0;0m INFO 12-04 14:02:20 [gpu_model_runner.py:3338] Model loading took 2.9110 GiB memory and 5.423149 seconds
[1;36m(EngineCore_DP0 pid=4004700)[0;0m INFO 12-04 14:02:36 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/6847b7a26b/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4004600)[0;0m INFO 12-04 14:02:36 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/6847b7a26b/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4004600)[0;0m INFO 12-04 14:02:37 [backends.py:647] Dynamo bytecode transform time: 15.96 s
[1;36m(EngineCore_DP0 pid=4004700)[0;0m INFO 12-04 14:02:37 [backends.py:647] Dynamo bytecode transform time: 15.98 s
[1;36m(EngineCore_DP0 pid=4004700)[0;0m INFO 12-04 14:02:42 [backends.py:210] Directly load the compiled graph(s) for dynamic shape from the cache, took 5.371 s
[1;36m(EngineCore_DP0 pid=4004600)[0;0m INFO 12-04 14:02:42 [backends.py:210] Directly load the compiled graph(s) for dynamic shape from the cache, took 5.372 s
[1;36m(EngineCore_DP0 pid=4004700)[0;0m INFO 12-04 14:02:44 [monitor.py:34] torch.compile takes 21.35 s in total
[1;36m(EngineCore_DP0 pid=4004600)[0;0m INFO 12-04 14:02:44 [monitor.py:34] torch.compile takes 21.33 s in total
[1;36m(EngineCore_DP0 pid=4004600)[0;0m INFO 12-04 14:02:46 [gpu_worker.py:359] Available KV cache memory: 30.65 GiB
[1;36m(EngineCore_DP0 pid=4004700)[0;0m INFO 12-04 14:02:46 [gpu_worker.py:359] Available KV cache memory: 32.40 GiB
[1;36m(EngineCore_DP0 pid=4004600)[0;0m INFO 12-04 14:02:46 [kv_cache_utils.py:1229] GPU KV cache size: 1,147,712 tokens
[1;36m(EngineCore_DP0 pid=4004600)[0;0m INFO 12-04 14:02:46 [kv_cache_utils.py:1234] Maximum concurrency for 131,072 tokens per request: 8.76x
[1;36m(EngineCore_DP0 pid=4004700)[0;0m INFO 12-04 14:02:46 [kv_cache_utils.py:1229] GPU KV cache size: 1,213,248 tokens
[1;36m(EngineCore_DP0 pid=4004700)[0;0m INFO 12-04 14:02:46 [kv_cache_utils.py:1234] Maximum concurrency for 131,072 tokens per request: 9.26x
[1;36m(EngineCore_DP0 pid=4004600)[0;0m INFO 12-04 14:02:46 [kernel_warmup.py:65] Warming up FlashInfer attention.
[1;36m(EngineCore_DP0 pid=4004700)[0;0m ERROR 12-04 14:02:46 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4004700)[0;0m ERROR 12-04 14:02:46 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4004700)[0;0m ERROR 12-04 14:02:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4004700)[0;0m ERROR 12-04 14:02:46 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4004700)[0;0m ERROR 12-04 14:02:46 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4004700)[0;0m ERROR 12-04 14:02:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4004700)[0;0m ERROR 12-04 14:02:46 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4004700)[0;0m ERROR 12-04 14:02:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4004700)[0;0m ERROR 12-04 14:02:46 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4004700)[0;0m ERROR 12-04 14:02:46 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4004700)[0;0m ERROR 12-04 14:02:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 247, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4004700)[0;0m ERROR 12-04 14:02:46 [core.py:842]     self.model_executor.initialize_from_config(kv_cache_configs)
[1;36m(EngineCore_DP0 pid=4004700)[0;0m ERROR 12-04 14:02:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 115, in initialize_from_config
[1;36m(EngineCore_DP0 pid=4004700)[0;0m ERROR 12-04 14:02:46 [core.py:842]     self.collective_rpc("initialize_from_config", args=(kv_cache_configs,))
[1;36m(EngineCore_DP0 pid=4004700)[0;0m ERROR 12-04 14:02:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4004700)[0;0m ERROR 12-04 14:02:46 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4004700)[0;0m ERROR 12-04 14:02:46 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4004700)[0;0m ERROR 12-04 14:02:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4004700)[0;0m ERROR 12-04 14:02:46 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4004700)[0;0m ERROR 12-04 14:02:46 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4004700)[0;0m ERROR 12-04 14:02:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 319, in initialize_from_config
[1;36m(EngineCore_DP0 pid=4004700)[0;0m ERROR 12-04 14:02:46 [core.py:842]     self.worker.initialize_from_config(kv_cache_config)  # type: ignore
[1;36m(EngineCore_DP0 pid=4004700)[0;0m ERROR 12-04 14:02:46 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4004700)[0;0m ERROR 12-04 14:02:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 404, in initialize_from_config
[1;36m(EngineCore_DP0 pid=4004700)[0;0m ERROR 12-04 14:02:46 [core.py:842]     self.model_runner.initialize_kv_cache(kv_cache_config)
[1;36m(EngineCore_DP0 pid=4004700)[0;0m ERROR 12-04 14:02:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 5007, in initialize_kv_cache
[1;36m(EngineCore_DP0 pid=4004700)[0;0m ERROR 12-04 14:02:46 [core.py:842]     kv_caches = self.initialize_kv_cache_tensors(
[1;36m(EngineCore_DP0 pid=4004700)[0;0m ERROR 12-04 14:02:46 [core.py:842]                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4004700)[0;0m ERROR 12-04 14:02:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4933, in initialize_kv_cache_tensors
[1;36m(EngineCore_DP0 pid=4004700)[0;0m ERROR 12-04 14:02:46 [core.py:842]     kv_cache_raw_tensors = self._allocate_kv_cache_tensors(kv_cache_config)
[1;36m(EngineCore_DP0 pid=4004700)[0;0m ERROR 12-04 14:02:46 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4004700)[0;0m ERROR 12-04 14:02:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4718, in _allocate_kv_cache_tensors
[1;36m(EngineCore_DP0 pid=4004700)[0;0m ERROR 12-04 14:02:46 [core.py:842]     tensor = torch.zeros(
[1;36m(EngineCore_DP0 pid=4004700)[0;0m ERROR 12-04 14:02:46 [core.py:842]              ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4004700)[0;0m ERROR 12-04 14:02:46 [core.py:842] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.16 GiB. GPU 0 has a total capacity of 44.29 GiB of which 61.69 MiB is free. Process 4004600 has 33.98 GiB memory in use. Including non-PyTorch memory, this process has 10.25 GiB memory in use. Of the allocated memory 9.90 GiB is allocated by PyTorch, and 134.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=4004700)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=4004700)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4004700)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=4004700)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=4004700)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=4004700)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=4004700)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=4004700)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=4004700)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4004700)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4004700)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4004700)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4004700)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=4004700)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4004700)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4004700)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4004700)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 247, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4004700)[0;0m     self.model_executor.initialize_from_config(kv_cache_configs)
[1;36m(EngineCore_DP0 pid=4004700)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 115, in initialize_from_config
[1;36m(EngineCore_DP0 pid=4004700)[0;0m     self.collective_rpc("initialize_from_config", args=(kv_cache_configs,))
[1;36m(EngineCore_DP0 pid=4004700)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4004700)[0;0m     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4004700)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4004700)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4004700)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4004700)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4004700)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 319, in initialize_from_config
[1;36m(EngineCore_DP0 pid=4004700)[0;0m     self.worker.initialize_from_config(kv_cache_config)  # type: ignore
[1;36m(EngineCore_DP0 pid=4004700)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4004700)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 404, in initialize_from_config
[1;36m(EngineCore_DP0 pid=4004700)[0;0m     self.model_runner.initialize_kv_cache(kv_cache_config)
[1;36m(EngineCore_DP0 pid=4004700)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 5007, in initialize_kv_cache
[1;36m(EngineCore_DP0 pid=4004700)[0;0m     kv_caches = self.initialize_kv_cache_tensors(
[1;36m(EngineCore_DP0 pid=4004700)[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4004700)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4933, in initialize_kv_cache_tensors
[1;36m(EngineCore_DP0 pid=4004700)[0;0m     kv_cache_raw_tensors = self._allocate_kv_cache_tensors(kv_cache_config)
[1;36m(EngineCore_DP0 pid=4004700)[0;0m                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4004700)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4718, in _allocate_kv_cache_tensors
[1;36m(EngineCore_DP0 pid=4004700)[0;0m     tensor = torch.zeros(
[1;36m(EngineCore_DP0 pid=4004700)[0;0m              ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4004700)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.16 GiB. GPU 0 has a total capacity of 44.29 GiB of which 61.69 MiB is free. Process 4004600 has 33.98 GiB memory in use. Including non-PyTorch memory, this process has 10.25 GiB memory in use. Of the allocated memory 9.90 GiB is allocated by PyTorch, and 134.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=4004600)[0;0m ERROR 12-04 14:02:46 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4004600)[0;0m ERROR 12-04 14:02:46 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4004600)[0;0m ERROR 12-04 14:02:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4004600)[0;0m ERROR 12-04 14:02:46 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4004600)[0;0m ERROR 12-04 14:02:46 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4004600)[0;0m ERROR 12-04 14:02:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4004600)[0;0m ERROR 12-04 14:02:46 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4004600)[0;0m ERROR 12-04 14:02:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4004600)[0;0m ERROR 12-04 14:02:46 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4004600)[0;0m ERROR 12-04 14:02:46 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4004600)[0;0m ERROR 12-04 14:02:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 247, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4004600)[0;0m ERROR 12-04 14:02:46 [core.py:842]     self.model_executor.initialize_from_config(kv_cache_configs)
[1;36m(EngineCore_DP0 pid=4004600)[0;0m ERROR 12-04 14:02:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 116, in initialize_from_config
[1;36m(EngineCore_DP0 pid=4004600)[0;0m ERROR 12-04 14:02:46 [core.py:842]     self.collective_rpc("compile_or_warm_up_model")
[1;36m(EngineCore_DP0 pid=4004600)[0;0m ERROR 12-04 14:02:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4004600)[0;0m ERROR 12-04 14:02:46 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4004600)[0;0m ERROR 12-04 14:02:46 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4004600)[0;0m ERROR 12-04 14:02:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4004600)[0;0m ERROR 12-04 14:02:46 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4004600)[0;0m ERROR 12-04 14:02:46 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4004600)[0;0m ERROR 12-04 14:02:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 425, in compile_or_warm_up_model
[1;36m(EngineCore_DP0 pid=4004600)[0;0m ERROR 12-04 14:02:46 [core.py:842]     kernel_warmup(self)
[1;36m(EngineCore_DP0 pid=4004600)[0;0m ERROR 12-04 14:02:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/warmup/kernel_warmup.py", line 68, in kernel_warmup
[1;36m(EngineCore_DP0 pid=4004600)[0;0m ERROR 12-04 14:02:46 [core.py:842]     worker.model_runner._dummy_run(
[1;36m(EngineCore_DP0 pid=4004600)[0;0m ERROR 12-04 14:02:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4004600)[0;0m ERROR 12-04 14:02:46 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4004600)[0;0m ERROR 12-04 14:02:46 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4004600)[0;0m ERROR 12-04 14:02:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3756, in _dummy_run
[1;36m(EngineCore_DP0 pid=4004600)[0;0m ERROR 12-04 14:02:46 [core.py:842]     attn_metadata, _ = self._build_attention_metadata(
[1;36m(EngineCore_DP0 pid=4004600)[0;0m ERROR 12-04 14:02:46 [core.py:842]                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4004600)[0;0m ERROR 12-04 14:02:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1576, in _build_attention_metadata
[1;36m(EngineCore_DP0 pid=4004600)[0;0m ERROR 12-04 14:02:46 [core.py:842]     attn_metadata_i = builder.build_for_cudagraph_capture(
[1;36m(EngineCore_DP0 pid=4004600)[0;0m ERROR 12-04 14:02:46 [core.py:842]                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4004600)[0;0m ERROR 12-04 14:02:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/attention/backends/utils.py", line 332, in build_for_cudagraph_capture
[1;36m(EngineCore_DP0 pid=4004600)[0;0m ERROR 12-04 14:02:46 [core.py:842]     return self.build(
[1;36m(EngineCore_DP0 pid=4004600)[0;0m ERROR 12-04 14:02:46 [core.py:842]            ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4004600)[0;0m ERROR 12-04 14:02:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/attention/backends/flashinfer.py", line 844, in build
[1;36m(EngineCore_DP0 pid=4004600)[0;0m ERROR 12-04 14:02:46 [core.py:842]     attn_metadata.prefill_wrapper = self._get_prefill_wrapper()
[1;36m(EngineCore_DP0 pid=4004600)[0;0m ERROR 12-04 14:02:46 [core.py:842]                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4004600)[0;0m ERROR 12-04 14:02:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/attention/backends/flashinfer.py", line 598, in _get_prefill_wrapper
[1;36m(EngineCore_DP0 pid=4004600)[0;0m ERROR 12-04 14:02:46 [core.py:842]     self._get_workspace_buffer(), get_kv_cache_layout()
[1;36m(EngineCore_DP0 pid=4004600)[0;0m ERROR 12-04 14:02:46 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4004600)[0;0m ERROR 12-04 14:02:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/attention/backends/flashinfer.py", line 583, in _get_workspace_buffer
[1;36m(EngineCore_DP0 pid=4004600)[0;0m ERROR 12-04 14:02:46 [core.py:842]     self._workspace_buffer = torch.zeros(
[1;36m(EngineCore_DP0 pid=4004600)[0;0m ERROR 12-04 14:02:46 [core.py:842]                              ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4004600)[0;0m ERROR 12-04 14:02:46 [core.py:842] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 394.00 MiB. GPU 0 has a total capacity of 44.29 GiB of which 61.69 MiB is free. Including non-PyTorch memory, this process has 33.98 GiB memory in use. Process 4004700 has 10.25 GiB memory in use. Of the allocated memory 33.61 GiB is allocated by PyTorch, and 161.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=4004600)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=4004600)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4004600)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=4004600)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=4004600)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=4004600)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=4004600)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=4004600)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=4004600)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4004600)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4004600)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4004600)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4004600)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=4004600)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4004600)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4004600)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4004600)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 247, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4004600)[0;0m     self.model_executor.initialize_from_config(kv_cache_configs)
[1;36m(EngineCore_DP0 pid=4004600)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 116, in initialize_from_config
[1;36m(EngineCore_DP0 pid=4004600)[0;0m     self.collective_rpc("compile_or_warm_up_model")
[1;36m(EngineCore_DP0 pid=4004600)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4004600)[0;0m     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4004600)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4004600)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4004600)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4004600)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4004600)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 425, in compile_or_warm_up_model
[1;36m(EngineCore_DP0 pid=4004600)[0;0m     kernel_warmup(self)
[1;36m(EngineCore_DP0 pid=4004600)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/warmup/kernel_warmup.py", line 68, in kernel_warmup
[1;36m(EngineCore_DP0 pid=4004600)[0;0m     worker.model_runner._dummy_run(
[1;36m(EngineCore_DP0 pid=4004600)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4004600)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4004600)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4004600)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3756, in _dummy_run
[1;36m(EngineCore_DP0 pid=4004600)[0;0m     attn_metadata, _ = self._build_attention_metadata(
[1;36m(EngineCore_DP0 pid=4004600)[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4004600)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1576, in _build_attention_metadata
[1;36m(EngineCore_DP0 pid=4004600)[0;0m     attn_metadata_i = builder.build_for_cudagraph_capture(
[1;36m(EngineCore_DP0 pid=4004600)[0;0m                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4004600)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/attention/backends/utils.py", line 332, in build_for_cudagraph_capture
[1;36m(EngineCore_DP0 pid=4004600)[0;0m     return self.build(
[1;36m(EngineCore_DP0 pid=4004600)[0;0m            ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4004600)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/attention/backends/flashinfer.py", line 844, in build
[1;36m(EngineCore_DP0 pid=4004600)[0;0m     attn_metadata.prefill_wrapper = self._get_prefill_wrapper()
[1;36m(EngineCore_DP0 pid=4004600)[0;0m                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4004600)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/attention/backends/flashinfer.py", line 598, in _get_prefill_wrapper
[1;36m(EngineCore_DP0 pid=4004600)[0;0m     self._get_workspace_buffer(), get_kv_cache_layout()
[1;36m(EngineCore_DP0 pid=4004600)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4004600)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/attention/backends/flashinfer.py", line 583, in _get_workspace_buffer
[1;36m(EngineCore_DP0 pid=4004600)[0;0m     self._workspace_buffer = torch.zeros(
[1;36m(EngineCore_DP0 pid=4004600)[0;0m                              ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4004600)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 394.00 MiB. GPU 0 has a total capacity of 44.29 GiB of which 61.69 MiB is free. Including non-PyTorch memory, this process has 33.98 GiB memory in use. Process 4004700 has 10.25 GiB memory in use. Of the allocated memory 33.61 GiB is allocated by PyTorch, and 161.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W1204 14:02:47.463258896 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank0]:[W1204 14:02:47.618302507 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: WeiboAI/VibeThinker-1.5B (backend=vllm)
[ModelCache] Using max_model_len=131072 for WeiboAI/VibeThinker-1.5B
[ModelCache] Detected 1 GPU(s), 44.3GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 14:03:08 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'WeiboAI/VibeThinker-1.5B'}
INFO 12-04 14:03:09 [model.py:631] Resolved architecture: Qwen2ForCausalLM
WARNING 12-04 14:03:09 [model.py:1921] Your device 'Quadro RTX 8000' (with compute capability 7.5) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.
WARNING 12-04 14:03:09 [model.py:1971] Casting torch.bfloat16 to torch.float16.
INFO 12-04 14:03:09 [model.py:1745] Using max model len 131072
INFO 12-04 14:03:09 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: WeiboAI/VibeThinker-1.5B (backend=vllm)
[ModelCache] Using max_model_len=131072 for WeiboAI/VibeThinker-1.5B
[ModelCache] Detected 1 GPU(s), 44.3GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 14:03:09 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'WeiboAI/VibeThinker-1.5B'}
INFO 12-04 14:03:09 [model.py:631] Resolved architecture: Qwen2ForCausalLM
WARNING 12-04 14:03:09 [model.py:1921] Your device 'Quadro RTX 8000' (with compute capability 7.5) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.
WARNING 12-04 14:03:09 [model.py:1971] Casting torch.bfloat16 to torch.float16.
INFO 12-04 14:03:09 [model.py:1745] Using max model len 131072
INFO 12-04 14:03:09 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4004968)[0;0m INFO 12-04 14:03:51 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='WeiboAI/VibeThinker-1.5B', speculative_config=None, tokenizer='WeiboAI/VibeThinker-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=WeiboAI/VibeThinker-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4004971)[0;0m INFO 12-04 14:03:51 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='WeiboAI/VibeThinker-1.5B', speculative_config=None, tokenizer='WeiboAI/VibeThinker-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=WeiboAI/VibeThinker-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4004971)[0;0m ERROR 12-04 14:03:53 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8
[1;36m(EngineCore_DP0 pid=4004968)[0;0m ERROR 12-04 14:03:53 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8
[1;36m(EngineCore_DP0 pid=4004968)[0;0m INFO 12-04 14:03:54 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.8:45889 backend=nccl
[1;36m(EngineCore_DP0 pid=4004971)[0;0m INFO 12-04 14:03:54 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.8:52641 backend=nccl
[W1204 14:03:54.610396805 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-23-0.rc.tch.harvard.edu]:45889 (errno: 97 - Address family not supported by protocol).
[W1204 14:03:54.611511371 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-23-0.rc.tch.harvard.edu]:52641 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4004971)[0;0m INFO 12-04 14:03:54 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4004968)[0;0m INFO 12-04 14:03:54 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4004968)[0;0m INFO 12-04 14:03:55 [gpu_model_runner.py:3259] Starting to load model WeiboAI/VibeThinker-1.5B...
[1;36m(EngineCore_DP0 pid=4004971)[0;0m INFO 12-04 14:03:55 [gpu_model_runner.py:3259] Starting to load model WeiboAI/VibeThinker-1.5B...
[1;36m(EngineCore_DP0 pid=4004971)[0;0m INFO 12-04 14:03:56 [cuda.py:418] Valid backends: ['FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4004971)[0;0m INFO 12-04 14:03:56 [cuda.py:427] Using FLASHINFER backend.
[1;36m(EngineCore_DP0 pid=4004968)[0;0m INFO 12-04 14:03:56 [cuda.py:418] Valid backends: ['FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4004968)[0;0m INFO 12-04 14:03:56 [cuda.py:427] Using FLASHINFER backend.
[1;36m(EngineCore_DP0 pid=4004968)[0;0m INFO 12-04 14:03:57 [weight_utils.py:481] No model.safetensors.index.json found in remote.
[1;36m(EngineCore_DP0 pid=4004968)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
[1;36m(EngineCore_DP0 pid=4004971)[0;0m INFO 12-04 14:03:57 [weight_utils.py:481] No model.safetensors.index.json found in remote.
[1;36m(EngineCore_DP0 pid=4004971)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
[1;36m(EngineCore_DP0 pid=4004968)[0;0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:04<00:00,  4.17s/it]
[1;36m(EngineCore_DP0 pid=4004968)[0;0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:04<00:00,  4.17s/it]
[1;36m(EngineCore_DP0 pid=4004968)[0;0m 
[1;36m(EngineCore_DP0 pid=4004968)[0;0m INFO 12-04 14:04:01 [default_loader.py:314] Loading weights took 4.30 seconds
[1;36m(EngineCore_DP0 pid=4004971)[0;0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:04<00:00,  4.27s/it]
[1;36m(EngineCore_DP0 pid=4004971)[0;0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:04<00:00,  4.27s/it]
[1;36m(EngineCore_DP0 pid=4004971)[0;0m 
[1;36m(EngineCore_DP0 pid=4004971)[0;0m INFO 12-04 14:04:01 [default_loader.py:314] Loading weights took 4.40 seconds
[1;36m(EngineCore_DP0 pid=4004968)[0;0m INFO 12-04 14:04:02 [gpu_model_runner.py:3338] Model loading took 2.9110 GiB memory and 5.916828 seconds
[1;36m(EngineCore_DP0 pid=4004971)[0;0m INFO 12-04 14:04:02 [gpu_model_runner.py:3338] Model loading took 2.9110 GiB memory and 6.152106 seconds
[1;36m(EngineCore_DP0 pid=4004968)[0;0m INFO 12-04 14:04:19 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/6847b7a26b/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4004971)[0;0m INFO 12-04 14:04:19 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/6847b7a26b/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4004971)[0;0m INFO 12-04 14:04:19 [backends.py:647] Dynamo bytecode transform time: 17.26 s
[1;36m(EngineCore_DP0 pid=4004968)[0;0m INFO 12-04 14:04:19 [backends.py:647] Dynamo bytecode transform time: 17.45 s
[1;36m(EngineCore_DP0 pid=4004971)[0;0m INFO 12-04 14:04:25 [backends.py:210] Directly load the compiled graph(s) for dynamic shape from the cache, took 5.452 s
[1;36m(EngineCore_DP0 pid=4004968)[0;0m INFO 12-04 14:04:25 [backends.py:210] Directly load the compiled graph(s) for dynamic shape from the cache, took 5.451 s
[1;36m(EngineCore_DP0 pid=4004968)[0;0m INFO 12-04 14:04:27 [monitor.py:34] torch.compile takes 22.90 s in total
[1;36m(EngineCore_DP0 pid=4004971)[0;0m INFO 12-04 14:04:27 [monitor.py:34] torch.compile takes 22.71 s in total
[1;36m(EngineCore_DP0 pid=4004968)[0;0m INFO 12-04 14:04:29 [gpu_worker.py:359] Available KV cache memory: 31.67 GiB
[1;36m(EngineCore_DP0 pid=4004971)[0;0m INFO 12-04 14:04:29 [gpu_worker.py:359] Available KV cache memory: 32.40 GiB
[1;36m(EngineCore_DP0 pid=4004968)[0;0m INFO 12-04 14:04:29 [kv_cache_utils.py:1229] GPU KV cache size: 1,185,888 tokens
[1;36m(EngineCore_DP0 pid=4004968)[0;0m INFO 12-04 14:04:29 [kv_cache_utils.py:1234] Maximum concurrency for 131,072 tokens per request: 9.05x
[1;36m(EngineCore_DP0 pid=4004971)[0;0m INFO 12-04 14:04:29 [kv_cache_utils.py:1229] GPU KV cache size: 1,213,184 tokens
[1;36m(EngineCore_DP0 pid=4004971)[0;0m INFO 12-04 14:04:29 [kv_cache_utils.py:1234] Maximum concurrency for 131,072 tokens per request: 9.26x
[1;36m(EngineCore_DP0 pid=4004971)[0;0m ERROR 12-04 14:04:29 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4004971)[0;0m ERROR 12-04 14:04:29 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4004971)[0;0m ERROR 12-04 14:04:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4004971)[0;0m ERROR 12-04 14:04:29 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4004971)[0;0m ERROR 12-04 14:04:29 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4004971)[0;0m ERROR 12-04 14:04:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4004971)[0;0m ERROR 12-04 14:04:29 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4004971)[0;0m ERROR 12-04 14:04:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4004971)[0;0m ERROR 12-04 14:04:29 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4004971)[0;0m ERROR 12-04 14:04:29 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4004971)[0;0m ERROR 12-04 14:04:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 247, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4004971)[0;0m ERROR 12-04 14:04:29 [core.py:842]     self.model_executor.initialize_from_config(kv_cache_configs)
[1;36m(EngineCore_DP0 pid=4004971)[0;0m ERROR 12-04 14:04:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 115, in initialize_from_config
[1;36m(EngineCore_DP0 pid=4004971)[0;0m ERROR 12-04 14:04:29 [core.py:842]     self.collective_rpc("initialize_from_config", args=(kv_cache_configs,))
[1;36m(EngineCore_DP0 pid=4004971)[0;0m ERROR 12-04 14:04:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4004971)[0;0m ERROR 12-04 14:04:29 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4004971)[0;0m ERROR 12-04 14:04:29 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4004971)[0;0m ERROR 12-04 14:04:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4004971)[0;0m ERROR 12-04 14:04:29 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4004971)[0;0m ERROR 12-04 14:04:29 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4004971)[0;0m ERROR 12-04 14:04:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 319, in initialize_from_config
[1;36m(EngineCore_DP0 pid=4004971)[0;0m ERROR 12-04 14:04:29 [core.py:842]     self.worker.initialize_from_config(kv_cache_config)  # type: ignore
[1;36m(EngineCore_DP0 pid=4004971)[0;0m ERROR 12-04 14:04:29 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4004971)[0;0m ERROR 12-04 14:04:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 404, in initialize_from_config
[1;36m(EngineCore_DP0 pid=4004971)[0;0m ERROR 12-04 14:04:29 [core.py:842]     self.model_runner.initialize_kv_cache(kv_cache_config)
[1;36m(EngineCore_DP0 pid=4004968)[0;0m ERROR 12-04 14:04:29 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4004968)[0;0m ERROR 12-04 14:04:29 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4004968)[0;0m ERROR 12-04 14:04:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4004968)[0;0m ERROR 12-04 14:04:29 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4004968)[0;0m ERROR 12-04 14:04:29 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4004968)[0;0m ERROR 12-04 14:04:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4004968)[0;0m ERROR 12-04 14:04:29 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4004968)[0;0m ERROR 12-04 14:04:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4004968)[0;0m ERROR 12-04 14:04:29 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4004968)[0;0m ERROR 12-04 14:04:29 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4004968)[0;0m ERROR 12-04 14:04:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 247, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4004968)[0;0m ERROR 12-04 14:04:29 [core.py:842]     self.model_executor.initialize_from_config(kv_cache_configs)
[1;36m(EngineCore_DP0 pid=4004968)[0;0m ERROR 12-04 14:04:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 115, in initialize_from_config
[1;36m(EngineCore_DP0 pid=4004968)[0;0m ERROR 12-04 14:04:29 [core.py:842]     self.collective_rpc("initialize_from_config", args=(kv_cache_configs,))
[1;36m(EngineCore_DP0 pid=4004968)[0;0m ERROR 12-04 14:04:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4004968)[0;0m ERROR 12-04 14:04:29 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4004968)[0;0m ERROR 12-04 14:04:29 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4004968)[0;0m ERROR 12-04 14:04:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4004968)[0;0m ERROR 12-04 14:04:29 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4004968)[0;0m ERROR 12-04 14:04:29 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4004968)[0;0m ERROR 12-04 14:04:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 319, in initialize_from_config
[1;36m(EngineCore_DP0 pid=4004968)[0;0m ERROR 12-04 14:04:29 [core.py:842]     self.worker.initialize_from_config(kv_cache_config)  # type: ignore
[1;36m(EngineCore_DP0 pid=4004968)[0;0m ERROR 12-04 14:04:29 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4004968)[0;0m ERROR 12-04 14:04:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 404, in initialize_from_config
[1;36m(EngineCore_DP0 pid=4004968)[0;0m ERROR 12-04 14:04:29 [core.py:842]     self.model_runner.initialize_kv_cache(kv_cache_config)
[1;36m(EngineCore_DP0 pid=4004971)[0;0m ERROR 12-04 14:04:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 5007, in initialize_kv_cache
[1;36m(EngineCore_DP0 pid=4004971)[0;0m ERROR 12-04 14:04:29 [core.py:842]     kv_caches = self.initialize_kv_cache_tensors(
[1;36m(EngineCore_DP0 pid=4004971)[0;0m ERROR 12-04 14:04:29 [core.py:842]                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4004971)[0;0m ERROR 12-04 14:04:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4933, in initialize_kv_cache_tensors
[1;36m(EngineCore_DP0 pid=4004971)[0;0m ERROR 12-04 14:04:29 [core.py:842]     kv_cache_raw_tensors = self._allocate_kv_cache_tensors(kv_cache_config)
[1;36m(EngineCore_DP0 pid=4004971)[0;0m ERROR 12-04 14:04:29 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4004971)[0;0m ERROR 12-04 14:04:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4718, in _allocate_kv_cache_tensors
[1;36m(EngineCore_DP0 pid=4004971)[0;0m ERROR 12-04 14:04:29 [core.py:842]     tensor = torch.zeros(
[1;36m(EngineCore_DP0 pid=4004971)[0;0m ERROR 12-04 14:04:29 [core.py:842]              ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4004971)[0;0m ERROR 12-04 14:04:29 [core.py:842] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.16 GiB. GPU 0 has a total capacity of 44.29 GiB of which 1.08 GiB is free. Including non-PyTorch memory, this process has 19.52 GiB memory in use. Process 4004968 has 23.69 GiB memory in use. Of the allocated memory 19.16 GiB is allocated by PyTorch, and 145.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=4004971)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=4004968)[0;0m ERROR 12-04 14:04:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 5007, in initialize_kv_cache
[1;36m(EngineCore_DP0 pid=4004968)[0;0m ERROR 12-04 14:04:29 [core.py:842]     kv_caches = self.initialize_kv_cache_tensors(
[1;36m(EngineCore_DP0 pid=4004968)[0;0m ERROR 12-04 14:04:29 [core.py:842]                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4004968)[0;0m ERROR 12-04 14:04:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4933, in initialize_kv_cache_tensors
[1;36m(EngineCore_DP0 pid=4004968)[0;0m ERROR 12-04 14:04:29 [core.py:842]     kv_cache_raw_tensors = self._allocate_kv_cache_tensors(kv_cache_config)
[1;36m(EngineCore_DP0 pid=4004968)[0;0m ERROR 12-04 14:04:29 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4004968)[0;0m ERROR 12-04 14:04:29 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4718, in _allocate_kv_cache_tensors
[1;36m(EngineCore_DP0 pid=4004968)[0;0m ERROR 12-04 14:04:29 [core.py:842]     tensor = torch.zeros(
[1;36m(EngineCore_DP0 pid=4004968)[0;0m ERROR 12-04 14:04:29 [core.py:842]              ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4004968)[0;0m ERROR 12-04 14:04:29 [core.py:842] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.13 GiB. GPU 0 has a total capacity of 44.29 GiB of which 1.08 GiB is free. Process 4004971 has 19.52 GiB memory in use. Including non-PyTorch memory, this process has 23.69 GiB memory in use. Of the allocated memory 23.32 GiB is allocated by PyTorch, and 162.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=4004968)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=4004971)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4004968)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4004971)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=4004971)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=4004971)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=4004971)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=4004971)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=4004971)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=4004971)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4004971)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4004971)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4004971)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4004971)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=4004971)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4004971)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4004971)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4004971)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 247, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4004971)[0;0m     self.model_executor.initialize_from_config(kv_cache_configs)
[1;36m(EngineCore_DP0 pid=4004971)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 115, in initialize_from_config
[1;36m(EngineCore_DP0 pid=4004971)[0;0m     self.collective_rpc("initialize_from_config", args=(kv_cache_configs,))
[1;36m(EngineCore_DP0 pid=4004971)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4004971)[0;0m     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4004971)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4004971)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4004971)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4004971)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4004971)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 319, in initialize_from_config
[1;36m(EngineCore_DP0 pid=4004971)[0;0m     self.worker.initialize_from_config(kv_cache_config)  # type: ignore
[1;36m(EngineCore_DP0 pid=4004971)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4004971)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 404, in initialize_from_config
[1;36m(EngineCore_DP0 pid=4004971)[0;0m     self.model_runner.initialize_kv_cache(kv_cache_config)
[1;36m(EngineCore_DP0 pid=4004971)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 5007, in initialize_kv_cache
[1;36m(EngineCore_DP0 pid=4004971)[0;0m     kv_caches = self.initialize_kv_cache_tensors(
[1;36m(EngineCore_DP0 pid=4004971)[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4004968)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=4004968)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=4004968)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=4004968)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=4004968)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=4004968)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=4004968)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4004968)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4004968)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4004968)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4004968)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=4004968)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4004968)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4004968)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4004968)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 247, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4004968)[0;0m     self.model_executor.initialize_from_config(kv_cache_configs)
[1;36m(EngineCore_DP0 pid=4004968)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 115, in initialize_from_config
[1;36m(EngineCore_DP0 pid=4004968)[0;0m     self.collective_rpc("initialize_from_config", args=(kv_cache_configs,))
[1;36m(EngineCore_DP0 pid=4004968)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4004968)[0;0m     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4004968)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4004968)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4004968)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4004968)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4004971)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4933, in initialize_kv_cache_tensors
[1;36m(EngineCore_DP0 pid=4004971)[0;0m     kv_cache_raw_tensors = self._allocate_kv_cache_tensors(kv_cache_config)
[1;36m(EngineCore_DP0 pid=4004971)[0;0m                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4004971)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4718, in _allocate_kv_cache_tensors
[1;36m(EngineCore_DP0 pid=4004971)[0;0m     tensor = torch.zeros(
[1;36m(EngineCore_DP0 pid=4004971)[0;0m              ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4004971)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.16 GiB. GPU 0 has a total capacity of 44.29 GiB of which 1.08 GiB is free. Including non-PyTorch memory, this process has 19.52 GiB memory in use. Process 4004968 has 23.69 GiB memory in use. Of the allocated memory 19.16 GiB is allocated by PyTorch, and 145.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=4004968)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 319, in initialize_from_config
[1;36m(EngineCore_DP0 pid=4004968)[0;0m     self.worker.initialize_from_config(kv_cache_config)  # type: ignore
[1;36m(EngineCore_DP0 pid=4004968)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4004968)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 404, in initialize_from_config
[1;36m(EngineCore_DP0 pid=4004968)[0;0m     self.model_runner.initialize_kv_cache(kv_cache_config)
[1;36m(EngineCore_DP0 pid=4004968)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 5007, in initialize_kv_cache
[1;36m(EngineCore_DP0 pid=4004968)[0;0m     kv_caches = self.initialize_kv_cache_tensors(
[1;36m(EngineCore_DP0 pid=4004968)[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4004968)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4933, in initialize_kv_cache_tensors
[1;36m(EngineCore_DP0 pid=4004968)[0;0m     kv_cache_raw_tensors = self._allocate_kv_cache_tensors(kv_cache_config)
[1;36m(EngineCore_DP0 pid=4004968)[0;0m                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4004968)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4718, in _allocate_kv_cache_tensors
[1;36m(EngineCore_DP0 pid=4004968)[0;0m     tensor = torch.zeros(
[1;36m(EngineCore_DP0 pid=4004968)[0;0m              ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4004968)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.13 GiB. GPU 0 has a total capacity of 44.29 GiB of which 1.08 GiB is free. Process 4004971 has 19.52 GiB memory in use. Including non-PyTorch memory, this process has 23.69 GiB memory in use. Of the allocated memory 23.32 GiB is allocated by PyTorch, and 162.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W1204 14:04:30.512791411 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank0]:[W1204 14:04:30.513363292 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: WeiboAI/VibeThinker-1.5B (backend=vllm)
[ModelCache] Using max_model_len=131072 for WeiboAI/VibeThinker-1.5B
[ModelCache] Detected 1 GPU(s), 44.3GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 14:04:51 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'WeiboAI/VibeThinker-1.5B'}
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: WeiboAI/VibeThinker-1.5B (backend=vllm)
[ModelCache] Using max_model_len=131072 for WeiboAI/VibeThinker-1.5B
[ModelCache] Detected 1 GPU(s), 44.3GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 14:04:52 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'WeiboAI/VibeThinker-1.5B'}
INFO 12-04 14:04:52 [model.py:631] Resolved architecture: Qwen2ForCausalLM
WARNING 12-04 14:04:52 [model.py:1921] Your device 'Quadro RTX 8000' (with compute capability 7.5) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.
WARNING 12-04 14:04:52 [model.py:1971] Casting torch.bfloat16 to torch.float16.
INFO 12-04 14:04:52 [model.py:1745] Using max model len 131072
INFO 12-04 14:04:52 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
INFO 12-04 14:04:52 [model.py:631] Resolved architecture: Qwen2ForCausalLM
WARNING 12-04 14:04:52 [model.py:1921] Your device 'Quadro RTX 8000' (with compute capability 7.5) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.
WARNING 12-04 14:04:52 [model.py:1971] Casting torch.bfloat16 to torch.float16.
INFO 12-04 14:04:52 [model.py:1745] Using max model len 131072
INFO 12-04 14:04:52 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4005179)[0;0m INFO 12-04 14:05:26 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='WeiboAI/VibeThinker-1.5B', speculative_config=None, tokenizer='WeiboAI/VibeThinker-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=WeiboAI/VibeThinker-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4005182)[0;0m INFO 12-04 14:05:26 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='WeiboAI/VibeThinker-1.5B', speculative_config=None, tokenizer='WeiboAI/VibeThinker-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=WeiboAI/VibeThinker-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4005179)[0;0m ERROR 12-04 14:05:28 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8
[1;36m(EngineCore_DP0 pid=4005182)[0;0m ERROR 12-04 14:05:28 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8
[1;36m(EngineCore_DP0 pid=4005179)[0;0m INFO 12-04 14:05:29 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.8:57665 backend=nccl
[1;36m(EngineCore_DP0 pid=4005182)[0;0m INFO 12-04 14:05:29 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.8:33435 backend=nccl
[W1204 14:05:29.144663605 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-23-0.rc.tch.harvard.edu]:57665 (errno: 97 - Address family not supported by protocol).
[W1204 14:05:29.146149031 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-23-0.rc.tch.harvard.edu]:33435 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4005182)[0;0m INFO 12-04 14:05:29 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4005179)[0;0m INFO 12-04 14:05:29 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4005179)[0;0m INFO 12-04 14:05:29 [gpu_model_runner.py:3259] Starting to load model WeiboAI/VibeThinker-1.5B...
[1;36m(EngineCore_DP0 pid=4005182)[0;0m INFO 12-04 14:05:29 [gpu_model_runner.py:3259] Starting to load model WeiboAI/VibeThinker-1.5B...
[1;36m(EngineCore_DP0 pid=4005179)[0;0m INFO 12-04 14:05:30 [cuda.py:418] Valid backends: ['FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4005179)[0;0m INFO 12-04 14:05:30 [cuda.py:427] Using FLASHINFER backend.
[1;36m(EngineCore_DP0 pid=4005182)[0;0m INFO 12-04 14:05:30 [cuda.py:418] Valid backends: ['FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4005182)[0;0m INFO 12-04 14:05:30 [cuda.py:427] Using FLASHINFER backend.
[1;36m(EngineCore_DP0 pid=4005182)[0;0m INFO 12-04 14:05:31 [weight_utils.py:481] No model.safetensors.index.json found in remote.
[1;36m(EngineCore_DP0 pid=4005182)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
[1;36m(EngineCore_DP0 pid=4005179)[0;0m INFO 12-04 14:05:31 [weight_utils.py:481] No model.safetensors.index.json found in remote.
[1;36m(EngineCore_DP0 pid=4005179)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
[1;36m(EngineCore_DP0 pid=4005182)[0;0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:04<00:00,  4.19s/it]
[1;36m(EngineCore_DP0 pid=4005182)[0;0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:04<00:00,  4.19s/it]
[1;36m(EngineCore_DP0 pid=4005182)[0;0m 
[1;36m(EngineCore_DP0 pid=4005182)[0;0m INFO 12-04 14:05:35 [default_loader.py:314] Loading weights took 4.32 seconds
[1;36m(EngineCore_DP0 pid=4005179)[0;0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:04<00:00,  4.23s/it]
[1;36m(EngineCore_DP0 pid=4005179)[0;0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:04<00:00,  4.23s/it]
[1;36m(EngineCore_DP0 pid=4005179)[0;0m 
[1;36m(EngineCore_DP0 pid=4005179)[0;0m INFO 12-04 14:05:35 [default_loader.py:314] Loading weights took 4.36 seconds
[1;36m(EngineCore_DP0 pid=4005182)[0;0m INFO 12-04 14:05:36 [gpu_model_runner.py:3338] Model loading took 2.9110 GiB memory and 5.575322 seconds
[1;36m(EngineCore_DP0 pid=4005179)[0;0m INFO 12-04 14:05:36 [gpu_model_runner.py:3338] Model loading took 2.9110 GiB memory and 5.823059 seconds
[1;36m(EngineCore_DP0 pid=4005179)[0;0m INFO 12-04 14:05:49 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/6847b7a26b/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4005182)[0;0m INFO 12-04 14:05:49 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/6847b7a26b/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4005179)[0;0m INFO 12-04 14:05:49 [backends.py:647] Dynamo bytecode transform time: 12.25 s
[1;36m(EngineCore_DP0 pid=4005182)[0;0m INFO 12-04 14:05:49 [backends.py:647] Dynamo bytecode transform time: 12.44 s
[1;36m(EngineCore_DP0 pid=4005182)[0;0m INFO 12-04 14:05:54 [backends.py:210] Directly load the compiled graph(s) for dynamic shape from the cache, took 5.282 s
[1;36m(EngineCore_DP0 pid=4005179)[0;0m INFO 12-04 14:05:54 [backends.py:210] Directly load the compiled graph(s) for dynamic shape from the cache, took 5.283 s
[1;36m(EngineCore_DP0 pid=4005182)[0;0m INFO 12-04 14:05:56 [monitor.py:34] torch.compile takes 17.72 s in total
[1;36m(EngineCore_DP0 pid=4005179)[0;0m INFO 12-04 14:05:56 [monitor.py:34] torch.compile takes 17.53 s in total
[1;36m(EngineCore_DP0 pid=4005179)[0;0m INFO 12-04 14:05:58 [gpu_worker.py:359] Available KV cache memory: 30.58 GiB
[1;36m(EngineCore_DP0 pid=4005182)[0;0m INFO 12-04 14:05:58 [gpu_worker.py:359] Available KV cache memory: 32.40 GiB
[1;36m(EngineCore_DP0 pid=4005179)[0;0m INFO 12-04 14:05:58 [kv_cache_utils.py:1229] GPU KV cache size: 1,145,296 tokens
[1;36m(EngineCore_DP0 pid=4005179)[0;0m INFO 12-04 14:05:58 [kv_cache_utils.py:1234] Maximum concurrency for 131,072 tokens per request: 8.74x
[1;36m(EngineCore_DP0 pid=4005179)[0;0m INFO 12-04 14:05:58 [kernel_warmup.py:65] Warming up FlashInfer attention.
[1;36m(EngineCore_DP0 pid=4005182)[0;0m INFO 12-04 14:05:58 [kv_cache_utils.py:1229] GPU KV cache size: 1,213,328 tokens
[1;36m(EngineCore_DP0 pid=4005182)[0;0m INFO 12-04 14:05:58 [kv_cache_utils.py:1234] Maximum concurrency for 131,072 tokens per request: 9.26x
[1;36m(EngineCore_DP0 pid=4005182)[0;0m ERROR 12-04 14:05:58 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4005182)[0;0m ERROR 12-04 14:05:58 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4005182)[0;0m ERROR 12-04 14:05:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4005182)[0;0m ERROR 12-04 14:05:58 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4005182)[0;0m ERROR 12-04 14:05:58 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005182)[0;0m ERROR 12-04 14:05:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4005182)[0;0m ERROR 12-04 14:05:58 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4005182)[0;0m ERROR 12-04 14:05:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4005182)[0;0m ERROR 12-04 14:05:58 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4005182)[0;0m ERROR 12-04 14:05:58 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005182)[0;0m ERROR 12-04 14:05:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 247, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4005182)[0;0m ERROR 12-04 14:05:58 [core.py:842]     self.model_executor.initialize_from_config(kv_cache_configs)
[1;36m(EngineCore_DP0 pid=4005182)[0;0m ERROR 12-04 14:05:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 115, in initialize_from_config
[1;36m(EngineCore_DP0 pid=4005182)[0;0m ERROR 12-04 14:05:58 [core.py:842]     self.collective_rpc("initialize_from_config", args=(kv_cache_configs,))
[1;36m(EngineCore_DP0 pid=4005182)[0;0m ERROR 12-04 14:05:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4005182)[0;0m ERROR 12-04 14:05:58 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4005182)[0;0m ERROR 12-04 14:05:58 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005182)[0;0m ERROR 12-04 14:05:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4005182)[0;0m ERROR 12-04 14:05:58 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4005182)[0;0m ERROR 12-04 14:05:58 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005182)[0;0m ERROR 12-04 14:05:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 319, in initialize_from_config
[1;36m(EngineCore_DP0 pid=4005182)[0;0m ERROR 12-04 14:05:58 [core.py:842]     self.worker.initialize_from_config(kv_cache_config)  # type: ignore
[1;36m(EngineCore_DP0 pid=4005182)[0;0m ERROR 12-04 14:05:58 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005182)[0;0m ERROR 12-04 14:05:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 404, in initialize_from_config
[1;36m(EngineCore_DP0 pid=4005182)[0;0m ERROR 12-04 14:05:58 [core.py:842]     self.model_runner.initialize_kv_cache(kv_cache_config)
[1;36m(EngineCore_DP0 pid=4005182)[0;0m ERROR 12-04 14:05:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 5007, in initialize_kv_cache
[1;36m(EngineCore_DP0 pid=4005182)[0;0m ERROR 12-04 14:05:58 [core.py:842]     kv_caches = self.initialize_kv_cache_tensors(
[1;36m(EngineCore_DP0 pid=4005182)[0;0m ERROR 12-04 14:05:58 [core.py:842]                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005182)[0;0m ERROR 12-04 14:05:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4933, in initialize_kv_cache_tensors
[1;36m(EngineCore_DP0 pid=4005182)[0;0m ERROR 12-04 14:05:58 [core.py:842]     kv_cache_raw_tensors = self._allocate_kv_cache_tensors(kv_cache_config)
[1;36m(EngineCore_DP0 pid=4005182)[0;0m ERROR 12-04 14:05:58 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005182)[0;0m ERROR 12-04 14:05:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4718, in _allocate_kv_cache_tensors
[1;36m(EngineCore_DP0 pid=4005182)[0;0m ERROR 12-04 14:05:58 [core.py:842]     tensor = torch.zeros(
[1;36m(EngineCore_DP0 pid=4005182)[0;0m ERROR 12-04 14:05:58 [core.py:842]              ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005182)[0;0m ERROR 12-04 14:05:58 [core.py:842] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.16 GiB. GPU 0 has a total capacity of 44.29 GiB of which 117.69 MiB is free. Process 4005179 has 33.93 GiB memory in use. Including non-PyTorch memory, this process has 10.25 GiB memory in use. Of the allocated memory 9.90 GiB is allocated by PyTorch, and 134.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=4005182)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=4005182)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4005182)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=4005182)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=4005182)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=4005182)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=4005182)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=4005182)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=4005182)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4005182)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4005182)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005182)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4005182)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=4005182)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4005182)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4005182)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005182)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 247, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4005182)[0;0m     self.model_executor.initialize_from_config(kv_cache_configs)
[1;36m(EngineCore_DP0 pid=4005182)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 115, in initialize_from_config
[1;36m(EngineCore_DP0 pid=4005182)[0;0m     self.collective_rpc("initialize_from_config", args=(kv_cache_configs,))
[1;36m(EngineCore_DP0 pid=4005182)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4005182)[0;0m     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4005182)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005182)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4005182)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4005182)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005182)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 319, in initialize_from_config
[1;36m(EngineCore_DP0 pid=4005182)[0;0m     self.worker.initialize_from_config(kv_cache_config)  # type: ignore
[1;36m(EngineCore_DP0 pid=4005182)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005182)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 404, in initialize_from_config
[1;36m(EngineCore_DP0 pid=4005182)[0;0m     self.model_runner.initialize_kv_cache(kv_cache_config)
[1;36m(EngineCore_DP0 pid=4005182)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 5007, in initialize_kv_cache
[1;36m(EngineCore_DP0 pid=4005182)[0;0m     kv_caches = self.initialize_kv_cache_tensors(
[1;36m(EngineCore_DP0 pid=4005182)[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005182)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4933, in initialize_kv_cache_tensors
[1;36m(EngineCore_DP0 pid=4005182)[0;0m     kv_cache_raw_tensors = self._allocate_kv_cache_tensors(kv_cache_config)
[1;36m(EngineCore_DP0 pid=4005182)[0;0m                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005182)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4718, in _allocate_kv_cache_tensors
[1;36m(EngineCore_DP0 pid=4005182)[0;0m     tensor = torch.zeros(
[1;36m(EngineCore_DP0 pid=4005182)[0;0m              ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005182)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.16 GiB. GPU 0 has a total capacity of 44.29 GiB of which 117.69 MiB is free. Process 4005179 has 33.93 GiB memory in use. Including non-PyTorch memory, this process has 10.25 GiB memory in use. Of the allocated memory 9.90 GiB is allocated by PyTorch, and 134.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=4005179)[0;0m ERROR 12-04 14:05:58 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4005179)[0;0m ERROR 12-04 14:05:58 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4005179)[0;0m ERROR 12-04 14:05:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4005179)[0;0m ERROR 12-04 14:05:58 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4005179)[0;0m ERROR 12-04 14:05:58 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005179)[0;0m ERROR 12-04 14:05:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4005179)[0;0m ERROR 12-04 14:05:58 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4005179)[0;0m ERROR 12-04 14:05:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4005179)[0;0m ERROR 12-04 14:05:58 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4005179)[0;0m ERROR 12-04 14:05:58 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005179)[0;0m ERROR 12-04 14:05:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 247, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4005179)[0;0m ERROR 12-04 14:05:58 [core.py:842]     self.model_executor.initialize_from_config(kv_cache_configs)
[1;36m(EngineCore_DP0 pid=4005179)[0;0m ERROR 12-04 14:05:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 116, in initialize_from_config
[1;36m(EngineCore_DP0 pid=4005179)[0;0m ERROR 12-04 14:05:58 [core.py:842]     self.collective_rpc("compile_or_warm_up_model")
[1;36m(EngineCore_DP0 pid=4005179)[0;0m ERROR 12-04 14:05:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4005179)[0;0m ERROR 12-04 14:05:58 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4005179)[0;0m ERROR 12-04 14:05:58 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005179)[0;0m ERROR 12-04 14:05:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4005179)[0;0m ERROR 12-04 14:05:58 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4005179)[0;0m ERROR 12-04 14:05:58 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005179)[0;0m ERROR 12-04 14:05:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 425, in compile_or_warm_up_model
[1;36m(EngineCore_DP0 pid=4005179)[0;0m ERROR 12-04 14:05:58 [core.py:842]     kernel_warmup(self)
[1;36m(EngineCore_DP0 pid=4005179)[0;0m ERROR 12-04 14:05:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/warmup/kernel_warmup.py", line 68, in kernel_warmup
[1;36m(EngineCore_DP0 pid=4005179)[0;0m ERROR 12-04 14:05:58 [core.py:842]     worker.model_runner._dummy_run(
[1;36m(EngineCore_DP0 pid=4005179)[0;0m ERROR 12-04 14:05:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4005179)[0;0m ERROR 12-04 14:05:58 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4005179)[0;0m ERROR 12-04 14:05:58 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005179)[0;0m ERROR 12-04 14:05:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3756, in _dummy_run
[1;36m(EngineCore_DP0 pid=4005179)[0;0m ERROR 12-04 14:05:58 [core.py:842]     attn_metadata, _ = self._build_attention_metadata(
[1;36m(EngineCore_DP0 pid=4005179)[0;0m ERROR 12-04 14:05:58 [core.py:842]                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005179)[0;0m ERROR 12-04 14:05:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1576, in _build_attention_metadata
[1;36m(EngineCore_DP0 pid=4005179)[0;0m ERROR 12-04 14:05:58 [core.py:842]     attn_metadata_i = builder.build_for_cudagraph_capture(
[1;36m(EngineCore_DP0 pid=4005179)[0;0m ERROR 12-04 14:05:58 [core.py:842]                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005179)[0;0m ERROR 12-04 14:05:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/attention/backends/utils.py", line 332, in build_for_cudagraph_capture
[1;36m(EngineCore_DP0 pid=4005179)[0;0m ERROR 12-04 14:05:58 [core.py:842]     return self.build(
[1;36m(EngineCore_DP0 pid=4005179)[0;0m ERROR 12-04 14:05:58 [core.py:842]            ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005179)[0;0m ERROR 12-04 14:05:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/attention/backends/flashinfer.py", line 844, in build
[1;36m(EngineCore_DP0 pid=4005179)[0;0m ERROR 12-04 14:05:58 [core.py:842]     attn_metadata.prefill_wrapper = self._get_prefill_wrapper()
[1;36m(EngineCore_DP0 pid=4005179)[0;0m ERROR 12-04 14:05:58 [core.py:842]                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005179)[0;0m ERROR 12-04 14:05:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/attention/backends/flashinfer.py", line 598, in _get_prefill_wrapper
[1;36m(EngineCore_DP0 pid=4005179)[0;0m ERROR 12-04 14:05:58 [core.py:842]     self._get_workspace_buffer(), get_kv_cache_layout()
[1;36m(EngineCore_DP0 pid=4005179)[0;0m ERROR 12-04 14:05:58 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005179)[0;0m ERROR 12-04 14:05:58 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/attention/backends/flashinfer.py", line 583, in _get_workspace_buffer
[1;36m(EngineCore_DP0 pid=4005179)[0;0m ERROR 12-04 14:05:58 [core.py:842]     self._workspace_buffer = torch.zeros(
[1;36m(EngineCore_DP0 pid=4005179)[0;0m ERROR 12-04 14:05:58 [core.py:842]                              ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005179)[0;0m ERROR 12-04 14:05:58 [core.py:842] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 394.00 MiB. GPU 0 has a total capacity of 44.29 GiB of which 117.69 MiB is free. Including non-PyTorch memory, this process has 33.93 GiB memory in use. Process 4005182 has 10.25 GiB memory in use. Of the allocated memory 33.54 GiB is allocated by PyTorch, and 171.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=4005179)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=4005179)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4005179)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=4005179)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=4005179)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=4005179)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=4005179)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=4005179)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=4005179)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4005179)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4005179)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005179)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4005179)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=4005179)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4005179)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4005179)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005179)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 247, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4005179)[0;0m     self.model_executor.initialize_from_config(kv_cache_configs)
[1;36m(EngineCore_DP0 pid=4005179)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 116, in initialize_from_config
[1;36m(EngineCore_DP0 pid=4005179)[0;0m     self.collective_rpc("compile_or_warm_up_model")
[1;36m(EngineCore_DP0 pid=4005179)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4005179)[0;0m     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4005179)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005179)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4005179)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4005179)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005179)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 425, in compile_or_warm_up_model
[1;36m(EngineCore_DP0 pid=4005179)[0;0m     kernel_warmup(self)
[1;36m(EngineCore_DP0 pid=4005179)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/warmup/kernel_warmup.py", line 68, in kernel_warmup
[1;36m(EngineCore_DP0 pid=4005179)[0;0m     worker.model_runner._dummy_run(
[1;36m(EngineCore_DP0 pid=4005179)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4005179)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4005179)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005179)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3756, in _dummy_run
[1;36m(EngineCore_DP0 pid=4005179)[0;0m     attn_metadata, _ = self._build_attention_metadata(
[1;36m(EngineCore_DP0 pid=4005179)[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005179)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1576, in _build_attention_metadata
[1;36m(EngineCore_DP0 pid=4005179)[0;0m     attn_metadata_i = builder.build_for_cudagraph_capture(
[1;36m(EngineCore_DP0 pid=4005179)[0;0m                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005179)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/attention/backends/utils.py", line 332, in build_for_cudagraph_capture
[1;36m(EngineCore_DP0 pid=4005179)[0;0m     return self.build(
[1;36m(EngineCore_DP0 pid=4005179)[0;0m            ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005179)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/attention/backends/flashinfer.py", line 844, in build
[1;36m(EngineCore_DP0 pid=4005179)[0;0m     attn_metadata.prefill_wrapper = self._get_prefill_wrapper()
[1;36m(EngineCore_DP0 pid=4005179)[0;0m                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005179)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/attention/backends/flashinfer.py", line 598, in _get_prefill_wrapper
[1;36m(EngineCore_DP0 pid=4005179)[0;0m     self._get_workspace_buffer(), get_kv_cache_layout()
[1;36m(EngineCore_DP0 pid=4005179)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005179)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/attention/backends/flashinfer.py", line 583, in _get_workspace_buffer
[1;36m(EngineCore_DP0 pid=4005179)[0;0m     self._workspace_buffer = torch.zeros(
[1;36m(EngineCore_DP0 pid=4005179)[0;0m                              ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005179)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 394.00 MiB. GPU 0 has a total capacity of 44.29 GiB of which 117.69 MiB is free. Including non-PyTorch memory, this process has 33.93 GiB memory in use. Process 4005182 has 10.25 GiB memory in use. Of the allocated memory 33.54 GiB is allocated by PyTorch, and 171.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W1204 14:05:59.406833309 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank0]:[W1204 14:05:59.673663486 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: WeiboAI/VibeThinker-1.5B (backend=vllm)
[ModelCache] Using max_model_len=131072 for WeiboAI/VibeThinker-1.5B
[ModelCache] Detected 1 GPU(s), 44.3GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 14:06:20 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'WeiboAI/VibeThinker-1.5B'}
INFO 12-04 14:06:21 [model.py:631] Resolved architecture: Qwen2ForCausalLM
WARNING 12-04 14:06:21 [model.py:1921] Your device 'Quadro RTX 8000' (with compute capability 7.5) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.
WARNING 12-04 14:06:21 [model.py:1971] Casting torch.bfloat16 to torch.float16.
INFO 12-04 14:06:21 [model.py:1745] Using max model len 131072
INFO 12-04 14:06:21 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: WeiboAI/VibeThinker-1.5B (backend=vllm)
[ModelCache] Using max_model_len=131072 for WeiboAI/VibeThinker-1.5B
[ModelCache] Detected 1 GPU(s), 44.3GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 14:06:21 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'WeiboAI/VibeThinker-1.5B'}
INFO 12-04 14:06:21 [model.py:631] Resolved architecture: Qwen2ForCausalLM
WARNING 12-04 14:06:21 [model.py:1921] Your device 'Quadro RTX 8000' (with compute capability 7.5) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.
WARNING 12-04 14:06:21 [model.py:1971] Casting torch.bfloat16 to torch.float16.
INFO 12-04 14:06:21 [model.py:1745] Using max model len 131072
INFO 12-04 14:06:21 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4005445)[0;0m INFO 12-04 14:06:45 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='WeiboAI/VibeThinker-1.5B', speculative_config=None, tokenizer='WeiboAI/VibeThinker-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=WeiboAI/VibeThinker-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4005448)[0;0m INFO 12-04 14:06:45 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='WeiboAI/VibeThinker-1.5B', speculative_config=None, tokenizer='WeiboAI/VibeThinker-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=WeiboAI/VibeThinker-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4005448)[0;0m ERROR 12-04 14:06:46 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8
[1;36m(EngineCore_DP0 pid=4005445)[0;0m ERROR 12-04 14:06:46 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8
[1;36m(EngineCore_DP0 pid=4005448)[0;0m INFO 12-04 14:06:48 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.8:49705 backend=nccl
[1;36m(EngineCore_DP0 pid=4005445)[0;0m INFO 12-04 14:06:48 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.8:37017 backend=nccl
[W1204 14:06:48.050345646 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-23-0.rc.tch.harvard.edu]:49705 (errno: 97 - Address family not supported by protocol).
[W1204 14:06:48.052750422 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-23-0.rc.tch.harvard.edu]:37017 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4005448)[0;0m INFO 12-04 14:06:48 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4005445)[0;0m INFO 12-04 14:06:48 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4005445)[0;0m INFO 12-04 14:06:48 [gpu_model_runner.py:3259] Starting to load model WeiboAI/VibeThinker-1.5B...
[1;36m(EngineCore_DP0 pid=4005448)[0;0m INFO 12-04 14:06:48 [gpu_model_runner.py:3259] Starting to load model WeiboAI/VibeThinker-1.5B...
[1;36m(EngineCore_DP0 pid=4005448)[0;0m INFO 12-04 14:06:50 [cuda.py:418] Valid backends: ['FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4005448)[0;0m INFO 12-04 14:06:50 [cuda.py:427] Using FLASHINFER backend.
[1;36m(EngineCore_DP0 pid=4005445)[0;0m INFO 12-04 14:06:50 [cuda.py:418] Valid backends: ['FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4005445)[0;0m INFO 12-04 14:06:50 [cuda.py:427] Using FLASHINFER backend.
[1;36m(EngineCore_DP0 pid=4005445)[0;0m INFO 12-04 14:06:50 [weight_utils.py:481] No model.safetensors.index.json found in remote.
[1;36m(EngineCore_DP0 pid=4005445)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
[1;36m(EngineCore_DP0 pid=4005448)[0;0m INFO 12-04 14:06:50 [weight_utils.py:481] No model.safetensors.index.json found in remote.
[1;36m(EngineCore_DP0 pid=4005448)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
[1;36m(EngineCore_DP0 pid=4005445)[0;0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.62s/it]
[1;36m(EngineCore_DP0 pid=4005445)[0;0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.62s/it]
[1;36m(EngineCore_DP0 pid=4005445)[0;0m 
[1;36m(EngineCore_DP0 pid=4005445)[0;0m INFO 12-04 14:06:54 [default_loader.py:314] Loading weights took 3.74 seconds
[1;36m(EngineCore_DP0 pid=4005448)[0;0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.64s/it]
[1;36m(EngineCore_DP0 pid=4005448)[0;0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.64s/it]
[1;36m(EngineCore_DP0 pid=4005448)[0;0m 
[1;36m(EngineCore_DP0 pid=4005448)[0;0m INFO 12-04 14:06:54 [default_loader.py:314] Loading weights took 3.77 seconds
[1;36m(EngineCore_DP0 pid=4005445)[0;0m INFO 12-04 14:06:55 [gpu_model_runner.py:3338] Model loading took 2.9110 GiB memory and 5.352352 seconds
[1;36m(EngineCore_DP0 pid=4005448)[0;0m INFO 12-04 14:06:55 [gpu_model_runner.py:3338] Model loading took 2.9110 GiB memory and 5.582241 seconds
[1;36m(EngineCore_DP0 pid=4005445)[0;0m INFO 12-04 14:07:09 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/6847b7a26b/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4005448)[0;0m INFO 12-04 14:07:09 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/6847b7a26b/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4005445)[0;0m INFO 12-04 14:07:09 [backends.py:647] Dynamo bytecode transform time: 14.44 s
[1;36m(EngineCore_DP0 pid=4005448)[0;0m INFO 12-04 14:07:09 [backends.py:647] Dynamo bytecode transform time: 14.19 s
[1;36m(EngineCore_DP0 pid=4005448)[0;0m INFO 12-04 14:07:15 [backends.py:210] Directly load the compiled graph(s) for dynamic shape from the cache, took 5.392 s
[1;36m(EngineCore_DP0 pid=4005445)[0;0m INFO 12-04 14:07:15 [backends.py:210] Directly load the compiled graph(s) for dynamic shape from the cache, took 5.397 s
[1;36m(EngineCore_DP0 pid=4005448)[0;0m INFO 12-04 14:07:17 [monitor.py:34] torch.compile takes 19.59 s in total
[1;36m(EngineCore_DP0 pid=4005445)[0;0m INFO 12-04 14:07:17 [monitor.py:34] torch.compile takes 19.84 s in total
[1;36m(EngineCore_DP0 pid=4005448)[0;0m INFO 12-04 14:07:19 [gpu_worker.py:359] Available KV cache memory: 30.58 GiB
[1;36m(EngineCore_DP0 pid=4005445)[0;0m INFO 12-04 14:07:19 [gpu_worker.py:359] Available KV cache memory: 32.40 GiB
[1;36m(EngineCore_DP0 pid=4005448)[0;0m INFO 12-04 14:07:19 [kv_cache_utils.py:1229] GPU KV cache size: 1,145,232 tokens
[1;36m(EngineCore_DP0 pid=4005448)[0;0m INFO 12-04 14:07:19 [kv_cache_utils.py:1234] Maximum concurrency for 131,072 tokens per request: 8.74x
[1;36m(EngineCore_DP0 pid=4005445)[0;0m INFO 12-04 14:07:19 [kv_cache_utils.py:1229] GPU KV cache size: 1,213,184 tokens
[1;36m(EngineCore_DP0 pid=4005445)[0;0m INFO 12-04 14:07:19 [kv_cache_utils.py:1234] Maximum concurrency for 131,072 tokens per request: 9.26x
[1;36m(EngineCore_DP0 pid=4005448)[0;0m INFO 12-04 14:07:19 [kernel_warmup.py:65] Warming up FlashInfer attention.
[1;36m(EngineCore_DP0 pid=4005445)[0;0m ERROR 12-04 14:07:19 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4005445)[0;0m ERROR 12-04 14:07:19 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4005445)[0;0m ERROR 12-04 14:07:19 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4005445)[0;0m ERROR 12-04 14:07:19 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4005445)[0;0m ERROR 12-04 14:07:19 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005445)[0;0m ERROR 12-04 14:07:19 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4005445)[0;0m ERROR 12-04 14:07:19 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4005445)[0;0m ERROR 12-04 14:07:19 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4005445)[0;0m ERROR 12-04 14:07:19 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4005445)[0;0m ERROR 12-04 14:07:19 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005445)[0;0m ERROR 12-04 14:07:19 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 247, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4005445)[0;0m ERROR 12-04 14:07:19 [core.py:842]     self.model_executor.initialize_from_config(kv_cache_configs)
[1;36m(EngineCore_DP0 pid=4005445)[0;0m ERROR 12-04 14:07:19 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 115, in initialize_from_config
[1;36m(EngineCore_DP0 pid=4005445)[0;0m ERROR 12-04 14:07:19 [core.py:842]     self.collective_rpc("initialize_from_config", args=(kv_cache_configs,))
[1;36m(EngineCore_DP0 pid=4005445)[0;0m ERROR 12-04 14:07:19 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4005445)[0;0m ERROR 12-04 14:07:19 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4005445)[0;0m ERROR 12-04 14:07:19 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005445)[0;0m ERROR 12-04 14:07:19 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4005445)[0;0m ERROR 12-04 14:07:19 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4005445)[0;0m ERROR 12-04 14:07:19 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005445)[0;0m ERROR 12-04 14:07:19 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 319, in initialize_from_config
[1;36m(EngineCore_DP0 pid=4005445)[0;0m ERROR 12-04 14:07:19 [core.py:842]     self.worker.initialize_from_config(kv_cache_config)  # type: ignore
[1;36m(EngineCore_DP0 pid=4005445)[0;0m ERROR 12-04 14:07:19 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005445)[0;0m ERROR 12-04 14:07:19 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 404, in initialize_from_config
[1;36m(EngineCore_DP0 pid=4005445)[0;0m ERROR 12-04 14:07:19 [core.py:842]     self.model_runner.initialize_kv_cache(kv_cache_config)
[1;36m(EngineCore_DP0 pid=4005445)[0;0m ERROR 12-04 14:07:19 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 5007, in initialize_kv_cache
[1;36m(EngineCore_DP0 pid=4005445)[0;0m ERROR 12-04 14:07:19 [core.py:842]     kv_caches = self.initialize_kv_cache_tensors(
[1;36m(EngineCore_DP0 pid=4005445)[0;0m ERROR 12-04 14:07:19 [core.py:842]                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005445)[0;0m ERROR 12-04 14:07:19 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4933, in initialize_kv_cache_tensors
[1;36m(EngineCore_DP0 pid=4005445)[0;0m ERROR 12-04 14:07:19 [core.py:842]     kv_cache_raw_tensors = self._allocate_kv_cache_tensors(kv_cache_config)
[1;36m(EngineCore_DP0 pid=4005445)[0;0m ERROR 12-04 14:07:19 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005445)[0;0m ERROR 12-04 14:07:19 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4718, in _allocate_kv_cache_tensors
[1;36m(EngineCore_DP0 pid=4005445)[0;0m ERROR 12-04 14:07:19 [core.py:842]     tensor = torch.zeros(
[1;36m(EngineCore_DP0 pid=4005445)[0;0m ERROR 12-04 14:07:19 [core.py:842]              ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005445)[0;0m ERROR 12-04 14:07:19 [core.py:842] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.16 GiB. GPU 0 has a total capacity of 44.29 GiB of which 117.69 MiB is free. Process 4005448 has 33.93 GiB memory in use. Including non-PyTorch memory, this process has 10.25 GiB memory in use. Of the allocated memory 9.90 GiB is allocated by PyTorch, and 135.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=4005445)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=4005445)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4005445)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=4005445)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=4005445)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=4005445)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=4005445)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=4005445)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=4005445)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4005445)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4005445)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005445)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4005445)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=4005445)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4005445)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4005445)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005445)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 247, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4005445)[0;0m     self.model_executor.initialize_from_config(kv_cache_configs)
[1;36m(EngineCore_DP0 pid=4005445)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 115, in initialize_from_config
[1;36m(EngineCore_DP0 pid=4005445)[0;0m     self.collective_rpc("initialize_from_config", args=(kv_cache_configs,))
[1;36m(EngineCore_DP0 pid=4005445)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4005445)[0;0m     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4005445)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005445)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4005445)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4005445)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005445)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 319, in initialize_from_config
[1;36m(EngineCore_DP0 pid=4005445)[0;0m     self.worker.initialize_from_config(kv_cache_config)  # type: ignore
[1;36m(EngineCore_DP0 pid=4005445)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005445)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 404, in initialize_from_config
[1;36m(EngineCore_DP0 pid=4005445)[0;0m     self.model_runner.initialize_kv_cache(kv_cache_config)
[1;36m(EngineCore_DP0 pid=4005445)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 5007, in initialize_kv_cache
[1;36m(EngineCore_DP0 pid=4005445)[0;0m     kv_caches = self.initialize_kv_cache_tensors(
[1;36m(EngineCore_DP0 pid=4005445)[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005445)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4933, in initialize_kv_cache_tensors
[1;36m(EngineCore_DP0 pid=4005445)[0;0m     kv_cache_raw_tensors = self._allocate_kv_cache_tensors(kv_cache_config)
[1;36m(EngineCore_DP0 pid=4005445)[0;0m                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005445)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4718, in _allocate_kv_cache_tensors
[1;36m(EngineCore_DP0 pid=4005445)[0;0m     tensor = torch.zeros(
[1;36m(EngineCore_DP0 pid=4005445)[0;0m              ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005445)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.16 GiB. GPU 0 has a total capacity of 44.29 GiB of which 117.69 MiB is free. Process 4005448 has 33.93 GiB memory in use. Including non-PyTorch memory, this process has 10.25 GiB memory in use. Of the allocated memory 9.90 GiB is allocated by PyTorch, and 135.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=4005448)[0;0m ERROR 12-04 14:07:19 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4005448)[0;0m ERROR 12-04 14:07:19 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4005448)[0;0m ERROR 12-04 14:07:19 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4005448)[0;0m ERROR 12-04 14:07:19 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4005448)[0;0m ERROR 12-04 14:07:19 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005448)[0;0m ERROR 12-04 14:07:19 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4005448)[0;0m ERROR 12-04 14:07:19 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4005448)[0;0m ERROR 12-04 14:07:19 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4005448)[0;0m ERROR 12-04 14:07:19 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4005448)[0;0m ERROR 12-04 14:07:19 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005448)[0;0m ERROR 12-04 14:07:19 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 247, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4005448)[0;0m ERROR 12-04 14:07:19 [core.py:842]     self.model_executor.initialize_from_config(kv_cache_configs)
[1;36m(EngineCore_DP0 pid=4005448)[0;0m ERROR 12-04 14:07:19 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 116, in initialize_from_config
[1;36m(EngineCore_DP0 pid=4005448)[0;0m ERROR 12-04 14:07:19 [core.py:842]     self.collective_rpc("compile_or_warm_up_model")
[1;36m(EngineCore_DP0 pid=4005448)[0;0m ERROR 12-04 14:07:19 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4005448)[0;0m ERROR 12-04 14:07:19 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4005448)[0;0m ERROR 12-04 14:07:19 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005448)[0;0m ERROR 12-04 14:07:19 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4005448)[0;0m ERROR 12-04 14:07:19 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4005448)[0;0m ERROR 12-04 14:07:19 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005448)[0;0m ERROR 12-04 14:07:19 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 425, in compile_or_warm_up_model
[1;36m(EngineCore_DP0 pid=4005448)[0;0m ERROR 12-04 14:07:19 [core.py:842]     kernel_warmup(self)
[1;36m(EngineCore_DP0 pid=4005448)[0;0m ERROR 12-04 14:07:19 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/warmup/kernel_warmup.py", line 68, in kernel_warmup
[1;36m(EngineCore_DP0 pid=4005448)[0;0m ERROR 12-04 14:07:19 [core.py:842]     worker.model_runner._dummy_run(
[1;36m(EngineCore_DP0 pid=4005448)[0;0m ERROR 12-04 14:07:19 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4005448)[0;0m ERROR 12-04 14:07:19 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4005448)[0;0m ERROR 12-04 14:07:19 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005448)[0;0m ERROR 12-04 14:07:19 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3756, in _dummy_run
[1;36m(EngineCore_DP0 pid=4005448)[0;0m ERROR 12-04 14:07:19 [core.py:842]     attn_metadata, _ = self._build_attention_metadata(
[1;36m(EngineCore_DP0 pid=4005448)[0;0m ERROR 12-04 14:07:19 [core.py:842]                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005448)[0;0m ERROR 12-04 14:07:19 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1576, in _build_attention_metadata
[1;36m(EngineCore_DP0 pid=4005448)[0;0m ERROR 12-04 14:07:19 [core.py:842]     attn_metadata_i = builder.build_for_cudagraph_capture(
[1;36m(EngineCore_DP0 pid=4005448)[0;0m ERROR 12-04 14:07:19 [core.py:842]                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005448)[0;0m ERROR 12-04 14:07:19 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/attention/backends/utils.py", line 332, in build_for_cudagraph_capture
[1;36m(EngineCore_DP0 pid=4005448)[0;0m ERROR 12-04 14:07:19 [core.py:842]     return self.build(
[1;36m(EngineCore_DP0 pid=4005448)[0;0m ERROR 12-04 14:07:19 [core.py:842]            ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005448)[0;0m ERROR 12-04 14:07:19 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/attention/backends/flashinfer.py", line 844, in build
[1;36m(EngineCore_DP0 pid=4005448)[0;0m ERROR 12-04 14:07:19 [core.py:842]     attn_metadata.prefill_wrapper = self._get_prefill_wrapper()
[1;36m(EngineCore_DP0 pid=4005448)[0;0m ERROR 12-04 14:07:19 [core.py:842]                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005448)[0;0m ERROR 12-04 14:07:19 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/attention/backends/flashinfer.py", line 598, in _get_prefill_wrapper
[1;36m(EngineCore_DP0 pid=4005448)[0;0m ERROR 12-04 14:07:19 [core.py:842]     self._get_workspace_buffer(), get_kv_cache_layout()
[1;36m(EngineCore_DP0 pid=4005448)[0;0m ERROR 12-04 14:07:19 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005448)[0;0m ERROR 12-04 14:07:19 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/attention/backends/flashinfer.py", line 583, in _get_workspace_buffer
[1;36m(EngineCore_DP0 pid=4005448)[0;0m ERROR 12-04 14:07:19 [core.py:842]     self._workspace_buffer = torch.zeros(
[1;36m(EngineCore_DP0 pid=4005448)[0;0m ERROR 12-04 14:07:19 [core.py:842]                              ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005448)[0;0m ERROR 12-04 14:07:19 [core.py:842] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 394.00 MiB. GPU 0 has a total capacity of 44.29 GiB of which 117.69 MiB is free. Including non-PyTorch memory, this process has 33.93 GiB memory in use. Process 4005445 has 10.25 GiB memory in use. Of the allocated memory 33.54 GiB is allocated by PyTorch, and 172.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=4005448)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=4005448)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4005448)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=4005448)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=4005448)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=4005448)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=4005448)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=4005448)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=4005448)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4005448)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4005448)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005448)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4005448)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=4005448)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4005448)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4005448)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005448)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 247, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4005448)[0;0m     self.model_executor.initialize_from_config(kv_cache_configs)
[1;36m(EngineCore_DP0 pid=4005448)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 116, in initialize_from_config
[1;36m(EngineCore_DP0 pid=4005448)[0;0m     self.collective_rpc("compile_or_warm_up_model")
[1;36m(EngineCore_DP0 pid=4005448)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4005448)[0;0m     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4005448)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005448)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4005448)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4005448)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005448)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 425, in compile_or_warm_up_model
[1;36m(EngineCore_DP0 pid=4005448)[0;0m     kernel_warmup(self)
[1;36m(EngineCore_DP0 pid=4005448)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/warmup/kernel_warmup.py", line 68, in kernel_warmup
[1;36m(EngineCore_DP0 pid=4005448)[0;0m     worker.model_runner._dummy_run(
[1;36m(EngineCore_DP0 pid=4005448)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4005448)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4005448)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005448)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3756, in _dummy_run
[1;36m(EngineCore_DP0 pid=4005448)[0;0m     attn_metadata, _ = self._build_attention_metadata(
[1;36m(EngineCore_DP0 pid=4005448)[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005448)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1576, in _build_attention_metadata
[1;36m(EngineCore_DP0 pid=4005448)[0;0m     attn_metadata_i = builder.build_for_cudagraph_capture(
[1;36m(EngineCore_DP0 pid=4005448)[0;0m                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005448)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/attention/backends/utils.py", line 332, in build_for_cudagraph_capture
[1;36m(EngineCore_DP0 pid=4005448)[0;0m     return self.build(
[1;36m(EngineCore_DP0 pid=4005448)[0;0m            ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005448)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/attention/backends/flashinfer.py", line 844, in build
[1;36m(EngineCore_DP0 pid=4005448)[0;0m     attn_metadata.prefill_wrapper = self._get_prefill_wrapper()
[1;36m(EngineCore_DP0 pid=4005448)[0;0m                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005448)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/attention/backends/flashinfer.py", line 598, in _get_prefill_wrapper
[1;36m(EngineCore_DP0 pid=4005448)[0;0m     self._get_workspace_buffer(), get_kv_cache_layout()
[1;36m(EngineCore_DP0 pid=4005448)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005448)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/attention/backends/flashinfer.py", line 583, in _get_workspace_buffer
[1;36m(EngineCore_DP0 pid=4005448)[0;0m     self._workspace_buffer = torch.zeros(
[1;36m(EngineCore_DP0 pid=4005448)[0;0m                              ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005448)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 394.00 MiB. GPU 0 has a total capacity of 44.29 GiB of which 117.69 MiB is free. Including non-PyTorch memory, this process has 33.93 GiB memory in use. Process 4005445 has 10.25 GiB memory in use. Of the allocated memory 33.54 GiB is allocated by PyTorch, and 172.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W1204 14:07:20.199044522 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank0]:[W1204 14:07:20.425998277 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: WeiboAI/VibeThinker-1.5B (backend=vllm)
[ModelCache] Using max_model_len=131072 for WeiboAI/VibeThinker-1.5B
[ModelCache] Detected 1 GPU(s), 44.3GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 14:07:41 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'WeiboAI/VibeThinker-1.5B'}
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: WeiboAI/VibeThinker-1.5B (backend=vllm)
[ModelCache] Using max_model_len=131072 for WeiboAI/VibeThinker-1.5B
[ModelCache] Detected 1 GPU(s), 44.3GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 14:07:41 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'WeiboAI/VibeThinker-1.5B'}
INFO 12-04 14:07:41 [model.py:631] Resolved architecture: Qwen2ForCausalLM
WARNING 12-04 14:07:41 [model.py:1921] Your device 'Quadro RTX 8000' (with compute capability 7.5) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.
WARNING 12-04 14:07:41 [model.py:1971] Casting torch.bfloat16 to torch.float16.
INFO 12-04 14:07:41 [model.py:1745] Using max model len 131072
INFO 12-04 14:07:41 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
INFO 12-04 14:07:42 [model.py:631] Resolved architecture: Qwen2ForCausalLM
WARNING 12-04 14:07:42 [model.py:1921] Your device 'Quadro RTX 8000' (with compute capability 7.5) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.
WARNING 12-04 14:07:42 [model.py:1971] Casting torch.bfloat16 to torch.float16.
INFO 12-04 14:07:42 [model.py:1745] Using max model len 131072
INFO 12-04 14:07:42 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4005723)[0;0m INFO 12-04 14:08:01 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='WeiboAI/VibeThinker-1.5B', speculative_config=None, tokenizer='WeiboAI/VibeThinker-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=WeiboAI/VibeThinker-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4005720)[0;0m INFO 12-04 14:08:01 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='WeiboAI/VibeThinker-1.5B', speculative_config=None, tokenizer='WeiboAI/VibeThinker-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=WeiboAI/VibeThinker-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4005723)[0;0m ERROR 12-04 14:08:03 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8
[1;36m(EngineCore_DP0 pid=4005720)[0;0m ERROR 12-04 14:08:03 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8
[1;36m(EngineCore_DP0 pid=4005720)[0;0m INFO 12-04 14:08:04 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.8:51545 backend=nccl
[1;36m(EngineCore_DP0 pid=4005723)[0;0m INFO 12-04 14:08:04 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.8:38329 backend=nccl
[W1204 14:08:04.469806102 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-23-0.rc.tch.harvard.edu]:51545 (errno: 97 - Address family not supported by protocol).
[W1204 14:08:04.472113888 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-23-0.rc.tch.harvard.edu]:38329 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4005723)[0;0m INFO 12-04 14:08:04 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4005720)[0;0m INFO 12-04 14:08:04 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4005723)[0;0m INFO 12-04 14:08:05 [gpu_model_runner.py:3259] Starting to load model WeiboAI/VibeThinker-1.5B...
[1;36m(EngineCore_DP0 pid=4005720)[0;0m INFO 12-04 14:08:05 [gpu_model_runner.py:3259] Starting to load model WeiboAI/VibeThinker-1.5B...
[1;36m(EngineCore_DP0 pid=4005720)[0;0m INFO 12-04 14:08:06 [cuda.py:418] Valid backends: ['FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4005720)[0;0m INFO 12-04 14:08:06 [cuda.py:427] Using FLASHINFER backend.
[1;36m(EngineCore_DP0 pid=4005723)[0;0m INFO 12-04 14:08:06 [cuda.py:418] Valid backends: ['FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4005723)[0;0m INFO 12-04 14:08:06 [cuda.py:427] Using FLASHINFER backend.
[1;36m(EngineCore_DP0 pid=4005723)[0;0m INFO 12-04 14:08:06 [weight_utils.py:481] No model.safetensors.index.json found in remote.
[1;36m(EngineCore_DP0 pid=4005723)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
[1;36m(EngineCore_DP0 pid=4005720)[0;0m INFO 12-04 14:08:07 [weight_utils.py:481] No model.safetensors.index.json found in remote.
[1;36m(EngineCore_DP0 pid=4005720)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
[1;36m(EngineCore_DP0 pid=4005723)[0;0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:04<00:00,  4.35s/it]
[1;36m(EngineCore_DP0 pid=4005723)[0;0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:04<00:00,  4.35s/it]
[1;36m(EngineCore_DP0 pid=4005723)[0;0m 
[1;36m(EngineCore_DP0 pid=4005723)[0;0m INFO 12-04 14:08:11 [default_loader.py:314] Loading weights took 4.48 seconds
[1;36m(EngineCore_DP0 pid=4005720)[0;0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:04<00:00,  4.41s/it]
[1;36m(EngineCore_DP0 pid=4005720)[0;0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:04<00:00,  4.41s/it]
[1;36m(EngineCore_DP0 pid=4005720)[0;0m 
[1;36m(EngineCore_DP0 pid=4005720)[0;0m INFO 12-04 14:08:11 [default_loader.py:314] Loading weights took 4.53 seconds
[1;36m(EngineCore_DP0 pid=4005723)[0;0m INFO 12-04 14:08:12 [gpu_model_runner.py:3338] Model loading took 2.9110 GiB memory and 5.972545 seconds
[1;36m(EngineCore_DP0 pid=4005720)[0;0m INFO 12-04 14:08:12 [gpu_model_runner.py:3338] Model loading took 2.9110 GiB memory and 6.202541 seconds
[1;36m(EngineCore_DP0 pid=4005720)[0;0m INFO 12-04 14:08:24 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/6847b7a26b/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4005723)[0;0m INFO 12-04 14:08:24 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/6847b7a26b/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4005720)[0;0m INFO 12-04 14:08:24 [backends.py:647] Dynamo bytecode transform time: 11.83 s
[1;36m(EngineCore_DP0 pid=4005723)[0;0m INFO 12-04 14:08:24 [backends.py:647] Dynamo bytecode transform time: 12.02 s
[1;36m(EngineCore_DP0 pid=4005723)[0;0m INFO 12-04 14:08:30 [backends.py:210] Directly load the compiled graph(s) for dynamic shape from the cache, took 5.556 s
[1;36m(EngineCore_DP0 pid=4005720)[0;0m INFO 12-04 14:08:30 [backends.py:210] Directly load the compiled graph(s) for dynamic shape from the cache, took 5.556 s
[1;36m(EngineCore_DP0 pid=4005723)[0;0m INFO 12-04 14:08:32 [monitor.py:34] torch.compile takes 17.57 s in total
[1;36m(EngineCore_DP0 pid=4005720)[0;0m INFO 12-04 14:08:32 [monitor.py:34] torch.compile takes 17.39 s in total
[1;36m(EngineCore_DP0 pid=4005723)[0;0m INFO 12-04 14:08:33 [gpu_worker.py:359] Available KV cache memory: 30.63 GiB
[1;36m(EngineCore_DP0 pid=4005720)[0;0m INFO 12-04 14:08:33 [gpu_worker.py:359] Available KV cache memory: 32.42 GiB
[1;36m(EngineCore_DP0 pid=4005723)[0;0m INFO 12-04 14:08:34 [kv_cache_utils.py:1229] GPU KV cache size: 1,147,056 tokens
[1;36m(EngineCore_DP0 pid=4005723)[0;0m INFO 12-04 14:08:34 [kv_cache_utils.py:1234] Maximum concurrency for 131,072 tokens per request: 8.75x
[1;36m(EngineCore_DP0 pid=4005720)[0;0m INFO 12-04 14:08:34 [kv_cache_utils.py:1229] GPU KV cache size: 1,213,904 tokens
[1;36m(EngineCore_DP0 pid=4005720)[0;0m INFO 12-04 14:08:34 [kv_cache_utils.py:1234] Maximum concurrency for 131,072 tokens per request: 9.26x
[1;36m(EngineCore_DP0 pid=4005723)[0;0m INFO 12-04 14:08:34 [kernel_warmup.py:65] Warming up FlashInfer attention.
[1;36m(EngineCore_DP0 pid=4005720)[0;0m ERROR 12-04 14:08:34 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4005720)[0;0m ERROR 12-04 14:08:34 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4005720)[0;0m ERROR 12-04 14:08:34 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4005720)[0;0m ERROR 12-04 14:08:34 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4005720)[0;0m ERROR 12-04 14:08:34 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005720)[0;0m ERROR 12-04 14:08:34 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4005720)[0;0m ERROR 12-04 14:08:34 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4005720)[0;0m ERROR 12-04 14:08:34 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4005720)[0;0m ERROR 12-04 14:08:34 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4005720)[0;0m ERROR 12-04 14:08:34 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005720)[0;0m ERROR 12-04 14:08:34 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 247, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4005720)[0;0m ERROR 12-04 14:08:34 [core.py:842]     self.model_executor.initialize_from_config(kv_cache_configs)
[1;36m(EngineCore_DP0 pid=4005720)[0;0m ERROR 12-04 14:08:34 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 115, in initialize_from_config
[1;36m(EngineCore_DP0 pid=4005720)[0;0m ERROR 12-04 14:08:34 [core.py:842]     self.collective_rpc("initialize_from_config", args=(kv_cache_configs,))
[1;36m(EngineCore_DP0 pid=4005720)[0;0m ERROR 12-04 14:08:34 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4005720)[0;0m ERROR 12-04 14:08:34 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4005720)[0;0m ERROR 12-04 14:08:34 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005720)[0;0m ERROR 12-04 14:08:34 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4005720)[0;0m ERROR 12-04 14:08:34 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4005720)[0;0m ERROR 12-04 14:08:34 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005720)[0;0m ERROR 12-04 14:08:34 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 319, in initialize_from_config
[1;36m(EngineCore_DP0 pid=4005720)[0;0m ERROR 12-04 14:08:34 [core.py:842]     self.worker.initialize_from_config(kv_cache_config)  # type: ignore
[1;36m(EngineCore_DP0 pid=4005720)[0;0m ERROR 12-04 14:08:34 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005720)[0;0m ERROR 12-04 14:08:34 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 404, in initialize_from_config
[1;36m(EngineCore_DP0 pid=4005720)[0;0m ERROR 12-04 14:08:34 [core.py:842]     self.model_runner.initialize_kv_cache(kv_cache_config)
[1;36m(EngineCore_DP0 pid=4005720)[0;0m ERROR 12-04 14:08:34 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 5007, in initialize_kv_cache
[1;36m(EngineCore_DP0 pid=4005720)[0;0m ERROR 12-04 14:08:34 [core.py:842]     kv_caches = self.initialize_kv_cache_tensors(
[1;36m(EngineCore_DP0 pid=4005720)[0;0m ERROR 12-04 14:08:34 [core.py:842]                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005720)[0;0m ERROR 12-04 14:08:34 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4933, in initialize_kv_cache_tensors
[1;36m(EngineCore_DP0 pid=4005720)[0;0m ERROR 12-04 14:08:34 [core.py:842]     kv_cache_raw_tensors = self._allocate_kv_cache_tensors(kv_cache_config)
[1;36m(EngineCore_DP0 pid=4005720)[0;0m ERROR 12-04 14:08:34 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005720)[0;0m ERROR 12-04 14:08:34 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4718, in _allocate_kv_cache_tensors
[1;36m(EngineCore_DP0 pid=4005720)[0;0m ERROR 12-04 14:08:34 [core.py:842]     tensor = torch.zeros(
[1;36m(EngineCore_DP0 pid=4005720)[0;0m ERROR 12-04 14:08:34 [core.py:842]              ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005720)[0;0m ERROR 12-04 14:08:34 [core.py:842] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.16 GiB. GPU 0 has a total capacity of 44.29 GiB of which 61.69 MiB is free. Process 4005723 has 33.98 GiB memory in use. Including non-PyTorch memory, this process has 10.25 GiB memory in use. Of the allocated memory 9.91 GiB is allocated by PyTorch, and 127.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=4005720)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=4005720)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4005720)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=4005720)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=4005720)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=4005720)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=4005720)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=4005720)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=4005720)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4005720)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4005720)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005720)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4005720)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=4005720)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4005720)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4005720)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005720)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 247, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4005720)[0;0m     self.model_executor.initialize_from_config(kv_cache_configs)
[1;36m(EngineCore_DP0 pid=4005720)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 115, in initialize_from_config
[1;36m(EngineCore_DP0 pid=4005720)[0;0m     self.collective_rpc("initialize_from_config", args=(kv_cache_configs,))
[1;36m(EngineCore_DP0 pid=4005720)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4005720)[0;0m     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4005720)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005720)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4005720)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4005720)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005720)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 319, in initialize_from_config
[1;36m(EngineCore_DP0 pid=4005720)[0;0m     self.worker.initialize_from_config(kv_cache_config)  # type: ignore
[1;36m(EngineCore_DP0 pid=4005720)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005720)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 404, in initialize_from_config
[1;36m(EngineCore_DP0 pid=4005720)[0;0m     self.model_runner.initialize_kv_cache(kv_cache_config)
[1;36m(EngineCore_DP0 pid=4005720)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 5007, in initialize_kv_cache
[1;36m(EngineCore_DP0 pid=4005720)[0;0m     kv_caches = self.initialize_kv_cache_tensors(
[1;36m(EngineCore_DP0 pid=4005720)[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005720)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4933, in initialize_kv_cache_tensors
[1;36m(EngineCore_DP0 pid=4005720)[0;0m     kv_cache_raw_tensors = self._allocate_kv_cache_tensors(kv_cache_config)
[1;36m(EngineCore_DP0 pid=4005720)[0;0m                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005720)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4718, in _allocate_kv_cache_tensors
[1;36m(EngineCore_DP0 pid=4005720)[0;0m     tensor = torch.zeros(
[1;36m(EngineCore_DP0 pid=4005720)[0;0m              ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005720)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.16 GiB. GPU 0 has a total capacity of 44.29 GiB of which 61.69 MiB is free. Process 4005723 has 33.98 GiB memory in use. Including non-PyTorch memory, this process has 10.25 GiB memory in use. Of the allocated memory 9.91 GiB is allocated by PyTorch, and 127.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=4005723)[0;0m ERROR 12-04 14:08:34 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4005723)[0;0m ERROR 12-04 14:08:34 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4005723)[0;0m ERROR 12-04 14:08:34 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4005723)[0;0m ERROR 12-04 14:08:34 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4005723)[0;0m ERROR 12-04 14:08:34 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005723)[0;0m ERROR 12-04 14:08:34 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4005723)[0;0m ERROR 12-04 14:08:34 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4005723)[0;0m ERROR 12-04 14:08:34 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4005723)[0;0m ERROR 12-04 14:08:34 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4005723)[0;0m ERROR 12-04 14:08:34 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005723)[0;0m ERROR 12-04 14:08:34 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 247, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4005723)[0;0m ERROR 12-04 14:08:34 [core.py:842]     self.model_executor.initialize_from_config(kv_cache_configs)
[1;36m(EngineCore_DP0 pid=4005723)[0;0m ERROR 12-04 14:08:34 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 116, in initialize_from_config
[1;36m(EngineCore_DP0 pid=4005723)[0;0m ERROR 12-04 14:08:34 [core.py:842]     self.collective_rpc("compile_or_warm_up_model")
[1;36m(EngineCore_DP0 pid=4005723)[0;0m ERROR 12-04 14:08:34 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4005723)[0;0m ERROR 12-04 14:08:34 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4005723)[0;0m ERROR 12-04 14:08:34 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005723)[0;0m ERROR 12-04 14:08:34 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4005723)[0;0m ERROR 12-04 14:08:34 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4005723)[0;0m ERROR 12-04 14:08:34 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005723)[0;0m ERROR 12-04 14:08:34 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 425, in compile_or_warm_up_model
[1;36m(EngineCore_DP0 pid=4005723)[0;0m ERROR 12-04 14:08:34 [core.py:842]     kernel_warmup(self)
[1;36m(EngineCore_DP0 pid=4005723)[0;0m ERROR 12-04 14:08:34 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/warmup/kernel_warmup.py", line 68, in kernel_warmup
[1;36m(EngineCore_DP0 pid=4005723)[0;0m ERROR 12-04 14:08:34 [core.py:842]     worker.model_runner._dummy_run(
[1;36m(EngineCore_DP0 pid=4005723)[0;0m ERROR 12-04 14:08:34 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4005723)[0;0m ERROR 12-04 14:08:34 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4005723)[0;0m ERROR 12-04 14:08:34 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005723)[0;0m ERROR 12-04 14:08:34 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3756, in _dummy_run
[1;36m(EngineCore_DP0 pid=4005723)[0;0m ERROR 12-04 14:08:34 [core.py:842]     attn_metadata, _ = self._build_attention_metadata(
[1;36m(EngineCore_DP0 pid=4005723)[0;0m ERROR 12-04 14:08:34 [core.py:842]                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005723)[0;0m ERROR 12-04 14:08:34 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1576, in _build_attention_metadata
[1;36m(EngineCore_DP0 pid=4005723)[0;0m ERROR 12-04 14:08:34 [core.py:842]     attn_metadata_i = builder.build_for_cudagraph_capture(
[1;36m(EngineCore_DP0 pid=4005723)[0;0m ERROR 12-04 14:08:34 [core.py:842]                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005723)[0;0m ERROR 12-04 14:08:34 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/attention/backends/utils.py", line 332, in build_for_cudagraph_capture
[1;36m(EngineCore_DP0 pid=4005723)[0;0m ERROR 12-04 14:08:34 [core.py:842]     return self.build(
[1;36m(EngineCore_DP0 pid=4005723)[0;0m ERROR 12-04 14:08:34 [core.py:842]            ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005723)[0;0m ERROR 12-04 14:08:34 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/attention/backends/flashinfer.py", line 844, in build
[1;36m(EngineCore_DP0 pid=4005723)[0;0m ERROR 12-04 14:08:34 [core.py:842]     attn_metadata.prefill_wrapper = self._get_prefill_wrapper()
[1;36m(EngineCore_DP0 pid=4005723)[0;0m ERROR 12-04 14:08:34 [core.py:842]                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005723)[0;0m ERROR 12-04 14:08:34 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/attention/backends/flashinfer.py", line 598, in _get_prefill_wrapper
[1;36m(EngineCore_DP0 pid=4005723)[0;0m ERROR 12-04 14:08:34 [core.py:842]     self._get_workspace_buffer(), get_kv_cache_layout()
[1;36m(EngineCore_DP0 pid=4005723)[0;0m ERROR 12-04 14:08:34 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005723)[0;0m ERROR 12-04 14:08:34 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/attention/backends/flashinfer.py", line 583, in _get_workspace_buffer
[1;36m(EngineCore_DP0 pid=4005723)[0;0m ERROR 12-04 14:08:34 [core.py:842]     self._workspace_buffer = torch.zeros(
[1;36m(EngineCore_DP0 pid=4005723)[0;0m ERROR 12-04 14:08:34 [core.py:842]                              ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005723)[0;0m ERROR 12-04 14:08:34 [core.py:842] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 394.00 MiB. GPU 0 has a total capacity of 44.29 GiB of which 61.69 MiB is free. Including non-PyTorch memory, this process has 33.98 GiB memory in use. Process 4005720 has 10.25 GiB memory in use. Of the allocated memory 33.59 GiB is allocated by PyTorch, and 179.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=4005723)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=4005723)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4005723)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=4005723)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=4005723)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=4005723)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=4005723)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=4005723)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=4005723)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4005723)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4005723)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005723)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4005723)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=4005723)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4005723)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4005723)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005723)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 247, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4005723)[0;0m     self.model_executor.initialize_from_config(kv_cache_configs)
[1;36m(EngineCore_DP0 pid=4005723)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 116, in initialize_from_config
[1;36m(EngineCore_DP0 pid=4005723)[0;0m     self.collective_rpc("compile_or_warm_up_model")
[1;36m(EngineCore_DP0 pid=4005723)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4005723)[0;0m     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4005723)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005723)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4005723)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4005723)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005723)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 425, in compile_or_warm_up_model
[1;36m(EngineCore_DP0 pid=4005723)[0;0m     kernel_warmup(self)
[1;36m(EngineCore_DP0 pid=4005723)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/warmup/kernel_warmup.py", line 68, in kernel_warmup
[1;36m(EngineCore_DP0 pid=4005723)[0;0m     worker.model_runner._dummy_run(
[1;36m(EngineCore_DP0 pid=4005723)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4005723)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4005723)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005723)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3756, in _dummy_run
[1;36m(EngineCore_DP0 pid=4005723)[0;0m     attn_metadata, _ = self._build_attention_metadata(
[1;36m(EngineCore_DP0 pid=4005723)[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005723)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1576, in _build_attention_metadata
[1;36m(EngineCore_DP0 pid=4005723)[0;0m     attn_metadata_i = builder.build_for_cudagraph_capture(
[1;36m(EngineCore_DP0 pid=4005723)[0;0m                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005723)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/attention/backends/utils.py", line 332, in build_for_cudagraph_capture
[1;36m(EngineCore_DP0 pid=4005723)[0;0m     return self.build(
[1;36m(EngineCore_DP0 pid=4005723)[0;0m            ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005723)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/attention/backends/flashinfer.py", line 844, in build
[1;36m(EngineCore_DP0 pid=4005723)[0;0m     attn_metadata.prefill_wrapper = self._get_prefill_wrapper()
[1;36m(EngineCore_DP0 pid=4005723)[0;0m                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005723)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/attention/backends/flashinfer.py", line 598, in _get_prefill_wrapper
[1;36m(EngineCore_DP0 pid=4005723)[0;0m     self._get_workspace_buffer(), get_kv_cache_layout()
[1;36m(EngineCore_DP0 pid=4005723)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005723)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/attention/backends/flashinfer.py", line 583, in _get_workspace_buffer
[1;36m(EngineCore_DP0 pid=4005723)[0;0m     self._workspace_buffer = torch.zeros(
[1;36m(EngineCore_DP0 pid=4005723)[0;0m                              ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005723)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 394.00 MiB. GPU 0 has a total capacity of 44.29 GiB of which 61.69 MiB is free. Including non-PyTorch memory, this process has 33.98 GiB memory in use. Process 4005720 has 10.25 GiB memory in use. Of the allocated memory 33.59 GiB is allocated by PyTorch, and 179.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W1204 14:08:35.176509632 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank0]:[W1204 14:08:35.315744160 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: WeiboAI/VibeThinker-1.5B (backend=vllm)
[ModelCache] Using max_model_len=131072 for WeiboAI/VibeThinker-1.5B
[ModelCache] Detected 1 GPU(s), 44.3GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 14:08:56 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'WeiboAI/VibeThinker-1.5B'}
INFO 12-04 14:08:57 [model.py:631] Resolved architecture: Qwen2ForCausalLM
WARNING 12-04 14:08:57 [model.py:1921] Your device 'Quadro RTX 8000' (with compute capability 7.5) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.
WARNING 12-04 14:08:57 [model.py:1971] Casting torch.bfloat16 to torch.float16.
INFO 12-04 14:08:57 [model.py:1745] Using max model len 131072
INFO 12-04 14:08:57 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: WeiboAI/VibeThinker-1.5B (backend=vllm)
[ModelCache] Using max_model_len=131072 for WeiboAI/VibeThinker-1.5B
[ModelCache] Detected 1 GPU(s), 44.3GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 14:08:57 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'WeiboAI/VibeThinker-1.5B'}
INFO 12-04 14:08:57 [model.py:631] Resolved architecture: Qwen2ForCausalLM
WARNING 12-04 14:08:57 [model.py:1921] Your device 'Quadro RTX 8000' (with compute capability 7.5) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.
WARNING 12-04 14:08:57 [model.py:1971] Casting torch.bfloat16 to torch.float16.
INFO 12-04 14:08:57 [model.py:1745] Using max model len 131072
INFO 12-04 14:08:57 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4005985)[0;0m INFO 12-04 14:09:25 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='WeiboAI/VibeThinker-1.5B', speculative_config=None, tokenizer='WeiboAI/VibeThinker-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=WeiboAI/VibeThinker-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4005982)[0;0m INFO 12-04 14:09:25 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='WeiboAI/VibeThinker-1.5B', speculative_config=None, tokenizer='WeiboAI/VibeThinker-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=WeiboAI/VibeThinker-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4005985)[0;0m ERROR 12-04 14:09:26 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8
[1;36m(EngineCore_DP0 pid=4005982)[0;0m ERROR 12-04 14:09:26 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8
[1;36m(EngineCore_DP0 pid=4005985)[0;0m INFO 12-04 14:09:27 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.8:35667 backend=nccl
[1;36m(EngineCore_DP0 pid=4005982)[0;0m INFO 12-04 14:09:27 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.8:47503 backend=nccl
[W1204 14:09:27.594079284 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-23-0.rc.tch.harvard.edu]:35667 (errno: 97 - Address family not supported by protocol).
[W1204 14:09:27.596609588 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-23-0.rc.tch.harvard.edu]:47503 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4005982)[0;0m INFO 12-04 14:09:27 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4005985)[0;0m INFO 12-04 14:09:27 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4005985)[0;0m INFO 12-04 14:09:28 [gpu_model_runner.py:3259] Starting to load model WeiboAI/VibeThinker-1.5B...
[1;36m(EngineCore_DP0 pid=4005982)[0;0m INFO 12-04 14:09:28 [gpu_model_runner.py:3259] Starting to load model WeiboAI/VibeThinker-1.5B...
[1;36m(EngineCore_DP0 pid=4005985)[0;0m INFO 12-04 14:09:29 [cuda.py:418] Valid backends: ['FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4005985)[0;0m INFO 12-04 14:09:29 [cuda.py:427] Using FLASHINFER backend.
[1;36m(EngineCore_DP0 pid=4005982)[0;0m INFO 12-04 14:09:29 [cuda.py:418] Valid backends: ['FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4005982)[0;0m INFO 12-04 14:09:29 [cuda.py:427] Using FLASHINFER backend.
[1;36m(EngineCore_DP0 pid=4005985)[0;0m INFO 12-04 14:09:30 [weight_utils.py:481] No model.safetensors.index.json found in remote.
[1;36m(EngineCore_DP0 pid=4005985)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
[1;36m(EngineCore_DP0 pid=4005982)[0;0m INFO 12-04 14:09:30 [weight_utils.py:481] No model.safetensors.index.json found in remote.
[1;36m(EngineCore_DP0 pid=4005982)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
[1;36m(EngineCore_DP0 pid=4005982)[0;0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.71s/it]
[1;36m(EngineCore_DP0 pid=4005982)[0;0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.71s/it]
[1;36m(EngineCore_DP0 pid=4005982)[0;0m 
[1;36m(EngineCore_DP0 pid=4005982)[0;0m INFO 12-04 14:09:33 [default_loader.py:314] Loading weights took 3.83 seconds
[1;36m(EngineCore_DP0 pid=4005985)[0;0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:04<00:00,  4.14s/it]
[1;36m(EngineCore_DP0 pid=4005985)[0;0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:04<00:00,  4.14s/it]
[1;36m(EngineCore_DP0 pid=4005985)[0;0m 
[1;36m(EngineCore_DP0 pid=4005985)[0;0m INFO 12-04 14:09:34 [default_loader.py:314] Loading weights took 4.27 seconds
[1;36m(EngineCore_DP0 pid=4005982)[0;0m INFO 12-04 14:09:34 [gpu_model_runner.py:3338] Model loading took 2.9110 GiB memory and 5.390126 seconds
[1;36m(EngineCore_DP0 pid=4005985)[0;0m INFO 12-04 14:09:34 [gpu_model_runner.py:3338] Model loading took 2.9110 GiB memory and 5.698456 seconds
[1;36m(EngineCore_DP0 pid=4005982)[0;0m INFO 12-04 14:09:47 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/6847b7a26b/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4005985)[0;0m INFO 12-04 14:09:47 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/6847b7a26b/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4005982)[0;0m INFO 12-04 14:09:47 [backends.py:647] Dynamo bytecode transform time: 12.61 s
[1;36m(EngineCore_DP0 pid=4005985)[0;0m INFO 12-04 14:09:47 [backends.py:647] Dynamo bytecode transform time: 12.35 s
[1;36m(EngineCore_DP0 pid=4005985)[0;0m INFO 12-04 14:09:53 [backends.py:210] Directly load the compiled graph(s) for dynamic shape from the cache, took 5.464 s
[1;36m(EngineCore_DP0 pid=4005982)[0;0m INFO 12-04 14:09:53 [backends.py:210] Directly load the compiled graph(s) for dynamic shape from the cache, took 5.456 s
[1;36m(EngineCore_DP0 pid=4005985)[0;0m INFO 12-04 14:09:55 [monitor.py:34] torch.compile takes 17.81 s in total
[1;36m(EngineCore_DP0 pid=4005982)[0;0m INFO 12-04 14:09:55 [monitor.py:34] torch.compile takes 18.06 s in total
[1;36m(EngineCore_DP0 pid=4005985)[0;0m INFO 12-04 14:09:56 [gpu_worker.py:359] Available KV cache memory: 30.61 GiB
[1;36m(EngineCore_DP0 pid=4005982)[0;0m INFO 12-04 14:09:56 [gpu_worker.py:359] Available KV cache memory: 32.42 GiB
[1;36m(EngineCore_DP0 pid=4005985)[0;0m INFO 12-04 14:09:57 [kv_cache_utils.py:1229] GPU KV cache size: 1,146,176 tokens
[1;36m(EngineCore_DP0 pid=4005985)[0;0m INFO 12-04 14:09:57 [kv_cache_utils.py:1234] Maximum concurrency for 131,072 tokens per request: 8.74x
[1;36m(EngineCore_DP0 pid=4005982)[0;0m INFO 12-04 14:09:57 [kv_cache_utils.py:1229] GPU KV cache size: 1,213,904 tokens
[1;36m(EngineCore_DP0 pid=4005982)[0;0m INFO 12-04 14:09:57 [kv_cache_utils.py:1234] Maximum concurrency for 131,072 tokens per request: 9.26x
[1;36m(EngineCore_DP0 pid=4005985)[0;0m INFO 12-04 14:09:57 [kernel_warmup.py:65] Warming up FlashInfer attention.
[1;36m(EngineCore_DP0 pid=4005982)[0;0m ERROR 12-04 14:09:57 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4005982)[0;0m ERROR 12-04 14:09:57 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4005982)[0;0m ERROR 12-04 14:09:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4005982)[0;0m ERROR 12-04 14:09:57 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4005982)[0;0m ERROR 12-04 14:09:57 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005982)[0;0m ERROR 12-04 14:09:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4005982)[0;0m ERROR 12-04 14:09:57 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4005982)[0;0m ERROR 12-04 14:09:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4005982)[0;0m ERROR 12-04 14:09:57 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4005982)[0;0m ERROR 12-04 14:09:57 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005982)[0;0m ERROR 12-04 14:09:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 247, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4005982)[0;0m ERROR 12-04 14:09:57 [core.py:842]     self.model_executor.initialize_from_config(kv_cache_configs)
[1;36m(EngineCore_DP0 pid=4005982)[0;0m ERROR 12-04 14:09:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 115, in initialize_from_config
[1;36m(EngineCore_DP0 pid=4005982)[0;0m ERROR 12-04 14:09:57 [core.py:842]     self.collective_rpc("initialize_from_config", args=(kv_cache_configs,))
[1;36m(EngineCore_DP0 pid=4005982)[0;0m ERROR 12-04 14:09:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4005982)[0;0m ERROR 12-04 14:09:57 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4005982)[0;0m ERROR 12-04 14:09:57 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005982)[0;0m ERROR 12-04 14:09:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4005982)[0;0m ERROR 12-04 14:09:57 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4005982)[0;0m ERROR 12-04 14:09:57 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005982)[0;0m ERROR 12-04 14:09:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 319, in initialize_from_config
[1;36m(EngineCore_DP0 pid=4005982)[0;0m ERROR 12-04 14:09:57 [core.py:842]     self.worker.initialize_from_config(kv_cache_config)  # type: ignore
[1;36m(EngineCore_DP0 pid=4005982)[0;0m ERROR 12-04 14:09:57 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005982)[0;0m ERROR 12-04 14:09:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 404, in initialize_from_config
[1;36m(EngineCore_DP0 pid=4005982)[0;0m ERROR 12-04 14:09:57 [core.py:842]     self.model_runner.initialize_kv_cache(kv_cache_config)
[1;36m(EngineCore_DP0 pid=4005982)[0;0m ERROR 12-04 14:09:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 5007, in initialize_kv_cache
[1;36m(EngineCore_DP0 pid=4005982)[0;0m ERROR 12-04 14:09:57 [core.py:842]     kv_caches = self.initialize_kv_cache_tensors(
[1;36m(EngineCore_DP0 pid=4005982)[0;0m ERROR 12-04 14:09:57 [core.py:842]                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005982)[0;0m ERROR 12-04 14:09:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4933, in initialize_kv_cache_tensors
[1;36m(EngineCore_DP0 pid=4005982)[0;0m ERROR 12-04 14:09:57 [core.py:842]     kv_cache_raw_tensors = self._allocate_kv_cache_tensors(kv_cache_config)
[1;36m(EngineCore_DP0 pid=4005982)[0;0m ERROR 12-04 14:09:57 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005982)[0;0m ERROR 12-04 14:09:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4718, in _allocate_kv_cache_tensors
[1;36m(EngineCore_DP0 pid=4005982)[0;0m ERROR 12-04 14:09:57 [core.py:842]     tensor = torch.zeros(
[1;36m(EngineCore_DP0 pid=4005982)[0;0m ERROR 12-04 14:09:57 [core.py:842]              ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005982)[0;0m ERROR 12-04 14:09:57 [core.py:842] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.16 GiB. GPU 0 has a total capacity of 44.29 GiB of which 117.69 MiB is free. Process 4005985 has 33.93 GiB memory in use. Including non-PyTorch memory, this process has 10.25 GiB memory in use. Of the allocated memory 9.91 GiB is allocated by PyTorch, and 127.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=4005982)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=4005982)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4005982)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=4005982)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=4005982)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=4005982)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=4005982)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=4005982)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=4005982)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4005982)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4005982)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005982)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4005982)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=4005982)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4005982)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4005982)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005982)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 247, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4005982)[0;0m     self.model_executor.initialize_from_config(kv_cache_configs)
[1;36m(EngineCore_DP0 pid=4005982)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 115, in initialize_from_config
[1;36m(EngineCore_DP0 pid=4005982)[0;0m     self.collective_rpc("initialize_from_config", args=(kv_cache_configs,))
[1;36m(EngineCore_DP0 pid=4005982)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4005982)[0;0m     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4005982)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005982)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4005982)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4005982)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005982)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 319, in initialize_from_config
[1;36m(EngineCore_DP0 pid=4005982)[0;0m     self.worker.initialize_from_config(kv_cache_config)  # type: ignore
[1;36m(EngineCore_DP0 pid=4005982)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005982)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 404, in initialize_from_config
[1;36m(EngineCore_DP0 pid=4005982)[0;0m     self.model_runner.initialize_kv_cache(kv_cache_config)
[1;36m(EngineCore_DP0 pid=4005982)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 5007, in initialize_kv_cache
[1;36m(EngineCore_DP0 pid=4005982)[0;0m     kv_caches = self.initialize_kv_cache_tensors(
[1;36m(EngineCore_DP0 pid=4005982)[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005982)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4933, in initialize_kv_cache_tensors
[1;36m(EngineCore_DP0 pid=4005982)[0;0m     kv_cache_raw_tensors = self._allocate_kv_cache_tensors(kv_cache_config)
[1;36m(EngineCore_DP0 pid=4005982)[0;0m                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005982)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4718, in _allocate_kv_cache_tensors
[1;36m(EngineCore_DP0 pid=4005982)[0;0m     tensor = torch.zeros(
[1;36m(EngineCore_DP0 pid=4005982)[0;0m              ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005982)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.16 GiB. GPU 0 has a total capacity of 44.29 GiB of which 117.69 MiB is free. Process 4005985 has 33.93 GiB memory in use. Including non-PyTorch memory, this process has 10.25 GiB memory in use. Of the allocated memory 9.91 GiB is allocated by PyTorch, and 127.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=4005985)[0;0m ERROR 12-04 14:09:57 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4005985)[0;0m ERROR 12-04 14:09:57 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4005985)[0;0m ERROR 12-04 14:09:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4005985)[0;0m ERROR 12-04 14:09:57 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4005985)[0;0m ERROR 12-04 14:09:57 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005985)[0;0m ERROR 12-04 14:09:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4005985)[0;0m ERROR 12-04 14:09:57 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4005985)[0;0m ERROR 12-04 14:09:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4005985)[0;0m ERROR 12-04 14:09:57 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4005985)[0;0m ERROR 12-04 14:09:57 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005985)[0;0m ERROR 12-04 14:09:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 247, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4005985)[0;0m ERROR 12-04 14:09:57 [core.py:842]     self.model_executor.initialize_from_config(kv_cache_configs)
[1;36m(EngineCore_DP0 pid=4005985)[0;0m ERROR 12-04 14:09:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 116, in initialize_from_config
[1;36m(EngineCore_DP0 pid=4005985)[0;0m ERROR 12-04 14:09:57 [core.py:842]     self.collective_rpc("compile_or_warm_up_model")
[1;36m(EngineCore_DP0 pid=4005985)[0;0m ERROR 12-04 14:09:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4005985)[0;0m ERROR 12-04 14:09:57 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4005985)[0;0m ERROR 12-04 14:09:57 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005985)[0;0m ERROR 12-04 14:09:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4005985)[0;0m ERROR 12-04 14:09:57 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4005985)[0;0m ERROR 12-04 14:09:57 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005985)[0;0m ERROR 12-04 14:09:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 425, in compile_or_warm_up_model
[1;36m(EngineCore_DP0 pid=4005985)[0;0m ERROR 12-04 14:09:57 [core.py:842]     kernel_warmup(self)
[1;36m(EngineCore_DP0 pid=4005985)[0;0m ERROR 12-04 14:09:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/warmup/kernel_warmup.py", line 68, in kernel_warmup
[1;36m(EngineCore_DP0 pid=4005985)[0;0m ERROR 12-04 14:09:57 [core.py:842]     worker.model_runner._dummy_run(
[1;36m(EngineCore_DP0 pid=4005985)[0;0m ERROR 12-04 14:09:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4005985)[0;0m ERROR 12-04 14:09:57 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4005985)[0;0m ERROR 12-04 14:09:57 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005985)[0;0m ERROR 12-04 14:09:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3756, in _dummy_run
[1;36m(EngineCore_DP0 pid=4005985)[0;0m ERROR 12-04 14:09:57 [core.py:842]     attn_metadata, _ = self._build_attention_metadata(
[1;36m(EngineCore_DP0 pid=4005985)[0;0m ERROR 12-04 14:09:57 [core.py:842]                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005985)[0;0m ERROR 12-04 14:09:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1576, in _build_attention_metadata
[1;36m(EngineCore_DP0 pid=4005985)[0;0m ERROR 12-04 14:09:57 [core.py:842]     attn_metadata_i = builder.build_for_cudagraph_capture(
[1;36m(EngineCore_DP0 pid=4005985)[0;0m ERROR 12-04 14:09:57 [core.py:842]                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005985)[0;0m ERROR 12-04 14:09:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/attention/backends/utils.py", line 332, in build_for_cudagraph_capture
[1;36m(EngineCore_DP0 pid=4005985)[0;0m ERROR 12-04 14:09:57 [core.py:842]     return self.build(
[1;36m(EngineCore_DP0 pid=4005985)[0;0m ERROR 12-04 14:09:57 [core.py:842]            ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005985)[0;0m ERROR 12-04 14:09:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/attention/backends/flashinfer.py", line 844, in build
[1;36m(EngineCore_DP0 pid=4005985)[0;0m ERROR 12-04 14:09:57 [core.py:842]     attn_metadata.prefill_wrapper = self._get_prefill_wrapper()
[1;36m(EngineCore_DP0 pid=4005985)[0;0m ERROR 12-04 14:09:57 [core.py:842]                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005985)[0;0m ERROR 12-04 14:09:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/attention/backends/flashinfer.py", line 598, in _get_prefill_wrapper
[1;36m(EngineCore_DP0 pid=4005985)[0;0m ERROR 12-04 14:09:57 [core.py:842]     self._get_workspace_buffer(), get_kv_cache_layout()
[1;36m(EngineCore_DP0 pid=4005985)[0;0m ERROR 12-04 14:09:57 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005985)[0;0m ERROR 12-04 14:09:57 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/attention/backends/flashinfer.py", line 583, in _get_workspace_buffer
[1;36m(EngineCore_DP0 pid=4005985)[0;0m ERROR 12-04 14:09:57 [core.py:842]     self._workspace_buffer = torch.zeros(
[1;36m(EngineCore_DP0 pid=4005985)[0;0m ERROR 12-04 14:09:57 [core.py:842]                              ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005985)[0;0m ERROR 12-04 14:09:57 [core.py:842] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 394.00 MiB. GPU 0 has a total capacity of 44.29 GiB of which 117.69 MiB is free. Including non-PyTorch memory, this process has 33.93 GiB memory in use. Process 4005982 has 10.25 GiB memory in use. Of the allocated memory 33.58 GiB is allocated by PyTorch, and 127.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=4005985)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=4005985)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4005985)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=4005985)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=4005985)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=4005985)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=4005985)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=4005985)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=4005985)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4005985)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4005985)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005985)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4005985)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=4005985)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4005985)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4005985)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005985)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 247, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4005985)[0;0m     self.model_executor.initialize_from_config(kv_cache_configs)
[1;36m(EngineCore_DP0 pid=4005985)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 116, in initialize_from_config
[1;36m(EngineCore_DP0 pid=4005985)[0;0m     self.collective_rpc("compile_or_warm_up_model")
[1;36m(EngineCore_DP0 pid=4005985)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4005985)[0;0m     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4005985)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005985)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4005985)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4005985)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005985)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 425, in compile_or_warm_up_model
[1;36m(EngineCore_DP0 pid=4005985)[0;0m     kernel_warmup(self)
[1;36m(EngineCore_DP0 pid=4005985)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/warmup/kernel_warmup.py", line 68, in kernel_warmup
[1;36m(EngineCore_DP0 pid=4005985)[0;0m     worker.model_runner._dummy_run(
[1;36m(EngineCore_DP0 pid=4005985)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4005985)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4005985)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005985)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3756, in _dummy_run
[1;36m(EngineCore_DP0 pid=4005985)[0;0m     attn_metadata, _ = self._build_attention_metadata(
[1;36m(EngineCore_DP0 pid=4005985)[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005985)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1576, in _build_attention_metadata
[1;36m(EngineCore_DP0 pid=4005985)[0;0m     attn_metadata_i = builder.build_for_cudagraph_capture(
[1;36m(EngineCore_DP0 pid=4005985)[0;0m                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005985)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/attention/backends/utils.py", line 332, in build_for_cudagraph_capture
[1;36m(EngineCore_DP0 pid=4005985)[0;0m     return self.build(
[1;36m(EngineCore_DP0 pid=4005985)[0;0m            ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005985)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/attention/backends/flashinfer.py", line 844, in build
[1;36m(EngineCore_DP0 pid=4005985)[0;0m     attn_metadata.prefill_wrapper = self._get_prefill_wrapper()
[1;36m(EngineCore_DP0 pid=4005985)[0;0m                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005985)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/attention/backends/flashinfer.py", line 598, in _get_prefill_wrapper
[1;36m(EngineCore_DP0 pid=4005985)[0;0m     self._get_workspace_buffer(), get_kv_cache_layout()
[1;36m(EngineCore_DP0 pid=4005985)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005985)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/attention/backends/flashinfer.py", line 583, in _get_workspace_buffer
[1;36m(EngineCore_DP0 pid=4005985)[0;0m     self._workspace_buffer = torch.zeros(
[1;36m(EngineCore_DP0 pid=4005985)[0;0m                              ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4005985)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 394.00 MiB. GPU 0 has a total capacity of 44.29 GiB of which 117.69 MiB is free. Including non-PyTorch memory, this process has 33.93 GiB memory in use. Process 4005982 has 10.25 GiB memory in use. Of the allocated memory 33.58 GiB is allocated by PyTorch, and 127.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W1204 14:09:58.966783087 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank0]:[W1204 14:09:58.247260174 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: WeiboAI/VibeThinker-1.5B (backend=vllm)
[ModelCache] Using max_model_len=131072 for WeiboAI/VibeThinker-1.5B
[ModelCache] Detected 1 GPU(s), 44.3GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 14:10:19 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'WeiboAI/VibeThinker-1.5B'}
INFO 12-04 14:10:19 [model.py:631] Resolved architecture: Qwen2ForCausalLM
WARNING 12-04 14:10:19 [model.py:1921] Your device 'Quadro RTX 8000' (with compute capability 7.5) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.
WARNING 12-04 14:10:19 [model.py:1971] Casting torch.bfloat16 to torch.float16.
INFO 12-04 14:10:19 [model.py:1745] Using max model len 131072
INFO 12-04 14:10:19 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: WeiboAI/VibeThinker-1.5B (backend=vllm)
[ModelCache] Using max_model_len=131072 for WeiboAI/VibeThinker-1.5B
[ModelCache] Detected 1 GPU(s), 44.3GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 14:10:19 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'WeiboAI/VibeThinker-1.5B'}
INFO 12-04 14:10:19 [model.py:631] Resolved architecture: Qwen2ForCausalLM
WARNING 12-04 14:10:19 [model.py:1921] Your device 'Quadro RTX 8000' (with compute capability 7.5) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.
WARNING 12-04 14:10:19 [model.py:1971] Casting torch.bfloat16 to torch.float16.
INFO 12-04 14:10:19 [model.py:1745] Using max model len 131072
INFO 12-04 14:10:19 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4006173)[0;0m INFO 12-04 14:10:44 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='WeiboAI/VibeThinker-1.5B', speculative_config=None, tokenizer='WeiboAI/VibeThinker-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=WeiboAI/VibeThinker-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4006170)[0;0m INFO 12-04 14:10:44 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='WeiboAI/VibeThinker-1.5B', speculative_config=None, tokenizer='WeiboAI/VibeThinker-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=WeiboAI/VibeThinker-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4006173)[0;0m ERROR 12-04 14:10:45 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8
[1;36m(EngineCore_DP0 pid=4006170)[0;0m ERROR 12-04 14:10:45 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8
[1;36m(EngineCore_DP0 pid=4006173)[0;0m INFO 12-04 14:10:46 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.8:35305 backend=nccl
[1;36m(EngineCore_DP0 pid=4006170)[0;0m INFO 12-04 14:10:46 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.8:53899 backend=nccl
[W1204 14:10:46.259515552 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-23-0.rc.tch.harvard.edu]:35305 (errno: 97 - Address family not supported by protocol).
[W1204 14:10:46.260969131 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-23-0.rc.tch.harvard.edu]:53899 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4006170)[0;0m INFO 12-04 14:10:46 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4006173)[0;0m INFO 12-04 14:10:46 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4006173)[0;0m INFO 12-04 14:10:46 [gpu_model_runner.py:3259] Starting to load model WeiboAI/VibeThinker-1.5B...
[1;36m(EngineCore_DP0 pid=4006170)[0;0m INFO 12-04 14:10:46 [gpu_model_runner.py:3259] Starting to load model WeiboAI/VibeThinker-1.5B...
[1;36m(EngineCore_DP0 pid=4006173)[0;0m INFO 12-04 14:10:47 [cuda.py:418] Valid backends: ['FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4006173)[0;0m INFO 12-04 14:10:47 [cuda.py:427] Using FLASHINFER backend.
[1;36m(EngineCore_DP0 pid=4006170)[0;0m INFO 12-04 14:10:47 [cuda.py:418] Valid backends: ['FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4006170)[0;0m INFO 12-04 14:10:47 [cuda.py:427] Using FLASHINFER backend.
[1;36m(EngineCore_DP0 pid=4006170)[0;0m INFO 12-04 14:10:48 [weight_utils.py:481] No model.safetensors.index.json found in remote.
[1;36m(EngineCore_DP0 pid=4006170)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
[1;36m(EngineCore_DP0 pid=4006173)[0;0m INFO 12-04 14:10:48 [weight_utils.py:481] No model.safetensors.index.json found in remote.
[1;36m(EngineCore_DP0 pid=4006173)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
[1;36m(EngineCore_DP0 pid=4006170)[0;0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:04<00:00,  4.19s/it]
[1;36m(EngineCore_DP0 pid=4006170)[0;0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:04<00:00,  4.19s/it]
[1;36m(EngineCore_DP0 pid=4006170)[0;0m 
[1;36m(EngineCore_DP0 pid=4006170)[0;0m INFO 12-04 14:10:52 [default_loader.py:314] Loading weights took 4.31 seconds
[1;36m(EngineCore_DP0 pid=4006173)[0;0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:04<00:00,  4.28s/it]
[1;36m(EngineCore_DP0 pid=4006173)[0;0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:04<00:00,  4.28s/it]
[1;36m(EngineCore_DP0 pid=4006173)[0;0m 
[1;36m(EngineCore_DP0 pid=4006173)[0;0m INFO 12-04 14:10:52 [default_loader.py:314] Loading weights took 4.41 seconds
[1;36m(EngineCore_DP0 pid=4006170)[0;0m INFO 12-04 14:10:53 [gpu_model_runner.py:3338] Model loading took 2.9110 GiB memory and 5.446895 seconds
[1;36m(EngineCore_DP0 pid=4006173)[0;0m INFO 12-04 14:10:53 [gpu_model_runner.py:3338] Model loading took 2.9110 GiB memory and 5.731724 seconds
[1;36m(EngineCore_DP0 pid=4006170)[0;0m INFO 12-04 14:11:07 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/6847b7a26b/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4006173)[0;0m INFO 12-04 14:11:07 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/6847b7a26b/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4006170)[0;0m INFO 12-04 14:11:07 [backends.py:647] Dynamo bytecode transform time: 13.76 s
[1;36m(EngineCore_DP0 pid=4006173)[0;0m INFO 12-04 14:11:07 [backends.py:647] Dynamo bytecode transform time: 13.52 s
[1;36m(EngineCore_DP0 pid=4006173)[0;0m INFO 12-04 14:11:13 [backends.py:210] Directly load the compiled graph(s) for dynamic shape from the cache, took 5.245 s
[1;36m(EngineCore_DP0 pid=4006170)[0;0m INFO 12-04 14:11:13 [backends.py:210] Directly load the compiled graph(s) for dynamic shape from the cache, took 5.255 s
[1;36m(EngineCore_DP0 pid=4006173)[0;0m INFO 12-04 14:11:15 [monitor.py:34] torch.compile takes 18.76 s in total
[1;36m(EngineCore_DP0 pid=4006170)[0;0m INFO 12-04 14:11:15 [monitor.py:34] torch.compile takes 19.02 s in total
[1;36m(EngineCore_DP0 pid=4006170)[0;0m INFO 12-04 14:11:16 [gpu_worker.py:359] Available KV cache memory: 30.58 GiB
[1;36m(EngineCore_DP0 pid=4006173)[0;0m INFO 12-04 14:11:16 [gpu_worker.py:359] Available KV cache memory: 32.40 GiB
[1;36m(EngineCore_DP0 pid=4006170)[0;0m INFO 12-04 14:11:16 [kv_cache_utils.py:1229] GPU KV cache size: 1,145,232 tokens
[1;36m(EngineCore_DP0 pid=4006170)[0;0m INFO 12-04 14:11:16 [kv_cache_utils.py:1234] Maximum concurrency for 131,072 tokens per request: 8.74x
[1;36m(EngineCore_DP0 pid=4006170)[0;0m INFO 12-04 14:11:17 [kernel_warmup.py:65] Warming up FlashInfer attention.
[1;36m(EngineCore_DP0 pid=4006173)[0;0m INFO 12-04 14:11:17 [kv_cache_utils.py:1229] GPU KV cache size: 1,213,184 tokens
[1;36m(EngineCore_DP0 pid=4006173)[0;0m INFO 12-04 14:11:17 [kv_cache_utils.py:1234] Maximum concurrency for 131,072 tokens per request: 9.26x
[1;36m(EngineCore_DP0 pid=4006173)[0;0m ERROR 12-04 14:11:17 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4006173)[0;0m ERROR 12-04 14:11:17 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4006173)[0;0m ERROR 12-04 14:11:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4006173)[0;0m ERROR 12-04 14:11:17 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4006173)[0;0m ERROR 12-04 14:11:17 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4006173)[0;0m ERROR 12-04 14:11:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4006173)[0;0m ERROR 12-04 14:11:17 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4006173)[0;0m ERROR 12-04 14:11:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4006173)[0;0m ERROR 12-04 14:11:17 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4006173)[0;0m ERROR 12-04 14:11:17 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4006173)[0;0m ERROR 12-04 14:11:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 247, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4006173)[0;0m ERROR 12-04 14:11:17 [core.py:842]     self.model_executor.initialize_from_config(kv_cache_configs)
[1;36m(EngineCore_DP0 pid=4006173)[0;0m ERROR 12-04 14:11:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 115, in initialize_from_config
[1;36m(EngineCore_DP0 pid=4006173)[0;0m ERROR 12-04 14:11:17 [core.py:842]     self.collective_rpc("initialize_from_config", args=(kv_cache_configs,))
[1;36m(EngineCore_DP0 pid=4006173)[0;0m ERROR 12-04 14:11:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4006173)[0;0m ERROR 12-04 14:11:17 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4006173)[0;0m ERROR 12-04 14:11:17 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4006173)[0;0m ERROR 12-04 14:11:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4006173)[0;0m ERROR 12-04 14:11:17 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4006173)[0;0m ERROR 12-04 14:11:17 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4006173)[0;0m ERROR 12-04 14:11:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 319, in initialize_from_config
[1;36m(EngineCore_DP0 pid=4006173)[0;0m ERROR 12-04 14:11:17 [core.py:842]     self.worker.initialize_from_config(kv_cache_config)  # type: ignore
[1;36m(EngineCore_DP0 pid=4006173)[0;0m ERROR 12-04 14:11:17 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4006173)[0;0m ERROR 12-04 14:11:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 404, in initialize_from_config
[1;36m(EngineCore_DP0 pid=4006173)[0;0m ERROR 12-04 14:11:17 [core.py:842]     self.model_runner.initialize_kv_cache(kv_cache_config)
[1;36m(EngineCore_DP0 pid=4006173)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=4006173)[0;0m ERROR 12-04 14:11:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 5007, in initialize_kv_cache
[1;36m(EngineCore_DP0 pid=4006173)[0;0m ERROR 12-04 14:11:17 [core.py:842]     kv_caches = self.initialize_kv_cache_tensors(
[1;36m(EngineCore_DP0 pid=4006173)[0;0m ERROR 12-04 14:11:17 [core.py:842]                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4006173)[0;0m ERROR 12-04 14:11:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4933, in initialize_kv_cache_tensors
[1;36m(EngineCore_DP0 pid=4006173)[0;0m ERROR 12-04 14:11:17 [core.py:842]     kv_cache_raw_tensors = self._allocate_kv_cache_tensors(kv_cache_config)
[1;36m(EngineCore_DP0 pid=4006173)[0;0m ERROR 12-04 14:11:17 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4006173)[0;0m ERROR 12-04 14:11:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4718, in _allocate_kv_cache_tensors
[1;36m(EngineCore_DP0 pid=4006173)[0;0m ERROR 12-04 14:11:17 [core.py:842]     tensor = torch.zeros(
[1;36m(EngineCore_DP0 pid=4006173)[0;0m ERROR 12-04 14:11:17 [core.py:842]              ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4006173)[0;0m ERROR 12-04 14:11:17 [core.py:842] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.16 GiB. GPU 0 has a total capacity of 44.29 GiB of which 117.69 MiB is free. Including non-PyTorch memory, this process has 10.25 GiB memory in use. Process 4006170 has 33.93 GiB memory in use. Of the allocated memory 9.90 GiB is allocated by PyTorch, and 135.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=4006173)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4006173)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=4006173)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=4006173)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=4006173)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=4006173)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=4006173)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=4006173)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4006173)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4006173)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4006173)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4006173)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=4006173)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4006173)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4006173)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4006173)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 247, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4006173)[0;0m     self.model_executor.initialize_from_config(kv_cache_configs)
[1;36m(EngineCore_DP0 pid=4006173)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 115, in initialize_from_config
[1;36m(EngineCore_DP0 pid=4006173)[0;0m     self.collective_rpc("initialize_from_config", args=(kv_cache_configs,))
[1;36m(EngineCore_DP0 pid=4006173)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4006173)[0;0m     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4006173)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4006173)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4006173)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4006173)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4006173)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 319, in initialize_from_config
[1;36m(EngineCore_DP0 pid=4006173)[0;0m     self.worker.initialize_from_config(kv_cache_config)  # type: ignore
[1;36m(EngineCore_DP0 pid=4006173)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4006173)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 404, in initialize_from_config
[1;36m(EngineCore_DP0 pid=4006173)[0;0m     self.model_runner.initialize_kv_cache(kv_cache_config)
[1;36m(EngineCore_DP0 pid=4006173)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 5007, in initialize_kv_cache
[1;36m(EngineCore_DP0 pid=4006173)[0;0m     kv_caches = self.initialize_kv_cache_tensors(
[1;36m(EngineCore_DP0 pid=4006173)[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4006173)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4933, in initialize_kv_cache_tensors
[1;36m(EngineCore_DP0 pid=4006173)[0;0m     kv_cache_raw_tensors = self._allocate_kv_cache_tensors(kv_cache_config)
[1;36m(EngineCore_DP0 pid=4006173)[0;0m                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4006173)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4718, in _allocate_kv_cache_tensors
[1;36m(EngineCore_DP0 pid=4006173)[0;0m     tensor = torch.zeros(
[1;36m(EngineCore_DP0 pid=4006173)[0;0m              ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4006173)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.16 GiB. GPU 0 has a total capacity of 44.29 GiB of which 117.69 MiB is free. Including non-PyTorch memory, this process has 10.25 GiB memory in use. Process 4006170 has 33.93 GiB memory in use. Of the allocated memory 9.90 GiB is allocated by PyTorch, and 135.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=4006170)[0;0m ERROR 12-04 14:11:17 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4006170)[0;0m ERROR 12-04 14:11:17 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4006170)[0;0m ERROR 12-04 14:11:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4006170)[0;0m ERROR 12-04 14:11:17 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4006170)[0;0m ERROR 12-04 14:11:17 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4006170)[0;0m ERROR 12-04 14:11:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4006170)[0;0m ERROR 12-04 14:11:17 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4006170)[0;0m ERROR 12-04 14:11:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4006170)[0;0m ERROR 12-04 14:11:17 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4006170)[0;0m ERROR 12-04 14:11:17 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4006170)[0;0m ERROR 12-04 14:11:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 247, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4006170)[0;0m ERROR 12-04 14:11:17 [core.py:842]     self.model_executor.initialize_from_config(kv_cache_configs)
[1;36m(EngineCore_DP0 pid=4006170)[0;0m ERROR 12-04 14:11:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 116, in initialize_from_config
[1;36m(EngineCore_DP0 pid=4006170)[0;0m ERROR 12-04 14:11:17 [core.py:842]     self.collective_rpc("compile_or_warm_up_model")
[1;36m(EngineCore_DP0 pid=4006170)[0;0m ERROR 12-04 14:11:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4006170)[0;0m ERROR 12-04 14:11:17 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4006170)[0;0m ERROR 12-04 14:11:17 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4006170)[0;0m ERROR 12-04 14:11:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4006170)[0;0m ERROR 12-04 14:11:17 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4006170)[0;0m ERROR 12-04 14:11:17 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4006170)[0;0m ERROR 12-04 14:11:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 425, in compile_or_warm_up_model
[1;36m(EngineCore_DP0 pid=4006170)[0;0m ERROR 12-04 14:11:17 [core.py:842]     kernel_warmup(self)
[1;36m(EngineCore_DP0 pid=4006170)[0;0m ERROR 12-04 14:11:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/warmup/kernel_warmup.py", line 68, in kernel_warmup
[1;36m(EngineCore_DP0 pid=4006170)[0;0m ERROR 12-04 14:11:17 [core.py:842]     worker.model_runner._dummy_run(
[1;36m(EngineCore_DP0 pid=4006170)[0;0m ERROR 12-04 14:11:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4006170)[0;0m ERROR 12-04 14:11:17 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4006170)[0;0m ERROR 12-04 14:11:17 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4006170)[0;0m ERROR 12-04 14:11:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3756, in _dummy_run
[1;36m(EngineCore_DP0 pid=4006170)[0;0m ERROR 12-04 14:11:17 [core.py:842]     attn_metadata, _ = self._build_attention_metadata(
[1;36m(EngineCore_DP0 pid=4006170)[0;0m ERROR 12-04 14:11:17 [core.py:842]                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4006170)[0;0m ERROR 12-04 14:11:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1576, in _build_attention_metadata
[1;36m(EngineCore_DP0 pid=4006170)[0;0m ERROR 12-04 14:11:17 [core.py:842]     attn_metadata_i = builder.build_for_cudagraph_capture(
[1;36m(EngineCore_DP0 pid=4006170)[0;0m ERROR 12-04 14:11:17 [core.py:842]                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4006170)[0;0m ERROR 12-04 14:11:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/attention/backends/utils.py", line 332, in build_for_cudagraph_capture
[1;36m(EngineCore_DP0 pid=4006170)[0;0m ERROR 12-04 14:11:17 [core.py:842]     return self.build(
[1;36m(EngineCore_DP0 pid=4006170)[0;0m ERROR 12-04 14:11:17 [core.py:842]            ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4006170)[0;0m ERROR 12-04 14:11:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/attention/backends/flashinfer.py", line 844, in build
[1;36m(EngineCore_DP0 pid=4006170)[0;0m ERROR 12-04 14:11:17 [core.py:842]     attn_metadata.prefill_wrapper = self._get_prefill_wrapper()
[1;36m(EngineCore_DP0 pid=4006170)[0;0m ERROR 12-04 14:11:17 [core.py:842]                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4006170)[0;0m ERROR 12-04 14:11:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/attention/backends/flashinfer.py", line 598, in _get_prefill_wrapper
[1;36m(EngineCore_DP0 pid=4006170)[0;0m ERROR 12-04 14:11:17 [core.py:842]     self._get_workspace_buffer(), get_kv_cache_layout()
[1;36m(EngineCore_DP0 pid=4006170)[0;0m ERROR 12-04 14:11:17 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4006170)[0;0m ERROR 12-04 14:11:17 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/attention/backends/flashinfer.py", line 583, in _get_workspace_buffer
[1;36m(EngineCore_DP0 pid=4006170)[0;0m ERROR 12-04 14:11:17 [core.py:842]     self._workspace_buffer = torch.zeros(
[1;36m(EngineCore_DP0 pid=4006170)[0;0m ERROR 12-04 14:11:17 [core.py:842]                              ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4006170)[0;0m ERROR 12-04 14:11:17 [core.py:842] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 394.00 MiB. GPU 0 has a total capacity of 44.29 GiB of which 117.69 MiB is free. Process 4006173 has 10.25 GiB memory in use. Including non-PyTorch memory, this process has 33.93 GiB memory in use. Of the allocated memory 33.54 GiB is allocated by PyTorch, and 172.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=4006170)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=4006170)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4006170)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=4006170)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=4006170)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=4006170)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=4006170)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=4006170)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=4006170)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4006170)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4006170)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4006170)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4006170)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=4006170)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4006170)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4006170)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4006170)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 247, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4006170)[0;0m     self.model_executor.initialize_from_config(kv_cache_configs)
[1;36m(EngineCore_DP0 pid=4006170)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 116, in initialize_from_config
[1;36m(EngineCore_DP0 pid=4006170)[0;0m     self.collective_rpc("compile_or_warm_up_model")
[1;36m(EngineCore_DP0 pid=4006170)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4006170)[0;0m     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4006170)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4006170)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4006170)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4006170)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4006170)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 425, in compile_or_warm_up_model
[1;36m(EngineCore_DP0 pid=4006170)[0;0m     kernel_warmup(self)
[1;36m(EngineCore_DP0 pid=4006170)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/warmup/kernel_warmup.py", line 68, in kernel_warmup
[1;36m(EngineCore_DP0 pid=4006170)[0;0m     worker.model_runner._dummy_run(
[1;36m(EngineCore_DP0 pid=4006170)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4006170)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4006170)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4006170)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3756, in _dummy_run
[1;36m(EngineCore_DP0 pid=4006170)[0;0m     attn_metadata, _ = self._build_attention_metadata(
[1;36m(EngineCore_DP0 pid=4006170)[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4006170)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1576, in _build_attention_metadata
[1;36m(EngineCore_DP0 pid=4006170)[0;0m     attn_metadata_i = builder.build_for_cudagraph_capture(
[1;36m(EngineCore_DP0 pid=4006170)[0;0m                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4006170)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/attention/backends/utils.py", line 332, in build_for_cudagraph_capture
[1;36m(EngineCore_DP0 pid=4006170)[0;0m     return self.build(
[1;36m(EngineCore_DP0 pid=4006170)[0;0m            ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4006170)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/attention/backends/flashinfer.py", line 844, in build
[1;36m(EngineCore_DP0 pid=4006170)[0;0m     attn_metadata.prefill_wrapper = self._get_prefill_wrapper()
[1;36m(EngineCore_DP0 pid=4006170)[0;0m                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4006170)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/attention/backends/flashinfer.py", line 598, in _get_prefill_wrapper
[1;36m(EngineCore_DP0 pid=4006170)[0;0m     self._get_workspace_buffer(), get_kv_cache_layout()
[1;36m(EngineCore_DP0 pid=4006170)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4006170)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/attention/backends/flashinfer.py", line 583, in _get_workspace_buffer
[1;36m(EngineCore_DP0 pid=4006170)[0;0m     self._workspace_buffer = torch.zeros(
[1;36m(EngineCore_DP0 pid=4006170)[0;0m                              ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4006170)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 394.00 MiB. GPU 0 has a total capacity of 44.29 GiB of which 117.69 MiB is free. Process 4006173 has 10.25 GiB memory in use. Including non-PyTorch memory, this process has 33.93 GiB memory in use. Of the allocated memory 33.54 GiB is allocated by PyTorch, and 172.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W1204 14:11:17.691645081 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank0]:[W1204 14:11:18.945937800 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: WeiboAI/VibeThinker-1.5B (backend=vllm)
[ModelCache] Using max_model_len=131072 for WeiboAI/VibeThinker-1.5B
[ModelCache] Detected 1 GPU(s), 44.3GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 14:11:39 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'WeiboAI/VibeThinker-1.5B'}
INFO 12-04 14:11:39 [model.py:631] Resolved architecture: Qwen2ForCausalLM
WARNING 12-04 14:11:39 [model.py:1921] Your device 'Quadro RTX 8000' (with compute capability 7.5) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.
WARNING 12-04 14:11:39 [model.py:1971] Casting torch.bfloat16 to torch.float16.
INFO 12-04 14:11:39 [model.py:1745] Using max model len 131072
INFO 12-04 14:11:39 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: WeiboAI/VibeThinker-1.5B (backend=vllm)
[ModelCache] Using max_model_len=131072 for WeiboAI/VibeThinker-1.5B
[ModelCache] Detected 1 GPU(s), 44.3GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 14:11:39 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'WeiboAI/VibeThinker-1.5B'}
INFO 12-04 14:11:39 [model.py:631] Resolved architecture: Qwen2ForCausalLM
WARNING 12-04 14:11:39 [model.py:1921] Your device 'Quadro RTX 8000' (with compute capability 7.5) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.
WARNING 12-04 14:11:39 [model.py:1971] Casting torch.bfloat16 to torch.float16.
INFO 12-04 14:11:39 [model.py:1745] Using max model len 131072
INFO 12-04 14:11:39 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4006360)[0;0m INFO 12-04 14:12:01 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='WeiboAI/VibeThinker-1.5B', speculative_config=None, tokenizer='WeiboAI/VibeThinker-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=WeiboAI/VibeThinker-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4006363)[0;0m INFO 12-04 14:12:01 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='WeiboAI/VibeThinker-1.5B', speculative_config=None, tokenizer='WeiboAI/VibeThinker-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=WeiboAI/VibeThinker-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4006363)[0;0m ERROR 12-04 14:12:03 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8
[1;36m(EngineCore_DP0 pid=4006360)[0;0m ERROR 12-04 14:12:03 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8
[1;36m(EngineCore_DP0 pid=4006360)[0;0m INFO 12-04 14:12:04 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.8:38533 backend=nccl
[1;36m(EngineCore_DP0 pid=4006363)[0;0m INFO 12-04 14:12:04 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.8:51071 backend=nccl
[W1204 14:12:04.952254409 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-23-0.rc.tch.harvard.edu]:38533 (errno: 97 - Address family not supported by protocol).
[W1204 14:12:04.953498487 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-23-0.rc.tch.harvard.edu]:51071 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4006363)[0;0m INFO 12-04 14:12:04 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4006360)[0;0m INFO 12-04 14:12:04 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4006360)[0;0m INFO 12-04 14:12:04 [gpu_model_runner.py:3259] Starting to load model WeiboAI/VibeThinker-1.5B...
[1;36m(EngineCore_DP0 pid=4006363)[0;0m INFO 12-04 14:12:04 [gpu_model_runner.py:3259] Starting to load model WeiboAI/VibeThinker-1.5B...
[1;36m(EngineCore_DP0 pid=4006360)[0;0m INFO 12-04 14:12:05 [cuda.py:418] Valid backends: ['FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4006360)[0;0m INFO 12-04 14:12:05 [cuda.py:427] Using FLASHINFER backend.
[1;36m(EngineCore_DP0 pid=4006363)[0;0m INFO 12-04 14:12:05 [cuda.py:418] Valid backends: ['FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4006363)[0;0m INFO 12-04 14:12:05 [cuda.py:427] Using FLASHINFER backend.
[1;36m(EngineCore_DP0 pid=4006363)[0;0m INFO 12-04 14:12:06 [weight_utils.py:481] No model.safetensors.index.json found in remote.
[1;36m(EngineCore_DP0 pid=4006363)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
[1;36m(EngineCore_DP0 pid=4006360)[0;0m INFO 12-04 14:12:06 [weight_utils.py:481] No model.safetensors.index.json found in remote.
[1;36m(EngineCore_DP0 pid=4006360)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
[1;36m(EngineCore_DP0 pid=4006363)[0;0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.66s/it]
[1;36m(EngineCore_DP0 pid=4006363)[0;0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.66s/it]
[1;36m(EngineCore_DP0 pid=4006363)[0;0m 
[1;36m(EngineCore_DP0 pid=4006363)[0;0m INFO 12-04 14:12:09 [default_loader.py:314] Loading weights took 3.78 seconds
[1;36m(EngineCore_DP0 pid=4006360)[0;0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:04<00:00,  4.12s/it]
[1;36m(EngineCore_DP0 pid=4006360)[0;0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:04<00:00,  4.12s/it]
[1;36m(EngineCore_DP0 pid=4006360)[0;0m 
[1;36m(EngineCore_DP0 pid=4006360)[0;0m INFO 12-04 14:12:10 [default_loader.py:314] Loading weights took 4.25 seconds
[1;36m(EngineCore_DP0 pid=4006363)[0;0m INFO 12-04 14:12:10 [gpu_model_runner.py:3338] Model loading took 2.9110 GiB memory and 4.950248 seconds
[1;36m(EngineCore_DP0 pid=4006360)[0;0m INFO 12-04 14:12:11 [gpu_model_runner.py:3338] Model loading took 2.9110 GiB memory and 5.559908 seconds
[1;36m(EngineCore_DP0 pid=4006360)[0;0m INFO 12-04 14:12:24 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/6847b7a26b/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4006363)[0;0m INFO 12-04 14:12:24 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/6847b7a26b/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4006360)[0;0m INFO 12-04 14:12:24 [backends.py:647] Dynamo bytecode transform time: 12.94 s
[1;36m(EngineCore_DP0 pid=4006363)[0;0m INFO 12-04 14:12:24 [backends.py:647] Dynamo bytecode transform time: 13.46 s
[1;36m(EngineCore_DP0 pid=4006363)[0;0m INFO 12-04 14:12:30 [backends.py:210] Directly load the compiled graph(s) for dynamic shape from the cache, took 5.226 s
[1;36m(EngineCore_DP0 pid=4006360)[0;0m INFO 12-04 14:12:30 [backends.py:210] Directly load the compiled graph(s) for dynamic shape from the cache, took 5.236 s
[1;36m(EngineCore_DP0 pid=4006363)[0;0m INFO 12-04 14:12:31 [monitor.py:34] torch.compile takes 18.69 s in total
[1;36m(EngineCore_DP0 pid=4006360)[0;0m INFO 12-04 14:12:31 [monitor.py:34] torch.compile takes 18.17 s in total
[1;36m(EngineCore_DP0 pid=4006363)[0;0m INFO 12-04 14:12:33 [gpu_worker.py:359] Available KV cache memory: 30.58 GiB
[1;36m(EngineCore_DP0 pid=4006360)[0;0m INFO 12-04 14:12:33 [gpu_worker.py:359] Available KV cache memory: 32.40 GiB
[1;36m(EngineCore_DP0 pid=4006363)[0;0m INFO 12-04 14:12:33 [kv_cache_utils.py:1229] GPU KV cache size: 1,145,232 tokens
[1;36m(EngineCore_DP0 pid=4006363)[0;0m INFO 12-04 14:12:33 [kv_cache_utils.py:1234] Maximum concurrency for 131,072 tokens per request: 8.74x
[1;36m(EngineCore_DP0 pid=4006360)[0;0m INFO 12-04 14:12:33 [kv_cache_utils.py:1229] GPU KV cache size: 1,213,184 tokens
[1;36m(EngineCore_DP0 pid=4006360)[0;0m INFO 12-04 14:12:33 [kv_cache_utils.py:1234] Maximum concurrency for 131,072 tokens per request: 9.26x
[1;36m(EngineCore_DP0 pid=4006363)[0;0m INFO 12-04 14:12:33 [kernel_warmup.py:65] Warming up FlashInfer attention.
[1;36m(EngineCore_DP0 pid=4006360)[0;0m ERROR 12-04 14:12:33 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4006360)[0;0m ERROR 12-04 14:12:33 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4006360)[0;0m ERROR 12-04 14:12:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4006360)[0;0m ERROR 12-04 14:12:33 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4006360)[0;0m ERROR 12-04 14:12:33 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4006360)[0;0m ERROR 12-04 14:12:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4006360)[0;0m ERROR 12-04 14:12:33 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4006360)[0;0m ERROR 12-04 14:12:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4006360)[0;0m ERROR 12-04 14:12:33 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4006360)[0;0m ERROR 12-04 14:12:33 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4006360)[0;0m ERROR 12-04 14:12:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 247, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4006360)[0;0m ERROR 12-04 14:12:33 [core.py:842]     self.model_executor.initialize_from_config(kv_cache_configs)
[1;36m(EngineCore_DP0 pid=4006360)[0;0m ERROR 12-04 14:12:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 115, in initialize_from_config
[1;36m(EngineCore_DP0 pid=4006360)[0;0m ERROR 12-04 14:12:33 [core.py:842]     self.collective_rpc("initialize_from_config", args=(kv_cache_configs,))
[1;36m(EngineCore_DP0 pid=4006360)[0;0m ERROR 12-04 14:12:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4006360)[0;0m ERROR 12-04 14:12:33 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4006360)[0;0m ERROR 12-04 14:12:33 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4006360)[0;0m ERROR 12-04 14:12:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4006360)[0;0m ERROR 12-04 14:12:33 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4006360)[0;0m ERROR 12-04 14:12:33 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4006360)[0;0m ERROR 12-04 14:12:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 319, in initialize_from_config
[1;36m(EngineCore_DP0 pid=4006360)[0;0m ERROR 12-04 14:12:33 [core.py:842]     self.worker.initialize_from_config(kv_cache_config)  # type: ignore
[1;36m(EngineCore_DP0 pid=4006360)[0;0m ERROR 12-04 14:12:33 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4006360)[0;0m ERROR 12-04 14:12:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 404, in initialize_from_config
[1;36m(EngineCore_DP0 pid=4006360)[0;0m ERROR 12-04 14:12:33 [core.py:842]     self.model_runner.initialize_kv_cache(kv_cache_config)
[1;36m(EngineCore_DP0 pid=4006360)[0;0m ERROR 12-04 14:12:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 5007, in initialize_kv_cache
[1;36m(EngineCore_DP0 pid=4006360)[0;0m ERROR 12-04 14:12:33 [core.py:842]     kv_caches = self.initialize_kv_cache_tensors(
[1;36m(EngineCore_DP0 pid=4006360)[0;0m ERROR 12-04 14:12:33 [core.py:842]                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4006360)[0;0m ERROR 12-04 14:12:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4933, in initialize_kv_cache_tensors
[1;36m(EngineCore_DP0 pid=4006360)[0;0m ERROR 12-04 14:12:33 [core.py:842]     kv_cache_raw_tensors = self._allocate_kv_cache_tensors(kv_cache_config)
[1;36m(EngineCore_DP0 pid=4006360)[0;0m ERROR 12-04 14:12:33 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4006360)[0;0m ERROR 12-04 14:12:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4718, in _allocate_kv_cache_tensors
[1;36m(EngineCore_DP0 pid=4006360)[0;0m ERROR 12-04 14:12:33 [core.py:842]     tensor = torch.zeros(
[1;36m(EngineCore_DP0 pid=4006360)[0;0m ERROR 12-04 14:12:33 [core.py:842]              ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4006360)[0;0m ERROR 12-04 14:12:33 [core.py:842] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.16 GiB. GPU 0 has a total capacity of 44.29 GiB of which 117.69 MiB is free. Process 4006363 has 33.93 GiB memory in use. Including non-PyTorch memory, this process has 10.25 GiB memory in use. Of the allocated memory 9.90 GiB is allocated by PyTorch, and 135.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=4006360)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=4006360)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4006360)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=4006360)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=4006360)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=4006360)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=4006360)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=4006360)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=4006360)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4006360)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4006360)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4006360)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4006360)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=4006360)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4006360)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4006360)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4006360)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 247, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4006360)[0;0m     self.model_executor.initialize_from_config(kv_cache_configs)
[1;36m(EngineCore_DP0 pid=4006360)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 115, in initialize_from_config
[1;36m(EngineCore_DP0 pid=4006360)[0;0m     self.collective_rpc("initialize_from_config", args=(kv_cache_configs,))
[1;36m(EngineCore_DP0 pid=4006360)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4006360)[0;0m     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4006360)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4006360)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4006360)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4006360)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4006360)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 319, in initialize_from_config
[1;36m(EngineCore_DP0 pid=4006360)[0;0m     self.worker.initialize_from_config(kv_cache_config)  # type: ignore
[1;36m(EngineCore_DP0 pid=4006360)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4006360)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 404, in initialize_from_config
[1;36m(EngineCore_DP0 pid=4006360)[0;0m     self.model_runner.initialize_kv_cache(kv_cache_config)
[1;36m(EngineCore_DP0 pid=4006360)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 5007, in initialize_kv_cache
[1;36m(EngineCore_DP0 pid=4006360)[0;0m     kv_caches = self.initialize_kv_cache_tensors(
[1;36m(EngineCore_DP0 pid=4006360)[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4006360)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4933, in initialize_kv_cache_tensors
[1;36m(EngineCore_DP0 pid=4006360)[0;0m     kv_cache_raw_tensors = self._allocate_kv_cache_tensors(kv_cache_config)
[1;36m(EngineCore_DP0 pid=4006360)[0;0m                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4006360)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4718, in _allocate_kv_cache_tensors
[1;36m(EngineCore_DP0 pid=4006360)[0;0m     tensor = torch.zeros(
[1;36m(EngineCore_DP0 pid=4006360)[0;0m              ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4006360)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.16 GiB. GPU 0 has a total capacity of 44.29 GiB of which 117.69 MiB is free. Process 4006363 has 33.93 GiB memory in use. Including non-PyTorch memory, this process has 10.25 GiB memory in use. Of the allocated memory 9.90 GiB is allocated by PyTorch, and 135.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=4006363)[0;0m ERROR 12-04 14:12:33 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4006363)[0;0m ERROR 12-04 14:12:33 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4006363)[0;0m ERROR 12-04 14:12:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4006363)[0;0m ERROR 12-04 14:12:33 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4006363)[0;0m ERROR 12-04 14:12:33 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4006363)[0;0m ERROR 12-04 14:12:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4006363)[0;0m ERROR 12-04 14:12:33 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4006363)[0;0m ERROR 12-04 14:12:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4006363)[0;0m ERROR 12-04 14:12:33 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4006363)[0;0m ERROR 12-04 14:12:33 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4006363)[0;0m ERROR 12-04 14:12:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 247, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4006363)[0;0m ERROR 12-04 14:12:33 [core.py:842]     self.model_executor.initialize_from_config(kv_cache_configs)
[1;36m(EngineCore_DP0 pid=4006363)[0;0m ERROR 12-04 14:12:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 116, in initialize_from_config
[1;36m(EngineCore_DP0 pid=4006363)[0;0m ERROR 12-04 14:12:33 [core.py:842]     self.collective_rpc("compile_or_warm_up_model")
[1;36m(EngineCore_DP0 pid=4006363)[0;0m ERROR 12-04 14:12:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4006363)[0;0m ERROR 12-04 14:12:33 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4006363)[0;0m ERROR 12-04 14:12:33 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4006363)[0;0m ERROR 12-04 14:12:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4006363)[0;0m ERROR 12-04 14:12:33 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4006363)[0;0m ERROR 12-04 14:12:33 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4006363)[0;0m ERROR 12-04 14:12:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 425, in compile_or_warm_up_model
[1;36m(EngineCore_DP0 pid=4006363)[0;0m ERROR 12-04 14:12:33 [core.py:842]     kernel_warmup(self)
[1;36m(EngineCore_DP0 pid=4006363)[0;0m ERROR 12-04 14:12:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/warmup/kernel_warmup.py", line 68, in kernel_warmup
[1;36m(EngineCore_DP0 pid=4006363)[0;0m ERROR 12-04 14:12:33 [core.py:842]     worker.model_runner._dummy_run(
[1;36m(EngineCore_DP0 pid=4006363)[0;0m ERROR 12-04 14:12:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4006363)[0;0m ERROR 12-04 14:12:33 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4006363)[0;0m ERROR 12-04 14:12:33 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4006363)[0;0m ERROR 12-04 14:12:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3756, in _dummy_run
[1;36m(EngineCore_DP0 pid=4006363)[0;0m ERROR 12-04 14:12:33 [core.py:842]     attn_metadata, _ = self._build_attention_metadata(
[1;36m(EngineCore_DP0 pid=4006363)[0;0m ERROR 12-04 14:12:33 [core.py:842]                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4006363)[0;0m ERROR 12-04 14:12:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1576, in _build_attention_metadata
[1;36m(EngineCore_DP0 pid=4006363)[0;0m ERROR 12-04 14:12:33 [core.py:842]     attn_metadata_i = builder.build_for_cudagraph_capture(
[1;36m(EngineCore_DP0 pid=4006363)[0;0m ERROR 12-04 14:12:33 [core.py:842]                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4006363)[0;0m ERROR 12-04 14:12:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/attention/backends/utils.py", line 332, in build_for_cudagraph_capture
[1;36m(EngineCore_DP0 pid=4006363)[0;0m ERROR 12-04 14:12:33 [core.py:842]     return self.build(
[1;36m(EngineCore_DP0 pid=4006363)[0;0m ERROR 12-04 14:12:33 [core.py:842]            ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4006363)[0;0m ERROR 12-04 14:12:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/attention/backends/flashinfer.py", line 844, in build
[1;36m(EngineCore_DP0 pid=4006363)[0;0m ERROR 12-04 14:12:33 [core.py:842]     attn_metadata.prefill_wrapper = self._get_prefill_wrapper()
[1;36m(EngineCore_DP0 pid=4006363)[0;0m ERROR 12-04 14:12:33 [core.py:842]                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4006363)[0;0m ERROR 12-04 14:12:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/attention/backends/flashinfer.py", line 598, in _get_prefill_wrapper
[1;36m(EngineCore_DP0 pid=4006363)[0;0m ERROR 12-04 14:12:33 [core.py:842]     self._get_workspace_buffer(), get_kv_cache_layout()
[1;36m(EngineCore_DP0 pid=4006363)[0;0m ERROR 12-04 14:12:33 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4006363)[0;0m ERROR 12-04 14:12:33 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/attention/backends/flashinfer.py", line 583, in _get_workspace_buffer
[1;36m(EngineCore_DP0 pid=4006363)[0;0m ERROR 12-04 14:12:33 [core.py:842]     self._workspace_buffer = torch.zeros(
[1;36m(EngineCore_DP0 pid=4006363)[0;0m ERROR 12-04 14:12:33 [core.py:842]                              ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4006363)[0;0m ERROR 12-04 14:12:33 [core.py:842] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 394.00 MiB. GPU 0 has a total capacity of 44.29 GiB of which 117.69 MiB is free. Including non-PyTorch memory, this process has 33.93 GiB memory in use. Process 4006360 has 10.25 GiB memory in use. Of the allocated memory 33.54 GiB is allocated by PyTorch, and 172.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=4006363)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=4006363)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4006363)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=4006363)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=4006363)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=4006363)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=4006363)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=4006363)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=4006363)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4006363)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4006363)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4006363)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4006363)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=4006363)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4006363)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4006363)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4006363)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 247, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4006363)[0;0m     self.model_executor.initialize_from_config(kv_cache_configs)
[1;36m(EngineCore_DP0 pid=4006363)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 116, in initialize_from_config
[1;36m(EngineCore_DP0 pid=4006363)[0;0m     self.collective_rpc("compile_or_warm_up_model")
[1;36m(EngineCore_DP0 pid=4006363)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4006363)[0;0m     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4006363)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4006363)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4006363)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4006363)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4006363)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 425, in compile_or_warm_up_model
[1;36m(EngineCore_DP0 pid=4006363)[0;0m     kernel_warmup(self)
[1;36m(EngineCore_DP0 pid=4006363)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/warmup/kernel_warmup.py", line 68, in kernel_warmup
[1;36m(EngineCore_DP0 pid=4006363)[0;0m     worker.model_runner._dummy_run(
[1;36m(EngineCore_DP0 pid=4006363)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=4006363)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4006363)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4006363)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3756, in _dummy_run
[1;36m(EngineCore_DP0 pid=4006363)[0;0m     attn_metadata, _ = self._build_attention_metadata(
[1;36m(EngineCore_DP0 pid=4006363)[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4006363)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1576, in _build_attention_metadata
[1;36m(EngineCore_DP0 pid=4006363)[0;0m     attn_metadata_i = builder.build_for_cudagraph_capture(
[1;36m(EngineCore_DP0 pid=4006363)[0;0m                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4006363)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/attention/backends/utils.py", line 332, in build_for_cudagraph_capture
[1;36m(EngineCore_DP0 pid=4006363)[0;0m     return self.build(
[1;36m(EngineCore_DP0 pid=4006363)[0;0m            ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4006363)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/attention/backends/flashinfer.py", line 844, in build
[1;36m(EngineCore_DP0 pid=4006363)[0;0m     attn_metadata.prefill_wrapper = self._get_prefill_wrapper()
[1;36m(EngineCore_DP0 pid=4006363)[0;0m                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4006363)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/attention/backends/flashinfer.py", line 598, in _get_prefill_wrapper
[1;36m(EngineCore_DP0 pid=4006363)[0;0m     self._get_workspace_buffer(), get_kv_cache_layout()
[1;36m(EngineCore_DP0 pid=4006363)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4006363)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/attention/backends/flashinfer.py", line 583, in _get_workspace_buffer
[1;36m(EngineCore_DP0 pid=4006363)[0;0m     self._workspace_buffer = torch.zeros(
[1;36m(EngineCore_DP0 pid=4006363)[0;0m                              ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4006363)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 394.00 MiB. GPU 0 has a total capacity of 44.29 GiB of which 117.69 MiB is free. Including non-PyTorch memory, this process has 33.93 GiB memory in use. Process 4006360 has 10.25 GiB memory in use. Of the allocated memory 33.54 GiB is allocated by PyTorch, and 172.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W1204 14:12:34.578641392 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank0]:[W1204 14:12:34.693180119 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: WeiboAI/VibeThinker-1.5B (backend=vllm)
[ModelCache] Using max_model_len=131072 for WeiboAI/VibeThinker-1.5B
[ModelCache] Detected 1 GPU(s), 44.3GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 14:12:56 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'WeiboAI/VibeThinker-1.5B'}
INFO 12-04 14:12:56 [model.py:631] Resolved architecture: Qwen2ForCausalLM
WARNING 12-04 14:12:56 [model.py:1921] Your device 'Quadro RTX 8000' (with compute capability 7.5) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.
WARNING 12-04 14:12:56 [model.py:1971] Casting torch.bfloat16 to torch.float16.
INFO 12-04 14:12:56 [model.py:1745] Using max model len 131072
INFO 12-04 14:12:56 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: WeiboAI/VibeThinker-1.5B (backend=vllm)
[ModelCache] Using max_model_len=131072 for WeiboAI/VibeThinker-1.5B
[ModelCache] Detected 1 GPU(s), 44.3GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 14:12:56 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'WeiboAI/VibeThinker-1.5B'}
INFO 12-04 14:12:56 [model.py:631] Resolved architecture: Qwen2ForCausalLM
WARNING 12-04 14:12:56 [model.py:1921] Your device 'Quadro RTX 8000' (with compute capability 7.5) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.
WARNING 12-04 14:12:56 [model.py:1971] Casting torch.bfloat16 to torch.float16.
INFO 12-04 14:12:56 [model.py:1745] Using max model len 131072
INFO 12-04 14:12:56 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4006629)[0;0m INFO 12-04 14:13:13 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='WeiboAI/VibeThinker-1.5B', speculative_config=None, tokenizer='WeiboAI/VibeThinker-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=WeiboAI/VibeThinker-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4006626)[0;0m INFO 12-04 14:13:13 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='WeiboAI/VibeThinker-1.5B', speculative_config=None, tokenizer='WeiboAI/VibeThinker-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=WeiboAI/VibeThinker-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4006629)[0;0m ERROR 12-04 14:13:14 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8
[1;36m(EngineCore_DP0 pid=4006626)[0;0m ERROR 12-04 14:13:14 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8
[1;36m(EngineCore_DP0 pid=4006629)[0;0m INFO 12-04 14:13:15 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.8:34133 backend=nccl
[1;36m(EngineCore_DP0 pid=4006626)[0;0m INFO 12-04 14:13:15 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.8:35723 backend=nccl
[W1204 14:13:15.507722351 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-23-0.rc.tch.harvard.edu]:34133 (errno: 97 - Address family not supported by protocol).
[W1204 14:13:15.510652823 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-23-0.rc.tch.harvard.edu]:35723 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4006626)[0;0m INFO 12-04 14:13:15 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4006629)[0;0m INFO 12-04 14:13:15 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4006626)[0;0m INFO 12-04 14:13:16 [gpu_model_runner.py:3259] Starting to load model WeiboAI/VibeThinker-1.5B...
[1;36m(EngineCore_DP0 pid=4006629)[0;0m INFO 12-04 14:13:16 [gpu_model_runner.py:3259] Starting to load model WeiboAI/VibeThinker-1.5B...
[1;36m(EngineCore_DP0 pid=4006626)[0;0m INFO 12-04 14:13:17 [cuda.py:418] Valid backends: ['FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4006626)[0;0m INFO 12-04 14:13:17 [cuda.py:427] Using FLASHINFER backend.
[1;36m(EngineCore_DP0 pid=4006629)[0;0m INFO 12-04 14:13:17 [cuda.py:418] Valid backends: ['FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=4006629)[0;0m INFO 12-04 14:13:17 [cuda.py:427] Using FLASHINFER backend.
[1;36m(EngineCore_DP0 pid=4006626)[0;0m INFO 12-04 14:13:17 [weight_utils.py:481] No model.safetensors.index.json found in remote.
[1;36m(EngineCore_DP0 pid=4006626)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
[1;36m(EngineCore_DP0 pid=4006629)[0;0m INFO 12-04 14:13:17 [weight_utils.py:481] No model.safetensors.index.json found in remote.
[1;36m(EngineCore_DP0 pid=4006629)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
[1;36m(EngineCore_DP0 pid=4006626)[0;0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:04<00:00,  4.25s/it]
[1;36m(EngineCore_DP0 pid=4006626)[0;0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:04<00:00,  4.25s/it]
[1;36m(EngineCore_DP0 pid=4006626)[0;0m 
[1;36m(EngineCore_DP0 pid=4006626)[0;0m INFO 12-04 14:13:22 [default_loader.py:314] Loading weights took 4.38 seconds
[1;36m(EngineCore_DP0 pid=4006629)[0;0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:04<00:00,  4.30s/it]
[1;36m(EngineCore_DP0 pid=4006629)[0;0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:04<00:00,  4.30s/it]
[1;36m(EngineCore_DP0 pid=4006629)[0;0m 
[1;36m(EngineCore_DP0 pid=4006629)[0;0m INFO 12-04 14:13:22 [default_loader.py:314] Loading weights took 4.43 seconds
[1;36m(EngineCore_DP0 pid=4006626)[0;0m INFO 12-04 14:13:22 [gpu_model_runner.py:3338] Model loading took 2.9110 GiB memory and 5.654095 seconds
[1;36m(EngineCore_DP0 pid=4006629)[0;0m INFO 12-04 14:13:22 [gpu_model_runner.py:3338] Model loading took 2.9110 GiB memory and 5.858545 seconds
[1;36m(EngineCore_DP0 pid=4006629)[0;0m INFO 12-04 14:13:36 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/6847b7a26b/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4006626)[0;0m INFO 12-04 14:13:36 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/6847b7a26b/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4006629)[0;0m INFO 12-04 14:13:36 [backends.py:647] Dynamo bytecode transform time: 13.29 s
[1;36m(EngineCore_DP0 pid=4006626)[0;0m INFO 12-04 14:13:36 [backends.py:647] Dynamo bytecode transform time: 13.48 s
[1;36m(EngineCore_DP0 pid=4006629)[0;0m INFO 12-04 14:13:42 [backends.py:210] Directly load the compiled graph(s) for dynamic shape from the cache, took 5.286 s
[1;36m(EngineCore_DP0 pid=4006626)[0;0m INFO 12-04 14:13:42 [backends.py:210] Directly load the compiled graph(s) for dynamic shape from the cache, took 5.294 s
[1;36m(EngineCore_DP0 pid=4006629)[0;0m INFO 12-04 14:13:44 [monitor.py:34] torch.compile takes 18.57 s in total
[1;36m(EngineCore_DP0 pid=4006626)[0;0m INFO 12-04 14:13:44 [monitor.py:34] torch.compile takes 18.77 s in total
[1;36m(EngineCore_DP0 pid=4006629)[0;0m INFO 12-04 14:13:45 [gpu_worker.py:359] Available KV cache memory: 31.10 GiB
[1;36m(EngineCore_DP0 pid=4006626)[0;0m INFO 12-04 14:13:45 [gpu_worker.py:359] Available KV cache memory: 32.40 GiB
[1;36m(EngineCore_DP0 pid=4006629)[0;0m INFO 12-04 14:13:46 [kv_cache_utils.py:1229] GPU KV cache size: 1,164,752 tokens
[1;36m(EngineCore_DP0 pid=4006629)[0;0m INFO 12-04 14:13:46 [kv_cache_utils.py:1234] Maximum concurrency for 131,072 tokens per request: 8.89x
[1;36m(EngineCore_DP0 pid=4006626)[0;0m INFO 12-04 14:13:46 [kv_cache_utils.py:1229] GPU KV cache size: 1,213,184 tokens
[1;36m(EngineCore_DP0 pid=4006626)[0;0m INFO 12-04 14:13:46 [kv_cache_utils.py:1234] Maximum concurrency for 131,072 tokens per request: 9.26x
[1;36m(EngineCore_DP0 pid=4006629)[0;0m INFO 12-04 14:13:46 [kernel_warmup.py:65] Warming up FlashInfer attention.
[1;36m(EngineCore_DP0 pid=4006626)[0;0m ERROR 12-04 14:13:46 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4006626)[0;0m ERROR 12-04 14:13:46 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4006626)[0;0m ERROR 12-04 14:13:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4006626)[0;0m ERROR 12-04 14:13:46 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4006626)[0;0m ERROR 12-04 14:13:46 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4006626)[0;0m ERROR 12-04 14:13:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4006626)[0;0m ERROR 12-04 14:13:46 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4006626)[0;0m ERROR 12-04 14:13:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4006626)[0;0m ERROR 12-04 14:13:46 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4006626)[0;0m ERROR 12-04 14:13:46 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4006626)[0;0m ERROR 12-04 14:13:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 247, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4006626)[0;0m ERROR 12-04 14:13:46 [core.py:842]     self.model_executor.initialize_from_config(kv_cache_configs)
[1;36m(EngineCore_DP0 pid=4006626)[0;0m ERROR 12-04 14:13:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 115, in initialize_from_config
[1;36m(EngineCore_DP0 pid=4006626)[0;0m ERROR 12-04 14:13:46 [core.py:842]     self.collective_rpc("initialize_from_config", args=(kv_cache_configs,))
[1;36m(EngineCore_DP0 pid=4006626)[0;0m ERROR 12-04 14:13:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4006626)[0;0m ERROR 12-04 14:13:46 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4006626)[0;0m ERROR 12-04 14:13:46 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4006626)[0;0m ERROR 12-04 14:13:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4006626)[0;0m ERROR 12-04 14:13:46 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4006626)[0;0m ERROR 12-04 14:13:46 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4006626)[0;0m ERROR 12-04 14:13:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 319, in initialize_from_config
[1;36m(EngineCore_DP0 pid=4006626)[0;0m ERROR 12-04 14:13:46 [core.py:842]     self.worker.initialize_from_config(kv_cache_config)  # type: ignore
[1;36m(EngineCore_DP0 pid=4006626)[0;0m ERROR 12-04 14:13:46 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4006626)[0;0m ERROR 12-04 14:13:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 404, in initialize_from_config
[1;36m(EngineCore_DP0 pid=4006626)[0;0m ERROR 12-04 14:13:46 [core.py:842]     self.model_runner.initialize_kv_cache(kv_cache_config)
[1;36m(EngineCore_DP0 pid=4006626)[0;0m ERROR 12-04 14:13:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 5007, in initialize_kv_cache
[1;36m(EngineCore_DP0 pid=4006626)[0;0m ERROR 12-04 14:13:46 [core.py:842]     kv_caches = self.initialize_kv_cache_tensors(
[1;36m(EngineCore_DP0 pid=4006626)[0;0m ERROR 12-04 14:13:46 [core.py:842]                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4006626)[0;0m ERROR 12-04 14:13:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4933, in initialize_kv_cache_tensors
[1;36m(EngineCore_DP0 pid=4006626)[0;0m ERROR 12-04 14:13:46 [core.py:842]     kv_cache_raw_tensors = self._allocate_kv_cache_tensors(kv_cache_config)
[1;36m(EngineCore_DP0 pid=4006626)[0;0m ERROR 12-04 14:13:46 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4006626)[0;0m ERROR 12-04 14:13:46 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4718, in _allocate_kv_cache_tensors
[1;36m(EngineCore_DP0 pid=4006626)[0;0m ERROR 12-04 14:13:46 [core.py:842]     tensor = torch.zeros(
[1;36m(EngineCore_DP0 pid=4006626)[0;0m ERROR 12-04 14:13:46 [core.py:842]              ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4006626)[0;0m ERROR 12-04 14:13:46 [core.py:842] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.16 GiB. GPU 0 has a total capacity of 44.29 GiB of which 799.69 MiB is free. Process 4006629 has 34.42 GiB memory in use. Including non-PyTorch memory, this process has 9.09 GiB memory in use. Of the allocated memory 8.74 GiB is allocated by PyTorch, and 134.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=4006626)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=4006626)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4006626)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=4006626)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=4006626)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=4006626)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=4006626)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=4006626)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=4006626)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4006626)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4006626)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4006626)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4006626)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=4006626)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=4006626)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=4006626)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4006626)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 247, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=4006626)[0;0m     self.model_executor.initialize_from_config(kv_cache_configs)
[1;36m(EngineCore_DP0 pid=4006626)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 115, in initialize_from_config
[1;36m(EngineCore_DP0 pid=4006626)[0;0m     self.collective_rpc("initialize_from_config", args=(kv_cache_configs,))
[1;36m(EngineCore_DP0 pid=4006626)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=4006626)[0;0m     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=4006626)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4006626)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=4006626)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4006626)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4006626)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 319, in initialize_from_config
[1;36m(EngineCore_DP0 pid=4006626)[0;0m     self.worker.initialize_from_config(kv_cache_config)  # type: ignore
[1;36m(EngineCore_DP0 pid=4006626)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4006626)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 404, in initialize_from_config
[1;36m(EngineCore_DP0 pid=4006626)[0;0m     self.model_runner.initialize_kv_cache(kv_cache_config)
[1;36m(EngineCore_DP0 pid=4006626)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 5007, in initialize_kv_cache
[1;36m(EngineCore_DP0 pid=4006626)[0;0m     kv_caches = self.initialize_kv_cache_tensors(
[1;36m(EngineCore_DP0 pid=4006626)[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4006626)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4933, in initialize_kv_cache_tensors
[1;36m(EngineCore_DP0 pid=4006626)[0;0m     kv_cache_raw_tensors = self._allocate_kv_cache_tensors(kv_cache_config)
[1;36m(EngineCore_DP0 pid=4006626)[0;0m                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4006626)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4718, in _allocate_kv_cache_tensors
[1;36m(EngineCore_DP0 pid=4006626)[0;0m     tensor = torch.zeros(
[1;36m(EngineCore_DP0 pid=4006626)[0;0m              ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4006626)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.16 GiB. GPU 0 has a total capacity of 44.29 GiB of which 799.69 MiB is free. Process 4006629 has 34.42 GiB memory in use. Including non-PyTorch memory, this process has 9.09 GiB memory in use. Of the allocated memory 8.74 GiB is allocated by PyTorch, and 134.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W1204 14:13:46.757996426 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[1;36m(EngineCore_DP0 pid=4006629)[0;0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/51 [00:00<?, ?it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   4%|▍         | 2/51 [00:00<00:02, 19.81it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  10%|▉         | 5/51 [00:00<00:02, 20.43it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|█▌        | 8/51 [00:00<00:02, 21.05it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|██▏       | 11/51 [00:00<00:01, 21.75it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|██▋       | 14/51 [00:00<00:01, 19.93it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 17/51 [00:00<00:01, 21.26it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  39%|███▉      | 20/51 [00:00<00:01, 23.40it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|████▌     | 23/51 [00:01<00:01, 25.09it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|█████     | 26/51 [00:01<00:00, 26.45it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  59%|█████▉    | 30/51 [00:01<00:00, 27.90it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 34/51 [00:01<00:00, 28.94it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▍  | 38/51 [00:01<00:00, 29.46it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|████████▏ | 42/51 [00:01<00:00, 29.87it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  90%|█████████ | 46/51 [00:01<00:00, 30.20it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  98%|█████████▊| 50/51 [00:01<00:00, 30.49it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:02<00:00, 23.78it/s]
[1;36m(EngineCore_DP0 pid=4006629)[0;0m Capturing CUDA graphs (decode, FULL):   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graphs (decode, FULL):   9%|▊         | 3/35 [00:00<00:01, 25.84it/s]Capturing CUDA graphs (decode, FULL):  17%|█▋        | 6/35 [00:00<00:01, 26.33it/s]Capturing CUDA graphs (decode, FULL):  26%|██▌       | 9/35 [00:00<00:00, 26.67it/s]Capturing CUDA graphs (decode, FULL):  34%|███▍      | 12/35 [00:00<00:00, 26.72it/s]Capturing CUDA graphs (decode, FULL):  43%|████▎     | 15/35 [00:00<00:00, 26.91it/s]Capturing CUDA graphs (decode, FULL):  51%|█████▏    | 18/35 [00:00<00:00, 26.98it/s]Capturing CUDA graphs (decode, FULL):  60%|██████    | 21/35 [00:00<00:00, 27.07it/s]Capturing CUDA graphs (decode, FULL):  69%|██████▊   | 24/35 [00:00<00:00, 26.70it/s]Capturing CUDA graphs (decode, FULL):  77%|███████▋  | 27/35 [00:01<00:00, 26.74it/s]Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 30/35 [00:01<00:00, 27.01it/s]Capturing CUDA graphs (decode, FULL):  94%|█████████▍| 33/35 [00:01<00:00, 27.23it/s]Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:01<00:00, 26.94it/s]
[1;36m(EngineCore_DP0 pid=4006629)[0;0m INFO 12-04 14:13:51 [gpu_model_runner.py:4244] Graph capturing finished in 4 secs, took -8.29 GiB
[1;36m(EngineCore_DP0 pid=4006629)[0;0m INFO 12-04 14:13:51 [core.py:250] init engine (profile, create kv cache, warmup model) took 28.25 seconds
INFO 12-04 14:13:52 [llm.py:352] Supported tasks: ['generate']
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 289.62it/s]
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: WeiboAI/VibeThinker-1.5B (backend=vllm)
[ModelCache] Using max_model_len=131072 for WeiboAI/VibeThinker-1.5B
[ModelCache] Detected 1 GPU(s), 44.3GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 14:14:08 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'WeiboAI/VibeThinker-1.5B'}
INFO 12-04 14:14:08 [model.py:631] Resolved architecture: Qwen2ForCausalLM
WARNING 12-04 14:14:08 [model.py:1921] Your device 'Quadro RTX 8000' (with compute capability 7.5) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.
WARNING 12-04 14:14:08 [model.py:1971] Casting torch.bfloat16 to torch.float16.
INFO 12-04 14:14:08 [model.py:1745] Using max model len 131072
INFO 12-04 14:14:08 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:17<00:00, 17.34s/it, est. speed input: 8.42 toks/s, output: 114.84 toks/s]Processed prompts: 100%|██████████| 1/1 [00:17<00:00, 17.34s/it, est. speed input: 8.42 toks/s, output: 114.84 toks/s]Processed prompts: 100%|██████████| 1/1 [00:17<00:00, 17.34s/it, est. speed input: 8.42 toks/s, output: 114.84 toks/s]
Agent 1 response: The three measurements of Jared's typing speed are 47 WPM, 52 WPM, and 57 WPM (since the final incre...

--- Problem 1/20, Round 1, Agent 2/3 ---
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 723.03it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:16<00:00, 16.14s/it, est. speed input: 8.86 toks/s, output: 113.75 toks/s]Processed prompts: 100%|██████████| 1/1 [00:16<00:00, 16.14s/it, est. speed input: 8.86 toks/s, output: 113.75 toks/s]Processed prompts: 100%|██████████| 1/1 [00:16<00:00, 16.14s/it, est. speed input: 8.86 toks/s, output: 113.75 toks/s]
Agent 2 response: Jared starts with an initial measurement of 47 WPM. After some lessons, his typing speed increases t...

--- Problem 1/20, Round 1, Agent 3/3 ---
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 975.42it/s]
[1;36m(EngineCore_DP0 pid=4006976)[0;0m INFO 12-04 14:14:45 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='WeiboAI/VibeThinker-1.5B', speculative_config=None, tokenizer='WeiboAI/VibeThinker-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=WeiboAI/VibeThinker-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4006976)[0;0m ERROR 12-04 14:14:47 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8
[1;36m(EngineCore_DP0 pid=4006976)[0;0m INFO 12-04 14:14:48 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.8:37307 backend=nccl
[W1204 14:14:48.320904656 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-23-0.rc.tch.harvard.edu]:37307 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4006976)[0;0m INFO 12-04 14:14:48 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4006976)[0;0m ERROR 12-04 14:14:48 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4006976)[0;0m ERROR 12-04 14:14:48 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4006976)[0;0m ERROR 12-04 14:14:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4006976)[0;0m ERROR 12-04 14:14:48 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4006976)[0;0m ERROR 12-04 14:14:48 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4006976)[0;0m ERROR 12-04 14:14:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4006976)[0;0m ERROR 12-04 14:14:48 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4006976)[0;0m ERROR 12-04 14:14:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=4006976)[0;0m ERROR 12-04 14:14:48 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=4006976)[0;0m ERROR 12-04 14:14:48 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4006976)[0;0m ERROR 12-04 14:14:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=4006976)[0;0m ERROR 12-04 14:14:48 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=4006976)[0;0m ERROR 12-04 14:14:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=4006976)[0;0m ERROR 12-04 14:14:48 [core.py:842]     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=4006976)[0;0m ERROR 12-04 14:14:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=4006976)[0;0m ERROR 12-04 14:14:48 [core.py:842]     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=4006976)[0;0m ERROR 12-04 14:14:48 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4006976)[0;0m ERROR 12-04 14:14:48 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=4006976)[0;0m ERROR 12-04 14:14:48 [core.py:842]     raise ValueError(
[1;36m(EngineCore_DP0 pid=4006976)[0;0m ERROR 12-04 14:14:48 [core.py:842] ValueError: Free memory on device (7.03/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[1;36m(EngineCore_DP0 pid=4006976)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=4006976)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4006976)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=4006976)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=4006976)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=4006976)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=4006976)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=4006976)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=4006976)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4006976)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4006976)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4006976)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4006976)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=4006976)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=4006976)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=4006976)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4006976)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=4006976)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=4006976)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=4006976)[0;0m     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=4006976)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=4006976)[0;0m     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=4006976)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4006976)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=4006976)[0;0m     raise ValueError(
[1;36m(EngineCore_DP0 pid=4006976)[0;0m ValueError: Free memory on device (7.03/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:22<00:00, 22.65s/it, est. speed input: 6.58 toks/s, output: 113.90 toks/s]Processed prompts: 100%|██████████| 1/1 [00:22<00:00, 22.65s/it, est. speed input: 6.58 toks/s, output: 113.90 toks/s]Processed prompts: 100%|██████████| 1/1 [00:22<00:00, 22.65s/it, est. speed input: 6.58 toks/s, output: 113.90 toks/s]
Agent 3 response: Jared starts at 47 WPM, increases to 52 WPM after some lessons, and then increases again by 5 WPM. T...

--- Problem 1/20, Round 2, Agent 1/3 ---
Agent 1 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 330.26it/s]
[rank0]:[W1204 14:14:49.400824476 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:10<00:00, 10.05s/it, est. speed input: 65.57 toks/s, output: 113.43 toks/s]Processed prompts: 100%|██████████| 1/1 [00:10<00:00, 10.05s/it, est. speed input: 65.57 toks/s, output: 113.43 toks/s]Processed prompts: 100%|██████████| 1/1 [00:10<00:00, 10.05s/it, est. speed input: 65.57 toks/s, output: 113.43 toks/s]
Agent 1 response: Jared's initial typing speed is 47 WPM. After some lessons, his typing speed increases to 52 WPM. He...

--- Problem 1/20, Round 2, Agent 2/3 ---
Agent 2 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 496.54it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.41s/it, est. speed input: 121.22 toks/s, output: 112.90 toks/s]Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.41s/it, est. speed input: 121.22 toks/s, output: 112.90 toks/s]Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.41s/it, est. speed input: 121.22 toks/s, output: 112.90 toks/s]
Agent 2 response: Jared's initial typing speed is 47 WPM. After some lessons, it increases to 52 WPM. If he increases ...

--- Problem 1/20, Round 2, Agent 3/3 ---
Agent 3 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 491.71it/s]
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: WeiboAI/VibeThinker-1.5B (backend=vllm)
[ModelCache] Using max_model_len=131072 for WeiboAI/VibeThinker-1.5B
[ModelCache] Detected 1 GPU(s), 44.3GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 14:15:10 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'WeiboAI/VibeThinker-1.5B'}
INFO 12-04 14:15:10 [model.py:631] Resolved architecture: Qwen2ForCausalLM
WARNING 12-04 14:15:10 [model.py:1921] Your device 'Quadro RTX 8000' (with compute capability 7.5) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.
WARNING 12-04 14:15:10 [model.py:1971] Casting torch.bfloat16 to torch.float16.
INFO 12-04 14:15:10 [model.py:1745] Using max model len 131072
INFO 12-04 14:15:10 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:10<00:00, 10.95s/it, est. speed input: 60.44 toks/s, output: 113.86 toks/s]Processed prompts: 100%|██████████| 1/1 [00:10<00:00, 10.95s/it, est. speed input: 60.44 toks/s, output: 113.86 toks/s]Processed prompts: 100%|██████████| 1/1 [00:10<00:00, 10.95s/it, est. speed input: 60.44 toks/s, output: 113.86 toks/s]
Agent 3 response: Jared's three measurements of typing speed are as follows: the initial speed of 47 WPM, an increase ...

--- Problem 1/20, Round 3, Agent 1/3 ---
Agent 1 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 192.19it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:09<00:00,  9.42s/it, est. speed input: 129.45 toks/s, output: 112.26 toks/s]Processed prompts: 100%|██████████| 1/1 [00:09<00:00,  9.42s/it, est. speed input: 129.45 toks/s, output: 112.26 toks/s]Processed prompts: 100%|██████████| 1/1 [00:09<00:00,  9.42s/it, est. speed input: 129.45 toks/s, output: 112.26 toks/s]
Agent 1 response: Jared's initial typing speed is 47 WPM. After some lessons, his typing speed increases to 52 WPM. He...

--- Problem 1/20, Round 3, Agent 2/3 ---
Agent 2 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 243.01it/s]
[1;36m(EngineCore_DP0 pid=4007074)[0;0m INFO 12-04 14:15:30 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='WeiboAI/VibeThinker-1.5B', speculative_config=None, tokenizer='WeiboAI/VibeThinker-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=WeiboAI/VibeThinker-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4007074)[0;0m ERROR 12-04 14:15:31 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:06<00:00,  6.69s/it, est. speed input: 181.79 toks/s, output: 112.03 toks/s]Processed prompts: 100%|██████████| 1/1 [00:06<00:00,  6.69s/it, est. speed input: 181.79 toks/s, output: 112.03 toks/s]Processed prompts: 100%|██████████| 1/1 [00:06<00:00,  6.70s/it, est. speed input: 181.79 toks/s, output: 112.03 toks/s]
Agent 2 response: Jared's initial typing speed is 47 WPM. After some lessons, it increases to 52 WPM. Increasing his t...

--- Problem 1/20, Round 3, Agent 3/3 ---
Agent 3 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 195.69it/s]
[1;36m(EngineCore_DP0 pid=4007074)[0;0m INFO 12-04 14:15:31 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.8:57245 backend=nccl
[W1204 14:15:31.716915079 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-23-0.rc.tch.harvard.edu]:57245 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4007074)[0;0m INFO 12-04 14:15:31 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4007074)[0;0m ERROR 12-04 14:15:32 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4007074)[0;0m ERROR 12-04 14:15:32 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4007074)[0;0m ERROR 12-04 14:15:32 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4007074)[0;0m ERROR 12-04 14:15:32 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4007074)[0;0m ERROR 12-04 14:15:32 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4007074)[0;0m ERROR 12-04 14:15:32 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4007074)[0;0m ERROR 12-04 14:15:32 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4007074)[0;0m ERROR 12-04 14:15:32 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=4007074)[0;0m ERROR 12-04 14:15:32 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=4007074)[0;0m ERROR 12-04 14:15:32 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4007074)[0;0m ERROR 12-04 14:15:32 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=4007074)[0;0m ERROR 12-04 14:15:32 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=4007074)[0;0m ERROR 12-04 14:15:32 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=4007074)[0;0m ERROR 12-04 14:15:32 [core.py:842]     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=4007074)[0;0m ERROR 12-04 14:15:32 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=4007074)[0;0m ERROR 12-04 14:15:32 [core.py:842]     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=4007074)[0;0m ERROR 12-04 14:15:32 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4007074)[0;0m ERROR 12-04 14:15:32 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=4007074)[0;0m ERROR 12-04 14:15:32 [core.py:842]     raise ValueError(
[1;36m(EngineCore_DP0 pid=4007074)[0;0m ERROR 12-04 14:15:32 [core.py:842] ValueError: Free memory on device (7.03/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[1;36m(EngineCore_DP0 pid=4007074)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=4007074)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4007074)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=4007074)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=4007074)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=4007074)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=4007074)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=4007074)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=4007074)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4007074)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4007074)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4007074)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4007074)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=4007074)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=4007074)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=4007074)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4007074)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=4007074)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=4007074)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=4007074)[0;0m     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=4007074)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=4007074)[0;0m     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=4007074)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4007074)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=4007074)[0;0m     raise ValueError(
[1;36m(EngineCore_DP0 pid=4007074)[0;0m ValueError: Free memory on device (7.03/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W1204 14:15:32.660084432 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.56s/it, est. speed input: 161.85 toks/s, output: 113.81 toks/s]Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.56s/it, est. speed input: 161.85 toks/s, output: 113.81 toks/s]Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.56s/it, est. speed input: 161.85 toks/s, output: 113.81 toks/s]
Agent 3 response: Jared's initial typing speed is 47 WPM. After some lessons, his typing speed increases to 52 WPM. He...

Running accuracy: 1.000 ± 0.000

--- Problem 2/20, Round 1, Agent 1/3 ---
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1160.89it/s]
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: WeiboAI/VibeThinker-1.5B (backend=vllm)
[ModelCache] Using max_model_len=131072 for WeiboAI/VibeThinker-1.5B
[ModelCache] Detected 1 GPU(s), 44.3GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 14:15:53 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'WeiboAI/VibeThinker-1.5B'}
INFO 12-04 14:15:53 [model.py:631] Resolved architecture: Qwen2ForCausalLM
WARNING 12-04 14:15:53 [model.py:1921] Your device 'Quadro RTX 8000' (with compute capability 7.5) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.
WARNING 12-04 14:15:53 [model.py:1971] Casting torch.bfloat16 to torch.float16.
INFO 12-04 14:15:53 [model.py:1745] Using max model len 131072
INFO 12-04 14:15:53 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4007139)[0;0m INFO 12-04 14:16:10 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='WeiboAI/VibeThinker-1.5B', speculative_config=None, tokenizer='WeiboAI/VibeThinker-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=WeiboAI/VibeThinker-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4007139)[0;0m ERROR 12-04 14:16:11 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8
[1;36m(EngineCore_DP0 pid=4007139)[0;0m INFO 12-04 14:16:12 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.8:39507 backend=nccl
[W1204 14:16:12.549038174 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-23-0.rc.tch.harvard.edu]:39507 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4007139)[0;0m INFO 12-04 14:16:12 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4007139)[0;0m ERROR 12-04 14:16:12 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4007139)[0;0m ERROR 12-04 14:16:12 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4007139)[0;0m ERROR 12-04 14:16:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4007139)[0;0m ERROR 12-04 14:16:12 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4007139)[0;0m ERROR 12-04 14:16:12 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4007139)[0;0m ERROR 12-04 14:16:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4007139)[0;0m ERROR 12-04 14:16:12 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4007139)[0;0m ERROR 12-04 14:16:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=4007139)[0;0m ERROR 12-04 14:16:12 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=4007139)[0;0m ERROR 12-04 14:16:12 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4007139)[0;0m ERROR 12-04 14:16:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=4007139)[0;0m ERROR 12-04 14:16:12 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=4007139)[0;0m ERROR 12-04 14:16:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=4007139)[0;0m ERROR 12-04 14:16:12 [core.py:842]     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=4007139)[0;0m ERROR 12-04 14:16:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=4007139)[0;0m ERROR 12-04 14:16:12 [core.py:842]     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=4007139)[0;0m ERROR 12-04 14:16:12 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4007139)[0;0m ERROR 12-04 14:16:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=4007139)[0;0m ERROR 12-04 14:16:12 [core.py:842]     raise ValueError(
[1;36m(EngineCore_DP0 pid=4007139)[0;0m ERROR 12-04 14:16:12 [core.py:842] ValueError: Free memory on device (7.03/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[1;36m(EngineCore_DP0 pid=4007139)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=4007139)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4007139)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=4007139)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=4007139)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=4007139)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=4007139)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=4007139)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=4007139)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4007139)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4007139)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4007139)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4007139)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=4007139)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=4007139)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=4007139)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4007139)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=4007139)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=4007139)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=4007139)[0;0m     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=4007139)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=4007139)[0;0m     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=4007139)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4007139)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=4007139)[0;0m     raise ValueError(
[1;36m(EngineCore_DP0 pid=4007139)[0;0m ValueError: Free memory on device (7.03/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W1204 14:16:13.493839591 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:39<00:00, 39.54s/it, est. speed input: 2.98 toks/s, output: 113.36 toks/s]Processed prompts: 100%|██████████| 1/1 [00:39<00:00, 39.54s/it, est. speed input: 2.98 toks/s, output: 113.36 toks/s]Processed prompts: 100%|██████████| 1/1 [00:39<00:00, 39.54s/it, est. speed input: 2.98 toks/s, output: 113.36 toks/s]
Agent 1 response: Jordan's two children each require 5 diaper changes per day, resulting in a total of \(2 \times 5 = ...

--- Problem 2/20, Round 1, Agent 2/3 ---
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1228.92it/s]
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: WeiboAI/VibeThinker-1.5B (backend=vllm)
[ModelCache] Using max_model_len=131072 for WeiboAI/VibeThinker-1.5B
[ModelCache] Detected 1 GPU(s), 44.3GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 14:16:34 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'WeiboAI/VibeThinker-1.5B'}
INFO 12-04 14:16:34 [model.py:631] Resolved architecture: Qwen2ForCausalLM
WARNING 12-04 14:16:34 [model.py:1921] Your device 'Quadro RTX 8000' (with compute capability 7.5) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.
WARNING 12-04 14:16:34 [model.py:1971] Casting torch.bfloat16 to torch.float16.
INFO 12-04 14:16:34 [model.py:1745] Using max model len 131072
INFO 12-04 14:16:34 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:32<00:00, 32.36s/it, est. speed input: 3.55 toks/s, output: 113.39 toks/s]Processed prompts: 100%|██████████| 1/1 [00:32<00:00, 32.36s/it, est. speed input: 3.55 toks/s, output: 113.39 toks/s]Processed prompts: 100%|██████████| 1/1 [00:32<00:00, 32.36s/it, est. speed input: 3.55 toks/s, output: 113.39 toks/s]
Agent 2 response: Jordan's two children each require 5 diaper changes per day, leading to a total of \(2 \times 5 = 10...

--- Problem 2/20, Round 1, Agent 3/3 ---
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1186.17it/s]
[1;36m(EngineCore_DP0 pid=4007216)[0;0m INFO 12-04 14:17:04 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='WeiboAI/VibeThinker-1.5B', speculative_config=None, tokenizer='WeiboAI/VibeThinker-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=WeiboAI/VibeThinker-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4007216)[0;0m ERROR 12-04 14:17:06 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8
[1;36m(EngineCore_DP0 pid=4007216)[0;0m INFO 12-04 14:17:07 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.8:44555 backend=nccl
[W1204 14:17:07.169134964 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-23-0.rc.tch.harvard.edu]:44555 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4007216)[0;0m INFO 12-04 14:17:07 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4007216)[0;0m ERROR 12-04 14:17:07 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4007216)[0;0m ERROR 12-04 14:17:07 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4007216)[0;0m ERROR 12-04 14:17:07 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4007216)[0;0m ERROR 12-04 14:17:07 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4007216)[0;0m ERROR 12-04 14:17:07 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4007216)[0;0m ERROR 12-04 14:17:07 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4007216)[0;0m ERROR 12-04 14:17:07 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4007216)[0;0m ERROR 12-04 14:17:07 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=4007216)[0;0m ERROR 12-04 14:17:07 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=4007216)[0;0m ERROR 12-04 14:17:07 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4007216)[0;0m ERROR 12-04 14:17:07 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=4007216)[0;0m ERROR 12-04 14:17:07 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=4007216)[0;0m ERROR 12-04 14:17:07 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=4007216)[0;0m ERROR 12-04 14:17:07 [core.py:842]     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=4007216)[0;0m ERROR 12-04 14:17:07 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=4007216)[0;0m ERROR 12-04 14:17:07 [core.py:842]     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=4007216)[0;0m ERROR 12-04 14:17:07 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4007216)[0;0m ERROR 12-04 14:17:07 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=4007216)[0;0m ERROR 12-04 14:17:07 [core.py:842]     raise ValueError(
[1;36m(EngineCore_DP0 pid=4007216)[0;0m ERROR 12-04 14:17:07 [core.py:842] ValueError: Free memory on device (7.03/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[1;36m(EngineCore_DP0 pid=4007216)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=4007216)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4007216)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=4007216)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=4007216)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=4007216)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=4007216)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=4007216)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=4007216)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4007216)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4007216)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4007216)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4007216)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=4007216)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=4007216)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=4007216)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4007216)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=4007216)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=4007216)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=4007216)[0;0m     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=4007216)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=4007216)[0;0m     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=4007216)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4007216)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=4007216)[0;0m     raise ValueError(
[1;36m(EngineCore_DP0 pid=4007216)[0;0m ValueError: Free memory on device (7.03/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W1204 14:17:08.169095696 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:21<00:00, 21.64s/it, est. speed input: 5.59 toks/s, output: 113.90 toks/s]Processed prompts: 100%|██████████| 1/1 [00:21<00:00, 21.64s/it, est. speed input: 5.59 toks/s, output: 113.90 toks/s]Processed prompts: 100%|██████████| 1/1 [00:21<00:00, 21.64s/it, est. speed input: 5.59 toks/s, output: 113.90 toks/s]
Agent 3 response: Jordan has 2 children, each requiring 5 diaper changes per day. This results in a total of \(2 \time...

--- Problem 2/20, Round 2, Agent 1/3 ---
Agent 1 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 452.07it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.98s/it, est. speed input: 295.76 toks/s, output: 114.37 toks/s]Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.98s/it, est. speed input: 295.76 toks/s, output: 114.37 toks/s]Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.98s/it, est. speed input: 295.76 toks/s, output: 114.37 toks/s]
Agent 1 response: Jordan's two children each require 5 diaper changes per day, so the total diaper changes needed per ...

--- Problem 2/20, Round 2, Agent 2/3 ---
Agent 2 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 528.12it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.26s/it, est. speed input: 258.22 toks/s, output: 114.52 toks/s]Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.26s/it, est. speed input: 258.22 toks/s, output: 114.52 toks/s]Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.26s/it, est. speed input: 258.22 toks/s, output: 114.52 toks/s]
Agent 2 response: Jordan's two children each require 5 diaper changes per day, resulting in a total of \(2 \times 5 = ...

--- Problem 2/20, Round 2, Agent 3/3 ---
Agent 3 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 527.06it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.23s/it, est. speed input: 265.13 toks/s, output: 114.59 toks/s]Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.23s/it, est. speed input: 265.13 toks/s, output: 114.59 toks/s]Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.23s/it, est. speed input: 265.13 toks/s, output: 114.59 toks/s]
Agent 3 response: Jordan's two children require a total of \(2 \times 5 = 10\) diaper changes per day. Jordan's wife c...

--- Problem 2/20, Round 3, Agent 1/3 ---
Agent 1 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 350.55it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.91s/it, est. speed input: 512.42 toks/s, output: 114.22 toks/s]Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.91s/it, est. speed input: 512.42 toks/s, output: 114.22 toks/s]Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.91s/it, est. speed input: 512.42 toks/s, output: 114.22 toks/s]
Agent 1 response: Jordan's two children each require 5 diaper changes per day, resulting in a total of \(2 \times 5 = ...

--- Problem 2/20, Round 3, Agent 2/3 ---
Agent 2 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 350.08it/s]
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: WeiboAI/VibeThinker-1.5B (backend=vllm)
[ModelCache] Using max_model_len=131072 for WeiboAI/VibeThinker-1.5B
[ModelCache] Detected 1 GPU(s), 44.3GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 14:17:29 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'WeiboAI/VibeThinker-1.5B'}
INFO 12-04 14:17:29 [model.py:631] Resolved architecture: Qwen2ForCausalLM
WARNING 12-04 14:17:29 [model.py:1921] Your device 'Quadro RTX 8000' (with compute capability 7.5) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.
WARNING 12-04 14:17:29 [model.py:1971] Casting torch.bfloat16 to torch.float16.
INFO 12-04 14:17:29 [model.py:1745] Using max model len 131072
INFO 12-04 14:17:29 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:12<00:00, 12.25s/it, est. speed input: 79.61 toks/s, output: 113.98 toks/s]Processed prompts: 100%|██████████| 1/1 [00:12<00:00, 12.25s/it, est. speed input: 79.61 toks/s, output: 113.98 toks/s]Processed prompts: 100%|██████████| 1/1 [00:12<00:00, 12.25s/it, est. speed input: 79.61 toks/s, output: 113.98 toks/s]
Agent 2 response: Jordan's two children each require 5 diaper changes per day, leading to a total of \(2 \times 5 = 10...

--- Problem 2/20, Round 3, Agent 3/3 ---
Agent 3 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 223.83it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.83s/it, est. speed input: 168.24 toks/s, output: 112.16 toks/s]Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.83s/it, est. speed input: 168.24 toks/s, output: 112.16 toks/s]Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.83s/it, est. speed input: 168.24 toks/s, output: 112.16 toks/s]
Agent 3 response: Jordan's two children require a total of \(2 \times 5 = 10\) diaper changes per day. Jordan's wife c...

Running accuracy: 1.000 ± 0.000

--- Problem 3/20, Round 1, Agent 1/3 ---
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 981.12it/s]
[1;36m(EngineCore_DP0 pid=4007276)[0;0m INFO 12-04 14:17:49 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='WeiboAI/VibeThinker-1.5B', speculative_config=None, tokenizer='WeiboAI/VibeThinker-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=WeiboAI/VibeThinker-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4007276)[0;0m ERROR 12-04 14:17:50 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8
[1;36m(EngineCore_DP0 pid=4007276)[0;0m INFO 12-04 14:17:51 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.8:48159 backend=nccl
[W1204 14:17:51.052380634 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-23-0.rc.tch.harvard.edu]:48159 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4007276)[0;0m INFO 12-04 14:17:51 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4007276)[0;0m ERROR 12-04 14:17:51 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4007276)[0;0m ERROR 12-04 14:17:51 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4007276)[0;0m ERROR 12-04 14:17:51 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4007276)[0;0m ERROR 12-04 14:17:51 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4007276)[0;0m ERROR 12-04 14:17:51 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4007276)[0;0m ERROR 12-04 14:17:51 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4007276)[0;0m ERROR 12-04 14:17:51 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4007276)[0;0m ERROR 12-04 14:17:51 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=4007276)[0;0m ERROR 12-04 14:17:51 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=4007276)[0;0m ERROR 12-04 14:17:51 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4007276)[0;0m ERROR 12-04 14:17:51 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=4007276)[0;0m ERROR 12-04 14:17:51 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=4007276)[0;0m ERROR 12-04 14:17:51 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=4007276)[0;0m ERROR 12-04 14:17:51 [core.py:842]     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=4007276)[0;0m ERROR 12-04 14:17:51 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=4007276)[0;0m ERROR 12-04 14:17:51 [core.py:842]     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=4007276)[0;0m ERROR 12-04 14:17:51 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4007276)[0;0m ERROR 12-04 14:17:51 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=4007276)[0;0m ERROR 12-04 14:17:51 [core.py:842]     raise ValueError(
[1;36m(EngineCore_DP0 pid=4007276)[0;0m ERROR 12-04 14:17:51 [core.py:842] ValueError: Free memory on device (7.03/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[1;36m(EngineCore_DP0 pid=4007276)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=4007276)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4007276)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=4007276)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=4007276)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=4007276)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=4007276)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=4007276)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=4007276)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4007276)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4007276)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4007276)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4007276)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=4007276)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=4007276)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=4007276)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4007276)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=4007276)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=4007276)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=4007276)[0;0m     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=4007276)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=4007276)[0;0m     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=4007276)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4007276)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=4007276)[0;0m     raise ValueError(
[1;36m(EngineCore_DP0 pid=4007276)[0;0m ValueError: Free memory on device (7.03/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W1204 14:17:52.021067767 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:17<00:00, 17.78s/it, est. speed input: 8.72 toks/s, output: 113.55 toks/s]Processed prompts: 100%|██████████| 1/1 [00:17<00:00, 17.78s/it, est. speed input: 8.72 toks/s, output: 113.55 toks/s]Processed prompts: 100%|██████████| 1/1 [00:17<00:00, 17.78s/it, est. speed input: 8.72 toks/s, output: 113.55 toks/s]
Agent 1 response: The maximum number of boxes that can be loaded onto the truck without exceeding the bridge's weight ...

--- Problem 3/20, Round 1, Agent 2/3 ---
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1108.14it/s]
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: WeiboAI/VibeThinker-1.5B (backend=vllm)
[ModelCache] Using max_model_len=131072 for WeiboAI/VibeThinker-1.5B
[ModelCache] Detected 1 GPU(s), 44.3GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 14:18:13 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'WeiboAI/VibeThinker-1.5B'}
INFO 12-04 14:18:13 [model.py:631] Resolved architecture: Qwen2ForCausalLM
WARNING 12-04 14:18:13 [model.py:1921] Your device 'Quadro RTX 8000' (with compute capability 7.5) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.
WARNING 12-04 14:18:13 [model.py:1971] Casting torch.bfloat16 to torch.float16.
INFO 12-04 14:18:13 [model.py:1745] Using max model len 131072
INFO 12-04 14:18:13 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:19<00:00, 19.28s/it, est. speed input: 7.88 toks/s, output: 114.68 toks/s]Processed prompts: 100%|██████████| 1/1 [00:19<00:00, 19.28s/it, est. speed input: 7.88 toks/s, output: 114.68 toks/s]Processed prompts: 100%|██████████| 1/1 [00:19<00:00, 19.28s/it, est. speed input: 7.88 toks/s, output: 114.68 toks/s]
Agent 2 response: The bridge's weight limit is 5000 pounds. The combined weight of the driver and the empty truck is 3...

--- Problem 3/20, Round 1, Agent 3/3 ---
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1037.17it/s]
[1;36m(EngineCore_DP0 pid=4007423)[0;0m INFO 12-04 14:18:28 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='WeiboAI/VibeThinker-1.5B', speculative_config=None, tokenizer='WeiboAI/VibeThinker-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=WeiboAI/VibeThinker-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4007423)[0;0m ERROR 12-04 14:18:30 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8
[1;36m(EngineCore_DP0 pid=4007423)[0;0m INFO 12-04 14:18:31 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.8:37597 backend=nccl
[W1204 14:18:31.127267633 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-23-0.rc.tch.harvard.edu]:37597 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4007423)[0;0m INFO 12-04 14:18:31 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4007423)[0;0m ERROR 12-04 14:18:31 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4007423)[0;0m ERROR 12-04 14:18:31 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4007423)[0;0m ERROR 12-04 14:18:31 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4007423)[0;0m ERROR 12-04 14:18:31 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4007423)[0;0m ERROR 12-04 14:18:31 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4007423)[0;0m ERROR 12-04 14:18:31 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4007423)[0;0m ERROR 12-04 14:18:31 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4007423)[0;0m ERROR 12-04 14:18:31 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=4007423)[0;0m ERROR 12-04 14:18:31 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=4007423)[0;0m ERROR 12-04 14:18:31 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4007423)[0;0m ERROR 12-04 14:18:31 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=4007423)[0;0m ERROR 12-04 14:18:31 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=4007423)[0;0m ERROR 12-04 14:18:31 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=4007423)[0;0m ERROR 12-04 14:18:31 [core.py:842]     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=4007423)[0;0m ERROR 12-04 14:18:31 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=4007423)[0;0m ERROR 12-04 14:18:31 [core.py:842]     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=4007423)[0;0m ERROR 12-04 14:18:31 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4007423)[0;0m ERROR 12-04 14:18:31 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=4007423)[0;0m ERROR 12-04 14:18:31 [core.py:842]     raise ValueError(
[1;36m(EngineCore_DP0 pid=4007423)[0;0m ERROR 12-04 14:18:31 [core.py:842] ValueError: Free memory on device (7.03/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[1;36m(EngineCore_DP0 pid=4007423)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=4007423)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4007423)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=4007423)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=4007423)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=4007423)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=4007423)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=4007423)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=4007423)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4007423)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4007423)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4007423)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4007423)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=4007423)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=4007423)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=4007423)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4007423)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=4007423)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=4007423)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=4007423)[0;0m     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=4007423)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=4007423)[0;0m     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=4007423)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4007423)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=4007423)[0;0m     raise ValueError(
[1;36m(EngineCore_DP0 pid=4007423)[0;0m ValueError: Free memory on device (7.03/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W1204 14:18:32.096021666 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:26<00:00, 26.17s/it, est. speed input: 6.04 toks/s, output: 113.24 toks/s]Processed prompts: 100%|██████████| 1/1 [00:26<00:00, 26.17s/it, est. speed input: 6.04 toks/s, output: 113.24 toks/s]Processed prompts: 100%|██████████| 1/1 [00:26<00:00, 26.17s/it, est. speed input: 6.04 toks/s, output: 113.24 toks/s]
Agent 3 response: The combined weight of the driver and the empty truck is 3755 pounds. The bridge's weight limit is 5...

--- Problem 3/20, Round 2, Agent 1/3 ---
Agent 1 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 376.34it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:10<00:00, 10.12s/it, est. speed input: 89.33 toks/s, output: 114.73 toks/s]Processed prompts: 100%|██████████| 1/1 [00:10<00:00, 10.12s/it, est. speed input: 89.33 toks/s, output: 114.73 toks/s]Processed prompts: 100%|██████████| 1/1 [00:10<00:00, 10.12s/it, est. speed input: 89.33 toks/s, output: 114.73 toks/s]
Agent 1 response: The bridge's weight limit is 5000 pounds, and the combined weight of the driver and the empty truck ...

--- Problem 3/20, Round 2, Agent 2/3 ---
Agent 2 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 398.66it/s]
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: WeiboAI/VibeThinker-1.5B (backend=vllm)
[ModelCache] Using max_model_len=131072 for WeiboAI/VibeThinker-1.5B
[ModelCache] Detected 1 GPU(s), 44.3GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 14:18:53 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'WeiboAI/VibeThinker-1.5B'}
INFO 12-04 14:18:53 [model.py:631] Resolved architecture: Qwen2ForCausalLM
WARNING 12-04 14:18:53 [model.py:1921] Your device 'Quadro RTX 8000' (with compute capability 7.5) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.
WARNING 12-04 14:18:53 [model.py:1971] Casting torch.bfloat16 to torch.float16.
INFO 12-04 14:18:53 [model.py:1745] Using max model len 131072
INFO 12-04 14:18:53 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4007583)[0;0m INFO 12-04 14:19:17 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='WeiboAI/VibeThinker-1.5B', speculative_config=None, tokenizer='WeiboAI/VibeThinker-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=WeiboAI/VibeThinker-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4007583)[0;0m ERROR 12-04 14:19:18 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8
[1;36m(EngineCore_DP0 pid=4007583)[0;0m INFO 12-04 14:19:19 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.8:37467 backend=nccl
[W1204 14:19:19.747516692 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-23-0.rc.tch.harvard.edu]:37467 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4007583)[0;0m INFO 12-04 14:19:19 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4007583)[0;0m ERROR 12-04 14:19:20 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4007583)[0;0m ERROR 12-04 14:19:20 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4007583)[0;0m ERROR 12-04 14:19:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4007583)[0;0m ERROR 12-04 14:19:20 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4007583)[0;0m ERROR 12-04 14:19:20 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4007583)[0;0m ERROR 12-04 14:19:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4007583)[0;0m ERROR 12-04 14:19:20 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4007583)[0;0m ERROR 12-04 14:19:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=4007583)[0;0m ERROR 12-04 14:19:20 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=4007583)[0;0m ERROR 12-04 14:19:20 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4007583)[0;0m ERROR 12-04 14:19:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=4007583)[0;0m ERROR 12-04 14:19:20 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=4007583)[0;0m ERROR 12-04 14:19:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=4007583)[0;0m ERROR 12-04 14:19:20 [core.py:842]     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=4007583)[0;0m ERROR 12-04 14:19:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=4007583)[0;0m ERROR 12-04 14:19:20 [core.py:842]     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=4007583)[0;0m ERROR 12-04 14:19:20 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4007583)[0;0m ERROR 12-04 14:19:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=4007583)[0;0m ERROR 12-04 14:19:20 [core.py:842]     raise ValueError(
[1;36m(EngineCore_DP0 pid=4007583)[0;0m ERROR 12-04 14:19:20 [core.py:842] ValueError: Free memory on device (7.03/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[1;36m(EngineCore_DP0 pid=4007583)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=4007583)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4007583)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=4007583)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=4007583)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=4007583)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=4007583)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=4007583)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=4007583)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4007583)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4007583)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4007583)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4007583)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=4007583)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=4007583)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=4007583)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4007583)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=4007583)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=4007583)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=4007583)[0;0m     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=4007583)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=4007583)[0;0m     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=4007583)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4007583)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=4007583)[0;0m     raise ValueError(
[1;36m(EngineCore_DP0 pid=4007583)[0;0m ValueError: Free memory on device (7.03/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W1204 14:19:20.703799652 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:35<00:00, 35.54s/it, est. speed input: 25.36 toks/s, output: 112.65 toks/s]Processed prompts: 100%|██████████| 1/1 [00:35<00:00, 35.54s/it, est. speed input: 25.36 toks/s, output: 112.65 toks/s]Processed prompts: 100%|██████████| 1/1 [00:35<00:00, 35.54s/it, est. speed input: 25.36 toks/s, output: 112.65 toks/s]
Agent 2 response: The bridge has a weight limit of 5000 pounds. The combined weight of the driver and the empty truck ...

--- Problem 3/20, Round 2, Agent 3/3 ---
Agent 3 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 394.20it/s]
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: WeiboAI/VibeThinker-1.5B (backend=vllm)
[ModelCache] Using max_model_len=131072 for WeiboAI/VibeThinker-1.5B
[ModelCache] Detected 1 GPU(s), 44.3GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 14:19:41 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'WeiboAI/VibeThinker-1.5B'}
INFO 12-04 14:19:42 [model.py:631] Resolved architecture: Qwen2ForCausalLM
WARNING 12-04 14:19:42 [model.py:1921] Your device 'Quadro RTX 8000' (with compute capability 7.5) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.
WARNING 12-04 14:19:42 [model.py:1971] Casting torch.bfloat16 to torch.float16.
INFO 12-04 14:19:42 [model.py:1745] Using max model len 131072
INFO 12-04 14:19:42 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:25<00:00, 25.24s/it, est. speed input: 35.93 toks/s, output: 113.50 toks/s]Processed prompts: 100%|██████████| 1/1 [00:25<00:00, 25.24s/it, est. speed input: 35.93 toks/s, output: 113.50 toks/s]Processed prompts: 100%|██████████| 1/1 [00:25<00:00, 25.24s/it, est. speed input: 35.93 toks/s, output: 113.50 toks/s]
Agent 3 response: The bridge has a weight limit of 5000 pounds. The combined weight of the driver and the empty truck ...

--- Problem 3/20, Round 3, Agent 1/3 ---
Agent 1 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 165.57it/s]
[1;36m(EngineCore_DP0 pid=4007763)[0;0m INFO 12-04 14:20:00 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='WeiboAI/VibeThinker-1.5B', speculative_config=None, tokenizer='WeiboAI/VibeThinker-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=WeiboAI/VibeThinker-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4007763)[0;0m ERROR 12-04 14:20:01 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8
[1;36m(EngineCore_DP0 pid=4007763)[0;0m INFO 12-04 14:20:02 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.8:50829 backend=nccl
[W1204 14:20:02.251686737 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-23-0.rc.tch.harvard.edu]:50829 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4007763)[0;0m INFO 12-04 14:20:02 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4007763)[0;0m ERROR 12-04 14:20:02 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4007763)[0;0m ERROR 12-04 14:20:02 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4007763)[0;0m ERROR 12-04 14:20:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4007763)[0;0m ERROR 12-04 14:20:02 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4007763)[0;0m ERROR 12-04 14:20:02 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4007763)[0;0m ERROR 12-04 14:20:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4007763)[0;0m ERROR 12-04 14:20:02 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4007763)[0;0m ERROR 12-04 14:20:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=4007763)[0;0m ERROR 12-04 14:20:02 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=4007763)[0;0m ERROR 12-04 14:20:02 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4007763)[0;0m ERROR 12-04 14:20:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=4007763)[0;0m ERROR 12-04 14:20:02 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=4007763)[0;0m ERROR 12-04 14:20:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=4007763)[0;0m ERROR 12-04 14:20:02 [core.py:842]     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=4007763)[0;0m ERROR 12-04 14:20:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=4007763)[0;0m ERROR 12-04 14:20:02 [core.py:842]     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=4007763)[0;0m ERROR 12-04 14:20:02 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4007763)[0;0m ERROR 12-04 14:20:02 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=4007763)[0;0m ERROR 12-04 14:20:02 [core.py:842]     raise ValueError(
[1;36m(EngineCore_DP0 pid=4007763)[0;0m ERROR 12-04 14:20:02 [core.py:842] ValueError: Free memory on device (7.03/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[1;36m(EngineCore_DP0 pid=4007763)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=4007763)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4007763)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=4007763)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=4007763)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=4007763)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=4007763)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=4007763)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=4007763)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4007763)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4007763)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4007763)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4007763)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=4007763)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=4007763)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=4007763)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4007763)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=4007763)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=4007763)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=4007763)[0;0m     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=4007763)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=4007763)[0;0m     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=4007763)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4007763)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=4007763)[0;0m     raise ValueError(
[1;36m(EngineCore_DP0 pid=4007763)[0;0m ValueError: Free memory on device (7.03/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W1204 14:20:03.253660490 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:13<00:00, 13.11s/it, est. speed input: 157.17 toks/s, output: 111.62 toks/s]Processed prompts: 100%|██████████| 1/1 [00:13<00:00, 13.11s/it, est. speed input: 157.17 toks/s, output: 111.62 toks/s]Processed prompts: 100%|██████████| 1/1 [00:13<00:00, 13.11s/it, est. speed input: 157.17 toks/s, output: 111.62 toks/s]
Agent 1 response: The bridge's weight limit is 5000 pounds. The combined weight of the driver and the empty truck is 3...

--- Problem 3/20, Round 3, Agent 2/3 ---
Agent 2 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 200.50it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:12<00:00, 12.24s/it, est. speed input: 168.11 toks/s, output: 114.01 toks/s]Processed prompts: 100%|██████████| 1/1 [00:12<00:00, 12.24s/it, est. speed input: 168.11 toks/s, output: 114.01 toks/s]Processed prompts: 100%|██████████| 1/1 [00:12<00:00, 12.24s/it, est. speed input: 168.11 toks/s, output: 114.01 toks/s]
Agent 2 response: The bridge's weight limit is 5000 pounds. The combined weight of the driver and the empty truck is 3...

--- Problem 3/20, Round 3, Agent 3/3 ---
Agent 3 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 184.38it/s]
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: WeiboAI/VibeThinker-1.5B (backend=vllm)
[ModelCache] Using max_model_len=131072 for WeiboAI/VibeThinker-1.5B
[ModelCache] Detected 1 GPU(s), 44.3GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 14:20:24 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'WeiboAI/VibeThinker-1.5B'}
INFO 12-04 14:20:24 [model.py:631] Resolved architecture: Qwen2ForCausalLM
WARNING 12-04 14:20:24 [model.py:1921] Your device 'Quadro RTX 8000' (with compute capability 7.5) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.
WARNING 12-04 14:20:24 [model.py:1971] Casting torch.bfloat16 to torch.float16.
INFO 12-04 14:20:24 [model.py:1745] Using max model len 131072
INFO 12-04 14:20:24 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:18<00:00, 18.67s/it, est. speed input: 110.52 toks/s, output: 112.50 toks/s]Processed prompts: 100%|██████████| 1/1 [00:18<00:00, 18.67s/it, est. speed input: 110.52 toks/s, output: 112.50 toks/s]Processed prompts: 100%|██████████| 1/1 [00:18<00:00, 18.67s/it, est. speed input: 110.52 toks/s, output: 112.50 toks/s]
Agent 3 response: The bridge has a weight limit of 5000 pounds. The combined weight of the driver and the empty truck ...

Running accuracy: 1.000 ± 0.000

--- Problem 4/20, Round 1, Agent 1/3 ---
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 768.47it/s]
[1;36m(EngineCore_DP0 pid=4009817)[0;0m INFO 12-04 14:20:51 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='WeiboAI/VibeThinker-1.5B', speculative_config=None, tokenizer='WeiboAI/VibeThinker-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=WeiboAI/VibeThinker-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4009817)[0;0m ERROR 12-04 14:20:53 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8
[1;36m(EngineCore_DP0 pid=4009817)[0;0m INFO 12-04 14:20:54 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.8:39153 backend=nccl
[W1204 14:20:54.690263773 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-23-0.rc.tch.harvard.edu]:39153 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4009817)[0;0m INFO 12-04 14:20:54 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4009817)[0;0m ERROR 12-04 14:20:55 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4009817)[0;0m ERROR 12-04 14:20:55 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4009817)[0;0m ERROR 12-04 14:20:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4009817)[0;0m ERROR 12-04 14:20:55 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4009817)[0;0m ERROR 12-04 14:20:55 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4009817)[0;0m ERROR 12-04 14:20:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4009817)[0;0m ERROR 12-04 14:20:55 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4009817)[0;0m ERROR 12-04 14:20:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=4009817)[0;0m ERROR 12-04 14:20:55 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=4009817)[0;0m ERROR 12-04 14:20:55 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4009817)[0;0m ERROR 12-04 14:20:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=4009817)[0;0m ERROR 12-04 14:20:55 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=4009817)[0;0m ERROR 12-04 14:20:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=4009817)[0;0m ERROR 12-04 14:20:55 [core.py:842]     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=4009817)[0;0m ERROR 12-04 14:20:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=4009817)[0;0m ERROR 12-04 14:20:55 [core.py:842]     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=4009817)[0;0m ERROR 12-04 14:20:55 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4009817)[0;0m ERROR 12-04 14:20:55 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=4009817)[0;0m ERROR 12-04 14:20:55 [core.py:842]     raise ValueError(
[1;36m(EngineCore_DP0 pid=4009817)[0;0m ERROR 12-04 14:20:55 [core.py:842] ValueError: Free memory on device (7.03/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[1;36m(EngineCore_DP0 pid=4009817)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=4009817)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4009817)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=4009817)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=4009817)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=4009817)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=4009817)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=4009817)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=4009817)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4009817)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4009817)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4009817)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4009817)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=4009817)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=4009817)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=4009817)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4009817)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=4009817)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=4009817)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=4009817)[0;0m     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=4009817)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=4009817)[0;0m     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=4009817)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4009817)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=4009817)[0;0m     raise ValueError(
[1;36m(EngineCore_DP0 pid=4009817)[0;0m ValueError: Free memory on device (7.03/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W1204 14:20:55.771498728 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:26<00:00, 26.05s/it, est. speed input: 5.03 toks/s, output: 113.74 toks/s]Processed prompts: 100%|██████████| 1/1 [00:26<00:00, 26.05s/it, est. speed input: 5.03 toks/s, output: 113.74 toks/s]Processed prompts: 100%|██████████| 1/1 [00:26<00:00, 26.05s/it, est. speed input: 5.03 toks/s, output: 113.74 toks/s]
Agent 1 response: Tim starts with 7 blue shoe boxes and 9 red shoe boxes. He uses 3 blue shoe boxes, leaving \(7 - 3 =...

--- Problem 4/20, Round 1, Agent 2/3 ---
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1160.89it/s]
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: WeiboAI/VibeThinker-1.5B (backend=vllm)
[ModelCache] Using max_model_len=131072 for WeiboAI/VibeThinker-1.5B
[ModelCache] Detected 1 GPU(s), 44.3GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 14:21:16 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'WeiboAI/VibeThinker-1.5B'}
INFO 12-04 14:21:17 [model.py:631] Resolved architecture: Qwen2ForCausalLM
WARNING 12-04 14:21:17 [model.py:1921] Your device 'Quadro RTX 8000' (with compute capability 7.5) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.
WARNING 12-04 14:21:17 [model.py:1971] Casting torch.bfloat16 to torch.float16.
INFO 12-04 14:21:17 [model.py:1745] Using max model len 131072
INFO 12-04 14:21:17 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:33<00:00, 33.05s/it, est. speed input: 3.87 toks/s, output: 112.76 toks/s]Processed prompts: 100%|██████████| 1/1 [00:33<00:00, 33.05s/it, est. speed input: 3.87 toks/s, output: 112.76 toks/s]Processed prompts: 100%|██████████| 1/1 [00:33<00:00, 33.05s/it, est. speed input: 3.87 toks/s, output: 112.76 toks/s]
Agent 2 response: Tim starts with 7 blue shoe boxes and 9 red shoe boxes. He uses 3 blue shoe boxes, leaving \(7 - 3 =...

--- Problem 4/20, Round 1, Agent 3/3 ---
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1023.50it/s]
[1;36m(EngineCore_DP0 pid=4009890)[0;0m INFO 12-04 14:21:43 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='WeiboAI/VibeThinker-1.5B', speculative_config=None, tokenizer='WeiboAI/VibeThinker-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=WeiboAI/VibeThinker-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4009890)[0;0m ERROR 12-04 14:21:44 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8
[1;36m(EngineCore_DP0 pid=4009890)[0;0m INFO 12-04 14:21:45 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.8:59443 backend=nccl
[W1204 14:21:45.482993019 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-23-0.rc.tch.harvard.edu]:59443 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4009890)[0;0m INFO 12-04 14:21:45 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4009890)[0;0m ERROR 12-04 14:21:45 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4009890)[0;0m ERROR 12-04 14:21:45 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4009890)[0;0m ERROR 12-04 14:21:45 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4009890)[0;0m ERROR 12-04 14:21:45 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4009890)[0;0m ERROR 12-04 14:21:45 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4009890)[0;0m ERROR 12-04 14:21:45 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4009890)[0;0m ERROR 12-04 14:21:45 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4009890)[0;0m ERROR 12-04 14:21:45 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=4009890)[0;0m ERROR 12-04 14:21:45 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=4009890)[0;0m ERROR 12-04 14:21:45 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4009890)[0;0m ERROR 12-04 14:21:45 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=4009890)[0;0m ERROR 12-04 14:21:45 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=4009890)[0;0m ERROR 12-04 14:21:45 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=4009890)[0;0m ERROR 12-04 14:21:45 [core.py:842]     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=4009890)[0;0m ERROR 12-04 14:21:45 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=4009890)[0;0m ERROR 12-04 14:21:45 [core.py:842]     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=4009890)[0;0m ERROR 12-04 14:21:45 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4009890)[0;0m ERROR 12-04 14:21:45 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=4009890)[0;0m ERROR 12-04 14:21:45 [core.py:842]     raise ValueError(
[1;36m(EngineCore_DP0 pid=4009890)[0;0m ERROR 12-04 14:21:45 [core.py:842] ValueError: Free memory on device (7.03/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[1;36m(EngineCore_DP0 pid=4009890)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=4009890)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4009890)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=4009890)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=4009890)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=4009890)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=4009890)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=4009890)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=4009890)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4009890)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4009890)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4009890)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4009890)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=4009890)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=4009890)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=4009890)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4009890)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=4009890)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=4009890)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=4009890)[0;0m     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=4009890)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=4009890)[0;0m     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=4009890)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4009890)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=4009890)[0;0m     raise ValueError(
[1;36m(EngineCore_DP0 pid=4009890)[0;0m ValueError: Free memory on device (7.03/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W1204 14:21:46.519711315 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:15<00:00, 15.73s/it, est. speed input: 8.52 toks/s, output: 113.60 toks/s]Processed prompts: 100%|██████████| 1/1 [00:15<00:00, 15.73s/it, est. speed input: 8.52 toks/s, output: 113.60 toks/s]Processed prompts: 100%|██████████| 1/1 [00:15<00:00, 15.73s/it, est. speed input: 8.52 toks/s, output: 113.60 toks/s]
Agent 3 response: Tim starts with 7 blue shoe boxes and 9 red shoe boxes.  
He uses 3 blue shoe boxes, so the remainin...

--- Problem 4/20, Round 2, Agent 1/3 ---
Agent 1 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 473.13it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:11<00:00, 11.49s/it, est. speed input: 57.08 toks/s, output: 114.87 toks/s]Processed prompts: 100%|██████████| 1/1 [00:11<00:00, 11.49s/it, est. speed input: 57.08 toks/s, output: 114.87 toks/s]Processed prompts: 100%|██████████| 1/1 [00:11<00:00, 11.49s/it, est. speed input: 57.08 toks/s, output: 114.87 toks/s]
Agent 1 response: Tim starts with 7 blue shoe boxes and 9 red shoe boxes. He uses 3 blue shoe boxes, leaving \(7 - 3 =...

--- Problem 4/20, Round 2, Agent 2/3 ---
Agent 2 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 477.22it/s]
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: WeiboAI/VibeThinker-1.5B (backend=vllm)
[ModelCache] Using max_model_len=131072 for WeiboAI/VibeThinker-1.5B
[ModelCache] Detected 1 GPU(s), 44.3GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 14:22:07 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'WeiboAI/VibeThinker-1.5B'}
INFO 12-04 14:22:07 [model.py:631] Resolved architecture: Qwen2ForCausalLM
WARNING 12-04 14:22:07 [model.py:1921] Your device 'Quadro RTX 8000' (with compute capability 7.5) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.
WARNING 12-04 14:22:07 [model.py:1971] Casting torch.bfloat16 to torch.float16.
INFO 12-04 14:22:07 [model.py:1745] Using max model len 131072
INFO 12-04 14:22:07 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:06<00:00,  6.04s/it, est. speed input: 108.13 toks/s, output: 113.93 toks/s]Processed prompts: 100%|██████████| 1/1 [00:06<00:00,  6.04s/it, est. speed input: 108.13 toks/s, output: 113.93 toks/s]Processed prompts: 100%|██████████| 1/1 [00:06<00:00,  6.04s/it, est. speed input: 108.13 toks/s, output: 113.93 toks/s]
Agent 2 response: Tim starts with 7 blue shoe boxes and 9 red shoe boxes. Using 3 blue shoe boxes leaves \(7 - 3 = 4\)...

--- Problem 4/20, Round 2, Agent 3/3 ---
Agent 3 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 470.48it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.32s/it, est. speed input: 284.63 toks/s, output: 111.43 toks/s]Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.32s/it, est. speed input: 284.63 toks/s, output: 111.43 toks/s]Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.32s/it, est. speed input: 284.63 toks/s, output: 111.43 toks/s]
Agent 3 response: Tim starts with 7 blue shoe boxes and 9 red shoe boxes.  
After using 3 blue shoe boxes, the remaini...

--- Problem 4/20, Round 3, Agent 1/3 ---
Agent 1 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 235.11it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.31s/it, est. speed input: 495.07 toks/s, output: 111.65 toks/s]Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.31s/it, est. speed input: 495.07 toks/s, output: 111.65 toks/s]Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.31s/it, est. speed input: 495.07 toks/s, output: 111.65 toks/s]
Agent 1 response: Tim starts with 7 blue shoe boxes and 9 red shoe boxes. After using 3 blue shoe boxes, the remaining...

--- Problem 4/20, Round 3, Agent 2/3 ---
Agent 2 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 226.92it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:08<00:00,  8.62s/it, est. speed input: 132.40 toks/s, output: 110.35 toks/s]Processed prompts: 100%|██████████| 1/1 [00:08<00:00,  8.62s/it, est. speed input: 132.40 toks/s, output: 110.35 toks/s]Processed prompts: 100%|██████████| 1/1 [00:08<00:00,  8.62s/it, est. speed input: 132.40 toks/s, output: 110.35 toks/s]
Agent 2 response: Tim starts with 7 blue shoe boxes and 9 red shoe boxes.  
After using 3 blue shoe boxes, the number ...

--- Problem 4/20, Round 3, Agent 3/3 ---
Agent 3 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 306.76it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.02s/it, est. speed input: 163.34 toks/s, output: 112.50 toks/s]Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.02s/it, est. speed input: 163.34 toks/s, output: 112.50 toks/s]Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.02s/it, est. speed input: 163.34 toks/s, output: 112.50 toks/s]
Agent 3 response: Tim starts with 7 blue shoe boxes and 9 red shoe boxes. After using 3 blue shoe boxes, the remaining...

Running accuracy: 1.000 ± 0.000

--- Problem 5/20, Round 1, Agent 1/3 ---
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 964.21it/s]
[1;36m(EngineCore_DP0 pid=4009962)[0;0m INFO 12-04 14:22:35 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='WeiboAI/VibeThinker-1.5B', speculative_config=None, tokenizer='WeiboAI/VibeThinker-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=WeiboAI/VibeThinker-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4009962)[0;0m ERROR 12-04 14:22:37 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8
[1;36m(EngineCore_DP0 pid=4009962)[0;0m INFO 12-04 14:22:38 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.8:59547 backend=nccl
[W1204 14:22:38.254058563 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-23-0.rc.tch.harvard.edu]:59547 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4009962)[0;0m INFO 12-04 14:22:38 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4009962)[0;0m ERROR 12-04 14:22:38 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4009962)[0;0m ERROR 12-04 14:22:38 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4009962)[0;0m ERROR 12-04 14:22:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4009962)[0;0m ERROR 12-04 14:22:38 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4009962)[0;0m ERROR 12-04 14:22:38 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4009962)[0;0m ERROR 12-04 14:22:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4009962)[0;0m ERROR 12-04 14:22:38 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4009962)[0;0m ERROR 12-04 14:22:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=4009962)[0;0m ERROR 12-04 14:22:38 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=4009962)[0;0m ERROR 12-04 14:22:38 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4009962)[0;0m ERROR 12-04 14:22:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=4009962)[0;0m ERROR 12-04 14:22:38 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=4009962)[0;0m ERROR 12-04 14:22:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=4009962)[0;0m ERROR 12-04 14:22:38 [core.py:842]     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=4009962)[0;0m ERROR 12-04 14:22:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=4009962)[0;0m ERROR 12-04 14:22:38 [core.py:842]     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=4009962)[0;0m ERROR 12-04 14:22:38 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4009962)[0;0m ERROR 12-04 14:22:38 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=4009962)[0;0m ERROR 12-04 14:22:38 [core.py:842]     raise ValueError(
[1;36m(EngineCore_DP0 pid=4009962)[0;0m ERROR 12-04 14:22:38 [core.py:842] ValueError: Free memory on device (7.03/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[1;36m(EngineCore_DP0 pid=4009962)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=4009962)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4009962)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=4009962)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=4009962)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=4009962)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=4009962)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=4009962)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=4009962)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4009962)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4009962)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4009962)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4009962)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=4009962)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=4009962)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=4009962)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4009962)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=4009962)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=4009962)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=4009962)[0;0m     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=4009962)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=4009962)[0;0m     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=4009962)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4009962)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=4009962)[0;0m     raise ValueError(
[1;36m(EngineCore_DP0 pid=4009962)[0;0m ValueError: Free memory on device (7.03/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W1204 14:22:39.290235760 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:15<00:00, 15.82s/it, est. speed input: 7.84 toks/s, output: 113.64 toks/s]Processed prompts: 100%|██████████| 1/1 [00:15<00:00, 15.82s/it, est. speed input: 7.84 toks/s, output: 113.64 toks/s]Processed prompts: 100%|██████████| 1/1 [00:15<00:00, 15.82s/it, est. speed input: 7.84 toks/s, output: 113.64 toks/s]
Agent 1 response: Dominick saw 20 helmets. Since there are half as many helmets as footballs, the number of footballs ...

--- Problem 5/20, Round 1, Agent 2/3 ---
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1195.64it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:09<00:00,  9.29s/it, est. speed input: 13.02 toks/s, output: 115.36 toks/s]Processed prompts: 100%|██████████| 1/1 [00:09<00:00,  9.29s/it, est. speed input: 13.02 toks/s, output: 115.36 toks/s]Processed prompts: 100%|██████████| 1/1 [00:09<00:00,  9.29s/it, est. speed input: 13.02 toks/s, output: 115.36 toks/s]
Agent 2 response: Dominick saw 20 helmets. Since the number of helmets is half the number of footballs, the number of ...

--- Problem 5/20, Round 1, Agent 3/3 ---
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1152.91it/s]
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: WeiboAI/VibeThinker-1.5B (backend=vllm)
[ModelCache] Using max_model_len=131072 for WeiboAI/VibeThinker-1.5B
[ModelCache] Detected 1 GPU(s), 44.3GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 14:23:00 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'WeiboAI/VibeThinker-1.5B'}
INFO 12-04 14:23:00 [model.py:631] Resolved architecture: Qwen2ForCausalLM
WARNING 12-04 14:23:00 [model.py:1921] Your device 'Quadro RTX 8000' (with compute capability 7.5) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.
WARNING 12-04 14:23:00 [model.py:1971] Casting torch.bfloat16 to torch.float16.
INFO 12-04 14:23:00 [model.py:1745] Using max model len 131072
INFO 12-04 14:23:00 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:19<00:00, 19.76s/it, est. speed input: 6.43 toks/s, output: 113.82 toks/s]Processed prompts: 100%|██████████| 1/1 [00:19<00:00, 19.76s/it, est. speed input: 6.43 toks/s, output: 113.82 toks/s]Processed prompts: 100%|██████████| 1/1 [00:19<00:00, 19.76s/it, est. speed input: 6.43 toks/s, output: 113.82 toks/s]
Agent 3 response: Dominick saw half as many robots as helmets and half as many helmets as footballs. Given there are 2...

--- Problem 5/20, Round 2, Agent 1/3 ---
Agent 1 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 435.68it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.56s/it, est. speed input: 274.60 toks/s, output: 111.71 toks/s]Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.56s/it, est. speed input: 274.60 toks/s, output: 111.71 toks/s]Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.56s/it, est. speed input: 274.60 toks/s, output: 111.71 toks/s]
Agent 1 response: Dominick saw 20 helmets.  
The number of footballs is twice the number of helmets because there are ...

--- Problem 5/20, Round 2, Agent 2/3 ---
Agent 2 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 419.85it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.63s/it, est. speed input: 265.83 toks/s, output: 113.17 toks/s]Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.63s/it, est. speed input: 265.83 toks/s, output: 113.17 toks/s]Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.63s/it, est. speed input: 265.83 toks/s, output: 113.17 toks/s]
Agent 2 response: Dominick saw 20 helmets. Since there are half as many helmets as footballs, the number of footballs ...

--- Problem 5/20, Round 2, Agent 3/3 ---
Agent 3 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 439.06it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.20s/it, est. speed input: 321.04 toks/s, output: 113.23 toks/s]Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.20s/it, est. speed input: 321.04 toks/s, output: 113.23 toks/s]Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.20s/it, est. speed input: 321.04 toks/s, output: 113.23 toks/s]
Agent 3 response: Dominick saw 20 helmets. Since there are half as many robots as helmets, the number of robots is \( ...

--- Problem 5/20, Round 3, Agent 1/3 ---
Agent 1 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 290.97it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:04<00:00,  4.67s/it, est. speed input: 256.56 toks/s, output: 112.95 toks/s]Processed prompts: 100%|██████████| 1/1 [00:04<00:00,  4.67s/it, est. speed input: 256.56 toks/s, output: 112.95 toks/s]Processed prompts: 100%|██████████| 1/1 [00:04<00:00,  4.67s/it, est. speed input: 256.56 toks/s, output: 112.95 toks/s]
Agent 1 response: Dominick saw 20 helmets.  
Since there are half as many helmets as footballs, the number of football...

--- Problem 5/20, Round 3, Agent 2/3 ---
Agent 2 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 284.96it/s]
[1;36m(EngineCore_DP0 pid=4010040)[0;0m INFO 12-04 14:23:28 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='WeiboAI/VibeThinker-1.5B', speculative_config=None, tokenizer='WeiboAI/VibeThinker-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=WeiboAI/VibeThinker-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4010040)[0;0m ERROR 12-04 14:23:30 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8
[1;36m(EngineCore_DP0 pid=4010040)[0;0m INFO 12-04 14:23:30 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.8:48449 backend=nccl
[W1204 14:23:30.748094246 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-23-0.rc.tch.harvard.edu]:48449 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4010040)[0;0m INFO 12-04 14:23:30 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4010040)[0;0m ERROR 12-04 14:23:31 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4010040)[0;0m ERROR 12-04 14:23:31 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4010040)[0;0m ERROR 12-04 14:23:31 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4010040)[0;0m ERROR 12-04 14:23:31 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4010040)[0;0m ERROR 12-04 14:23:31 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4010040)[0;0m ERROR 12-04 14:23:31 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4010040)[0;0m ERROR 12-04 14:23:31 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4010040)[0;0m ERROR 12-04 14:23:31 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=4010040)[0;0m ERROR 12-04 14:23:31 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=4010040)[0;0m ERROR 12-04 14:23:31 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4010040)[0;0m ERROR 12-04 14:23:31 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=4010040)[0;0m ERROR 12-04 14:23:31 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=4010040)[0;0m ERROR 12-04 14:23:31 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=4010040)[0;0m ERROR 12-04 14:23:31 [core.py:842]     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=4010040)[0;0m ERROR 12-04 14:23:31 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=4010040)[0;0m ERROR 12-04 14:23:31 [core.py:842]     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=4010040)[0;0m ERROR 12-04 14:23:31 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4010040)[0;0m ERROR 12-04 14:23:31 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=4010040)[0;0m ERROR 12-04 14:23:31 [core.py:842]     raise ValueError(
[1;36m(EngineCore_DP0 pid=4010040)[0;0m ERROR 12-04 14:23:31 [core.py:842] ValueError: Free memory on device (7.03/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[1;36m(EngineCore_DP0 pid=4010040)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=4010040)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4010040)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=4010040)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=4010040)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=4010040)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=4010040)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=4010040)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=4010040)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4010040)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4010040)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4010040)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4010040)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=4010040)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=4010040)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=4010040)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4010040)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=4010040)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=4010040)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=4010040)[0;0m     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=4010040)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=4010040)[0;0m     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=4010040)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4010040)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=4010040)[0;0m     raise ValueError(
[1;36m(EngineCore_DP0 pid=4010040)[0;0m ValueError: Free memory on device (7.03/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W1204 14:23:31.768568295 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.05s/it, est. speed input: 236.23 toks/s, output: 111.58 toks/s]Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.05s/it, est. speed input: 236.23 toks/s, output: 111.58 toks/s]Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.05s/it, est. speed input: 236.23 toks/s, output: 111.58 toks/s]
Agent 2 response: Dominick saw 20 helmets. Since there are half as many helmets as footballs, the number of footballs ...

--- Problem 5/20, Round 3, Agent 3/3 ---
Agent 3 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 181.16it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.32s/it, est. speed input: 225.37 toks/s, output: 114.19 toks/s]Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.32s/it, est. speed input: 225.37 toks/s, output: 114.19 toks/s]Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.32s/it, est. speed input: 225.37 toks/s, output: 114.19 toks/s]
Agent 3 response: Dominick saw 20 helmets.  
The number of robots is half the number of helmets: \( 20 \div 2 = 10 \)....

Running accuracy: 1.000 ± 0.000

--- Problem 6/20, Round 1, Agent 1/3 ---
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1051.20it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:13<00:00, 13.19s/it, est. speed input: 10.76 toks/s, output: 115.22 toks/s]Processed prompts: 100%|██████████| 1/1 [00:13<00:00, 13.19s/it, est. speed input: 10.76 toks/s, output: 115.22 toks/s]Processed prompts: 100%|██████████| 1/1 [00:13<00:00, 13.19s/it, est. speed input: 10.76 toks/s, output: 115.22 toks/s]
Agent 1 response: The school auditorium has 4 rows with 18 seats each, totaling \(4 \times 18 = 72\) seats. One-fourth...

--- Problem 6/20, Round 1, Agent 2/3 ---
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1147.55it/s]
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: WeiboAI/VibeThinker-1.5B (backend=vllm)
[ModelCache] Using max_model_len=131072 for WeiboAI/VibeThinker-1.5B
[ModelCache] Detected 1 GPU(s), 44.3GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 14:23:52 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'WeiboAI/VibeThinker-1.5B'}
INFO 12-04 14:23:53 [model.py:631] Resolved architecture: Qwen2ForCausalLM
WARNING 12-04 14:23:53 [model.py:1921] Your device 'Quadro RTX 8000' (with compute capability 7.5) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.
WARNING 12-04 14:23:53 [model.py:1971] Casting torch.bfloat16 to torch.float16.
INFO 12-04 14:23:53 [model.py:1745] Using max model len 131072
INFO 12-04 14:23:53 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:14<00:00, 14.31s/it, est. speed input: 9.71 toks/s, output: 113.84 toks/s]Processed prompts: 100%|██████████| 1/1 [00:14<00:00, 14.31s/it, est. speed input: 9.71 toks/s, output: 113.84 toks/s]Processed prompts: 100%|██████████| 1/1 [00:14<00:00, 14.31s/it, est. speed input: 9.71 toks/s, output: 113.84 toks/s]
Agent 2 response: The school auditorium has 4 rows of seats with 18 seats per row, totaling \(4 \times 18 = 72\) seats...

--- Problem 6/20, Round 1, Agent 3/3 ---
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 811.59it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.69s/it, est. speed input: 18.85 toks/s, output: 113.86 toks/s]Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.69s/it, est. speed input: 18.85 toks/s, output: 113.86 toks/s]Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.69s/it, est. speed input: 18.85 toks/s, output: 113.86 toks/s]
Agent 3 response: The school auditorium has 4 rows with 18 seats each, totaling \(4 \times 18 = 72\) seats. One-fourth...

--- Problem 6/20, Round 2, Agent 1/3 ---
Agent 1 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 355.12it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:11<00:00, 11.81s/it, est. speed input: 67.16 toks/s, output: 113.31 toks/s]Processed prompts: 100%|██████████| 1/1 [00:11<00:00, 11.81s/it, est. speed input: 67.16 toks/s, output: 113.31 toks/s]Processed prompts: 100%|██████████| 1/1 [00:11<00:00, 11.81s/it, est. speed input: 67.16 toks/s, output: 113.31 toks/s]
Agent 1 response: The school auditorium has 4 rows with 18 seats each, so the total number of seats is \(4 \times 18 =...

--- Problem 6/20, Round 2, Agent 2/3 ---
Agent 2 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 353.74it/s]
[1;36m(EngineCore_DP0 pid=4010191)[0;0m INFO 12-04 14:24:25 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='WeiboAI/VibeThinker-1.5B', speculative_config=None, tokenizer='WeiboAI/VibeThinker-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=WeiboAI/VibeThinker-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4010191)[0;0m ERROR 12-04 14:24:26 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8
[1;36m(EngineCore_DP0 pid=4010191)[0;0m INFO 12-04 14:24:28 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.8:38067 backend=nccl
[W1204 14:24:28.215254614 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-23-0.rc.tch.harvard.edu]:38067 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4010191)[0;0m INFO 12-04 14:24:28 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4010191)[0;0m ERROR 12-04 14:24:28 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4010191)[0;0m ERROR 12-04 14:24:28 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4010191)[0;0m ERROR 12-04 14:24:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4010191)[0;0m ERROR 12-04 14:24:28 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4010191)[0;0m ERROR 12-04 14:24:28 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4010191)[0;0m ERROR 12-04 14:24:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4010191)[0;0m ERROR 12-04 14:24:28 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4010191)[0;0m ERROR 12-04 14:24:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=4010191)[0;0m ERROR 12-04 14:24:28 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=4010191)[0;0m ERROR 12-04 14:24:28 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4010191)[0;0m ERROR 12-04 14:24:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=4010191)[0;0m ERROR 12-04 14:24:28 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=4010191)[0;0m ERROR 12-04 14:24:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=4010191)[0;0m ERROR 12-04 14:24:28 [core.py:842]     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=4010191)[0;0m ERROR 12-04 14:24:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=4010191)[0;0m ERROR 12-04 14:24:28 [core.py:842]     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=4010191)[0;0m ERROR 12-04 14:24:28 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4010191)[0;0m ERROR 12-04 14:24:28 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=4010191)[0;0m ERROR 12-04 14:24:28 [core.py:842]     raise ValueError(
[1;36m(EngineCore_DP0 pid=4010191)[0;0m ERROR 12-04 14:24:28 [core.py:842] ValueError: Free memory on device (7.03/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[1;36m(EngineCore_DP0 pid=4010191)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=4010191)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4010191)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=4010191)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=4010191)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=4010191)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=4010191)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=4010191)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=4010191)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4010191)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4010191)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4010191)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4010191)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=4010191)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=4010191)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=4010191)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4010191)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=4010191)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=4010191)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=4010191)[0;0m     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=4010191)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=4010191)[0;0m     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=4010191)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4010191)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=4010191)[0;0m     raise ValueError(
[1;36m(EngineCore_DP0 pid=4010191)[0;0m ValueError: Free memory on device (7.03/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W1204 14:24:29.321185605 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:13<00:00, 13.51s/it, est. speed input: 58.45 toks/s, output: 113.36 toks/s]Processed prompts: 100%|██████████| 1/1 [00:13<00:00, 13.51s/it, est. speed input: 58.45 toks/s, output: 113.36 toks/s]Processed prompts: 100%|██████████| 1/1 [00:13<00:00, 13.52s/it, est. speed input: 58.45 toks/s, output: 113.36 toks/s]
Agent 2 response: The school auditorium has 4 rows of seats with 18 seats per row, so the total number of seats is \(4...

--- Problem 6/20, Round 2, Agent 3/3 ---
Agent 3 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 407.89it/s]
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: WeiboAI/VibeThinker-1.5B (backend=vllm)
[ModelCache] Using max_model_len=131072 for WeiboAI/VibeThinker-1.5B
[ModelCache] Detected 1 GPU(s), 44.3GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 14:24:50 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'WeiboAI/VibeThinker-1.5B'}
INFO 12-04 14:24:50 [model.py:631] Resolved architecture: Qwen2ForCausalLM
WARNING 12-04 14:24:50 [model.py:1921] Your device 'Quadro RTX 8000' (with compute capability 7.5) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.
WARNING 12-04 14:24:50 [model.py:1971] Casting torch.bfloat16 to torch.float16.
INFO 12-04 14:24:50 [model.py:1745] Using max model len 131072
INFO 12-04 14:24:50 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:14<00:00, 14.98s/it, est. speed input: 53.14 toks/s, output: 114.15 toks/s]Processed prompts: 100%|██████████| 1/1 [00:14<00:00, 14.98s/it, est. speed input: 53.14 toks/s, output: 114.15 toks/s]Processed prompts: 100%|██████████| 1/1 [00:14<00:00, 14.98s/it, est. speed input: 53.14 toks/s, output: 114.15 toks/s]
Agent 3 response: The school auditorium has 4 rows with 18 seats each, resulting in a total of \(4 \times 18 = 72\) se...

--- Problem 6/20, Round 3, Agent 1/3 ---
Agent 1 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 218.64it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:09<00:00,  9.06s/it, est. speed input: 177.28 toks/s, output: 112.26 toks/s]Processed prompts: 100%|██████████| 1/1 [00:09<00:00,  9.06s/it, est. speed input: 177.28 toks/s, output: 112.26 toks/s]Processed prompts: 100%|██████████| 1/1 [00:09<00:00,  9.06s/it, est. speed input: 177.28 toks/s, output: 112.26 toks/s]
Agent 1 response: The school auditorium has 4 rows with 18 seats each, resulting in a total of \(4 \times 18 = 72\) se...

--- Problem 6/20, Round 3, Agent 2/3 ---
Agent 2 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 148.60it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.66s/it, est. speed input: 283.13 toks/s, output: 112.33 toks/s]Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.66s/it, est. speed input: 283.13 toks/s, output: 112.33 toks/s]Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.66s/it, est. speed input: 283.13 toks/s, output: 112.33 toks/s]
Agent 2 response: The school auditorium has 4 rows of seats, each with 18 seats, so the total number of seats is \(4 \...

--- Problem 6/20, Round 3, Agent 3/3 ---
Agent 3 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 214.52it/s]
[1;36m(EngineCore_DP0 pid=4010377)[0;0m INFO 12-04 14:25:14 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='WeiboAI/VibeThinker-1.5B', speculative_config=None, tokenizer='WeiboAI/VibeThinker-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=WeiboAI/VibeThinker-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4010377)[0;0m ERROR 12-04 14:25:15 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8
[1;36m(EngineCore_DP0 pid=4010377)[0;0m INFO 12-04 14:25:16 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.8:37673 backend=nccl
[W1204 14:25:16.889713974 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-23-0.rc.tch.harvard.edu]:37673 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4010377)[0;0m INFO 12-04 14:25:16 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4010377)[0;0m ERROR 12-04 14:25:16 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4010377)[0;0m ERROR 12-04 14:25:16 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4010377)[0;0m ERROR 12-04 14:25:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4010377)[0;0m ERROR 12-04 14:25:16 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4010377)[0;0m ERROR 12-04 14:25:16 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4010377)[0;0m ERROR 12-04 14:25:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4010377)[0;0m ERROR 12-04 14:25:16 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4010377)[0;0m ERROR 12-04 14:25:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=4010377)[0;0m ERROR 12-04 14:25:16 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=4010377)[0;0m ERROR 12-04 14:25:16 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4010377)[0;0m ERROR 12-04 14:25:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=4010377)[0;0m ERROR 12-04 14:25:16 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=4010377)[0;0m ERROR 12-04 14:25:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=4010377)[0;0m ERROR 12-04 14:25:16 [core.py:842]     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=4010377)[0;0m ERROR 12-04 14:25:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=4010377)[0;0m ERROR 12-04 14:25:16 [core.py:842]     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=4010377)[0;0m ERROR 12-04 14:25:16 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4010377)[0;0m ERROR 12-04 14:25:16 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=4010377)[0;0m ERROR 12-04 14:25:16 [core.py:842]     raise ValueError(
[1;36m(EngineCore_DP0 pid=4010377)[0;0m ERROR 12-04 14:25:16 [core.py:842] ValueError: Free memory on device (7.03/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[1;36m(EngineCore_DP0 pid=4010377)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=4010377)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4010377)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=4010377)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=4010377)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=4010377)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=4010377)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=4010377)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=4010377)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4010377)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4010377)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4010377)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4010377)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=4010377)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=4010377)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=4010377)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4010377)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=4010377)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=4010377)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=4010377)[0;0m     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=4010377)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=4010377)[0;0m     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=4010377)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4010377)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=4010377)[0;0m     raise ValueError(
[1;36m(EngineCore_DP0 pid=4010377)[0;0m ValueError: Free memory on device (7.03/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W1204 14:25:17.920762480 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:15<00:00, 15.23s/it, est. speed input: 105.64 toks/s, output: 112.60 toks/s]Processed prompts: 100%|██████████| 1/1 [00:15<00:00, 15.23s/it, est. speed input: 105.64 toks/s, output: 112.60 toks/s]Processed prompts: 100%|██████████| 1/1 [00:15<00:00, 15.23s/it, est. speed input: 105.64 toks/s, output: 112.60 toks/s]
Agent 3 response: The school auditorium has 4 rows with 18 seats each, so the total number of seats is \(4 \times 18 =...

Running accuracy: 1.000 ± 0.000

--- Problem 7/20, Round 1, Agent 1/3 ---
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1149.12it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:11<00:00, 11.01s/it, est. speed input: 9.99 toks/s, output: 115.35 toks/s]Processed prompts: 100%|██████████| 1/1 [00:11<00:00, 11.01s/it, est. speed input: 9.99 toks/s, output: 115.35 toks/s]Processed prompts: 100%|██████████| 1/1 [00:11<00:00, 11.01s/it, est. speed input: 9.99 toks/s, output: 115.35 toks/s]
Agent 1 response: The total number of legs is calculated by considering each category of pets:

- **Dogs**: 5 dogs × 4...

--- Problem 7/20, Round 1, Agent 2/3 ---
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1245.34it/s]
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: WeiboAI/VibeThinker-1.5B (backend=vllm)
[ModelCache] Using max_model_len=131072 for WeiboAI/VibeThinker-1.5B
[ModelCache] Detected 1 GPU(s), 44.3GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 14:25:38 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'WeiboAI/VibeThinker-1.5B'}
INFO 12-04 14:25:38 [model.py:631] Resolved architecture: Qwen2ForCausalLM
WARNING 12-04 14:25:38 [model.py:1921] Your device 'Quadro RTX 8000' (with compute capability 7.5) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.
WARNING 12-04 14:25:38 [model.py:1971] Casting torch.bfloat16 to torch.float16.
INFO 12-04 14:25:38 [model.py:1745] Using max model len 131072
INFO 12-04 14:25:38 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:10<00:00, 10.73s/it, est. speed input: 9.98 toks/s, output: 113.83 toks/s]Processed prompts: 100%|██████████| 1/1 [00:10<00:00, 10.73s/it, est. speed input: 9.98 toks/s, output: 113.83 toks/s]Processed prompts: 100%|██████████| 1/1 [00:10<00:00, 10.73s/it, est. speed input: 9.98 toks/s, output: 113.83 toks/s]
Agent 2 response: The problem involves calculating the total number of legs for dogs, cats, and birds. Each dog and ca...

--- Problem 7/20, Round 1, Agent 3/3 ---
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 886.93it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:10<00:00, 10.55s/it, est. speed input: 10.71 toks/s, output: 113.59 toks/s]Processed prompts: 100%|██████████| 1/1 [00:10<00:00, 10.55s/it, est. speed input: 10.71 toks/s, output: 113.59 toks/s]Processed prompts: 100%|██████████| 1/1 [00:10<00:00, 10.55s/it, est. speed input: 10.71 toks/s, output: 113.59 toks/s]
Agent 3 response: The pet store has 5 dogs, each with 4 legs, contributing \(5 \times 4 = 20\) legs. There are 2 cats,...

--- Problem 7/20, Round 2, Agent 1/3 ---
Agent 1 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 446.01it/s]
[1;36m(EngineCore_DP0 pid=4010545)[0;0m INFO 12-04 14:26:01 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='WeiboAI/VibeThinker-1.5B', speculative_config=None, tokenizer='WeiboAI/VibeThinker-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=WeiboAI/VibeThinker-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4010545)[0;0m ERROR 12-04 14:26:02 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8
[1;36m(EngineCore_DP0 pid=4010545)[0;0m INFO 12-04 14:26:03 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.8:50837 backend=nccl
[W1204 14:26:03.738043052 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-23-0.rc.tch.harvard.edu]:50837 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4010545)[0;0m INFO 12-04 14:26:03 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4010545)[0;0m ERROR 12-04 14:26:04 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4010545)[0;0m ERROR 12-04 14:26:04 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4010545)[0;0m ERROR 12-04 14:26:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4010545)[0;0m ERROR 12-04 14:26:04 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4010545)[0;0m ERROR 12-04 14:26:04 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4010545)[0;0m ERROR 12-04 14:26:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4010545)[0;0m ERROR 12-04 14:26:04 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4010545)[0;0m ERROR 12-04 14:26:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=4010545)[0;0m ERROR 12-04 14:26:04 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=4010545)[0;0m ERROR 12-04 14:26:04 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4010545)[0;0m ERROR 12-04 14:26:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=4010545)[0;0m ERROR 12-04 14:26:04 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=4010545)[0;0m ERROR 12-04 14:26:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=4010545)[0;0m ERROR 12-04 14:26:04 [core.py:842]     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=4010545)[0;0m ERROR 12-04 14:26:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=4010545)[0;0m ERROR 12-04 14:26:04 [core.py:842]     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=4010545)[0;0m ERROR 12-04 14:26:04 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4010545)[0;0m ERROR 12-04 14:26:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=4010545)[0;0m ERROR 12-04 14:26:04 [core.py:842]     raise ValueError(
[1;36m(EngineCore_DP0 pid=4010545)[0;0m ERROR 12-04 14:26:04 [core.py:842] ValueError: Free memory on device (7.03/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[1;36m(EngineCore_DP0 pid=4010545)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=4010545)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4010545)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=4010545)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=4010545)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=4010545)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=4010545)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=4010545)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=4010545)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4010545)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4010545)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4010545)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4010545)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=4010545)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=4010545)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=4010545)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4010545)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=4010545)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=4010545)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=4010545)[0;0m     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=4010545)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=4010545)[0;0m     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=4010545)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4010545)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=4010545)[0;0m     raise ValueError(
[1;36m(EngineCore_DP0 pid=4010545)[0;0m ValueError: Free memory on device (7.03/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W1204 14:26:04.772635127 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:10<00:00, 10.65s/it, est. speed input: 53.05 toks/s, output: 112.68 toks/s]Processed prompts: 100%|██████████| 1/1 [00:10<00:00, 10.65s/it, est. speed input: 53.05 toks/s, output: 112.68 toks/s]Processed prompts: 100%|██████████| 1/1 [00:10<00:00, 10.65s/it, est. speed input: 53.05 toks/s, output: 112.68 toks/s]
Agent 1 response: The pet store has 5 dogs, each with 4 legs, contributing \(5 \times 4 = 20\) legs.  
There are 2 cat...

--- Problem 7/20, Round 2, Agent 2/3 ---
Agent 2 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 526.59it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.69s/it, est. speed input: 98.80 toks/s, output: 114.97 toks/s]Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.69s/it, est. speed input: 98.80 toks/s, output: 114.97 toks/s]Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.69s/it, est. speed input: 98.80 toks/s, output: 114.97 toks/s]
Agent 2 response: To determine the total number of legs for the pets in the store, we calculate the legs contributed b...

--- Problem 7/20, Round 2, Agent 3/3 ---
Agent 3 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 524.81it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:10<00:00, 10.89s/it, est. speed input: 52.13 toks/s, output: 115.01 toks/s]Processed prompts: 100%|██████████| 1/1 [00:10<00:00, 10.89s/it, est. speed input: 52.13 toks/s, output: 115.01 toks/s]Processed prompts: 100%|██████████| 1/1 [00:10<00:00, 10.90s/it, est. speed input: 52.13 toks/s, output: 115.01 toks/s]
Agent 3 response: The pet store has 5 dogs, each with 4 legs, contributing \(5 \times 4 = 20\) legs.  
There are 2 cat...

--- Problem 7/20, Round 3, Agent 1/3 ---
Agent 1 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 314.16it/s]
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: WeiboAI/VibeThinker-1.5B (backend=vllm)
[ModelCache] Using max_model_len=131072 for WeiboAI/VibeThinker-1.5B
[ModelCache] Detected 1 GPU(s), 44.3GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 14:26:25 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'WeiboAI/VibeThinker-1.5B'}
INFO 12-04 14:26:26 [model.py:631] Resolved architecture: Qwen2ForCausalLM
WARNING 12-04 14:26:26 [model.py:1921] Your device 'Quadro RTX 8000' (with compute capability 7.5) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.
WARNING 12-04 14:26:26 [model.py:1971] Casting torch.bfloat16 to torch.float16.
INFO 12-04 14:26:26 [model.py:1745] Using max model len 131072
INFO 12-04 14:26:26 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:08<00:00,  8.65s/it, est. speed input: 121.89 toks/s, output: 113.44 toks/s]Processed prompts: 100%|██████████| 1/1 [00:08<00:00,  8.65s/it, est. speed input: 121.89 toks/s, output: 113.44 toks/s]Processed prompts: 100%|██████████| 1/1 [00:08<00:00,  8.65s/it, est. speed input: 121.89 toks/s, output: 113.44 toks/s]
Agent 1 response: The pet store has 5 dogs, each with 4 legs, contributing \(5 \times 4 = 20\) legs.  
There are 2 cat...

--- Problem 7/20, Round 3, Agent 2/3 ---
Agent 2 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 206.73it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:04<00:00,  4.78s/it, est. speed input: 219.81 toks/s, output: 112.52 toks/s]Processed prompts: 100%|██████████| 1/1 [00:04<00:00,  4.78s/it, est. speed input: 219.81 toks/s, output: 112.52 toks/s]Processed prompts: 100%|██████████| 1/1 [00:04<00:00,  4.78s/it, est. speed input: 219.81 toks/s, output: 112.52 toks/s]
Agent 2 response: To find the total number of legs for the pets in the store, calculate the legs for each type of anim...

--- Problem 7/20, Round 3, Agent 3/3 ---
Agent 3 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 306.83it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:14<00:00, 14.46s/it, est. speed input: 73.12 toks/s, output: 113.17 toks/s]Processed prompts: 100%|██████████| 1/1 [00:14<00:00, 14.46s/it, est. speed input: 73.12 toks/s, output: 113.17 toks/s]Processed prompts: 100%|██████████| 1/1 [00:14<00:00, 14.46s/it, est. speed input: 73.12 toks/s, output: 113.17 toks/s]
Agent 3 response: The pet store has 5 dogs, each with 4 legs, contributing \(5 \times 4 = 20\) legs.  
There are 2 cat...

Running accuracy: 1.000 ± 0.000

--- Problem 8/20, Round 1, Agent 1/3 ---
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 742.22it/s]
[1;36m(EngineCore_DP0 pid=4010601)[0;0m INFO 12-04 14:26:50 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='WeiboAI/VibeThinker-1.5B', speculative_config=None, tokenizer='WeiboAI/VibeThinker-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=WeiboAI/VibeThinker-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4010601)[0;0m ERROR 12-04 14:26:51 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8
[1;36m(EngineCore_DP0 pid=4010601)[0;0m INFO 12-04 14:26:52 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.8:59273 backend=nccl
[W1204 14:26:52.673482438 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-23-0.rc.tch.harvard.edu]:59273 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4010601)[0;0m INFO 12-04 14:26:52 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4010601)[0;0m ERROR 12-04 14:26:53 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4010601)[0;0m ERROR 12-04 14:26:53 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4010601)[0;0m ERROR 12-04 14:26:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4010601)[0;0m ERROR 12-04 14:26:53 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4010601)[0;0m ERROR 12-04 14:26:53 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4010601)[0;0m ERROR 12-04 14:26:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4010601)[0;0m ERROR 12-04 14:26:53 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4010601)[0;0m ERROR 12-04 14:26:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=4010601)[0;0m ERROR 12-04 14:26:53 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=4010601)[0;0m ERROR 12-04 14:26:53 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4010601)[0;0m ERROR 12-04 14:26:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=4010601)[0;0m ERROR 12-04 14:26:53 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=4010601)[0;0m ERROR 12-04 14:26:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=4010601)[0;0m ERROR 12-04 14:26:53 [core.py:842]     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=4010601)[0;0m ERROR 12-04 14:26:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=4010601)[0;0m ERROR 12-04 14:26:53 [core.py:842]     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=4010601)[0;0m ERROR 12-04 14:26:53 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4010601)[0;0m ERROR 12-04 14:26:53 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=4010601)[0;0m ERROR 12-04 14:26:53 [core.py:842]     raise ValueError(
[1;36m(EngineCore_DP0 pid=4010601)[0;0m ERROR 12-04 14:26:53 [core.py:842] ValueError: Free memory on device (7.03/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[1;36m(EngineCore_DP0 pid=4010601)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=4010601)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4010601)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=4010601)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=4010601)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=4010601)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=4010601)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=4010601)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=4010601)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4010601)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4010601)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4010601)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4010601)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=4010601)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=4010601)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=4010601)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4010601)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=4010601)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=4010601)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=4010601)[0;0m     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=4010601)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=4010601)[0;0m     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=4010601)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4010601)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=4010601)[0;0m     raise ValueError(
[1;36m(EngineCore_DP0 pid=4010601)[0;0m ValueError: Free memory on device (7.03/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W1204 14:26:53.698096090 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:06<00:00,  6.63s/it, est. speed input: 21.25 toks/s, output: 113.21 toks/s]Processed prompts: 100%|██████████| 1/1 [00:06<00:00,  6.63s/it, est. speed input: 21.25 toks/s, output: 113.21 toks/s]Processed prompts: 100%|██████████| 1/1 [00:06<00:00,  6.63s/it, est. speed input: 21.25 toks/s, output: 113.21 toks/s]
Agent 1 response: The speed of the slow lane car is half of 60 mph, which is \(30\) mph. To find the time taken to tra...

--- Problem 8/20, Round 1, Agent 2/3 ---
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1118.48it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:06<00:00,  6.82s/it, est. speed input: 20.22 toks/s, output: 115.32 toks/s]Processed prompts: 100%|██████████| 1/1 [00:06<00:00,  6.82s/it, est. speed input: 20.22 toks/s, output: 115.32 toks/s]Processed prompts: 100%|██████████| 1/1 [00:06<00:00,  6.82s/it, est. speed input: 20.22 toks/s, output: 115.32 toks/s]
Agent 2 response: The fast lane car travels at 60 mph, and the slow lane car travels at half that speed, which is 30 m...

--- Problem 8/20, Round 1, Agent 3/3 ---
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1143.80it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.19s/it, est. speed input: 20.03 toks/s, output: 115.31 toks/s]Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.19s/it, est. speed input: 20.03 toks/s, output: 115.31 toks/s]Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.19s/it, est. speed input: 20.03 toks/s, output: 115.31 toks/s]
Agent 3 response: The car in the slow lane travels at half the speed of the fast lane car, which is \( \frac{60}{2} = ...

--- Problem 8/20, Round 2, Agent 1/3 ---
Agent 1 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 450.95it/s]
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: WeiboAI/VibeThinker-1.5B (backend=vllm)
[ModelCache] Using max_model_len=131072 for WeiboAI/VibeThinker-1.5B
[ModelCache] Detected 1 GPU(s), 44.3GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 14:27:14 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'WeiboAI/VibeThinker-1.5B'}
INFO 12-04 14:27:15 [model.py:631] Resolved architecture: Qwen2ForCausalLM
WARNING 12-04 14:27:15 [model.py:1921] Your device 'Quadro RTX 8000' (with compute capability 7.5) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.
WARNING 12-04 14:27:15 [model.py:1971] Casting torch.bfloat16 to torch.float16.
INFO 12-04 14:27:15 [model.py:1745] Using max model len 131072
INFO 12-04 14:27:15 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:06<00:00,  6.60s/it, est. speed input: 100.90 toks/s, output: 113.63 toks/s]Processed prompts: 100%|██████████| 1/1 [00:06<00:00,  6.60s/it, est. speed input: 100.90 toks/s, output: 113.63 toks/s]Processed prompts: 100%|██████████| 1/1 [00:06<00:00,  6.60s/it, est. speed input: 100.90 toks/s, output: 113.63 toks/s]
Agent 1 response: The car in the slow lane travels at half the speed of the fast lane car. Given that the fast lane ca...

--- Problem 8/20, Round 2, Agent 2/3 ---
Agent 2 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 438.18it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:06<00:00,  6.17s/it, est. speed input: 107.45 toks/s, output: 112.31 toks/s]Processed prompts: 100%|██████████| 1/1 [00:06<00:00,  6.17s/it, est. speed input: 107.45 toks/s, output: 112.31 toks/s]Processed prompts: 100%|██████████| 1/1 [00:06<00:00,  6.17s/it, est. speed input: 107.45 toks/s, output: 112.31 toks/s]
Agent 2 response: The car in the fast lane travels at 60 miles per hour, so the car in the slow lane travels at half t...

--- Problem 8/20, Round 2, Agent 3/3 ---
Agent 3 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 308.84it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.10s/it, est. speed input: 131.20 toks/s, output: 112.77 toks/s]Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.10s/it, est. speed input: 131.20 toks/s, output: 112.77 toks/s]Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.10s/it, est. speed input: 131.20 toks/s, output: 112.77 toks/s]
Agent 3 response: The car in the fast lane travels at 60 miles per hour. The car in the slow lane travels at half that...

--- Problem 8/20, Round 3, Agent 1/3 ---
Agent 1 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 228.61it/s]
[1;36m(EngineCore_DP0 pid=4010680)[0;0m INFO 12-04 14:27:36 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='WeiboAI/VibeThinker-1.5B', speculative_config=None, tokenizer='WeiboAI/VibeThinker-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=WeiboAI/VibeThinker-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4010680)[0;0m ERROR 12-04 14:27:38 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8
[1;36m(EngineCore_DP0 pid=4010680)[0;0m INFO 12-04 14:27:39 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.8:39831 backend=nccl
[W1204 14:27:39.988800607 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-23-0.rc.tch.harvard.edu]:39831 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4010680)[0;0m INFO 12-04 14:27:39 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4010680)[0;0m ERROR 12-04 14:27:39 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4010680)[0;0m ERROR 12-04 14:27:39 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4010680)[0;0m ERROR 12-04 14:27:39 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4010680)[0;0m ERROR 12-04 14:27:39 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4010680)[0;0m ERROR 12-04 14:27:39 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4010680)[0;0m ERROR 12-04 14:27:39 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4010680)[0;0m ERROR 12-04 14:27:39 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4010680)[0;0m ERROR 12-04 14:27:39 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=4010680)[0;0m ERROR 12-04 14:27:39 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=4010680)[0;0m ERROR 12-04 14:27:39 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4010680)[0;0m ERROR 12-04 14:27:39 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=4010680)[0;0m ERROR 12-04 14:27:39 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=4010680)[0;0m ERROR 12-04 14:27:39 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=4010680)[0;0m ERROR 12-04 14:27:39 [core.py:842]     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=4010680)[0;0m ERROR 12-04 14:27:39 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=4010680)[0;0m ERROR 12-04 14:27:39 [core.py:842]     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=4010680)[0;0m ERROR 12-04 14:27:39 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4010680)[0;0m ERROR 12-04 14:27:39 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=4010680)[0;0m ERROR 12-04 14:27:39 [core.py:842]     raise ValueError(
[1;36m(EngineCore_DP0 pid=4010680)[0;0m ERROR 12-04 14:27:39 [core.py:842] ValueError: Free memory on device (7.03/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[1;36m(EngineCore_DP0 pid=4010680)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=4010680)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4010680)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=4010680)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=4010680)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=4010680)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=4010680)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=4010680)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=4010680)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4010680)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4010680)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4010680)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4010680)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=4010680)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=4010680)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=4010680)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4010680)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=4010680)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=4010680)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=4010680)[0;0m     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=4010680)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=4010680)[0;0m     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=4010680)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4010680)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=4010680)[0;0m     raise ValueError(
[1;36m(EngineCore_DP0 pid=4010680)[0;0m ValueError: Free memory on device (7.03/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W1204 14:27:40.096365782 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:20<00:00, 20.91s/it, est. speed input: 67.16 toks/s, output: 111.40 toks/s]Processed prompts: 100%|██████████| 1/1 [00:20<00:00, 20.91s/it, est. speed input: 67.16 toks/s, output: 111.40 toks/s]Processed prompts: 100%|██████████| 1/1 [00:20<00:00, 20.91s/it, est. speed input: 67.16 toks/s, output: 111.40 toks/s]
Agent 1 response: The fast lane car travels at 60 miles per hour, so the slow lane car, traveling at half that speed, ...

--- Problem 8/20, Round 3, Agent 2/3 ---
Agent 2 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 199.67it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.28s/it, est. speed input: 192.55 toks/s, output: 113.80 toks/s]Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.28s/it, est. speed input: 192.55 toks/s, output: 113.80 toks/s]Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.28s/it, est. speed input: 192.55 toks/s, output: 113.80 toks/s]
Agent 2 response: The fast lane car travels at 60 miles per hour, so the slow lane car travels at half that speed, whi...

--- Problem 8/20, Round 3, Agent 3/3 ---
Agent 3 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 237.23it/s]
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: WeiboAI/VibeThinker-1.5B (backend=vllm)
[ModelCache] Using max_model_len=131072 for WeiboAI/VibeThinker-1.5B
[ModelCache] Detected 1 GPU(s), 44.3GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 14:28:01 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'WeiboAI/VibeThinker-1.5B'}
INFO 12-04 14:28:01 [model.py:631] Resolved architecture: Qwen2ForCausalLM
WARNING 12-04 14:28:01 [model.py:1921] Your device 'Quadro RTX 8000' (with compute capability 7.5) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.
WARNING 12-04 14:28:01 [model.py:1971] Casting torch.bfloat16 to torch.float16.
INFO 12-04 14:28:01 [model.py:1745] Using max model len 131072
INFO 12-04 14:28:01 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.03s/it, est. speed input: 279.48 toks/s, output: 113.02 toks/s]Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.03s/it, est. speed input: 279.48 toks/s, output: 113.02 toks/s]Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.03s/it, est. speed input: 279.48 toks/s, output: 113.02 toks/s]
Agent 3 response: The car in the slow lane travels at half the speed of the fast lane car, which is \( \frac{60}{2} = ...

Running accuracy: 1.000 ± 0.000

--- Problem 9/20, Round 1, Agent 1/3 ---
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 730.71it/s]
[1;36m(EngineCore_DP0 pid=4010769)[0;0m INFO 12-04 14:28:25 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='WeiboAI/VibeThinker-1.5B', speculative_config=None, tokenizer='WeiboAI/VibeThinker-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=WeiboAI/VibeThinker-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4010769)[0;0m ERROR 12-04 14:28:26 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8
[1;36m(EngineCore_DP0 pid=4010769)[0;0m INFO 12-04 14:28:26 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.8:54253 backend=nccl
[W1204 14:28:27.888429825 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-23-0.rc.tch.harvard.edu]:54253 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4010769)[0;0m INFO 12-04 14:28:27 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4010769)[0;0m ERROR 12-04 14:28:27 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4010769)[0;0m ERROR 12-04 14:28:27 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4010769)[0;0m ERROR 12-04 14:28:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4010769)[0;0m ERROR 12-04 14:28:27 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4010769)[0;0m ERROR 12-04 14:28:27 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4010769)[0;0m ERROR 12-04 14:28:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4010769)[0;0m ERROR 12-04 14:28:27 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4010769)[0;0m ERROR 12-04 14:28:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=4010769)[0;0m ERROR 12-04 14:28:27 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=4010769)[0;0m ERROR 12-04 14:28:27 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4010769)[0;0m ERROR 12-04 14:28:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=4010769)[0;0m ERROR 12-04 14:28:27 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=4010769)[0;0m ERROR 12-04 14:28:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=4010769)[0;0m ERROR 12-04 14:28:27 [core.py:842]     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=4010769)[0;0m ERROR 12-04 14:28:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=4010769)[0;0m ERROR 12-04 14:28:27 [core.py:842]     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=4010769)[0;0m ERROR 12-04 14:28:27 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4010769)[0;0m ERROR 12-04 14:28:27 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=4010769)[0;0m ERROR 12-04 14:28:27 [core.py:842]     raise ValueError(
[1;36m(EngineCore_DP0 pid=4010769)[0;0m ERROR 12-04 14:28:27 [core.py:842] ValueError: Free memory on device (7.03/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[1;36m(EngineCore_DP0 pid=4010769)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=4010769)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4010769)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=4010769)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=4010769)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=4010769)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=4010769)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=4010769)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=4010769)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4010769)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4010769)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4010769)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4010769)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=4010769)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=4010769)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=4010769)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4010769)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=4010769)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=4010769)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=4010769)[0;0m     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=4010769)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=4010769)[0;0m     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=4010769)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4010769)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=4010769)[0;0m     raise ValueError(
[1;36m(EngineCore_DP0 pid=4010769)[0;0m ValueError: Free memory on device (7.03/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W1204 14:28:28.925904550 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:39<00:00, 39.49s/it, est. speed input: 3.82 toks/s, output: 111.99 toks/s]Processed prompts: 100%|██████████| 1/1 [00:39<00:00, 39.49s/it, est. speed input: 3.82 toks/s, output: 111.99 toks/s]Processed prompts: 100%|██████████| 1/1 [00:39<00:00, 39.49s/it, est. speed input: 3.82 toks/s, output: 111.99 toks/s]
Agent 1 response: Joe has $50 to buy an outfit with a 30% off sale. The original prices of the shirt and shorts are $2...

--- Problem 9/20, Round 1, Agent 2/3 ---
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 874.00it/s]
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: WeiboAI/VibeThinker-1.5B (backend=vllm)
[ModelCache] Using max_model_len=131072 for WeiboAI/VibeThinker-1.5B
[ModelCache] Detected 1 GPU(s), 44.3GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 14:28:49 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'WeiboAI/VibeThinker-1.5B'}
INFO 12-04 14:28:49 [model.py:631] Resolved architecture: Qwen2ForCausalLM
WARNING 12-04 14:28:49 [model.py:1921] Your device 'Quadro RTX 8000' (with compute capability 7.5) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.
WARNING 12-04 14:28:49 [model.py:1971] Casting torch.bfloat16 to torch.float16.
INFO 12-04 14:28:49 [model.py:1745] Using max model len 131072
INFO 12-04 14:28:49 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:14<00:00, 14.05s/it, est. speed input: 10.53 toks/s, output: 113.93 toks/s]Processed prompts: 100%|██████████| 1/1 [00:14<00:00, 14.05s/it, est. speed input: 10.53 toks/s, output: 113.93 toks/s]Processed prompts: 100%|██████████| 1/1 [00:14<00:00, 14.05s/it, est. speed input: 10.53 toks/s, output: 113.93 toks/s]
Agent 2 response: Joe has a total of $50 to spend on the shirt and shorts. The shirt costs $25 and the shorts cost $35...

--- Problem 9/20, Round 1, Agent 3/3 ---
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1057.83it/s]
[1;36m(EngineCore_DP0 pid=4010926)[0;0m INFO 12-04 14:29:17 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='WeiboAI/VibeThinker-1.5B', speculative_config=None, tokenizer='WeiboAI/VibeThinker-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=WeiboAI/VibeThinker-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4010926)[0;0m ERROR 12-04 14:29:20 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8
[1;36m(EngineCore_DP0 pid=4010926)[0;0m INFO 12-04 14:29:21 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.8:56331 backend=nccl
[W1204 14:29:21.432876694 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-23-0.rc.tch.harvard.edu]:56331 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4010926)[0;0m INFO 12-04 14:29:21 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4010926)[0;0m ERROR 12-04 14:29:21 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4010926)[0;0m ERROR 12-04 14:29:21 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4010926)[0;0m ERROR 12-04 14:29:21 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4010926)[0;0m ERROR 12-04 14:29:21 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4010926)[0;0m ERROR 12-04 14:29:21 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4010926)[0;0m ERROR 12-04 14:29:21 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4010926)[0;0m ERROR 12-04 14:29:21 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4010926)[0;0m ERROR 12-04 14:29:21 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=4010926)[0;0m ERROR 12-04 14:29:21 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=4010926)[0;0m ERROR 12-04 14:29:21 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4010926)[0;0m ERROR 12-04 14:29:21 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=4010926)[0;0m ERROR 12-04 14:29:21 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=4010926)[0;0m ERROR 12-04 14:29:21 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=4010926)[0;0m ERROR 12-04 14:29:21 [core.py:842]     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=4010926)[0;0m ERROR 12-04 14:29:21 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=4010926)[0;0m ERROR 12-04 14:29:21 [core.py:842]     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=4010926)[0;0m ERROR 12-04 14:29:21 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4010926)[0;0m ERROR 12-04 14:29:21 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=4010926)[0;0m ERROR 12-04 14:29:21 [core.py:842]     raise ValueError(
[1;36m(EngineCore_DP0 pid=4010926)[0;0m ERROR 12-04 14:29:21 [core.py:842] ValueError: Free memory on device (7.03/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[1;36m(EngineCore_DP0 pid=4010926)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=4010926)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4010926)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=4010926)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=4010926)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=4010926)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=4010926)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=4010926)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=4010926)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4010926)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4010926)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4010926)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4010926)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=4010926)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=4010926)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=4010926)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4010926)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=4010926)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=4010926)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=4010926)[0;0m     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=4010926)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=4010926)[0;0m     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=4010926)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4010926)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=4010926)[0;0m     raise ValueError(
[1;36m(EngineCore_DP0 pid=4010926)[0;0m ValueError: Free memory on device (7.03/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W1204 14:29:22.506170748 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:42<00:00, 42.98s/it, est. speed input: 3.58 toks/s, output: 113.30 toks/s]Processed prompts: 100%|██████████| 1/1 [00:42<00:00, 42.98s/it, est. speed input: 3.58 toks/s, output: 113.30 toks/s]Processed prompts: 100%|██████████| 1/1 [00:42<00:00, 42.98s/it, est. speed input: 3.58 toks/s, output: 113.30 toks/s]
Agent 3 response: Joe has $50 to spend on an outfit that includes a 30% off sale. The shirt and shorts have original p...

--- Problem 9/20, Round 2, Agent 1/3 ---
Agent 1 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 379.51it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:03<00:00,  3.27s/it, est. speed input: 240.18 toks/s, output: 114.12 toks/s]Processed prompts: 100%|██████████| 1/1 [00:03<00:00,  3.27s/it, est. speed input: 240.18 toks/s, output: 114.12 toks/s]Processed prompts: 100%|██████████| 1/1 [00:03<00:00,  3.27s/it, est. speed input: 240.18 toks/s, output: 114.12 toks/s]
Agent 1 response: Joe has $50 to spend on a shirt that costs $25 and a pair of shorts that cost $35, making the total ...

--- Problem 9/20, Round 2, Agent 2/3 ---
Agent 2 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 400.64it/s]
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: WeiboAI/VibeThinker-1.5B (backend=vllm)
[ModelCache] Using max_model_len=131072 for WeiboAI/VibeThinker-1.5B
[ModelCache] Detected 1 GPU(s), 44.3GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 14:29:43 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'WeiboAI/VibeThinker-1.5B'}
INFO 12-04 14:29:43 [model.py:631] Resolved architecture: Qwen2ForCausalLM
WARNING 12-04 14:29:43 [model.py:1921] Your device 'Quadro RTX 8000' (with compute capability 7.5) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.
WARNING 12-04 14:29:43 [model.py:1971] Casting torch.bfloat16 to torch.float16.
INFO 12-04 14:29:43 [model.py:1745] Using max model len 131072
INFO 12-04 14:29:43 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=4011039)[0;0m INFO 12-04 14:30:09 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='WeiboAI/VibeThinker-1.5B', speculative_config=None, tokenizer='WeiboAI/VibeThinker-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=WeiboAI/VibeThinker-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4011039)[0;0m ERROR 12-04 14:30:10 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8
[1;36m(EngineCore_DP0 pid=4011039)[0;0m INFO 12-04 14:30:11 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.8:33539 backend=nccl
[W1204 14:30:11.719484349 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-23-0.rc.tch.harvard.edu]:33539 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4011039)[0;0m INFO 12-04 14:30:11 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4011039)[0;0m ERROR 12-04 14:30:12 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4011039)[0;0m ERROR 12-04 14:30:12 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4011039)[0;0m ERROR 12-04 14:30:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4011039)[0;0m ERROR 12-04 14:30:12 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4011039)[0;0m ERROR 12-04 14:30:12 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4011039)[0;0m ERROR 12-04 14:30:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4011039)[0;0m ERROR 12-04 14:30:12 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4011039)[0;0m ERROR 12-04 14:30:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=4011039)[0;0m ERROR 12-04 14:30:12 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=4011039)[0;0m ERROR 12-04 14:30:12 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4011039)[0;0m ERROR 12-04 14:30:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=4011039)[0;0m ERROR 12-04 14:30:12 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=4011039)[0;0m ERROR 12-04 14:30:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=4011039)[0;0m ERROR 12-04 14:30:12 [core.py:842]     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=4011039)[0;0m ERROR 12-04 14:30:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=4011039)[0;0m ERROR 12-04 14:30:12 [core.py:842]     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=4011039)[0;0m ERROR 12-04 14:30:12 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4011039)[0;0m ERROR 12-04 14:30:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=4011039)[0;0m ERROR 12-04 14:30:12 [core.py:842]     raise ValueError(
[1;36m(EngineCore_DP0 pid=4011039)[0;0m ERROR 12-04 14:30:12 [core.py:842] ValueError: Free memory on device (7.03/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[1;36m(EngineCore_DP0 pid=4011039)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=4011039)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4011039)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=4011039)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=4011039)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=4011039)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=4011039)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=4011039)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=4011039)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4011039)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4011039)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4011039)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4011039)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=4011039)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=4011039)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=4011039)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4011039)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=4011039)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=4011039)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=4011039)[0;0m     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=4011039)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=4011039)[0;0m     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=4011039)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4011039)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=4011039)[0;0m     raise ValueError(
[1;36m(EngineCore_DP0 pid=4011039)[0;0m ValueError: Free memory on device (7.03/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W1204 14:30:12.785856743 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:33<00:00, 33.13s/it, est. speed input: 23.60 toks/s, output: 112.42 toks/s]Processed prompts: 100%|██████████| 1/1 [00:33<00:00, 33.13s/it, est. speed input: 23.60 toks/s, output: 112.42 toks/s]Processed prompts: 100%|██████████| 1/1 [00:33<00:00, 33.14s/it, est. speed input: 23.60 toks/s, output: 112.42 toks/s]
Agent 2 response: Joe has $50 to spend on a shirt that costs $25 and a pair of shorts that costs $35. The total origin...

--- Problem 9/20, Round 2, Agent 3/3 ---
Agent 3 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 403.30it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:14<00:00, 14.64s/it, est. speed input: 53.81 toks/s, output: 114.59 toks/s]Processed prompts: 100%|██████████| 1/1 [00:14<00:00, 14.64s/it, est. speed input: 53.81 toks/s, output: 114.59 toks/s]Processed prompts: 100%|██████████| 1/1 [00:14<00:00, 14.64s/it, est. speed input: 53.81 toks/s, output: 114.59 toks/s]
Agent 3 response: The total original price of the shirt and shorts is $25 + $35 = $60.  
With a 30% off sale, the disc...

--- Problem 9/20, Round 3, Agent 1/3 ---
Agent 1 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 243.61it/s]
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: WeiboAI/VibeThinker-1.5B (backend=vllm)
[ModelCache] Using max_model_len=131072 for WeiboAI/VibeThinker-1.5B
[ModelCache] Detected 1 GPU(s), 44.3GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 14:30:33 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'WeiboAI/VibeThinker-1.5B'}
INFO 12-04 14:30:34 [model.py:631] Resolved architecture: Qwen2ForCausalLM
WARNING 12-04 14:30:34 [model.py:1921] Your device 'Quadro RTX 8000' (with compute capability 7.5) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.
WARNING 12-04 14:30:34 [model.py:1971] Casting torch.bfloat16 to torch.float16.
INFO 12-04 14:30:34 [model.py:1745] Using max model len 131072
INFO 12-04 14:30:34 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:28<00:00, 28.57s/it, est. speed input: 48.45 toks/s, output: 112.69 toks/s]Processed prompts: 100%|██████████| 1/1 [00:28<00:00, 28.57s/it, est. speed input: 48.45 toks/s, output: 112.69 toks/s]Processed prompts: 100%|██████████| 1/1 [00:28<00:00, 28.57s/it, est. speed input: 48.45 toks/s, output: 112.69 toks/s]
Agent 1 response: Joe has a total of $50 to spend on the shirt and shorts. The shirt costs $25 and the shorts cost $35...

--- Problem 9/20, Round 3, Agent 2/3 ---
Agent 2 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 226.74it/s]
[1;36m(EngineCore_DP0 pid=4011144)[0;0m INFO 12-04 14:31:08 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='WeiboAI/VibeThinker-1.5B', speculative_config=None, tokenizer='WeiboAI/VibeThinker-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=WeiboAI/VibeThinker-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4011144)[0;0m ERROR 12-04 14:31:09 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8
[1;36m(EngineCore_DP0 pid=4011144)[0;0m INFO 12-04 14:31:11 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.8:53625 backend=nccl
[W1204 14:31:11.946372383 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-23-0.rc.tch.harvard.edu]:53625 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4011144)[0;0m INFO 12-04 14:31:11 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4011144)[0;0m ERROR 12-04 14:31:11 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4011144)[0;0m ERROR 12-04 14:31:11 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4011144)[0;0m ERROR 12-04 14:31:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4011144)[0;0m ERROR 12-04 14:31:11 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4011144)[0;0m ERROR 12-04 14:31:11 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4011144)[0;0m ERROR 12-04 14:31:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4011144)[0;0m ERROR 12-04 14:31:11 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4011144)[0;0m ERROR 12-04 14:31:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=4011144)[0;0m ERROR 12-04 14:31:11 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=4011144)[0;0m ERROR 12-04 14:31:11 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4011144)[0;0m ERROR 12-04 14:31:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=4011144)[0;0m ERROR 12-04 14:31:11 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=4011144)[0;0m ERROR 12-04 14:31:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=4011144)[0;0m ERROR 12-04 14:31:11 [core.py:842]     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=4011144)[0;0m ERROR 12-04 14:31:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=4011144)[0;0m ERROR 12-04 14:31:11 [core.py:842]     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=4011144)[0;0m ERROR 12-04 14:31:11 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4011144)[0;0m ERROR 12-04 14:31:11 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=4011144)[0;0m ERROR 12-04 14:31:11 [core.py:842]     raise ValueError(
[1;36m(EngineCore_DP0 pid=4011144)[0;0m ERROR 12-04 14:31:11 [core.py:842] ValueError: Free memory on device (7.03/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[1;36m(EngineCore_DP0 pid=4011144)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=4011144)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4011144)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=4011144)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=4011144)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=4011144)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=4011144)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=4011144)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=4011144)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4011144)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4011144)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4011144)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4011144)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=4011144)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=4011144)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=4011144)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4011144)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=4011144)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=4011144)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=4011144)[0;0m     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=4011144)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=4011144)[0;0m     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=4011144)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4011144)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=4011144)[0;0m     raise ValueError(
[1;36m(EngineCore_DP0 pid=4011144)[0;0m ValueError: Free memory on device (7.03/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W1204 14:31:12.025910766 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:32<00:00, 32.14s/it, est. speed input: 42.97 toks/s, output: 113.23 toks/s]Processed prompts: 100%|██████████| 1/1 [00:32<00:00, 32.14s/it, est. speed input: 42.97 toks/s, output: 113.23 toks/s]Processed prompts: 100%|██████████| 1/1 [00:32<00:00, 32.14s/it, est. speed input: 42.97 toks/s, output: 113.23 toks/s]
Agent 2 response: Joe has a total of $50 to spend on the shirt and shorts. The original price of the shirt is $25 and ...

--- Problem 9/20, Round 3, Agent 3/3 ---
Agent 3 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 227.83it/s]
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: WeiboAI/VibeThinker-1.5B (backend=vllm)
[ModelCache] Using max_model_len=131072 for WeiboAI/VibeThinker-1.5B
[ModelCache] Detected 1 GPU(s), 44.3GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 14:31:33 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'WeiboAI/VibeThinker-1.5B'}
INFO 12-04 14:31:33 [model.py:631] Resolved architecture: Qwen2ForCausalLM
WARNING 12-04 14:31:33 [model.py:1921] Your device 'Quadro RTX 8000' (with compute capability 7.5) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.
WARNING 12-04 14:31:33 [model.py:1971] Casting torch.bfloat16 to torch.float16.
INFO 12-04 14:31:33 [model.py:1745] Using max model len 131072
INFO 12-04 14:31:33 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:27<00:00, 27.53s/it, est. speed input: 50.38 toks/s, output: 113.07 toks/s]Processed prompts: 100%|██████████| 1/1 [00:27<00:00, 27.53s/it, est. speed input: 50.38 toks/s, output: 113.07 toks/s]Processed prompts: 100%|██████████| 1/1 [00:27<00:00, 27.53s/it, est. speed input: 50.38 toks/s, output: 113.07 toks/s]
Agent 3 response: The shirt costs $25 and the shorts cost $35, so the total original price is $25 + $35 = $60.  
With ...

Running accuracy: 1.000 ± 0.000

--- Problem 10/20, Round 1, Agent 1/3 ---
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1127.80it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.30s/it, est. speed input: 21.49 toks/s, output: 114.83 toks/s]Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.30s/it, est. speed input: 21.49 toks/s, output: 114.83 toks/s]Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.30s/it, est. speed input: 21.49 toks/s, output: 114.83 toks/s]
Agent 1 response: The glass of milk contains 8 ounces, and John drinks 2 glasses. This amounts to  \(2 \times 8 = 16\)...

--- Problem 10/20, Round 1, Agent 2/3 ---
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1146.30it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:04<00:00,  4.83s/it, est. speed input: 22.96 toks/s, output: 114.39 toks/s]Processed prompts: 100%|██████████| 1/1 [00:04<00:00,  4.83s/it, est. speed input: 22.96 toks/s, output: 114.39 toks/s]Processed prompts: 100%|██████████| 1/1 [00:04<00:00,  4.83s/it, est. speed input: 22.96 toks/s, output: 114.39 toks/s]
Agent 2 response: John drinks 2 glasses of milk, with each glass being 8 ounces. Therefore, the total amount of milk h...

--- Problem 10/20, Round 1, Agent 3/3 ---
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 729.70it/s]
[1;36m(EngineCore_DP0 pid=4011405)[0;0m INFO 12-04 14:32:15 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='WeiboAI/VibeThinker-1.5B', speculative_config=None, tokenizer='WeiboAI/VibeThinker-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=WeiboAI/VibeThinker-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:08<00:00,  8.31s/it, est. speed input: 14.09 toks/s, output: 113.90 toks/s]Processed prompts: 100%|██████████| 1/1 [00:08<00:00,  8.31s/it, est. speed input: 14.09 toks/s, output: 113.90 toks/s]Processed prompts: 100%|██████████| 1/1 [00:08<00:00,  8.31s/it, est. speed input: 14.09 toks/s, output: 113.90 toks/s]
Agent 3 response: John drinks 2 glasses of milk, and each glass is 8 ounces. Therefore, the total amount of milk he co...

--- Problem 10/20, Round 2, Agent 1/3 ---
Agent 1 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 454.57it/s]
[1;36m(EngineCore_DP0 pid=4011405)[0;0m ERROR 12-04 14:32:17 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8
[1;36m(EngineCore_DP0 pid=4011405)[0;0m INFO 12-04 14:32:19 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.8:48323 backend=nccl
[W1204 14:32:19.807222935 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-23-0.rc.tch.harvard.edu]:48323 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4011405)[0;0m INFO 12-04 14:32:19 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4011405)[0;0m ERROR 12-04 14:32:20 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4011405)[0;0m ERROR 12-04 14:32:20 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4011405)[0;0m ERROR 12-04 14:32:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4011405)[0;0m ERROR 12-04 14:32:20 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4011405)[0;0m ERROR 12-04 14:32:20 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4011405)[0;0m ERROR 12-04 14:32:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4011405)[0;0m ERROR 12-04 14:32:20 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4011405)[0;0m ERROR 12-04 14:32:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=4011405)[0;0m ERROR 12-04 14:32:20 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=4011405)[0;0m ERROR 12-04 14:32:20 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4011405)[0;0m ERROR 12-04 14:32:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=4011405)[0;0m ERROR 12-04 14:32:20 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=4011405)[0;0m ERROR 12-04 14:32:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=4011405)[0;0m ERROR 12-04 14:32:20 [core.py:842]     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=4011405)[0;0m ERROR 12-04 14:32:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=4011405)[0;0m ERROR 12-04 14:32:20 [core.py:842]     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=4011405)[0;0m ERROR 12-04 14:32:20 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4011405)[0;0m ERROR 12-04 14:32:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=4011405)[0;0m ERROR 12-04 14:32:20 [core.py:842]     raise ValueError(
[1;36m(EngineCore_DP0 pid=4011405)[0;0m ERROR 12-04 14:32:20 [core.py:842] ValueError: Free memory on device (7.03/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[1;36m(EngineCore_DP0 pid=4011405)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=4011405)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4011405)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=4011405)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=4011405)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=4011405)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=4011405)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=4011405)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=4011405)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4011405)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4011405)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4011405)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4011405)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=4011405)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=4011405)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=4011405)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4011405)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=4011405)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=4011405)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=4011405)[0;0m     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=4011405)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=4011405)[0;0m     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=4011405)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4011405)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=4011405)[0;0m     raise ValueError(
[1;36m(EngineCore_DP0 pid=4011405)[0;0m ValueError: Free memory on device (7.03/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W1204 14:32:21.888878100 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:11<00:00, 11.77s/it, est. speed input: 41.65 toks/s, output: 113.89 toks/s]Processed prompts: 100%|██████████| 1/1 [00:11<00:00, 11.77s/it, est. speed input: 41.65 toks/s, output: 113.89 toks/s]Processed prompts: 100%|██████████| 1/1 [00:11<00:00, 11.77s/it, est. speed input: 41.65 toks/s, output: 113.89 toks/s]
Agent 1 response: To determine how many calories John consumed, we start by calculating the total amount of milk he dr...

--- Problem 10/20, Round 2, Agent 2/3 ---
Agent 2 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 556.94it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:08<00:00,  8.02s/it, est. speed input: 60.70 toks/s, output: 115.03 toks/s]Processed prompts: 100%|██████████| 1/1 [00:08<00:00,  8.02s/it, est. speed input: 60.70 toks/s, output: 115.03 toks/s]Processed prompts: 100%|██████████| 1/1 [00:08<00:00,  8.02s/it, est. speed input: 60.70 toks/s, output: 115.03 toks/s]
Agent 2 response: John consumes 2 glasses of milk, each being 8 ounces. The total volume of milk he drinks is:  
\(2 \...

--- Problem 10/20, Round 2, Agent 3/3 ---
Agent 3 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 542.04it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:04<00:00,  4.19s/it, est. speed input: 117.57 toks/s, output: 114.94 toks/s]Processed prompts: 100%|██████████| 1/1 [00:04<00:00,  4.19s/it, est. speed input: 117.57 toks/s, output: 114.94 toks/s]Processed prompts: 100%|██████████| 1/1 [00:04<00:00,  4.19s/it, est. speed input: 117.57 toks/s, output: 114.94 toks/s]
Agent 3 response: John drinks 2 glasses of milk, each containing 8 ounces. The total amount of milk he consumed is:
\[...

--- Problem 10/20, Round 3, Agent 1/3 ---
Agent 1 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 337.62it/s]
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: WeiboAI/VibeThinker-1.5B (backend=vllm)
[ModelCache] Using max_model_len=131072 for WeiboAI/VibeThinker-1.5B
[ModelCache] Detected 1 GPU(s), 44.3GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 14:32:42 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'WeiboAI/VibeThinker-1.5B'}
INFO 12-04 14:32:42 [model.py:631] Resolved architecture: Qwen2ForCausalLM
WARNING 12-04 14:32:42 [model.py:1921] Your device 'Quadro RTX 8000' (with compute capability 7.5) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.
WARNING 12-04 14:32:42 [model.py:1971] Casting torch.bfloat16 to torch.float16.
INFO 12-04 14:32:42 [model.py:1745] Using max model len 131072
INFO 12-04 14:32:42 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.03s/it, est. speed input: 183.28 toks/s, output: 113.31 toks/s]Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.03s/it, est. speed input: 183.28 toks/s, output: 113.31 toks/s]Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.03s/it, est. speed input: 183.28 toks/s, output: 113.31 toks/s]
Agent 1 response: To determine the total calories John consumed, first calculate the total amount of milk he drank. Ea...

--- Problem 10/20, Round 3, Agent 2/3 ---
Agent 2 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 308.18it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:04<00:00,  4.67s/it, est. speed input: 196.70 toks/s, output: 113.01 toks/s]Processed prompts: 100%|██████████| 1/1 [00:04<00:00,  4.67s/it, est. speed input: 196.70 toks/s, output: 113.01 toks/s]Processed prompts: 100%|██████████| 1/1 [00:04<00:00,  4.67s/it, est. speed input: 196.70 toks/s, output: 113.01 toks/s]
Agent 2 response: John consumes 2 glasses of milk, each being 8 ounces. The total volume of milk he drank is \(2 \time...

--- Problem 10/20, Round 3, Agent 3/3 ---
Agent 3 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 326.86it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.61s/it, est. speed input: 164.78 toks/s, output: 112.58 toks/s]Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.61s/it, est. speed input: 164.78 toks/s, output: 112.58 toks/s]Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.61s/it, est. speed input: 164.78 toks/s, output: 112.58 toks/s]
Agent 3 response: John drinks 2 glasses of milk, each containing 8 ounces. The total amount of milk he consumed is:
\[...

Running accuracy: 1.000 ± 0.000

--- Problem 11/20, Round 1, Agent 1/3 ---
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1013.36it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:03<00:00,  3.84s/it, est. speed input: 32.06 toks/s, output: 114.42 toks/s]Processed prompts: 100%|██████████| 1/1 [00:03<00:00,  3.84s/it, est. speed input: 32.06 toks/s, output: 114.42 toks/s]Processed prompts: 100%|██████████| 1/1 [00:03<00:00,  3.84s/it, est. speed input: 32.06 toks/s, output: 114.42 toks/s]
Agent 1 response: Johnny started with an allowance of $20 and added $10, making a total of $30. This sum was invested,...

--- Problem 11/20, Round 1, Agent 2/3 ---
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1118.48it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:03<00:00,  3.17s/it, est. speed input: 37.84 toks/s, output: 114.47 toks/s]Processed prompts: 100%|██████████| 1/1 [00:03<00:00,  3.17s/it, est. speed input: 37.84 toks/s, output: 114.47 toks/s]Processed prompts: 100%|██████████| 1/1 [00:03<00:00,  3.17s/it, est. speed input: 37.84 toks/s, output: 114.47 toks/s]
Agent 2 response: Johnny initially had $20 and added $10, resulting in a total of $30. This sum tripled over one year,...

--- Problem 11/20, Round 1, Agent 3/3 ---
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 983.42it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:04<00:00,  4.60s/it, est. speed input: 27.41 toks/s, output: 113.78 toks/s]Processed prompts: 100%|██████████| 1/1 [00:04<00:00,  4.60s/it, est. speed input: 27.41 toks/s, output: 113.78 toks/s]Processed prompts: 100%|██████████| 1/1 [00:04<00:00,  4.60s/it, est. speed input: 27.41 toks/s, output: 113.78 toks/s]
Agent 3 response: Johnny started with an allowance of $20 and added $10, resulting in a total of $30. This sum was the...

--- Problem 11/20, Round 2, Agent 1/3 ---
Agent 1 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 384.06it/s]
[1;36m(EngineCore_DP0 pid=4011994)[0;0m INFO 12-04 14:33:10 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='WeiboAI/VibeThinker-1.5B', speculative_config=None, tokenizer='WeiboAI/VibeThinker-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=WeiboAI/VibeThinker-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4011994)[0;0m ERROR 12-04 14:33:11 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8
[1;36m(EngineCore_DP0 pid=4011994)[0;0m INFO 12-04 14:33:11 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.8:49227 backend=nccl
[W1204 14:33:11.810745689 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-23-0.rc.tch.harvard.edu]:49227 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4011994)[0;0m INFO 12-04 14:33:11 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4011994)[0;0m ERROR 12-04 14:33:12 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4011994)[0;0m ERROR 12-04 14:33:12 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4011994)[0;0m ERROR 12-04 14:33:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4011994)[0;0m ERROR 12-04 14:33:12 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4011994)[0;0m ERROR 12-04 14:33:12 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4011994)[0;0m ERROR 12-04 14:33:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4011994)[0;0m ERROR 12-04 14:33:12 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4011994)[0;0m ERROR 12-04 14:33:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=4011994)[0;0m ERROR 12-04 14:33:12 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=4011994)[0;0m ERROR 12-04 14:33:12 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4011994)[0;0m ERROR 12-04 14:33:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=4011994)[0;0m ERROR 12-04 14:33:12 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=4011994)[0;0m ERROR 12-04 14:33:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=4011994)[0;0m ERROR 12-04 14:33:12 [core.py:842]     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=4011994)[0;0m ERROR 12-04 14:33:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=4011994)[0;0m ERROR 12-04 14:33:12 [core.py:842]     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=4011994)[0;0m ERROR 12-04 14:33:12 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4011994)[0;0m ERROR 12-04 14:33:12 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=4011994)[0;0m ERROR 12-04 14:33:12 [core.py:842]     raise ValueError(
[1;36m(EngineCore_DP0 pid=4011994)[0;0m ERROR 12-04 14:33:12 [core.py:842] ValueError: Free memory on device (7.03/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[1;36m(EngineCore_DP0 pid=4011994)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=4011994)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4011994)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=4011994)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=4011994)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=4011994)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=4011994)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=4011994)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=4011994)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4011994)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4011994)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4011994)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4011994)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=4011994)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=4011994)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=4011994)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4011994)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=4011994)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=4011994)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=4011994)[0;0m     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=4011994)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=4011994)[0;0m     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=4011994)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4011994)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=4011994)[0;0m     raise ValueError(
[1;36m(EngineCore_DP0 pid=4011994)[0;0m ValueError: Free memory on device (7.03/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W1204 14:33:12.817312437 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:15<00:00, 15.69s/it, est. speed input: 28.80 toks/s, output: 113.68 toks/s]Processed prompts: 100%|██████████| 1/1 [00:15<00:00, 15.69s/it, est. speed input: 28.80 toks/s, output: 113.68 toks/s]Processed prompts: 100%|██████████| 1/1 [00:15<00:00, 15.69s/it, est. speed input: 28.80 toks/s, output: 113.68 toks/s]
Agent 1 response: Johnny began with an allowance of $20 and added an extra $10, resulting in a total sum of $30. This ...

--- Problem 11/20, Round 2, Agent 2/3 ---
Agent 2 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 612.04it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:06<00:00,  6.19s/it, est. speed input: 72.58 toks/s, output: 115.09 toks/s]Processed prompts: 100%|██████████| 1/1 [00:06<00:00,  6.19s/it, est. speed input: 72.58 toks/s, output: 115.09 toks/s]Processed prompts: 100%|██████████| 1/1 [00:06<00:00,  6.19s/it, est. speed input: 72.58 toks/s, output: 115.09 toks/s]
Agent 2 response: Johnny started with an allowance of $20 and added $10, resulting in a total of $30. This sum was the...

--- Problem 11/20, Round 2, Agent 3/3 ---
Agent 3 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 614.91it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:03<00:00,  3.64s/it, est. speed input: 124.83 toks/s, output: 114.95 toks/s]Processed prompts: 100%|██████████| 1/1 [00:03<00:00,  3.64s/it, est. speed input: 124.83 toks/s, output: 114.95 toks/s]Processed prompts: 100%|██████████| 1/1 [00:03<00:00,  3.65s/it, est. speed input: 124.83 toks/s, output: 114.95 toks/s]
Agent 3 response: Johnny began with an allowance of $20 and added $10, resulting in a total of $30. This amount was in...

--- Problem 11/20, Round 3, Agent 1/3 ---
Agent 1 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 365.33it/s]
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: WeiboAI/VibeThinker-1.5B (backend=vllm)
[ModelCache] Using max_model_len=131072 for WeiboAI/VibeThinker-1.5B
[ModelCache] Detected 1 GPU(s), 44.3GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 14:33:33 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'WeiboAI/VibeThinker-1.5B'}
INFO 12-04 14:33:34 [model.py:631] Resolved architecture: Qwen2ForCausalLM
WARNING 12-04 14:33:34 [model.py:1921] Your device 'Quadro RTX 8000' (with compute capability 7.5) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.
WARNING 12-04 14:33:34 [model.py:1971] Casting torch.bfloat16 to torch.float16.
INFO 12-04 14:33:34 [model.py:1745] Using max model len 131072
INFO 12-04 14:33:34 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.78s/it, est. speed input: 142.39 toks/s, output: 112.80 toks/s]Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.78s/it, est. speed input: 142.39 toks/s, output: 112.80 toks/s]Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.78s/it, est. speed input: 142.39 toks/s, output: 112.80 toks/s]
Agent 1 response: Johnny started with an allowance of $20 and added $10, resulting in a total of $30. This sum was inv...

--- Problem 11/20, Round 3, Agent 2/3 ---
Agent 2 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 338.69it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:04<00:00,  4.27s/it, est. speed input: 192.13 toks/s, output: 113.17 toks/s]Processed prompts: 100%|██████████| 1/1 [00:04<00:00,  4.27s/it, est. speed input: 192.13 toks/s, output: 113.17 toks/s]Processed prompts: 100%|██████████| 1/1 [00:04<00:00,  4.27s/it, est. speed input: 192.13 toks/s, output: 113.17 toks/s]
Agent 2 response: Johnny initially had $20 and added $10, resulting in a total of $30. This sum was then invested, whi...

--- Problem 11/20, Round 3, Agent 3/3 ---
Agent 3 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 331.43it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:03<00:00,  3.31s/it, est. speed input: 249.18 toks/s, output: 112.22 toks/s]Processed prompts: 100%|██████████| 1/1 [00:03<00:00,  3.31s/it, est. speed input: 249.18 toks/s, output: 112.22 toks/s]Processed prompts: 100%|██████████| 1/1 [00:03<00:00,  3.32s/it, est. speed input: 249.18 toks/s, output: 112.22 toks/s]
Agent 3 response: Johnny started with an allowance of $20 and added $10, resulting in a total of $30. This amount was ...

Running accuracy: 1.000 ± 0.000

--- Problem 12/20, Round 1, Agent 1/3 ---
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 947.22it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.21s/it, est. speed input: 17.88 toks/s, output: 113.38 toks/s]Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.21s/it, est. speed input: 17.88 toks/s, output: 113.38 toks/s]Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.21s/it, est. speed input: 17.88 toks/s, output: 113.38 toks/s]
Agent 1 response: The beanstalk was 3 inches tall after the first week. In the second week, it doubled in height, so \...

--- Problem 12/20, Round 1, Agent 2/3 ---
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1116.69it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.82s/it, est. speed input: 21.65 toks/s, output: 111.53 toks/s]Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.82s/it, est. speed input: 21.65 toks/s, output: 111.53 toks/s]Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.82s/it, est. speed input: 21.65 toks/s, output: 111.53 toks/s]
Agent 2 response: The beanstalk started at 3 inches after the first week. In the second week, it doubled in height:  
...

--- Problem 12/20, Round 1, Agent 3/3 ---
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1103.18it/s]
[1;36m(EngineCore_DP0 pid=4012469)[0;0m INFO 12-04 14:33:59 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='WeiboAI/VibeThinker-1.5B', speculative_config=None, tokenizer='WeiboAI/VibeThinker-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=WeiboAI/VibeThinker-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4012469)[0;0m ERROR 12-04 14:34:01 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8
[1;36m(EngineCore_DP0 pid=4012469)[0;0m INFO 12-04 14:34:03 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.8:53991 backend=nccl
[W1204 14:34:03.120024786 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-23-0.rc.tch.harvard.edu]:53991 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4012469)[0;0m INFO 12-04 14:34:03 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4012469)[0;0m ERROR 12-04 14:34:03 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4012469)[0;0m ERROR 12-04 14:34:03 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4012469)[0;0m ERROR 12-04 14:34:03 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4012469)[0;0m ERROR 12-04 14:34:03 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4012469)[0;0m ERROR 12-04 14:34:03 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4012469)[0;0m ERROR 12-04 14:34:03 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4012469)[0;0m ERROR 12-04 14:34:03 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4012469)[0;0m ERROR 12-04 14:34:03 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=4012469)[0;0m ERROR 12-04 14:34:03 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=4012469)[0;0m ERROR 12-04 14:34:03 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4012469)[0;0m ERROR 12-04 14:34:03 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=4012469)[0;0m ERROR 12-04 14:34:03 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=4012469)[0;0m ERROR 12-04 14:34:03 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=4012469)[0;0m ERROR 12-04 14:34:03 [core.py:842]     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=4012469)[0;0m ERROR 12-04 14:34:03 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=4012469)[0;0m ERROR 12-04 14:34:03 [core.py:842]     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=4012469)[0;0m ERROR 12-04 14:34:03 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4012469)[0;0m ERROR 12-04 14:34:03 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=4012469)[0;0m ERROR 12-04 14:34:03 [core.py:842]     raise ValueError(
[1;36m(EngineCore_DP0 pid=4012469)[0;0m ERROR 12-04 14:34:03 [core.py:842] ValueError: Free memory on device (7.03/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[1;36m(EngineCore_DP0 pid=4012469)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=4012469)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4012469)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=4012469)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=4012469)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=4012469)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=4012469)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=4012469)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=4012469)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4012469)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4012469)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4012469)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4012469)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=4012469)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=4012469)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=4012469)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4012469)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=4012469)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=4012469)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=4012469)[0;0m     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=4012469)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=4012469)[0;0m     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=4012469)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4012469)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=4012469)[0;0m     raise ValueError(
[1;36m(EngineCore_DP0 pid=4012469)[0;0m ValueError: Free memory on device (7.03/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W1204 14:34:04.239006484 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.95s/it, est. speed input: 22.20 toks/s, output: 110.81 toks/s]Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.95s/it, est. speed input: 22.20 toks/s, output: 110.81 toks/s]Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.95s/it, est. speed input: 22.20 toks/s, output: 110.81 toks/s]
Agent 3 response: The beanstalk was 3 inches tall after the first week. In the second week, it doubled in height, resu...

--- Problem 12/20, Round 2, Agent 1/3 ---
Agent 1 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 437.59it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:03<00:00,  3.41s/it, est. speed input: 139.21 toks/s, output: 114.01 toks/s]Processed prompts: 100%|██████████| 1/1 [00:03<00:00,  3.41s/it, est. speed input: 139.21 toks/s, output: 114.01 toks/s]Processed prompts: 100%|██████████| 1/1 [00:03<00:00,  3.41s/it, est. speed input: 139.21 toks/s, output: 114.01 toks/s]
Agent 1 response: The beanstalk was 3 inches tall after the first week. In the second week, it doubled in height:  
\(...

--- Problem 12/20, Round 2, Agent 2/3 ---
Agent 2 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 543.30it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.49s/it, est. speed input: 189.22 toks/s, output: 113.45 toks/s]Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.49s/it, est. speed input: 189.22 toks/s, output: 113.45 toks/s]Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.49s/it, est. speed input: 189.22 toks/s, output: 113.45 toks/s]
Agent 2 response: The beanstalk was 3 inches tall after the first week. Doubling in the second week gives \(3 \times 2...

--- Problem 12/20, Round 2, Agent 3/3 ---
Agent 3 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 551.52it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.81s/it, est. speed input: 61.18 toks/s, output: 113.02 toks/s]Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.81s/it, est. speed input: 61.18 toks/s, output: 113.02 toks/s]Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.81s/it, est. speed input: 61.18 toks/s, output: 113.02 toks/s]
Agent 3 response: The beanstalk was 3 inches tall after the first week. In the second week, it doubled in height, resu...

--- Problem 12/20, Round 3, Agent 1/3 ---
Agent 1 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 381.13it/s]
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: WeiboAI/VibeThinker-1.5B (backend=vllm)
[ModelCache] Using max_model_len=131072 for WeiboAI/VibeThinker-1.5B
[ModelCache] Detected 1 GPU(s), 44.3GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 14:34:25 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'WeiboAI/VibeThinker-1.5B'}
INFO 12-04 14:34:25 [model.py:631] Resolved architecture: Qwen2ForCausalLM
WARNING 12-04 14:34:25 [model.py:1921] Your device 'Quadro RTX 8000' (with compute capability 7.5) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.
WARNING 12-04 14:34:25 [model.py:1971] Casting torch.bfloat16 to torch.float16.
INFO 12-04 14:34:25 [model.py:1745] Using max model len 131072
INFO 12-04 14:34:25 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:12<00:00, 12.28s/it, est. speed input: 66.29 toks/s, output: 113.03 toks/s]Processed prompts: 100%|██████████| 1/1 [00:12<00:00, 12.28s/it, est. speed input: 66.29 toks/s, output: 113.03 toks/s]Processed prompts: 100%|██████████| 1/1 [00:12<00:00, 12.28s/it, est. speed input: 66.29 toks/s, output: 113.03 toks/s]
Agent 1 response: The beanstalk was 3 inches tall after the first week.  
In the second week, it doubled in height: \(...

--- Problem 12/20, Round 3, Agent 2/3 ---
Agent 2 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 367.28it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:19<00:00, 19.08s/it, est. speed input: 42.50 toks/s, output: 113.13 toks/s]Processed prompts: 100%|██████████| 1/1 [00:19<00:00, 19.08s/it, est. speed input: 42.50 toks/s, output: 113.13 toks/s]Processed prompts: 100%|██████████| 1/1 [00:19<00:00, 19.08s/it, est. speed input: 42.50 toks/s, output: 113.13 toks/s]
Agent 2 response: The beanstalk is 3 inches tall after the first week.  
After the second week, it doubles in height: ...

--- Problem 12/20, Round 3, Agent 3/3 ---
Agent 3 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 331.49it/s]
[1;36m(EngineCore_DP0 pid=4012892)[0;0m INFO 12-04 14:34:51 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='WeiboAI/VibeThinker-1.5B', speculative_config=None, tokenizer='WeiboAI/VibeThinker-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=WeiboAI/VibeThinker-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4012892)[0;0m ERROR 12-04 14:34:52 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8
[1;36m(EngineCore_DP0 pid=4012892)[0;0m INFO 12-04 14:34:53 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.8:43779 backend=nccl
[W1204 14:34:53.772815962 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-23-0.rc.tch.harvard.edu]:43779 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4012892)[0;0m INFO 12-04 14:34:53 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4012892)[0;0m ERROR 12-04 14:34:54 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4012892)[0;0m ERROR 12-04 14:34:54 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4012892)[0;0m ERROR 12-04 14:34:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4012892)[0;0m ERROR 12-04 14:34:54 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4012892)[0;0m ERROR 12-04 14:34:54 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4012892)[0;0m ERROR 12-04 14:34:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4012892)[0;0m ERROR 12-04 14:34:54 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4012892)[0;0m ERROR 12-04 14:34:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=4012892)[0;0m ERROR 12-04 14:34:54 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=4012892)[0;0m ERROR 12-04 14:34:54 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4012892)[0;0m ERROR 12-04 14:34:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=4012892)[0;0m ERROR 12-04 14:34:54 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=4012892)[0;0m ERROR 12-04 14:34:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=4012892)[0;0m ERROR 12-04 14:34:54 [core.py:842]     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=4012892)[0;0m ERROR 12-04 14:34:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=4012892)[0;0m ERROR 12-04 14:34:54 [core.py:842]     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=4012892)[0;0m ERROR 12-04 14:34:54 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4012892)[0;0m ERROR 12-04 14:34:54 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=4012892)[0;0m ERROR 12-04 14:34:54 [core.py:842]     raise ValueError(
[1;36m(EngineCore_DP0 pid=4012892)[0;0m ERROR 12-04 14:34:54 [core.py:842] ValueError: Free memory on device (7.03/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[1;36m(EngineCore_DP0 pid=4012892)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=4012892)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4012892)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=4012892)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=4012892)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=4012892)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=4012892)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=4012892)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=4012892)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4012892)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4012892)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4012892)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4012892)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=4012892)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=4012892)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=4012892)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4012892)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=4012892)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=4012892)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=4012892)[0;0m     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=4012892)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=4012892)[0;0m     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=4012892)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4012892)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=4012892)[0;0m     raise ValueError(
[1;36m(EngineCore_DP0 pid=4012892)[0;0m ValueError: Free memory on device (7.03/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W1204 14:34:54.786940536 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:14<00:00, 14.47s/it, est. speed input: 56.46 toks/s, output: 113.81 toks/s]Processed prompts: 100%|██████████| 1/1 [00:14<00:00, 14.47s/it, est. speed input: 56.46 toks/s, output: 113.81 toks/s]Processed prompts: 100%|██████████| 1/1 [00:14<00:00, 14.47s/it, est. speed input: 56.46 toks/s, output: 113.81 toks/s]
Agent 3 response: The beanstalk was 3 inches tall after the first week.  
In the second week, it doubled in height, so...

Running accuracy: 1.000 ± 0.000

--- Problem 13/20, Round 1, Agent 1/3 ---
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 980.66it/s]
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: WeiboAI/VibeThinker-1.5B (backend=vllm)
[ModelCache] Using max_model_len=131072 for WeiboAI/VibeThinker-1.5B
[ModelCache] Detected 1 GPU(s), 44.3GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 14:35:15 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'WeiboAI/VibeThinker-1.5B'}
INFO 12-04 14:35:16 [model.py:631] Resolved architecture: Qwen2ForCausalLM
WARNING 12-04 14:35:16 [model.py:1921] Your device 'Quadro RTX 8000' (with compute capability 7.5) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.
WARNING 12-04 14:35:16 [model.py:1971] Casting torch.bfloat16 to torch.float16.
INFO 12-04 14:35:16 [model.py:1745] Using max model len 131072
INFO 12-04 14:35:16 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:31<00:00, 31.57s/it, est. speed input: 4.97 toks/s, output: 113.59 toks/s]Processed prompts: 100%|██████████| 1/1 [00:31<00:00, 31.57s/it, est. speed input: 4.97 toks/s, output: 113.59 toks/s]Processed prompts: 100%|██████████| 1/1 [00:31<00:00, 31.57s/it, est. speed input: 4.97 toks/s, output: 113.59 toks/s]
Agent 1 response: Christina recorded her mood over 30 days, with 12 good days, 8 bad days, and 10 neutral days. The fi...

--- Problem 13/20, Round 1, Agent 2/3 ---
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1032.57it/s]
[1;36m(EngineCore_DP0 pid=4013215)[0;0m INFO 12-04 14:35:40 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='WeiboAI/VibeThinker-1.5B', speculative_config=None, tokenizer='WeiboAI/VibeThinker-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=WeiboAI/VibeThinker-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4013215)[0;0m ERROR 12-04 14:35:42 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8
[1;36m(EngineCore_DP0 pid=4013215)[0;0m INFO 12-04 14:35:44 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.8:52005 backend=nccl
[W1204 14:35:44.520818578 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-23-0.rc.tch.harvard.edu]:52005 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4013215)[0;0m INFO 12-04 14:35:44 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4013215)[0;0m ERROR 12-04 14:35:44 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4013215)[0;0m ERROR 12-04 14:35:44 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4013215)[0;0m ERROR 12-04 14:35:44 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4013215)[0;0m ERROR 12-04 14:35:44 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4013215)[0;0m ERROR 12-04 14:35:44 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4013215)[0;0m ERROR 12-04 14:35:44 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4013215)[0;0m ERROR 12-04 14:35:44 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4013215)[0;0m ERROR 12-04 14:35:44 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=4013215)[0;0m ERROR 12-04 14:35:44 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=4013215)[0;0m ERROR 12-04 14:35:44 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4013215)[0;0m ERROR 12-04 14:35:44 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=4013215)[0;0m ERROR 12-04 14:35:44 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=4013215)[0;0m ERROR 12-04 14:35:44 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=4013215)[0;0m ERROR 12-04 14:35:44 [core.py:842]     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=4013215)[0;0m ERROR 12-04 14:35:44 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=4013215)[0;0m ERROR 12-04 14:35:44 [core.py:842]     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=4013215)[0;0m ERROR 12-04 14:35:44 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4013215)[0;0m ERROR 12-04 14:35:44 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=4013215)[0;0m ERROR 12-04 14:35:44 [core.py:842]     raise ValueError(
[1;36m(EngineCore_DP0 pid=4013215)[0;0m ERROR 12-04 14:35:44 [core.py:842] ValueError: Free memory on device (7.03/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[1;36m(EngineCore_DP0 pid=4013215)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=4013215)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4013215)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=4013215)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=4013215)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=4013215)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=4013215)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=4013215)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=4013215)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4013215)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4013215)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4013215)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4013215)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=4013215)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=4013215)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=4013215)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4013215)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=4013215)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=4013215)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=4013215)[0;0m     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=4013215)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=4013215)[0;0m     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=4013215)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4013215)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=4013215)[0;0m     raise ValueError(
[1;36m(EngineCore_DP0 pid=4013215)[0;0m ValueError: Free memory on device (7.03/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W1204 14:35:45.579783261 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: WeiboAI/VibeThinker-1.5B (backend=vllm)
[ModelCache] Using max_model_len=131072 for WeiboAI/VibeThinker-1.5B
[ModelCache] Detected 1 GPU(s), 44.3GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 14:36:06 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'WeiboAI/VibeThinker-1.5B'}
INFO 12-04 14:36:07 [model.py:631] Resolved architecture: Qwen2ForCausalLM
WARNING 12-04 14:36:07 [model.py:1921] Your device 'Quadro RTX 8000' (with compute capability 7.5) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.
WARNING 12-04 14:36:07 [model.py:1971] Casting torch.bfloat16 to torch.float16.
INFO 12-04 14:36:07 [model.py:1745] Using max model len 131072
INFO 12-04 14:36:07 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:44<00:00, 44.05s/it, est. speed input: 3.50 toks/s, output: 113.41 toks/s]Processed prompts: 100%|██████████| 1/1 [00:44<00:00, 44.05s/it, est. speed input: 3.50 toks/s, output: 113.41 toks/s]Processed prompts: 100%|██████████| 1/1 [00:44<00:00, 44.05s/it, est. speed input: 3.50 toks/s, output: 113.41 toks/s]
Agent 2 response: Christina's mood record over 30 days includes 12 good days, 8 bad days, and 10 neutral days. The fir...

--- Problem 13/20, Round 1, Agent 3/3 ---
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 923.65it/s]
[1;36m(EngineCore_DP0 pid=4013499)[0;0m INFO 12-04 14:36:34 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='WeiboAI/VibeThinker-1.5B', speculative_config=None, tokenizer='WeiboAI/VibeThinker-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=WeiboAI/VibeThinker-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4013499)[0;0m ERROR 12-04 14:36:36 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8
[1;36m(EngineCore_DP0 pid=4013499)[0;0m INFO 12-04 14:36:37 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.8:52499 backend=nccl
[W1204 14:36:37.892873358 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-23-0.rc.tch.harvard.edu]:52499 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4013499)[0;0m INFO 12-04 14:36:37 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4013499)[0;0m ERROR 12-04 14:36:37 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4013499)[0;0m ERROR 12-04 14:36:37 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4013499)[0;0m ERROR 12-04 14:36:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4013499)[0;0m ERROR 12-04 14:36:37 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4013499)[0;0m ERROR 12-04 14:36:37 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4013499)[0;0m ERROR 12-04 14:36:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4013499)[0;0m ERROR 12-04 14:36:37 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4013499)[0;0m ERROR 12-04 14:36:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=4013499)[0;0m ERROR 12-04 14:36:37 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=4013499)[0;0m ERROR 12-04 14:36:37 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4013499)[0;0m ERROR 12-04 14:36:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=4013499)[0;0m ERROR 12-04 14:36:37 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=4013499)[0;0m ERROR 12-04 14:36:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=4013499)[0;0m ERROR 12-04 14:36:37 [core.py:842]     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=4013499)[0;0m ERROR 12-04 14:36:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=4013499)[0;0m ERROR 12-04 14:36:37 [core.py:842]     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=4013499)[0;0m ERROR 12-04 14:36:37 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4013499)[0;0m ERROR 12-04 14:36:37 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=4013499)[0;0m ERROR 12-04 14:36:37 [core.py:842]     raise ValueError(
[1;36m(EngineCore_DP0 pid=4013499)[0;0m ERROR 12-04 14:36:37 [core.py:842] ValueError: Free memory on device (7.03/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[1;36m(EngineCore_DP0 pid=4013499)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=4013499)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4013499)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=4013499)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=4013499)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=4013499)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=4013499)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=4013499)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=4013499)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4013499)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4013499)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4013499)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4013499)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=4013499)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=4013499)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=4013499)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4013499)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=4013499)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=4013499)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=4013499)[0;0m     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=4013499)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=4013499)[0;0m     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=4013499)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4013499)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=4013499)[0;0m     raise ValueError(
[1;36m(EngineCore_DP0 pid=4013499)[0;0m ValueError: Free memory on device (7.03/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W1204 14:36:38.910572072 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: WeiboAI/VibeThinker-1.5B (backend=vllm)
[ModelCache] Using max_model_len=131072 for WeiboAI/VibeThinker-1.5B
[ModelCache] Detected 1 GPU(s), 44.3GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 14:36:59 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'WeiboAI/VibeThinker-1.5B'}
INFO 12-04 14:36:59 [model.py:631] Resolved architecture: Qwen2ForCausalLM
WARNING 12-04 14:36:59 [model.py:1921] Your device 'Quadro RTX 8000' (with compute capability 7.5) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.
WARNING 12-04 14:36:59 [model.py:1971] Casting torch.bfloat16 to torch.float16.
INFO 12-04 14:36:59 [model.py:1745] Using max model len 131072
INFO 12-04 14:36:59 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:44<00:00, 44.28s/it, est. speed input: 3.61 toks/s, output: 113.34 toks/s]Processed prompts: 100%|██████████| 1/1 [00:44<00:00, 44.28s/it, est. speed input: 3.61 toks/s, output: 113.34 toks/s]Processed prompts: 100%|██████████| 1/1 [00:44<00:00, 44.28s/it, est. speed input: 3.61 toks/s, output: 113.34 toks/s]
Agent 3 response: Christina's month consists of 30 days, with 12 good days, 8 bad days, and 10 neutral days. The first...

--- Problem 13/20, Round 2, Agent 1/3 ---
Agent 1 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 215.60it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.81s/it, est. speed input: 145.92 toks/s, output: 112.35 toks/s]Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.81s/it, est. speed input: 145.92 toks/s, output: 112.35 toks/s]Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.81s/it, est. speed input: 145.92 toks/s, output: 112.35 toks/s]
Agent 1 response: Christina recorded her mood over 30 days, with 12 good days, 8 bad days, and 10 neutral days (since ...

--- Problem 13/20, Round 2, Agent 2/3 ---
Agent 2 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 241.82it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:04<00:00,  4.58s/it, est. speed input: 248.15 toks/s, output: 112.50 toks/s]Processed prompts: 100%|██████████| 1/1 [00:04<00:00,  4.58s/it, est. speed input: 248.15 toks/s, output: 112.50 toks/s]Processed prompts: 100%|██████████| 1/1 [00:04<00:00,  4.58s/it, est. speed input: 248.15 toks/s, output: 112.50 toks/s]
Agent 2 response: Christina's month has 30 days, with 12 good days, 8 bad days, and 10 neutral days. The first 8 days ...

--- Problem 13/20, Round 2, Agent 3/3 ---
Agent 3 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 269.23it/s]
[1;36m(EngineCore_DP0 pid=4013661)[0;0m INFO 12-04 14:37:21 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='WeiboAI/VibeThinker-1.5B', speculative_config=None, tokenizer='WeiboAI/VibeThinker-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=WeiboAI/VibeThinker-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4013661)[0;0m ERROR 12-04 14:37:22 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8
[1;36m(EngineCore_DP0 pid=4013661)[0;0m INFO 12-04 14:37:23 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.8:48933 backend=nccl
[W1204 14:37:23.387191990 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-23-0.rc.tch.harvard.edu]:48933 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4013661)[0;0m INFO 12-04 14:37:23 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4013661)[0;0m ERROR 12-04 14:37:23 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4013661)[0;0m ERROR 12-04 14:37:23 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4013661)[0;0m ERROR 12-04 14:37:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4013661)[0;0m ERROR 12-04 14:37:23 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4013661)[0;0m ERROR 12-04 14:37:23 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4013661)[0;0m ERROR 12-04 14:37:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4013661)[0;0m ERROR 12-04 14:37:23 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4013661)[0;0m ERROR 12-04 14:37:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=4013661)[0;0m ERROR 12-04 14:37:23 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=4013661)[0;0m ERROR 12-04 14:37:23 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4013661)[0;0m ERROR 12-04 14:37:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=4013661)[0;0m ERROR 12-04 14:37:23 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=4013661)[0;0m ERROR 12-04 14:37:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=4013661)[0;0m ERROR 12-04 14:37:23 [core.py:842]     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=4013661)[0;0m ERROR 12-04 14:37:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=4013661)[0;0m ERROR 12-04 14:37:23 [core.py:842]     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=4013661)[0;0m ERROR 12-04 14:37:23 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4013661)[0;0m ERROR 12-04 14:37:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=4013661)[0;0m ERROR 12-04 14:37:23 [core.py:842]     raise ValueError(
[1;36m(EngineCore_DP0 pid=4013661)[0;0m ERROR 12-04 14:37:23 [core.py:842] ValueError: Free memory on device (7.03/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[1;36m(EngineCore_DP0 pid=4013661)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=4013661)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4013661)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=4013661)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=4013661)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=4013661)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=4013661)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=4013661)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=4013661)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4013661)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4013661)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4013661)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4013661)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=4013661)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=4013661)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=4013661)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4013661)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=4013661)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=4013661)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=4013661)[0;0m     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=4013661)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=4013661)[0;0m     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=4013661)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4013661)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=4013661)[0;0m     raise ValueError(
[1;36m(EngineCore_DP0 pid=4013661)[0;0m ValueError: Free memory on device (7.03/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W1204 14:37:24.430445962 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:08<00:00,  8.84s/it, est. speed input: 129.15 toks/s, output: 111.62 toks/s]Processed prompts: 100%|██████████| 1/1 [00:08<00:00,  8.84s/it, est. speed input: 129.15 toks/s, output: 111.62 toks/s]Processed prompts: 100%|██████████| 1/1 [00:08<00:00,  8.84s/it, est. speed input: 129.15 toks/s, output: 111.62 toks/s]
Agent 3 response: Christina's month has 30 days, with 12 good days, 8 bad days, and 10 neutral days. The first 8 days ...

--- Problem 13/20, Round 3, Agent 1/3 ---
Agent 1 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 156.66it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.38s/it, est. speed input: 286.70 toks/s, output: 113.68 toks/s]Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.38s/it, est. speed input: 286.70 toks/s, output: 113.68 toks/s]Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.38s/it, est. speed input: 286.70 toks/s, output: 113.68 toks/s]
Agent 1 response: Christina recorded her mood over 30 days, with 12 good days, 8 bad days, and 10 neutral days (since ...

--- Problem 13/20, Round 3, Agent 2/3 ---
Agent 2 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 166.16it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:06<00:00,  6.70s/it, est. speed input: 315.58 toks/s, output: 113.66 toks/s]Processed prompts: 100%|██████████| 1/1 [00:06<00:00,  6.70s/it, est. speed input: 315.58 toks/s, output: 113.66 toks/s]Processed prompts: 100%|██████████| 1/1 [00:06<00:00,  6.70s/it, est. speed input: 315.58 toks/s, output: 113.66 toks/s]
Agent 2 response: Christina's month has 30 days, with 12 good days, 8 bad days, and 10 neutral days (since 30 - 12 - 8...

--- Problem 13/20, Round 3, Agent 3/3 ---
Agent 3 receiving other agents' responses...
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 167.08it/s]
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: WeiboAI/VibeThinker-1.5B (backend=vllm)
[ModelCache] Using max_model_len=131072 for WeiboAI/VibeThinker-1.5B
[ModelCache] Detected 1 GPU(s), 44.3GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 14:37:45 [utils.py:253] non-default args: {'max_model_len': 131072, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'WeiboAI/VibeThinker-1.5B'}
INFO 12-04 14:37:45 [model.py:631] Resolved architecture: Qwen2ForCausalLM
WARNING 12-04 14:37:45 [model.py:1921] Your device 'Quadro RTX 8000' (with compute capability 7.5) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.
WARNING 12-04 14:37:45 [model.py:1971] Casting torch.bfloat16 to torch.float16.
INFO 12-04 14:37:45 [model.py:1745] Using max model len 131072
INFO 12-04 14:37:45 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:08<00:00,  8.06s/it, est. speed input: 263.02 toks/s, output: 112.95 toks/s]Processed prompts: 100%|██████████| 1/1 [00:08<00:00,  8.06s/it, est. speed input: 263.02 toks/s, output: 112.95 toks/s]Processed prompts: 100%|██████████| 1/1 [00:08<00:00,  8.06s/it, est. speed input: 263.02 toks/s, output: 112.95 toks/s]
Agent 3 response: Christina's month has 30 days, with 12 good days, 8 bad days, and 10 neutral days. The first 8 days ...

Running accuracy: 1.000 ± 0.000

--- Problem 14/20, Round 1, Agent 1/3 ---
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1129.32it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:08<00:00,  8.86s/it, est. speed input: 13.44 toks/s, output: 113.49 toks/s]Processed prompts: 100%|██████████| 1/1 [00:08<00:00,  8.86s/it, est. speed input: 13.44 toks/s, output: 113.49 toks/s]Processed prompts: 100%|██████████| 1/1 [00:08<00:00,  8.86s/it, est. speed input: 13.44 toks/s, output: 113.49 toks/s]
Agent 1 response: Valerie's salary is given as $5000, which is half of her brother's salary. Therefore, her brother's ...

--- Problem 14/20, Round 1, Agent 2/3 ---
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 871.09it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:14<00:00, 14.03s/it, est. speed input: 8.27 toks/s, output: 113.66 toks/s]Processed prompts: 100%|██████████| 1/1 [00:14<00:00, 14.03s/it, est. speed input: 8.27 toks/s, output: 113.66 toks/s]Processed prompts: 100%|██████████| 1/1 [00:14<00:00, 14.03s/it, est. speed input: 8.27 toks/s, output: 113.66 toks/s]
Agent 2 response: Valerie's salary is given as $5000, which is half of her brother's salary. Therefore, her brother's ...

--- Problem 14/20, Round 1, Agent 3/3 ---
[ModelCache] Using cached model: WeiboAI/VibeThinker-1.5B
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1107.26it/s]
[1;36m(EngineCore_DP0 pid=4014116)[0;0m INFO 12-04 14:38:11 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='WeiboAI/VibeThinker-1.5B', speculative_config=None, tokenizer='WeiboAI/VibeThinker-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=WeiboAI/VibeThinker-1.5B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=4014116)[0;0m ERROR 12-04 14:38:13 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8
[1;36m(EngineCore_DP0 pid=4014116)[0;0m INFO 12-04 14:38:13 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.8:49731 backend=nccl
[W1204 14:38:13.745503299 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-23-0.rc.tch.harvard.edu]:49731 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4014116)[0;0m INFO 12-04 14:38:13 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4014116)[0;0m ERROR 12-04 14:38:14 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=4014116)[0;0m ERROR 12-04 14:38:14 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4014116)[0;0m ERROR 12-04 14:38:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4014116)[0;0m ERROR 12-04 14:38:14 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4014116)[0;0m ERROR 12-04 14:38:14 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4014116)[0;0m ERROR 12-04 14:38:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4014116)[0;0m ERROR 12-04 14:38:14 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=4014116)[0;0m ERROR 12-04 14:38:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=4014116)[0;0m ERROR 12-04 14:38:14 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=4014116)[0;0m ERROR 12-04 14:38:14 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4014116)[0;0m ERROR 12-04 14:38:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=4014116)[0;0m ERROR 12-04 14:38:14 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=4014116)[0;0m ERROR 12-04 14:38:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=4014116)[0;0m ERROR 12-04 14:38:14 [core.py:842]     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=4014116)[0;0m ERROR 12-04 14:38:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=4014116)[0;0m ERROR 12-04 14:38:14 [core.py:842]     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=4014116)[0;0m ERROR 12-04 14:38:14 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4014116)[0;0m ERROR 12-04 14:38:14 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=4014116)[0;0m ERROR 12-04 14:38:14 [core.py:842]     raise ValueError(
[1;36m(EngineCore_DP0 pid=4014116)[0;0m ERROR 12-04 14:38:14 [core.py:842] ValueError: Free memory on device (7.03/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[1;36m(EngineCore_DP0 pid=4014116)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=4014116)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=4014116)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=4014116)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=4014116)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=4014116)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=4014116)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=4014116)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=4014116)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=4014116)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=4014116)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4014116)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=4014116)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=4014116)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=4014116)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=4014116)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4014116)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=4014116)[0;0m     self._init_executor()
[1;36m(EngineCore_DP0 pid=4014116)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
[1;36m(EngineCore_DP0 pid=4014116)[0;0m     self.driver_worker.init_device()
[1;36m(EngineCore_DP0 pid=4014116)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
[1;36m(EngineCore_DP0 pid=4014116)[0;0m     self.worker.init_device()  # type: ignore
[1;36m(EngineCore_DP0 pid=4014116)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=4014116)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
[1;36m(EngineCore_DP0 pid=4014116)[0;0m     raise ValueError(
[1;36m(EngineCore_DP0 pid=4014116)[0;0m ValueError: Free memory on device (7.03/44.29 GiB) on startup is less than desired GPU memory utilization (0.9, 39.86 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W1204 14:38:14.775234231 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
slurmstepd-gpu-23-0: error: *** STEP 12735854.0 ON gpu-23-0 CANCELLED AT 2025-12-04T14:38:17 ***
