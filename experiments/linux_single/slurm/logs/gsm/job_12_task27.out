Using persona diversity with 7 different personas
============================================================
GSM Task - Multiagent Debate
============================================================
Model: mistralai/Mistral-7B-Instruct-v0.3
Persona diversity mode:
  Agent 1: a Kantian deontologist who judges all actions strictly by th...
  Agent 2: a Gothic novelist who focuses on dramatic irony, foreshadowi...
  Agent 3: a Zen master who communicates only through non-sequiturs, ko...
  Agent 4: a deep-sea volcanologist focused on extremes of pressure, he...
  Agent 5: a systems engineer who focuses strictly on modularity, inter...
  Agent 6: an expert chess grandmaster who analyzes all moves based on ...
  Agent 7: a hermetic alchemist who seeks to transmute the problem into...
Agents: 7
Rounds: 3
Problems: 20
Dataset: /home/ch269957/projects/slm_multiagent_debate/data/gsm8k/test.jsonl
Generation params: {'temperature': 1.0, 'max_tokens': None, 'top_p': 1.0, 'n': 1}
============================================================

--- Problem 1/20, Round 1, Agent 1/7 ---
[ModelCache] Loading model: mistralai/Mistral-7B-Instruct-v0.3 (backend=vllm)
[ModelCache] Using max_model_len=32768 for mistralai/Mistral-7B-Instruct-v0.3
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 14:29:47 [utils.py:253] non-default args: {'max_model_len': 32768, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'mistralai/Mistral-7B-Instruct-v0.3'}
Using persona diversity with 7 different personas
============================================================
GSM Task - Multiagent Debate
============================================================
Model: mistralai/Mistral-7B-Instruct-v0.3
Persona diversity mode:
  Agent 1: a Kantian deontologist who judges all actions strictly by th...
  Agent 2: a Gothic novelist who focuses on dramatic irony, foreshadowi...
  Agent 3: a Zen master who communicates only through non-sequiturs, ko...
  Agent 4: a deep-sea volcanologist focused on extremes of pressure, he...
  Agent 5: a systems engineer who focuses strictly on modularity, inter...
  Agent 6: an expert chess grandmaster who analyzes all moves based on ...
  Agent 7: a hermetic alchemist who seeks to transmute the problem into...
Agents: 7
Rounds: 3
Problems: 20
Dataset: /home/ch269957/projects/slm_multiagent_debate/data/gsm8k/test.jsonl
Generation params: {'temperature': 1.0, 'max_tokens': None, 'top_p': 1.0, 'n': 1}
============================================================

--- Problem 1/20, Round 1, Agent 1/7 ---
[ModelCache] Loading model: mistralai/Mistral-7B-Instruct-v0.3 (backend=vllm)
[ModelCache] Using max_model_len=32768 for mistralai/Mistral-7B-Instruct-v0.3
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 14:29:47 [utils.py:253] non-default args: {'max_model_len': 32768, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'mistralai/Mistral-7B-Instruct-v0.3'}
INFO 12-04 14:29:48 [model.py:631] Resolved architecture: MistralForCausalLM
INFO 12-04 14:29:48 [model.py:631] Resolved architecture: MistralForCausalLM
INFO 12-04 14:29:48 [model.py:1745] Using max model len 32768
INFO 12-04 14:29:48 [model.py:1745] Using max model len 32768
INFO 12-04 14:29:48 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
INFO 12-04 14:29:48 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/transformers_utils/tokenizer.py:287: FutureWarning: It is strongly recommended to run mistral models with `--tokenizer-mode "mistral"` to ensure correct encoding and decoding.
  return get_tokenizer(
/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/transformers_utils/tokenizer.py:287: FutureWarning: It is strongly recommended to run mistral models with `--tokenizer-mode "mistral"` to ensure correct encoding and decoding.
  return get_tokenizer(
[1;36m(EngineCore_DP0 pid=569452)[0;0m INFO 12-04 14:30:02 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='mistralai/Mistral-7B-Instruct-v0.3', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.3', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=mistralai/Mistral-7B-Instruct-v0.3, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=569456)[0;0m INFO 12-04 14:30:02 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='mistralai/Mistral-7B-Instruct-v0.3', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.3', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=mistralai/Mistral-7B-Instruct-v0.3, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=569452)[0;0m INFO 12-04 14:30:04 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.9:40951 backend=nccl
[1;36m(EngineCore_DP0 pid=569456)[0;0m INFO 12-04 14:30:04 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.9:54971 backend=nccl
[W1204 14:30:04.811501008 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-21-2.rc.tch.harvard.edu]:40951 (errno: 97 - Address family not supported by protocol).
[W1204 14:30:04.814581435 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-21-2.rc.tch.harvard.edu]:54971 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=569456)[0;0m INFO 12-04 14:30:04 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=569452)[0;0m INFO 12-04 14:30:04 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=569456)[0;0m INFO 12-04 14:30:04 [gpu_model_runner.py:3259] Starting to load model mistralai/Mistral-7B-Instruct-v0.3...
[1;36m(EngineCore_DP0 pid=569452)[0;0m INFO 12-04 14:30:04 [gpu_model_runner.py:3259] Starting to load model mistralai/Mistral-7B-Instruct-v0.3...
[1;36m(EngineCore_DP0 pid=569456)[0;0m INFO 12-04 14:30:05 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=569452)[0;0m INFO 12-04 14:30:05 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=569456)[0;0m INFO 12-04 14:30:05 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=569452)[0;0m INFO 12-04 14:30:05 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=569452)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]
[1;36m(EngineCore_DP0 pid=569456)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]
[1;36m(EngineCore_DP0 pid=569456)[0;0m Loading safetensors checkpoint shards:  33% Completed | 1/3 [00:15<00:30, 15.16s/it]
[1;36m(EngineCore_DP0 pid=569452)[0;0m Loading safetensors checkpoint shards:  33% Completed | 1/3 [00:15<00:30, 15.38s/it]
[1;36m(EngineCore_DP0 pid=569456)[0;0m Loading safetensors checkpoint shards:  67% Completed | 2/3 [00:32<00:16, 16.33s/it]
[1;36m(EngineCore_DP0 pid=569452)[0;0m Loading safetensors checkpoint shards:  67% Completed | 2/3 [00:32<00:16, 16.42s/it]
[1;36m(EngineCore_DP0 pid=569456)[0;0m Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:48<00:00, 16.49s/it]
[1;36m(EngineCore_DP0 pid=569452)[0;0m Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:49<00:00, 16.54s/it]
[1;36m(EngineCore_DP0 pid=569456)[0;0m Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:48<00:00, 16.33s/it]
[1;36m(EngineCore_DP0 pid=569452)[0;0m Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:49<00:00, 16.40s/it]
[1;36m(EngineCore_DP0 pid=569452)[0;0m 
[1;36m(EngineCore_DP0 pid=569456)[0;0m 
[1;36m(EngineCore_DP0 pid=569452)[0;0m INFO 12-04 14:30:55 [default_loader.py:314] Loading weights took 49.39 seconds
[1;36m(EngineCore_DP0 pid=569456)[0;0m INFO 12-04 14:30:55 [default_loader.py:314] Loading weights took 49.20 seconds
[1;36m(EngineCore_DP0 pid=569452)[0;0m INFO 12-04 14:30:56 [gpu_model_runner.py:3338] Model loading took 13.5084 GiB memory and 50.531784 seconds
[1;36m(EngineCore_DP0 pid=569456)[0;0m INFO 12-04 14:30:56 [gpu_model_runner.py:3338] Model loading took 13.5084 GiB memory and 50.561333 seconds
[1;36m(EngineCore_DP0 pid=569456)[0;0m INFO 12-04 14:31:07 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/0ef74f8a17/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=569452)[0;0m INFO 12-04 14:31:07 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/0ef74f8a17/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=569456)[0;0m INFO 12-04 14:31:07 [backends.py:647] Dynamo bytecode transform time: 10.63 s
[1;36m(EngineCore_DP0 pid=569452)[0;0m INFO 12-04 14:31:07 [backends.py:647] Dynamo bytecode transform time: 10.66 s
[1;36m(EngineCore_DP0 pid=569452)[0;0m [rank0]:W1204 14:31:08.475000 569452 site-packages/torch/_inductor/triton_bundler.py:396] [0/0] Directory /tmp/torchinductor_ch269957/triton/0/tmp.15bbe5c3-e35b-4ba4-a92b-d9b62a04b440 is not empty - skipping!
[1;36m(EngineCore_DP0 pid=569452)[0;0m [rank0]:W1204 14:31:08.477000 569452 site-packages/torch/_inductor/triton_bundler.py:396] [0/0] Directory /tmp/torchinductor_ch269957/triton/0/tmp.886d9928-7a61-4ed2-839f-43cb43b2c6ac is not empty - skipping!
[1;36m(EngineCore_DP0 pid=569452)[0;0m [rank0]:W1204 14:31:08.721000 569452 site-packages/torch/_inductor/triton_bundler.py:396] [0/0] Directory /tmp/torchinductor_ch269957/triton/0/tmp.47c958b2-ff4a-48b5-8d15-671bb3101dea is not empty - skipping!
[1;36m(EngineCore_DP0 pid=569452)[0;0m [rank0]:W1204 14:31:08.723000 569452 site-packages/torch/_inductor/triton_bundler.py:396] [0/0] Directory /tmp/torchinductor_ch269957/triton/0/tmp.a54e4870-f6e1-4548-88bf-93a617366c06 is not empty - skipping!
[1;36m(EngineCore_DP0 pid=569452)[0;0m [rank0]:W1204 14:31:14.978000 569452 site-packages/torch/_inductor/triton_bundler.py:396] [0/0] Directory /tmp/torchinductor_ch269957/triton/0/tmp.1a45cef0-4dae-423c-9702-3f1f7acbf959 is not empty - skipping!
[1;36m(EngineCore_DP0 pid=569456)[0;0m INFO 12-04 14:31:14 [backends.py:210] Directly load the compiled graph(s) for dynamic shape from the cache, took 6.563 s
[1;36m(EngineCore_DP0 pid=569452)[0;0m INFO 12-04 14:31:14 [backends.py:210] Directly load the compiled graph(s) for dynamic shape from the cache, took 6.563 s
[1;36m(EngineCore_DP0 pid=569456)[0;0m INFO 12-04 14:31:17 [monitor.py:34] torch.compile takes 17.19 s in total
[1;36m(EngineCore_DP0 pid=569452)[0;0m INFO 12-04 14:31:17 [monitor.py:34] torch.compile takes 17.23 s in total
[1;36m(EngineCore_DP0 pid=569456)[0;0m INFO 12-04 14:31:20 [gpu_worker.py:359] Available KV cache memory: 11.53 GiB
[1;36m(EngineCore_DP0 pid=569452)[0;0m INFO 12-04 14:31:20 [gpu_worker.py:359] Available KV cache memory: 11.97 GiB
[1;36m(EngineCore_DP0 pid=569452)[0;0m INFO 12-04 14:31:20 [kv_cache_utils.py:1229] GPU KV cache size: 98,016 tokens
[1;36m(EngineCore_DP0 pid=569456)[0;0m INFO 12-04 14:31:20 [kv_cache_utils.py:1229] GPU KV cache size: 94,432 tokens
[1;36m(EngineCore_DP0 pid=569452)[0;0m INFO 12-04 14:31:20 [kv_cache_utils.py:1234] Maximum concurrency for 32,768 tokens per request: 2.99x
[1;36m(EngineCore_DP0 pid=569456)[0;0m INFO 12-04 14:31:20 [kv_cache_utils.py:1234] Maximum concurrency for 32,768 tokens per request: 2.88x
[1;36m(EngineCore_DP0 pid=569452)[0;0m ERROR 12-04 14:31:20 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=569452)[0;0m ERROR 12-04 14:31:20 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=569452)[0;0m ERROR 12-04 14:31:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=569452)[0;0m ERROR 12-04 14:31:20 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=569452)[0;0m ERROR 12-04 14:31:20 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=569452)[0;0m ERROR 12-04 14:31:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=569452)[0;0m ERROR 12-04 14:31:20 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=569452)[0;0m ERROR 12-04 14:31:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=569452)[0;0m ERROR 12-04 14:31:20 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=569452)[0;0m ERROR 12-04 14:31:20 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=569452)[0;0m ERROR 12-04 14:31:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 247, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=569452)[0;0m ERROR 12-04 14:31:20 [core.py:842]     self.model_executor.initialize_from_config(kv_cache_configs)
[1;36m(EngineCore_DP0 pid=569452)[0;0m ERROR 12-04 14:31:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 115, in initialize_from_config
[1;36m(EngineCore_DP0 pid=569452)[0;0m ERROR 12-04 14:31:20 [core.py:842]     self.collective_rpc("initialize_from_config", args=(kv_cache_configs,))
[1;36m(EngineCore_DP0 pid=569452)[0;0m ERROR 12-04 14:31:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=569452)[0;0m ERROR 12-04 14:31:20 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=569452)[0;0m ERROR 12-04 14:31:20 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=569452)[0;0m ERROR 12-04 14:31:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=569452)[0;0m ERROR 12-04 14:31:20 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=569452)[0;0m ERROR 12-04 14:31:20 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=569452)[0;0m ERROR 12-04 14:31:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 319, in initialize_from_config
[1;36m(EngineCore_DP0 pid=569452)[0;0m ERROR 12-04 14:31:20 [core.py:842]     self.worker.initialize_from_config(kv_cache_config)  # type: ignore
[1;36m(EngineCore_DP0 pid=569452)[0;0m ERROR 12-04 14:31:20 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=569452)[0;0m ERROR 12-04 14:31:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 404, in initialize_from_config
[1;36m(EngineCore_DP0 pid=569452)[0;0m ERROR 12-04 14:31:20 [core.py:842]     self.model_runner.initialize_kv_cache(kv_cache_config)
[1;36m(EngineCore_DP0 pid=569452)[0;0m ERROR 12-04 14:31:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 5007, in initialize_kv_cache
[1;36m(EngineCore_DP0 pid=569452)[0;0m ERROR 12-04 14:31:20 [core.py:842]     kv_caches = self.initialize_kv_cache_tensors(
[1;36m(EngineCore_DP0 pid=569452)[0;0m ERROR 12-04 14:31:20 [core.py:842]                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=569452)[0;0m ERROR 12-04 14:31:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4933, in initialize_kv_cache_tensors
[1;36m(EngineCore_DP0 pid=569452)[0;0m ERROR 12-04 14:31:20 [core.py:842]     kv_cache_raw_tensors = self._allocate_kv_cache_tensors(kv_cache_config)
[1;36m(EngineCore_DP0 pid=569452)[0;0m ERROR 12-04 14:31:20 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=569452)[0;0m ERROR 12-04 14:31:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4718, in _allocate_kv_cache_tensors
[1;36m(EngineCore_DP0 pid=569452)[0;0m ERROR 12-04 14:31:20 [core.py:842]     tensor = torch.zeros(
[1;36m(EngineCore_DP0 pid=569452)[0;0m ERROR 12-04 14:31:20 [core.py:842]              ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=569452)[0;0m ERROR 12-04 14:31:20 [core.py:842] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 384.00 MiB. GPU 0 has a total capacity of 44.42 GiB of which 12.50 MiB is free. Process 569456 has 22.23 GiB memory in use. Including non-PyTorch memory, this process has 22.17 GiB memory in use. Of the allocated memory 21.81 GiB is allocated by PyTorch, and 27.83 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=569452)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=569452)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=569452)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=569452)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=569452)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=569452)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=569452)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=569452)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=569452)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=569452)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=569452)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=569452)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=569452)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=569452)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=569452)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=569452)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=569452)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 247, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=569452)[0;0m     self.model_executor.initialize_from_config(kv_cache_configs)
[1;36m(EngineCore_DP0 pid=569452)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 115, in initialize_from_config
[1;36m(EngineCore_DP0 pid=569452)[0;0m     self.collective_rpc("initialize_from_config", args=(kv_cache_configs,))
[1;36m(EngineCore_DP0 pid=569452)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=569452)[0;0m     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=569452)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=569452)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=569452)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=569452)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=569452)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 319, in initialize_from_config
[1;36m(EngineCore_DP0 pid=569452)[0;0m     self.worker.initialize_from_config(kv_cache_config)  # type: ignore
[1;36m(EngineCore_DP0 pid=569452)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=569452)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 404, in initialize_from_config
[1;36m(EngineCore_DP0 pid=569452)[0;0m     self.model_runner.initialize_kv_cache(kv_cache_config)
[1;36m(EngineCore_DP0 pid=569452)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 5007, in initialize_kv_cache
[1;36m(EngineCore_DP0 pid=569452)[0;0m     kv_caches = self.initialize_kv_cache_tensors(
[1;36m(EngineCore_DP0 pid=569452)[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=569452)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4933, in initialize_kv_cache_tensors
[1;36m(EngineCore_DP0 pid=569452)[0;0m     kv_cache_raw_tensors = self._allocate_kv_cache_tensors(kv_cache_config)
[1;36m(EngineCore_DP0 pid=569452)[0;0m                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=569452)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4718, in _allocate_kv_cache_tensors
[1;36m(EngineCore_DP0 pid=569452)[0;0m     tensor = torch.zeros(
[1;36m(EngineCore_DP0 pid=569452)[0;0m              ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=569452)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 384.00 MiB. GPU 0 has a total capacity of 44.42 GiB of which 12.50 MiB is free. Process 569456 has 22.23 GiB memory in use. Including non-PyTorch memory, this process has 22.17 GiB memory in use. Of the allocated memory 21.81 GiB is allocated by PyTorch, and 27.83 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=569456)[0;0m ERROR 12-04 14:31:20 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=569456)[0;0m ERROR 12-04 14:31:20 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=569456)[0;0m ERROR 12-04 14:31:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=569456)[0;0m ERROR 12-04 14:31:20 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=569456)[0;0m ERROR 12-04 14:31:20 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=569456)[0;0m ERROR 12-04 14:31:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=569456)[0;0m ERROR 12-04 14:31:20 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=569456)[0;0m ERROR 12-04 14:31:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=569456)[0;0m ERROR 12-04 14:31:20 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=569456)[0;0m ERROR 12-04 14:31:20 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=569456)[0;0m ERROR 12-04 14:31:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 247, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=569456)[0;0m ERROR 12-04 14:31:20 [core.py:842]     self.model_executor.initialize_from_config(kv_cache_configs)
[1;36m(EngineCore_DP0 pid=569456)[0;0m ERROR 12-04 14:31:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 115, in initialize_from_config
[1;36m(EngineCore_DP0 pid=569456)[0;0m ERROR 12-04 14:31:20 [core.py:842]     self.collective_rpc("initialize_from_config", args=(kv_cache_configs,))
[1;36m(EngineCore_DP0 pid=569456)[0;0m ERROR 12-04 14:31:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=569456)[0;0m ERROR 12-04 14:31:20 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=569456)[0;0m ERROR 12-04 14:31:20 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=569456)[0;0m ERROR 12-04 14:31:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=569456)[0;0m ERROR 12-04 14:31:20 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=569456)[0;0m ERROR 12-04 14:31:20 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=569456)[0;0m ERROR 12-04 14:31:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 319, in initialize_from_config
[1;36m(EngineCore_DP0 pid=569456)[0;0m ERROR 12-04 14:31:20 [core.py:842]     self.worker.initialize_from_config(kv_cache_config)  # type: ignore
[1;36m(EngineCore_DP0 pid=569456)[0;0m ERROR 12-04 14:31:20 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=569456)[0;0m ERROR 12-04 14:31:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 404, in initialize_from_config
[1;36m(EngineCore_DP0 pid=569456)[0;0m ERROR 12-04 14:31:20 [core.py:842]     self.model_runner.initialize_kv_cache(kv_cache_config)
[1;36m(EngineCore_DP0 pid=569456)[0;0m ERROR 12-04 14:31:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 5007, in initialize_kv_cache
[1;36m(EngineCore_DP0 pid=569456)[0;0m ERROR 12-04 14:31:20 [core.py:842]     kv_caches = self.initialize_kv_cache_tensors(
[1;36m(EngineCore_DP0 pid=569456)[0;0m ERROR 12-04 14:31:20 [core.py:842]                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=569456)[0;0m ERROR 12-04 14:31:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4933, in initialize_kv_cache_tensors
[1;36m(EngineCore_DP0 pid=569456)[0;0m ERROR 12-04 14:31:20 [core.py:842]     kv_cache_raw_tensors = self._allocate_kv_cache_tensors(kv_cache_config)
[1;36m(EngineCore_DP0 pid=569456)[0;0m ERROR 12-04 14:31:20 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=569456)[0;0m ERROR 12-04 14:31:20 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4718, in _allocate_kv_cache_tensors
[1;36m(EngineCore_DP0 pid=569456)[0;0m ERROR 12-04 14:31:20 [core.py:842]     tensor = torch.zeros(
[1;36m(EngineCore_DP0 pid=569456)[0;0m ERROR 12-04 14:31:20 [core.py:842]              ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=569456)[0;0m ERROR 12-04 14:31:20 [core.py:842] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 370.00 MiB. GPU 0 has a total capacity of 44.42 GiB of which 12.50 MiB is free. Including non-PyTorch memory, this process has 22.23 GiB memory in use. Process 569452 has 22.17 GiB memory in use. Of the allocated memory 21.87 GiB is allocated by PyTorch, and 28.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=569456)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=569456)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=569456)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=569456)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=569456)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=569456)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=569456)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=569456)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=569456)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=569456)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=569456)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=569456)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=569456)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=569456)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=569456)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=569456)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=569456)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 247, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=569456)[0;0m     self.model_executor.initialize_from_config(kv_cache_configs)
[1;36m(EngineCore_DP0 pid=569456)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 115, in initialize_from_config
[1;36m(EngineCore_DP0 pid=569456)[0;0m     self.collective_rpc("initialize_from_config", args=(kv_cache_configs,))
[1;36m(EngineCore_DP0 pid=569456)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=569456)[0;0m     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=569456)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=569456)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=569456)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=569456)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=569456)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 319, in initialize_from_config
[1;36m(EngineCore_DP0 pid=569456)[0;0m     self.worker.initialize_from_config(kv_cache_config)  # type: ignore
[1;36m(EngineCore_DP0 pid=569456)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=569456)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 404, in initialize_from_config
[1;36m(EngineCore_DP0 pid=569456)[0;0m     self.model_runner.initialize_kv_cache(kv_cache_config)
[1;36m(EngineCore_DP0 pid=569456)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 5007, in initialize_kv_cache
[1;36m(EngineCore_DP0 pid=569456)[0;0m     kv_caches = self.initialize_kv_cache_tensors(
[1;36m(EngineCore_DP0 pid=569456)[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=569456)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4933, in initialize_kv_cache_tensors
[1;36m(EngineCore_DP0 pid=569456)[0;0m     kv_cache_raw_tensors = self._allocate_kv_cache_tensors(kv_cache_config)
[1;36m(EngineCore_DP0 pid=569456)[0;0m                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=569456)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4718, in _allocate_kv_cache_tensors
[1;36m(EngineCore_DP0 pid=569456)[0;0m     tensor = torch.zeros(
[1;36m(EngineCore_DP0 pid=569456)[0;0m              ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=569456)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 370.00 MiB. GPU 0 has a total capacity of 44.42 GiB of which 12.50 MiB is free. Including non-PyTorch memory, this process has 22.23 GiB memory in use. Process 569452 has 22.17 GiB memory in use. Of the allocated memory 21.87 GiB is allocated by PyTorch, and 28.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W1204 14:31:21.949688304 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank0]:[W1204 14:31:21.949688775 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: mistralai/Mistral-7B-Instruct-v0.3 (backend=vllm)
[ModelCache] Using max_model_len=32768 for mistralai/Mistral-7B-Instruct-v0.3
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 14:31:42 [utils.py:253] non-default args: {'max_model_len': 32768, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'mistralai/Mistral-7B-Instruct-v0.3'}
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: mistralai/Mistral-7B-Instruct-v0.3 (backend=vllm)
[ModelCache] Using max_model_len=32768 for mistralai/Mistral-7B-Instruct-v0.3
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 14:31:42 [utils.py:253] non-default args: {'max_model_len': 32768, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'mistralai/Mistral-7B-Instruct-v0.3'}
INFO 12-04 14:31:43 [model.py:631] Resolved architecture: MistralForCausalLM
INFO 12-04 14:31:43 [model.py:1745] Using max model len 32768
INFO 12-04 14:31:43 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
INFO 12-04 14:31:43 [model.py:631] Resolved architecture: MistralForCausalLM
INFO 12-04 14:31:43 [model.py:1745] Using max model len 32768
INFO 12-04 14:31:43 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=570079)[0;0m INFO 12-04 14:31:58 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='mistralai/Mistral-7B-Instruct-v0.3', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.3', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=mistralai/Mistral-7B-Instruct-v0.3, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=570072)[0;0m INFO 12-04 14:31:58 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='mistralai/Mistral-7B-Instruct-v0.3', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.3', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=mistralai/Mistral-7B-Instruct-v0.3, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=570079)[0;0m INFO 12-04 14:31:59 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.9:43013 backend=nccl
[1;36m(EngineCore_DP0 pid=570072)[0;0m INFO 12-04 14:31:59 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.9:49445 backend=nccl
[W1204 14:31:59.150211013 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-21-2.rc.tch.harvard.edu]:49445 (errno: 97 - Address family not supported by protocol).
[W1204 14:31:59.150666151 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-21-2.rc.tch.harvard.edu]:43013 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=570079)[0;0m INFO 12-04 14:31:59 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=570072)[0;0m INFO 12-04 14:31:59 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=570079)[0;0m INFO 12-04 14:32:00 [gpu_model_runner.py:3259] Starting to load model mistralai/Mistral-7B-Instruct-v0.3...
[1;36m(EngineCore_DP0 pid=570072)[0;0m INFO 12-04 14:32:00 [gpu_model_runner.py:3259] Starting to load model mistralai/Mistral-7B-Instruct-v0.3...
[1;36m(EngineCore_DP0 pid=570072)[0;0m INFO 12-04 14:32:01 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=570072)[0;0m INFO 12-04 14:32:01 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=570079)[0;0m INFO 12-04 14:32:01 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=570079)[0;0m INFO 12-04 14:32:01 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=570072)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]
[1;36m(EngineCore_DP0 pid=570079)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]
[1;36m(EngineCore_DP0 pid=570079)[0;0m Loading safetensors checkpoint shards:  33% Completed | 1/3 [00:08<00:17,  8.98s/it]
[1;36m(EngineCore_DP0 pid=570072)[0;0m Loading safetensors checkpoint shards:  33% Completed | 1/3 [00:09<00:18,  9.10s/it]
[1;36m(EngineCore_DP0 pid=570079)[0;0m Loading safetensors checkpoint shards:  67% Completed | 2/3 [00:16<00:08,  8.32s/it]
[1;36m(EngineCore_DP0 pid=570072)[0;0m Loading safetensors checkpoint shards:  67% Completed | 2/3 [00:16<00:08,  8.37s/it]
[1;36m(EngineCore_DP0 pid=570079)[0;0m Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:38<00:00, 14.53s/it]
[1;36m(EngineCore_DP0 pid=570072)[0;0m Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:38<00:00, 14.55s/it]
[1;36m(EngineCore_DP0 pid=570072)[0;0m Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:38<00:00, 12.96s/it]
[1;36m(EngineCore_DP0 pid=570079)[0;0m Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:38<00:00, 12.92s/it]
[1;36m(EngineCore_DP0 pid=570072)[0;0m 
[1;36m(EngineCore_DP0 pid=570079)[0;0m 
[1;36m(EngineCore_DP0 pid=570079)[0;0m INFO 12-04 14:32:40 [default_loader.py:314] Loading weights took 38.97 seconds
[1;36m(EngineCore_DP0 pid=570072)[0;0m INFO 12-04 14:32:40 [default_loader.py:314] Loading weights took 39.08 seconds
[1;36m(EngineCore_DP0 pid=570079)[0;0m INFO 12-04 14:32:41 [gpu_model_runner.py:3338] Model loading took 13.5084 GiB memory and 40.143093 seconds
[1;36m(EngineCore_DP0 pid=570072)[0;0m INFO 12-04 14:32:41 [gpu_model_runner.py:3338] Model loading took 13.5084 GiB memory and 40.144586 seconds
[1;36m(EngineCore_DP0 pid=570072)[0;0m INFO 12-04 14:32:51 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/0ef74f8a17/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=570079)[0;0m INFO 12-04 14:32:51 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/0ef74f8a17/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=570079)[0;0m INFO 12-04 14:32:51 [backends.py:647] Dynamo bytecode transform time: 9.85 s
[1;36m(EngineCore_DP0 pid=570072)[0;0m INFO 12-04 14:32:51 [backends.py:647] Dynamo bytecode transform time: 9.85 s
[1;36m(EngineCore_DP0 pid=570072)[0;0m [rank0]:W1204 14:32:54.821000 570072 site-packages/torch/_inductor/codecache.py:1021] [0/0] fx graph cache unable to load compiled graph
[1;36m(EngineCore_DP0 pid=570072)[0;0m [rank0]:W1204 14:32:54.821000 570072 site-packages/torch/_inductor/codecache.py:1021] [0/0] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=570072)[0;0m [rank0]:W1204 14:32:54.821000 570072 site-packages/torch/_inductor/codecache.py:1021] [0/0]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/codecache.py", line 1017, in iterate_over_candidates
[1;36m(EngineCore_DP0 pid=570072)[0;0m [rank0]:W1204 14:32:54.821000 570072 site-packages/torch/_inductor/codecache.py:1021] [0/0]     with open(os.path.join(subdir, path), "rb") as f:
[1;36m(EngineCore_DP0 pid=570072)[0;0m [rank0]:W1204 14:32:54.821000 570072 site-packages/torch/_inductor/codecache.py:1021] [0/0]          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=570072)[0;0m [rank0]:W1204 14:32:54.821000 570072 site-packages/torch/_inductor/codecache.py:1021] [0/0] FileNotFoundError: [Errno 2] No such file or directory: '/tmp/torchinductor_ch269957/fxgraph/ol/folwqklezpc3cya2ynm7y23gllyyiu4bk25nyp4ryjevzdnys7b2/.570079.140292187255808.tmp'
[1;36m(EngineCore_DP0 pid=570079)[0;0m INFO 12-04 14:32:59 [backends.py:210] Directly load the compiled graph(s) for dynamic shape from the cache, took 6.658 s
[1;36m(EngineCore_DP0 pid=570072)[0;0m INFO 12-04 14:32:59 [backends.py:210] Directly load the compiled graph(s) for dynamic shape from the cache, took 6.656 s
[1;36m(EngineCore_DP0 pid=570072)[0;0m INFO 12-04 14:33:01 [monitor.py:34] torch.compile takes 16.51 s in total
[1;36m(EngineCore_DP0 pid=570079)[0;0m INFO 12-04 14:33:01 [monitor.py:34] torch.compile takes 16.51 s in total
[1;36m(EngineCore_DP0 pid=570079)[0;0m INFO 12-04 14:33:04 [gpu_worker.py:359] Available KV cache memory: 10.79 GiB
[1;36m(EngineCore_DP0 pid=570072)[0;0m INFO 12-04 14:33:04 [gpu_worker.py:359] Available KV cache memory: 11.97 GiB
[1;36m(EngineCore_DP0 pid=570079)[0;0m INFO 12-04 14:33:04 [kv_cache_utils.py:1229] GPU KV cache size: 88,416 tokens
[1;36m(EngineCore_DP0 pid=570079)[0;0m INFO 12-04 14:33:04 [kv_cache_utils.py:1234] Maximum concurrency for 32,768 tokens per request: 2.70x
[1;36m(EngineCore_DP0 pid=570072)[0;0m INFO 12-04 14:33:04 [kv_cache_utils.py:1229] GPU KV cache size: 98,016 tokens
[1;36m(EngineCore_DP0 pid=570072)[0;0m INFO 12-04 14:33:04 [kv_cache_utils.py:1234] Maximum concurrency for 32,768 tokens per request: 2.99x
[1;36m(EngineCore_DP0 pid=570072)[0;0m ERROR 12-04 14:33:04 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=570072)[0;0m ERROR 12-04 14:33:04 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=570072)[0;0m ERROR 12-04 14:33:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=570072)[0;0m ERROR 12-04 14:33:04 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=570072)[0;0m ERROR 12-04 14:33:04 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=570072)[0;0m ERROR 12-04 14:33:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=570072)[0;0m ERROR 12-04 14:33:04 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=570072)[0;0m ERROR 12-04 14:33:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=570072)[0;0m ERROR 12-04 14:33:04 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=570072)[0;0m ERROR 12-04 14:33:04 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=570072)[0;0m ERROR 12-04 14:33:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 247, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=570072)[0;0m ERROR 12-04 14:33:04 [core.py:842]     self.model_executor.initialize_from_config(kv_cache_configs)
[1;36m(EngineCore_DP0 pid=570072)[0;0m ERROR 12-04 14:33:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 115, in initialize_from_config
[1;36m(EngineCore_DP0 pid=570072)[0;0m ERROR 12-04 14:33:04 [core.py:842]     self.collective_rpc("initialize_from_config", args=(kv_cache_configs,))
[1;36m(EngineCore_DP0 pid=570072)[0;0m ERROR 12-04 14:33:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=570072)[0;0m ERROR 12-04 14:33:04 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=570072)[0;0m ERROR 12-04 14:33:04 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=570072)[0;0m ERROR 12-04 14:33:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=570072)[0;0m ERROR 12-04 14:33:04 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=570072)[0;0m ERROR 12-04 14:33:04 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=570072)[0;0m ERROR 12-04 14:33:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 319, in initialize_from_config
[1;36m(EngineCore_DP0 pid=570072)[0;0m ERROR 12-04 14:33:04 [core.py:842]     self.worker.initialize_from_config(kv_cache_config)  # type: ignore
[1;36m(EngineCore_DP0 pid=570072)[0;0m ERROR 12-04 14:33:04 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=570072)[0;0m ERROR 12-04 14:33:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 404, in initialize_from_config
[1;36m(EngineCore_DP0 pid=570072)[0;0m ERROR 12-04 14:33:04 [core.py:842]     self.model_runner.initialize_kv_cache(kv_cache_config)
[1;36m(EngineCore_DP0 pid=570072)[0;0m ERROR 12-04 14:33:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 5007, in initialize_kv_cache
[1;36m(EngineCore_DP0 pid=570072)[0;0m ERROR 12-04 14:33:04 [core.py:842]     kv_caches = self.initialize_kv_cache_tensors(
[1;36m(EngineCore_DP0 pid=570072)[0;0m ERROR 12-04 14:33:04 [core.py:842]                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=570072)[0;0m ERROR 12-04 14:33:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4933, in initialize_kv_cache_tensors
[1;36m(EngineCore_DP0 pid=570072)[0;0m ERROR 12-04 14:33:04 [core.py:842]     kv_cache_raw_tensors = self._allocate_kv_cache_tensors(kv_cache_config)
[1;36m(EngineCore_DP0 pid=570072)[0;0m ERROR 12-04 14:33:04 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=570072)[0;0m ERROR 12-04 14:33:04 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4718, in _allocate_kv_cache_tensors
[1;36m(EngineCore_DP0 pid=570072)[0;0m ERROR 12-04 14:33:04 [core.py:842]     tensor = torch.zeros(
[1;36m(EngineCore_DP0 pid=570072)[0;0m ERROR 12-04 14:33:04 [core.py:842]              ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=570072)[0;0m ERROR 12-04 14:33:04 [core.py:842] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 384.00 MiB. GPU 0 has a total capacity of 44.42 GiB of which 138.50 MiB is free. Process 570079 has 24.73 GiB memory in use. Including non-PyTorch memory, this process has 19.54 GiB memory in use. Of the allocated memory 19.19 GiB is allocated by PyTorch, and 19.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=570072)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=570072)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=570072)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=570072)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=570072)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=570072)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=570072)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=570072)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=570072)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=570072)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=570072)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=570072)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=570072)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=570072)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=570072)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=570072)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=570072)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 247, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=570072)[0;0m     self.model_executor.initialize_from_config(kv_cache_configs)
[1;36m(EngineCore_DP0 pid=570072)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 115, in initialize_from_config
[1;36m(EngineCore_DP0 pid=570072)[0;0m     self.collective_rpc("initialize_from_config", args=(kv_cache_configs,))
[1;36m(EngineCore_DP0 pid=570072)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=570072)[0;0m     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=570072)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=570072)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=570072)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=570072)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=570072)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 319, in initialize_from_config
[1;36m(EngineCore_DP0 pid=570072)[0;0m     self.worker.initialize_from_config(kv_cache_config)  # type: ignore
[1;36m(EngineCore_DP0 pid=570072)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=570072)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 404, in initialize_from_config
[1;36m(EngineCore_DP0 pid=570072)[0;0m     self.model_runner.initialize_kv_cache(kv_cache_config)
[1;36m(EngineCore_DP0 pid=570072)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 5007, in initialize_kv_cache
[1;36m(EngineCore_DP0 pid=570072)[0;0m     kv_caches = self.initialize_kv_cache_tensors(
[1;36m(EngineCore_DP0 pid=570072)[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=570072)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4933, in initialize_kv_cache_tensors
[1;36m(EngineCore_DP0 pid=570072)[0;0m     kv_cache_raw_tensors = self._allocate_kv_cache_tensors(kv_cache_config)
[1;36m(EngineCore_DP0 pid=570072)[0;0m                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=570072)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4718, in _allocate_kv_cache_tensors
[1;36m(EngineCore_DP0 pid=570072)[0;0m     tensor = torch.zeros(
[1;36m(EngineCore_DP0 pid=570072)[0;0m              ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=570072)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 384.00 MiB. GPU 0 has a total capacity of 44.42 GiB of which 138.50 MiB is free. Process 570079 has 24.73 GiB memory in use. Including non-PyTorch memory, this process has 19.54 GiB memory in use. Of the allocated memory 19.19 GiB is allocated by PyTorch, and 19.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=570079)[0;0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/51 [00:00<?, ?it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   2%|▏         | 1/51 [00:00<00:05,  9.63it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   2%|▏         | 1/51 [00:00<00:05,  8.70it/s]
[rank0]:[W1204 14:33:05.143268931 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[1;36m(EngineCore_DP0 pid=570079)[0;0m ERROR 12-04 14:33:05 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=570079)[0;0m ERROR 12-04 14:33:05 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=570079)[0;0m ERROR 12-04 14:33:05 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=570079)[0;0m ERROR 12-04 14:33:05 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=570079)[0;0m ERROR 12-04 14:33:05 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=570079)[0;0m ERROR 12-04 14:33:05 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=570079)[0;0m ERROR 12-04 14:33:05 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=570079)[0;0m ERROR 12-04 14:33:05 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=570079)[0;0m ERROR 12-04 14:33:05 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=570079)[0;0m ERROR 12-04 14:33:05 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=570079)[0;0m ERROR 12-04 14:33:05 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 247, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=570079)[0;0m ERROR 12-04 14:33:05 [core.py:842]     self.model_executor.initialize_from_config(kv_cache_configs)
[1;36m(EngineCore_DP0 pid=570079)[0;0m ERROR 12-04 14:33:05 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 116, in initialize_from_config
[1;36m(EngineCore_DP0 pid=570079)[0;0m ERROR 12-04 14:33:05 [core.py:842]     self.collective_rpc("compile_or_warm_up_model")
[1;36m(EngineCore_DP0 pid=570079)[0;0m ERROR 12-04 14:33:05 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=570079)[0;0m ERROR 12-04 14:33:05 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=570079)[0;0m ERROR 12-04 14:33:05 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=570079)[0;0m ERROR 12-04 14:33:05 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=570079)[0;0m ERROR 12-04 14:33:05 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=570079)[0;0m ERROR 12-04 14:33:05 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=570079)[0;0m ERROR 12-04 14:33:05 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 429, in compile_or_warm_up_model
[1;36m(EngineCore_DP0 pid=570079)[0;0m ERROR 12-04 14:33:05 [core.py:842]     cuda_graph_memory_bytes = self.model_runner.capture_model()
[1;36m(EngineCore_DP0 pid=570079)[0;0m ERROR 12-04 14:33:05 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=570079)[0;0m ERROR 12-04 14:33:05 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4201, in capture_model
[1;36m(EngineCore_DP0 pid=570079)[0;0m ERROR 12-04 14:33:05 [core.py:842]     self._capture_cudagraphs(
[1;36m(EngineCore_DP0 pid=570079)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=570079)[0;0m ERROR 12-04 14:33:05 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4298, in _capture_cudagraphs
[1;36m(EngineCore_DP0 pid=570079)[0;0m ERROR 12-04 14:33:05 [core.py:842]     self._dummy_run(
[1;36m(EngineCore_DP0 pid=570079)[0;0m ERROR 12-04 14:33:05 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=570079)[0;0m ERROR 12-04 14:33:05 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=570079)[0;0m ERROR 12-04 14:33:05 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=570079)[0;0m ERROR 12-04 14:33:05 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=570079)[0;0m ERROR 12-04 14:33:05 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=570079)[0;0m ERROR 12-04 14:33:05 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=570079)[0;0m ERROR 12-04 14:33:05 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=570079)[0;0m ERROR 12-04 14:33:05 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=570079)[0;0m ERROR 12-04 14:33:05 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=570079)[0;0m ERROR 12-04 14:33:05 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=570079)[0;0m ERROR 12-04 14:33:05 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=570079)[0;0m ERROR 12-04 14:33:05 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=570079)[0;0m ERROR 12-04 14:33:05 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=570079)[0;0m ERROR 12-04 14:33:05 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=570079)[0;0m ERROR 12-04 14:33:05 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=570079)[0;0m ERROR 12-04 14:33:05 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=570079)[0;0m ERROR 12-04 14:33:05 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=570079)[0;0m ERROR 12-04 14:33:05 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=570079)[0;0m ERROR 12-04 14:33:05 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 399, in __call__
[1;36m(EngineCore_DP0 pid=570079)[0;0m ERROR 12-04 14:33:05 [core.py:842]     return TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=570079)[0;0m ERROR 12-04 14:33:05 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=570079)[0;0m ERROR 12-04 14:33:05 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 152, in __call__
[1;36m(EngineCore_DP0 pid=570079)[0;0m ERROR 12-04 14:33:05 [core.py:842]     return self.forward(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=570079)[0;0m ERROR 12-04 14:33:05 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=570079)[0;0m ERROR 12-04 14:33:05 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 430, in forward
[1;36m(EngineCore_DP0 pid=570079)[0;0m ERROR 12-04 14:33:05 [core.py:842]     def forward(
[1;36m(EngineCore_DP0 pid=570079)[0;0m ERROR 12-04 14:33:05 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
[1;36m(EngineCore_DP0 pid=570079)[0;0m ERROR 12-04 14:33:05 [core.py:842]     return fn(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=570079)[0;0m ERROR 12-04 14:33:05 [core.py:842]            ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=570079)[0;0m ERROR 12-04 14:33:05 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/caching.py", line 53, in __call__
[1;36m(EngineCore_DP0 pid=570079)[0;0m ERROR 12-04 14:33:05 [core.py:842]     return self.optimized_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=570079)[0;0m ERROR 12-04 14:33:05 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=570079)[0;0m ERROR 12-04 14:33:05 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/graph_module.py", line 837, in call_wrapped
[1;36m(EngineCore_DP0 pid=570079)[0;0m ERROR 12-04 14:33:05 [core.py:842]     return self._wrapped_call(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=570079)[0;0m ERROR 12-04 14:33:05 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=570079)[0;0m ERROR 12-04 14:33:05 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/graph_module.py", line 413, in __call__
[1;36m(EngineCore_DP0 pid=570079)[0;0m ERROR 12-04 14:33:05 [core.py:842]     raise e
[1;36m(EngineCore_DP0 pid=570079)[0;0m ERROR 12-04 14:33:05 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/graph_module.py", line 400, in __call__
[1;36m(EngineCore_DP0 pid=570079)[0;0m ERROR 12-04 14:33:05 [core.py:842]     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
[1;36m(EngineCore_DP0 pid=570079)[0;0m ERROR 12-04 14:33:05 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=570079)[0;0m ERROR 12-04 14:33:05 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=570079)[0;0m ERROR 12-04 14:33:05 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=570079)[0;0m ERROR 12-04 14:33:05 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=570079)[0;0m ERROR 12-04 14:33:05 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=570079)[0;0m ERROR 12-04 14:33:05 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=570079)[0;0m ERROR 12-04 14:33:05 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=570079)[0;0m ERROR 12-04 14:33:05 [core.py:842]   File "<eval_with_key>.66", line 209, in forward
[1;36m(EngineCore_DP0 pid=570079)[0;0m ERROR 12-04 14:33:05 [core.py:842]     submod_2 = self.submod_2(getitem_3, s72, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_, getitem_4, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_3 = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_ = getitem_4 = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_ = None
[1;36m(EngineCore_DP0 pid=570079)[0;0m ERROR 12-04 14:33:05 [core.py:842]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=570079)[0;0m ERROR 12-04 14:33:05 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=570079)[0;0m ERROR 12-04 14:33:05 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=570079)[0;0m ERROR 12-04 14:33:05 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=570079)[0;0m ERROR 12-04 14:33:05 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/piecewise_backend.py", line 99, in __call__
[1;36m(EngineCore_DP0 pid=570079)[0;0m ERROR 12-04 14:33:05 [core.py:842]     return self.compiled_graph_for_general_shape(*args)
[1;36m(EngineCore_DP0 pid=570079)[0;0m ERROR 12-04 14:33:05 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=570079)[0;0m ERROR 12-04 14:33:05 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 268, in compiled_graph_wrapper
[1;36m(EngineCore_DP0 pid=570079)[0;0m ERROR 12-04 14:33:05 [core.py:842]     graph_output = inductor_compiled_graph(*args)
[1;36m(EngineCore_DP0 pid=570079)[0;0m ERROR 12-04 14:33:05 [core.py:842]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=570079)[0;0m ERROR 12-04 14:33:05 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
[1;36m(EngineCore_DP0 pid=570079)[0;0m ERROR 12-04 14:33:05 [core.py:842]     return self._compiled_fn(*args)
[1;36m(EngineCore_DP0 pid=570079)[0;0m ERROR 12-04 14:33:05 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=570079)[0;0m ERROR 12-04 14:33:05 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 184, in <lambda>
[1;36m(EngineCore_DP0 pid=570079)[0;0m ERROR 12-04 14:33:05 [core.py:842]     return CompiledArtifact(lambda *args: compiled_fn(list(args)), None)
[1;36m(EngineCore_DP0 pid=570079)[0;0m ERROR 12-04 14:33:05 [core.py:842]                                           ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=570079)[0;0m ERROR 12-04 14:33:05 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
[1;36m(EngineCore_DP0 pid=570079)[0;0m ERROR 12-04 14:33:05 [core.py:842]     all_outs = call_func_at_runtime_with_args(
[1;36m(EngineCore_DP0 pid=570079)[0;0m ERROR 12-04 14:33:05 [core.py:842]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=570079)[0;0m ERROR 12-04 14:33:05 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
[1;36m(EngineCore_DP0 pid=570079)[0;0m ERROR 12-04 14:33:05 [core.py:842]     out = normalize_as_list(f(args))
[1;36m(EngineCore_DP0 pid=570079)[0;0m ERROR 12-04 14:33:05 [core.py:842]                             ^^^^^^^
[1;36m(EngineCore_DP0 pid=570079)[0;0m ERROR 12-04 14:33:05 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
[1;36m(EngineCore_DP0 pid=570079)[0;0m ERROR 12-04 14:33:05 [core.py:842]     return compiled_fn(runtime_args)
[1;36m(EngineCore_DP0 pid=570079)[0;0m ERROR 12-04 14:33:05 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=570079)[0;0m ERROR 12-04 14:33:05 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/output_code.py", line 613, in __call__
[1;36m(EngineCore_DP0 pid=570079)[0;0m ERROR 12-04 14:33:05 [core.py:842]     return self.current_callable(inputs)
[1;36m(EngineCore_DP0 pid=570079)[0;0m ERROR 12-04 14:33:05 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=570079)[0;0m ERROR 12-04 14:33:05 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/utils.py", line 2962, in run
[1;36m(EngineCore_DP0 pid=570079)[0;0m ERROR 12-04 14:33:05 [core.py:842]     out = model(new_inputs)
[1;36m(EngineCore_DP0 pid=570079)[0;0m ERROR 12-04 14:33:05 [core.py:842]           ^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=570079)[0;0m ERROR 12-04 14:33:05 [core.py:842]   File "/tmp/torchinductor_ch269957/e6/ce6cgw7n42hkhqbz26qv76rj3cvq3sj2bzahzg62722bds6daraz.py", line 906, in call
[1;36m(EngineCore_DP0 pid=570079)[0;0m ERROR 12-04 14:33:05 [core.py:842]     buf3 = empty_strided_cuda((s72, 28672), (28672, 1), torch.bfloat16)
[1;36m(EngineCore_DP0 pid=570079)[0;0m ERROR 12-04 14:33:05 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=570079)[0;0m ERROR 12-04 14:33:05 [core.py:842] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 44.42 GiB of which 10.50 MiB is free. Including non-PyTorch memory, this process has 24.86 GiB memory in use. Process 570072 has 19.54 GiB memory in use. Of the allocated memory 24.50 GiB is allocated by PyTorch, with 75.88 MiB allocated in private pools (e.g., CUDA Graphs), and 21.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=570079)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=570079)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=570079)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=570079)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=570079)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=570079)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=570079)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=570079)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=570079)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=570079)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=570079)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=570079)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=570079)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=570079)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=570079)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=570079)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 247, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=570079)[0;0m     self.model_executor.initialize_from_config(kv_cache_configs)
[1;36m(EngineCore_DP0 pid=570079)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 116, in initialize_from_config
[1;36m(EngineCore_DP0 pid=570079)[0;0m     self.collective_rpc("compile_or_warm_up_model")
[1;36m(EngineCore_DP0 pid=570079)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=570079)[0;0m     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=570079)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=570079)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=570079)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=570079)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=570079)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 429, in compile_or_warm_up_model
[1;36m(EngineCore_DP0 pid=570079)[0;0m     cuda_graph_memory_bytes = self.model_runner.capture_model()
[1;36m(EngineCore_DP0 pid=570079)[0;0m                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=570079)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4201, in capture_model
[1;36m(EngineCore_DP0 pid=570079)[0;0m     self._capture_cudagraphs(
[1;36m(EngineCore_DP0 pid=570079)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4298, in _capture_cudagraphs
[1;36m(EngineCore_DP0 pid=570079)[0;0m     self._dummy_run(
[1;36m(EngineCore_DP0 pid=570079)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=570079)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=570079)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=570079)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=570079)[0;0m     outputs = self.model(
[1;36m(EngineCore_DP0 pid=570079)[0;0m               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=570079)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=570079)[0;0m     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=570079)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=570079)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=570079)[0;0m     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=570079)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=570079)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=570079)[0;0m     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=570079)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=570079)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=570079)[0;0m     model_output = self.model(
[1;36m(EngineCore_DP0 pid=570079)[0;0m                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=570079)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 399, in __call__
[1;36m(EngineCore_DP0 pid=570079)[0;0m     return TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=570079)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=570079)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 152, in __call__
[1;36m(EngineCore_DP0 pid=570079)[0;0m     return self.forward(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=570079)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=570079)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 430, in forward
[1;36m(EngineCore_DP0 pid=570079)[0;0m     def forward(
[1;36m(EngineCore_DP0 pid=570079)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
[1;36m(EngineCore_DP0 pid=570079)[0;0m     return fn(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=570079)[0;0m            ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=570079)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/caching.py", line 53, in __call__
[1;36m(EngineCore_DP0 pid=570079)[0;0m     return self.optimized_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=570079)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=570079)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/graph_module.py", line 837, in call_wrapped
[1;36m(EngineCore_DP0 pid=570079)[0;0m     return self._wrapped_call(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=570079)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=570079)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/graph_module.py", line 413, in __call__
[1;36m(EngineCore_DP0 pid=570079)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=570079)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/graph_module.py", line 400, in __call__
[1;36m(EngineCore_DP0 pid=570079)[0;0m     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
[1;36m(EngineCore_DP0 pid=570079)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=570079)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=570079)[0;0m     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=570079)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=570079)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=570079)[0;0m     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=570079)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=570079)[0;0m   File "<eval_with_key>.66", line 209, in forward
[1;36m(EngineCore_DP0 pid=570079)[0;0m     submod_2 = self.submod_2(getitem_3, s72, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_, getitem_4, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_3 = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_ = getitem_4 = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_ = None
[1;36m(EngineCore_DP0 pid=570079)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=570079)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=570079)[0;0m     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=570079)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=570079)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/piecewise_backend.py", line 99, in __call__
[1;36m(EngineCore_DP0 pid=570079)[0;0m     return self.compiled_graph_for_general_shape(*args)
[1;36m(EngineCore_DP0 pid=570079)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=570079)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 268, in compiled_graph_wrapper
[1;36m(EngineCore_DP0 pid=570079)[0;0m     graph_output = inductor_compiled_graph(*args)
[1;36m(EngineCore_DP0 pid=570079)[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=570079)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
[1;36m(EngineCore_DP0 pid=570079)[0;0m     return self._compiled_fn(*args)
[1;36m(EngineCore_DP0 pid=570079)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=570079)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 184, in <lambda>
[1;36m(EngineCore_DP0 pid=570079)[0;0m     return CompiledArtifact(lambda *args: compiled_fn(list(args)), None)
[1;36m(EngineCore_DP0 pid=570079)[0;0m                                           ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=570079)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
[1;36m(EngineCore_DP0 pid=570079)[0;0m     all_outs = call_func_at_runtime_with_args(
[1;36m(EngineCore_DP0 pid=570079)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=570079)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
[1;36m(EngineCore_DP0 pid=570079)[0;0m     out = normalize_as_list(f(args))
[1;36m(EngineCore_DP0 pid=570079)[0;0m                             ^^^^^^^
[1;36m(EngineCore_DP0 pid=570079)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
[1;36m(EngineCore_DP0 pid=570079)[0;0m     return compiled_fn(runtime_args)
[1;36m(EngineCore_DP0 pid=570079)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=570079)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/output_code.py", line 613, in __call__
[1;36m(EngineCore_DP0 pid=570079)[0;0m     return self.current_callable(inputs)
[1;36m(EngineCore_DP0 pid=570079)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=570079)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/utils.py", line 2962, in run
[1;36m(EngineCore_DP0 pid=570079)[0;0m     out = model(new_inputs)
[1;36m(EngineCore_DP0 pid=570079)[0;0m           ^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=570079)[0;0m   File "/tmp/torchinductor_ch269957/e6/ce6cgw7n42hkhqbz26qv76rj3cvq3sj2bzahzg62722bds6daraz.py", line 906, in call
[1;36m(EngineCore_DP0 pid=570079)[0;0m     buf3 = empty_strided_cuda((s72, 28672), (28672, 1), torch.bfloat16)
[1;36m(EngineCore_DP0 pid=570079)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=570079)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 44.42 GiB of which 10.50 MiB is free. Including non-PyTorch memory, this process has 24.86 GiB memory in use. Process 570072 has 19.54 GiB memory in use. Of the allocated memory 24.50 GiB is allocated by PyTorch, with 75.88 MiB allocated in private pools (e.g., CUDA Graphs), and 21.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W1204 14:33:06.953822060 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: mistralai/Mistral-7B-Instruct-v0.3 (backend=vllm)
[ModelCache] Using max_model_len=32768 for mistralai/Mistral-7B-Instruct-v0.3
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 14:33:27 [utils.py:253] non-default args: {'max_model_len': 32768, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'mistralai/Mistral-7B-Instruct-v0.3'}
INFO 12-04 14:33:27 [model.py:631] Resolved architecture: MistralForCausalLM
INFO 12-04 14:33:27 [model.py:1745] Using max model len 32768
INFO 12-04 14:33:27 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: mistralai/Mistral-7B-Instruct-v0.3 (backend=vllm)
[ModelCache] Using max_model_len=32768 for mistralai/Mistral-7B-Instruct-v0.3
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 14:33:27 [utils.py:253] non-default args: {'max_model_len': 32768, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'mistralai/Mistral-7B-Instruct-v0.3'}
INFO 12-04 14:33:27 [model.py:631] Resolved architecture: MistralForCausalLM
INFO 12-04 14:33:27 [model.py:1745] Using max model len 32768
INFO 12-04 14:33:27 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=571153)[0;0m INFO 12-04 14:33:40 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='mistralai/Mistral-7B-Instruct-v0.3', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.3', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=mistralai/Mistral-7B-Instruct-v0.3, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=571148)[0;0m INFO 12-04 14:33:40 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='mistralai/Mistral-7B-Instruct-v0.3', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.3', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=mistralai/Mistral-7B-Instruct-v0.3, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=571153)[0;0m INFO 12-04 14:33:42 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.9:59695 backend=nccl
[1;36m(EngineCore_DP0 pid=571148)[0;0m INFO 12-04 14:33:42 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.9:50743 backend=nccl
[W1204 14:33:42.775077031 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-21-2.rc.tch.harvard.edu]:59695 (errno: 97 - Address family not supported by protocol).
[W1204 14:33:42.775085054 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-21-2.rc.tch.harvard.edu]:50743 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=571148)[0;0m INFO 12-04 14:33:42 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=571153)[0;0m INFO 12-04 14:33:42 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=571153)[0;0m INFO 12-04 14:33:42 [gpu_model_runner.py:3259] Starting to load model mistralai/Mistral-7B-Instruct-v0.3...
[1;36m(EngineCore_DP0 pid=571148)[0;0m INFO 12-04 14:33:42 [gpu_model_runner.py:3259] Starting to load model mistralai/Mistral-7B-Instruct-v0.3...
[1;36m(EngineCore_DP0 pid=571148)[0;0m INFO 12-04 14:33:43 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=571148)[0;0m INFO 12-04 14:33:43 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=571153)[0;0m INFO 12-04 14:33:43 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=571153)[0;0m INFO 12-04 14:33:43 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=571153)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]
[1;36m(EngineCore_DP0 pid=571148)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]
[1;36m(EngineCore_DP0 pid=571153)[0;0m Loading safetensors checkpoint shards:  33% Completed | 1/3 [00:08<00:17,  8.87s/it]
[1;36m(EngineCore_DP0 pid=571148)[0;0m Loading safetensors checkpoint shards:  33% Completed | 1/3 [00:08<00:17,  8.76s/it]
[1;36m(EngineCore_DP0 pid=571153)[0;0m Loading safetensors checkpoint shards:  67% Completed | 2/3 [00:12<00:05,  5.63s/it]
[1;36m(EngineCore_DP0 pid=571148)[0;0m Loading safetensors checkpoint shards:  67% Completed | 2/3 [00:12<00:05,  5.58s/it]
[1;36m(EngineCore_DP0 pid=571153)[0;0m Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:16<00:00,  5.10s/it]
[1;36m(EngineCore_DP0 pid=571148)[0;0m Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:16<00:00,  5.08s/it]
[1;36m(EngineCore_DP0 pid=571153)[0;0m Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:16<00:00,  5.57s/it]
[1;36m(EngineCore_DP0 pid=571153)[0;0m 
[1;36m(EngineCore_DP0 pid=571148)[0;0m Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:16<00:00,  5.53s/it]
[1;36m(EngineCore_DP0 pid=571148)[0;0m 
[1;36m(EngineCore_DP0 pid=571153)[0;0m INFO 12-04 14:34:01 [default_loader.py:314] Loading weights took 16.90 seconds
[1;36m(EngineCore_DP0 pid=571148)[0;0m INFO 12-04 14:34:01 [default_loader.py:314] Loading weights took 16.79 seconds
[1;36m(EngineCore_DP0 pid=571148)[0;0m INFO 12-04 14:34:01 [gpu_model_runner.py:3338] Model loading took 13.5084 GiB memory and 17.995536 seconds
[1;36m(EngineCore_DP0 pid=571153)[0;0m INFO 12-04 14:34:01 [gpu_model_runner.py:3338] Model loading took 13.5084 GiB memory and 17.994359 seconds
[1;36m(EngineCore_DP0 pid=571153)[0;0m INFO 12-04 14:34:11 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/0ef74f8a17/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=571153)[0;0m INFO 12-04 14:34:11 [backends.py:647] Dynamo bytecode transform time: 9.81 s
[1;36m(EngineCore_DP0 pid=571148)[0;0m INFO 12-04 14:34:11 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/0ef74f8a17/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=571148)[0;0m INFO 12-04 14:34:11 [backends.py:647] Dynamo bytecode transform time: 9.84 s
[1;36m(EngineCore_DP0 pid=571148)[0;0m INFO 12-04 14:34:17 [backends.py:210] Directly load the compiled graph(s) for dynamic shape from the cache, took 4.812 s
[1;36m(EngineCore_DP0 pid=571153)[0;0m INFO 12-04 14:34:17 [backends.py:210] Directly load the compiled graph(s) for dynamic shape from the cache, took 4.841 s
[1;36m(EngineCore_DP0 pid=571153)[0;0m INFO 12-04 14:34:19 [monitor.py:34] torch.compile takes 14.65 s in total
[1;36m(EngineCore_DP0 pid=571148)[0;0m INFO 12-04 14:34:19 [monitor.py:34] torch.compile takes 14.65 s in total
[1;36m(EngineCore_DP0 pid=571153)[0;0m INFO 12-04 14:34:22 [gpu_worker.py:359] Available KV cache memory: 10.79 GiB
[1;36m(EngineCore_DP0 pid=571148)[0;0m INFO 12-04 14:34:22 [gpu_worker.py:359] Available KV cache memory: 11.97 GiB
[1;36m(EngineCore_DP0 pid=571153)[0;0m INFO 12-04 14:34:22 [kv_cache_utils.py:1229] GPU KV cache size: 88,416 tokens
[1;36m(EngineCore_DP0 pid=571153)[0;0m INFO 12-04 14:34:22 [kv_cache_utils.py:1234] Maximum concurrency for 32,768 tokens per request: 2.70x
[1;36m(EngineCore_DP0 pid=571148)[0;0m INFO 12-04 14:34:22 [kv_cache_utils.py:1229] GPU KV cache size: 98,016 tokens
[1;36m(EngineCore_DP0 pid=571148)[0;0m INFO 12-04 14:34:22 [kv_cache_utils.py:1234] Maximum concurrency for 32,768 tokens per request: 2.99x
[1;36m(EngineCore_DP0 pid=571148)[0;0m ERROR 12-04 14:34:22 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=571148)[0;0m ERROR 12-04 14:34:22 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=571148)[0;0m ERROR 12-04 14:34:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=571148)[0;0m ERROR 12-04 14:34:22 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=571148)[0;0m ERROR 12-04 14:34:22 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=571148)[0;0m ERROR 12-04 14:34:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=571148)[0;0m ERROR 12-04 14:34:22 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=571148)[0;0m ERROR 12-04 14:34:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=571148)[0;0m ERROR 12-04 14:34:22 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=571148)[0;0m ERROR 12-04 14:34:22 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=571148)[0;0m ERROR 12-04 14:34:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 247, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=571148)[0;0m ERROR 12-04 14:34:22 [core.py:842]     self.model_executor.initialize_from_config(kv_cache_configs)
[1;36m(EngineCore_DP0 pid=571148)[0;0m ERROR 12-04 14:34:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 115, in initialize_from_config
[1;36m(EngineCore_DP0 pid=571148)[0;0m ERROR 12-04 14:34:22 [core.py:842]     self.collective_rpc("initialize_from_config", args=(kv_cache_configs,))
[1;36m(EngineCore_DP0 pid=571148)[0;0m ERROR 12-04 14:34:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=571148)[0;0m ERROR 12-04 14:34:22 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=571148)[0;0m ERROR 12-04 14:34:22 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=571148)[0;0m ERROR 12-04 14:34:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=571148)[0;0m ERROR 12-04 14:34:22 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=571148)[0;0m ERROR 12-04 14:34:22 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=571148)[0;0m ERROR 12-04 14:34:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 319, in initialize_from_config
[1;36m(EngineCore_DP0 pid=571148)[0;0m ERROR 12-04 14:34:22 [core.py:842]     self.worker.initialize_from_config(kv_cache_config)  # type: ignore
[1;36m(EngineCore_DP0 pid=571148)[0;0m ERROR 12-04 14:34:22 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=571148)[0;0m ERROR 12-04 14:34:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 404, in initialize_from_config
[1;36m(EngineCore_DP0 pid=571148)[0;0m ERROR 12-04 14:34:22 [core.py:842]     self.model_runner.initialize_kv_cache(kv_cache_config)
[1;36m(EngineCore_DP0 pid=571148)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=571148)[0;0m ERROR 12-04 14:34:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 5007, in initialize_kv_cache
[1;36m(EngineCore_DP0 pid=571148)[0;0m ERROR 12-04 14:34:22 [core.py:842]     kv_caches = self.initialize_kv_cache_tensors(
[1;36m(EngineCore_DP0 pid=571148)[0;0m ERROR 12-04 14:34:22 [core.py:842]                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=571148)[0;0m ERROR 12-04 14:34:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4933, in initialize_kv_cache_tensors
[1;36m(EngineCore_DP0 pid=571148)[0;0m ERROR 12-04 14:34:22 [core.py:842]     kv_cache_raw_tensors = self._allocate_kv_cache_tensors(kv_cache_config)
[1;36m(EngineCore_DP0 pid=571148)[0;0m ERROR 12-04 14:34:22 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=571148)[0;0m ERROR 12-04 14:34:22 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4718, in _allocate_kv_cache_tensors
[1;36m(EngineCore_DP0 pid=571148)[0;0m ERROR 12-04 14:34:22 [core.py:842]     tensor = torch.zeros(
[1;36m(EngineCore_DP0 pid=571148)[0;0m ERROR 12-04 14:34:22 [core.py:842]              ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=571148)[0;0m ERROR 12-04 14:34:22 [core.py:842] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 384.00 MiB. GPU 0 has a total capacity of 44.42 GiB of which 138.50 MiB is free. Process 571153 has 24.73 GiB memory in use. Including non-PyTorch memory, this process has 19.54 GiB memory in use. Of the allocated memory 19.19 GiB is allocated by PyTorch, and 19.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=571148)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=571148)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=571148)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=571148)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=571148)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=571148)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=571148)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=571148)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=571148)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=571148)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=571148)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=571148)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=571148)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=571148)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=571148)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=571148)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 247, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=571148)[0;0m     self.model_executor.initialize_from_config(kv_cache_configs)
[1;36m(EngineCore_DP0 pid=571148)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 115, in initialize_from_config
[1;36m(EngineCore_DP0 pid=571148)[0;0m     self.collective_rpc("initialize_from_config", args=(kv_cache_configs,))
[1;36m(EngineCore_DP0 pid=571148)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=571148)[0;0m     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=571148)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=571148)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=571148)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=571148)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=571148)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 319, in initialize_from_config
[1;36m(EngineCore_DP0 pid=571148)[0;0m     self.worker.initialize_from_config(kv_cache_config)  # type: ignore
[1;36m(EngineCore_DP0 pid=571148)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=571148)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 404, in initialize_from_config
[1;36m(EngineCore_DP0 pid=571148)[0;0m     self.model_runner.initialize_kv_cache(kv_cache_config)
[1;36m(EngineCore_DP0 pid=571148)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 5007, in initialize_kv_cache
[1;36m(EngineCore_DP0 pid=571148)[0;0m     kv_caches = self.initialize_kv_cache_tensors(
[1;36m(EngineCore_DP0 pid=571148)[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=571148)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4933, in initialize_kv_cache_tensors
[1;36m(EngineCore_DP0 pid=571148)[0;0m     kv_cache_raw_tensors = self._allocate_kv_cache_tensors(kv_cache_config)
[1;36m(EngineCore_DP0 pid=571148)[0;0m                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=571148)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4718, in _allocate_kv_cache_tensors
[1;36m(EngineCore_DP0 pid=571148)[0;0m     tensor = torch.zeros(
[1;36m(EngineCore_DP0 pid=571148)[0;0m              ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=571148)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 384.00 MiB. GPU 0 has a total capacity of 44.42 GiB of which 138.50 MiB is free. Process 571153 has 24.73 GiB memory in use. Including non-PyTorch memory, this process has 19.54 GiB memory in use. Of the allocated memory 19.19 GiB is allocated by PyTorch, and 19.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=571153)[0;0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/51 [00:00<?, ?it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   2%|▏         | 1/51 [00:00<00:05,  9.63it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   2%|▏         | 1/51 [00:00<00:05,  8.74it/s]
[1;36m(EngineCore_DP0 pid=571153)[0;0m ERROR 12-04 14:34:23 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=571153)[0;0m ERROR 12-04 14:34:23 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=571153)[0;0m ERROR 12-04 14:34:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=571153)[0;0m ERROR 12-04 14:34:23 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=571153)[0;0m ERROR 12-04 14:34:23 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=571153)[0;0m ERROR 12-04 14:34:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=571153)[0;0m ERROR 12-04 14:34:23 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=571153)[0;0m ERROR 12-04 14:34:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=571153)[0;0m ERROR 12-04 14:34:23 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=571153)[0;0m ERROR 12-04 14:34:23 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=571153)[0;0m ERROR 12-04 14:34:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 247, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=571153)[0;0m ERROR 12-04 14:34:23 [core.py:842]     self.model_executor.initialize_from_config(kv_cache_configs)
[1;36m(EngineCore_DP0 pid=571153)[0;0m ERROR 12-04 14:34:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 116, in initialize_from_config
[1;36m(EngineCore_DP0 pid=571153)[0;0m ERROR 12-04 14:34:23 [core.py:842]     self.collective_rpc("compile_or_warm_up_model")
[1;36m(EngineCore_DP0 pid=571153)[0;0m ERROR 12-04 14:34:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=571153)[0;0m ERROR 12-04 14:34:23 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=571153)[0;0m ERROR 12-04 14:34:23 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=571153)[0;0m ERROR 12-04 14:34:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=571153)[0;0m ERROR 12-04 14:34:23 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=571153)[0;0m ERROR 12-04 14:34:23 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=571153)[0;0m ERROR 12-04 14:34:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 429, in compile_or_warm_up_model
[1;36m(EngineCore_DP0 pid=571153)[0;0m ERROR 12-04 14:34:23 [core.py:842]     cuda_graph_memory_bytes = self.model_runner.capture_model()
[1;36m(EngineCore_DP0 pid=571153)[0;0m ERROR 12-04 14:34:23 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=571153)[0;0m ERROR 12-04 14:34:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4201, in capture_model
[1;36m(EngineCore_DP0 pid=571153)[0;0m ERROR 12-04 14:34:23 [core.py:842]     self._capture_cudagraphs(
[1;36m(EngineCore_DP0 pid=571153)[0;0m ERROR 12-04 14:34:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4298, in _capture_cudagraphs
[1;36m(EngineCore_DP0 pid=571153)[0;0m ERROR 12-04 14:34:23 [core.py:842]     self._dummy_run(
[1;36m(EngineCore_DP0 pid=571153)[0;0m ERROR 12-04 14:34:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=571153)[0;0m ERROR 12-04 14:34:23 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=571153)[0;0m ERROR 12-04 14:34:23 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=571153)[0;0m ERROR 12-04 14:34:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=571153)[0;0m ERROR 12-04 14:34:23 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=571153)[0;0m ERROR 12-04 14:34:23 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=571153)[0;0m ERROR 12-04 14:34:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=571153)[0;0m ERROR 12-04 14:34:23 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=571153)[0;0m ERROR 12-04 14:34:23 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=571153)[0;0m ERROR 12-04 14:34:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=571153)[0;0m ERROR 12-04 14:34:23 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=571153)[0;0m ERROR 12-04 14:34:23 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=571153)[0;0m ERROR 12-04 14:34:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=571153)[0;0m ERROR 12-04 14:34:23 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=571153)[0;0m ERROR 12-04 14:34:23 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=571153)[0;0m ERROR 12-04 14:34:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=571153)[0;0m ERROR 12-04 14:34:23 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=571153)[0;0m ERROR 12-04 14:34:23 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=571153)[0;0m ERROR 12-04 14:34:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 399, in __call__
[1;36m(EngineCore_DP0 pid=571153)[0;0m ERROR 12-04 14:34:23 [core.py:842]     return TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=571153)[0;0m ERROR 12-04 14:34:23 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=571153)[0;0m ERROR 12-04 14:34:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 152, in __call__
[1;36m(EngineCore_DP0 pid=571153)[0;0m ERROR 12-04 14:34:23 [core.py:842]     return self.forward(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=571153)[0;0m ERROR 12-04 14:34:23 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=571153)[0;0m ERROR 12-04 14:34:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 430, in forward
[1;36m(EngineCore_DP0 pid=571153)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=571153)[0;0m ERROR 12-04 14:34:23 [core.py:842]     def forward(
[1;36m(EngineCore_DP0 pid=571153)[0;0m ERROR 12-04 14:34:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
[1;36m(EngineCore_DP0 pid=571153)[0;0m ERROR 12-04 14:34:23 [core.py:842]     return fn(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=571153)[0;0m ERROR 12-04 14:34:23 [core.py:842]            ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=571153)[0;0m ERROR 12-04 14:34:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/caching.py", line 53, in __call__
[1;36m(EngineCore_DP0 pid=571153)[0;0m ERROR 12-04 14:34:23 [core.py:842]     return self.optimized_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=571153)[0;0m ERROR 12-04 14:34:23 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=571153)[0;0m ERROR 12-04 14:34:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/graph_module.py", line 837, in call_wrapped
[1;36m(EngineCore_DP0 pid=571153)[0;0m ERROR 12-04 14:34:23 [core.py:842]     return self._wrapped_call(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=571153)[0;0m ERROR 12-04 14:34:23 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=571153)[0;0m ERROR 12-04 14:34:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/graph_module.py", line 413, in __call__
[1;36m(EngineCore_DP0 pid=571153)[0;0m ERROR 12-04 14:34:23 [core.py:842]     raise e
[1;36m(EngineCore_DP0 pid=571153)[0;0m ERROR 12-04 14:34:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/graph_module.py", line 400, in __call__
[1;36m(EngineCore_DP0 pid=571153)[0;0m ERROR 12-04 14:34:23 [core.py:842]     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
[1;36m(EngineCore_DP0 pid=571153)[0;0m ERROR 12-04 14:34:23 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=571153)[0;0m ERROR 12-04 14:34:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=571153)[0;0m ERROR 12-04 14:34:23 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=571153)[0;0m ERROR 12-04 14:34:23 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=571153)[0;0m ERROR 12-04 14:34:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=571153)[0;0m ERROR 12-04 14:34:23 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=571153)[0;0m ERROR 12-04 14:34:23 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=571153)[0;0m ERROR 12-04 14:34:23 [core.py:842]   File "<eval_with_key>.66", line 209, in forward
[1;36m(EngineCore_DP0 pid=571153)[0;0m ERROR 12-04 14:34:23 [core.py:842]     submod_2 = self.submod_2(getitem_3, s72, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_, getitem_4, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_3 = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_ = getitem_4 = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_ = None
[1;36m(EngineCore_DP0 pid=571153)[0;0m ERROR 12-04 14:34:23 [core.py:842]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=571153)[0;0m ERROR 12-04 14:34:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=571153)[0;0m ERROR 12-04 14:34:23 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=571153)[0;0m ERROR 12-04 14:34:23 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=571153)[0;0m ERROR 12-04 14:34:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/piecewise_backend.py", line 99, in __call__
[1;36m(EngineCore_DP0 pid=571153)[0;0m ERROR 12-04 14:34:23 [core.py:842]     return self.compiled_graph_for_general_shape(*args)
[1;36m(EngineCore_DP0 pid=571153)[0;0m ERROR 12-04 14:34:23 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=571153)[0;0m ERROR 12-04 14:34:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 268, in compiled_graph_wrapper
[1;36m(EngineCore_DP0 pid=571153)[0;0m ERROR 12-04 14:34:23 [core.py:842]     graph_output = inductor_compiled_graph(*args)
[1;36m(EngineCore_DP0 pid=571153)[0;0m ERROR 12-04 14:34:23 [core.py:842]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=571153)[0;0m ERROR 12-04 14:34:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
[1;36m(EngineCore_DP0 pid=571153)[0;0m ERROR 12-04 14:34:23 [core.py:842]     return self._compiled_fn(*args)
[1;36m(EngineCore_DP0 pid=571153)[0;0m ERROR 12-04 14:34:23 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=571153)[0;0m ERROR 12-04 14:34:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 184, in <lambda>
[1;36m(EngineCore_DP0 pid=571153)[0;0m ERROR 12-04 14:34:23 [core.py:842]     return CompiledArtifact(lambda *args: compiled_fn(list(args)), None)
[1;36m(EngineCore_DP0 pid=571153)[0;0m ERROR 12-04 14:34:23 [core.py:842]                                           ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=571153)[0;0m ERROR 12-04 14:34:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
[1;36m(EngineCore_DP0 pid=571153)[0;0m ERROR 12-04 14:34:23 [core.py:842]     all_outs = call_func_at_runtime_with_args(
[1;36m(EngineCore_DP0 pid=571153)[0;0m ERROR 12-04 14:34:23 [core.py:842]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=571153)[0;0m ERROR 12-04 14:34:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
[1;36m(EngineCore_DP0 pid=571153)[0;0m ERROR 12-04 14:34:23 [core.py:842]     out = normalize_as_list(f(args))
[1;36m(EngineCore_DP0 pid=571153)[0;0m ERROR 12-04 14:34:23 [core.py:842]                             ^^^^^^^
[1;36m(EngineCore_DP0 pid=571153)[0;0m ERROR 12-04 14:34:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
[1;36m(EngineCore_DP0 pid=571153)[0;0m ERROR 12-04 14:34:23 [core.py:842]     return compiled_fn(runtime_args)
[1;36m(EngineCore_DP0 pid=571153)[0;0m ERROR 12-04 14:34:23 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=571153)[0;0m ERROR 12-04 14:34:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/output_code.py", line 613, in __call__
[1;36m(EngineCore_DP0 pid=571153)[0;0m ERROR 12-04 14:34:23 [core.py:842]     return self.current_callable(inputs)
[1;36m(EngineCore_DP0 pid=571153)[0;0m ERROR 12-04 14:34:23 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=571153)[0;0m ERROR 12-04 14:34:23 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/utils.py", line 2962, in run
[1;36m(EngineCore_DP0 pid=571153)[0;0m ERROR 12-04 14:34:23 [core.py:842]     out = model(new_inputs)
[1;36m(EngineCore_DP0 pid=571153)[0;0m ERROR 12-04 14:34:23 [core.py:842]           ^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=571153)[0;0m ERROR 12-04 14:34:23 [core.py:842]   File "/tmp/torchinductor_ch269957/e6/ce6cgw7n42hkhqbz26qv76rj3cvq3sj2bzahzg62722bds6daraz.py", line 906, in call
[1;36m(EngineCore_DP0 pid=571153)[0;0m ERROR 12-04 14:34:23 [core.py:842]     buf3 = empty_strided_cuda((s72, 28672), (28672, 1), torch.bfloat16)
[1;36m(EngineCore_DP0 pid=571153)[0;0m ERROR 12-04 14:34:23 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=571153)[0;0m ERROR 12-04 14:34:23 [core.py:842] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 44.42 GiB of which 10.50 MiB is free. Including non-PyTorch memory, this process has 24.86 GiB memory in use. Process 571148 has 19.54 GiB memory in use. Of the allocated memory 24.50 GiB is allocated by PyTorch, with 75.88 MiB allocated in private pools (e.g., CUDA Graphs), and 21.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=571153)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=571153)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=571153)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=571153)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=571153)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=571153)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=571153)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=571153)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=571153)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=571153)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=571153)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=571153)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=571153)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=571153)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=571153)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=571153)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 247, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=571153)[0;0m     self.model_executor.initialize_from_config(kv_cache_configs)
[1;36m(EngineCore_DP0 pid=571153)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 116, in initialize_from_config
[1;36m(EngineCore_DP0 pid=571153)[0;0m     self.collective_rpc("compile_or_warm_up_model")
[1;36m(EngineCore_DP0 pid=571153)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=571153)[0;0m     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=571153)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=571153)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=571153)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=571153)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=571153)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 429, in compile_or_warm_up_model
[1;36m(EngineCore_DP0 pid=571153)[0;0m     cuda_graph_memory_bytes = self.model_runner.capture_model()
[1;36m(EngineCore_DP0 pid=571153)[0;0m                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=571153)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4201, in capture_model
[1;36m(EngineCore_DP0 pid=571153)[0;0m     self._capture_cudagraphs(
[1;36m(EngineCore_DP0 pid=571153)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4298, in _capture_cudagraphs
[1;36m(EngineCore_DP0 pid=571153)[0;0m     self._dummy_run(
[1;36m(EngineCore_DP0 pid=571153)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=571153)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=571153)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=571153)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=571153)[0;0m     outputs = self.model(
[1;36m(EngineCore_DP0 pid=571153)[0;0m               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=571153)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=571153)[0;0m     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=571153)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=571153)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=571153)[0;0m     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=571153)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=571153)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=571153)[0;0m     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=571153)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=571153)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=571153)[0;0m     model_output = self.model(
[1;36m(EngineCore_DP0 pid=571153)[0;0m                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=571153)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 399, in __call__
[1;36m(EngineCore_DP0 pid=571153)[0;0m     return TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=571153)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=571153)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 152, in __call__
[1;36m(EngineCore_DP0 pid=571153)[0;0m     return self.forward(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=571153)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=571153)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 430, in forward
[1;36m(EngineCore_DP0 pid=571153)[0;0m     def forward(
[1;36m(EngineCore_DP0 pid=571153)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
[1;36m(EngineCore_DP0 pid=571153)[0;0m     return fn(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=571153)[0;0m            ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=571153)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/caching.py", line 53, in __call__
[1;36m(EngineCore_DP0 pid=571153)[0;0m     return self.optimized_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=571153)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=571153)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/graph_module.py", line 837, in call_wrapped
[1;36m(EngineCore_DP0 pid=571153)[0;0m     return self._wrapped_call(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=571153)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=571153)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/graph_module.py", line 413, in __call__
[1;36m(EngineCore_DP0 pid=571153)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=571153)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/graph_module.py", line 400, in __call__
[1;36m(EngineCore_DP0 pid=571153)[0;0m     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
[1;36m(EngineCore_DP0 pid=571153)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=571153)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=571153)[0;0m     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=571153)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=571153)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=571153)[0;0m     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=571153)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=571153)[0;0m   File "<eval_with_key>.66", line 209, in forward
[1;36m(EngineCore_DP0 pid=571153)[0;0m     submod_2 = self.submod_2(getitem_3, s72, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_, getitem_4, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_3 = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_ = getitem_4 = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_ = None
[1;36m(EngineCore_DP0 pid=571153)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=571153)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=571153)[0;0m     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=571153)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=571153)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/piecewise_backend.py", line 99, in __call__
[1;36m(EngineCore_DP0 pid=571153)[0;0m     return self.compiled_graph_for_general_shape(*args)
[1;36m(EngineCore_DP0 pid=571153)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=571153)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 268, in compiled_graph_wrapper
[1;36m(EngineCore_DP0 pid=571153)[0;0m     graph_output = inductor_compiled_graph(*args)
[1;36m(EngineCore_DP0 pid=571153)[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=571153)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
[1;36m(EngineCore_DP0 pid=571153)[0;0m     return self._compiled_fn(*args)
[1;36m(EngineCore_DP0 pid=571153)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=571153)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 184, in <lambda>
[1;36m(EngineCore_DP0 pid=571153)[0;0m     return CompiledArtifact(lambda *args: compiled_fn(list(args)), None)
[1;36m(EngineCore_DP0 pid=571153)[0;0m                                           ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=571153)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
[1;36m(EngineCore_DP0 pid=571153)[0;0m     all_outs = call_func_at_runtime_with_args(
[1;36m(EngineCore_DP0 pid=571153)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=571153)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
[1;36m(EngineCore_DP0 pid=571153)[0;0m     out = normalize_as_list(f(args))
[1;36m(EngineCore_DP0 pid=571153)[0;0m                             ^^^^^^^
[1;36m(EngineCore_DP0 pid=571153)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
[1;36m(EngineCore_DP0 pid=571153)[0;0m     return compiled_fn(runtime_args)
[1;36m(EngineCore_DP0 pid=571153)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=571153)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/output_code.py", line 613, in __call__
[1;36m(EngineCore_DP0 pid=571153)[0;0m     return self.current_callable(inputs)
[1;36m(EngineCore_DP0 pid=571153)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=571153)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/utils.py", line 2962, in run
[1;36m(EngineCore_DP0 pid=571153)[0;0m     out = model(new_inputs)
[1;36m(EngineCore_DP0 pid=571153)[0;0m           ^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=571153)[0;0m   File "/tmp/torchinductor_ch269957/e6/ce6cgw7n42hkhqbz26qv76rj3cvq3sj2bzahzg62722bds6daraz.py", line 906, in call
[1;36m(EngineCore_DP0 pid=571153)[0;0m     buf3 = empty_strided_cuda((s72, 28672), (28672, 1), torch.bfloat16)
[1;36m(EngineCore_DP0 pid=571153)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=571153)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 44.42 GiB of which 10.50 MiB is free. Including non-PyTorch memory, this process has 24.86 GiB memory in use. Process 571148 has 19.54 GiB memory in use. Of the allocated memory 24.50 GiB is allocated by PyTorch, with 75.88 MiB allocated in private pools (e.g., CUDA Graphs), and 21.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W1204 14:34:23.084745869 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank0]:[W1204 14:34:24.845867396 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: mistralai/Mistral-7B-Instruct-v0.3 (backend=vllm)
[ModelCache] Using max_model_len=32768 for mistralai/Mistral-7B-Instruct-v0.3
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 14:34:44 [utils.py:253] non-default args: {'max_model_len': 32768, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'mistralai/Mistral-7B-Instruct-v0.3'}
INFO 12-04 14:34:45 [model.py:631] Resolved architecture: MistralForCausalLM
INFO 12-04 14:34:45 [model.py:1745] Using max model len 32768
INFO 12-04 14:34:45 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: mistralai/Mistral-7B-Instruct-v0.3 (backend=vllm)
[ModelCache] Using max_model_len=32768 for mistralai/Mistral-7B-Instruct-v0.3
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 14:34:45 [utils.py:253] non-default args: {'max_model_len': 32768, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'mistralai/Mistral-7B-Instruct-v0.3'}
INFO 12-04 14:34:45 [model.py:631] Resolved architecture: MistralForCausalLM
INFO 12-04 14:34:45 [model.py:1745] Using max model len 32768
INFO 12-04 14:34:45 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=572804)[0;0m INFO 12-04 14:34:59 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='mistralai/Mistral-7B-Instruct-v0.3', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.3', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=mistralai/Mistral-7B-Instruct-v0.3, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=572801)[0;0m INFO 12-04 14:34:59 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='mistralai/Mistral-7B-Instruct-v0.3', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.3', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=mistralai/Mistral-7B-Instruct-v0.3, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=572804)[0;0m INFO 12-04 14:35:01 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.9:56807 backend=nccl
[1;36m(EngineCore_DP0 pid=572801)[0;0m INFO 12-04 14:35:01 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.9:47713 backend=nccl
[W1204 14:35:01.959528192 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-21-2.rc.tch.harvard.edu]:56807 (errno: 97 - Address family not supported by protocol).
[W1204 14:35:01.963059375 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-21-2.rc.tch.harvard.edu]:47713 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=572804)[0;0m INFO 12-04 14:35:01 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=572801)[0;0m INFO 12-04 14:35:01 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=572804)[0;0m INFO 12-04 14:35:01 [gpu_model_runner.py:3259] Starting to load model mistralai/Mistral-7B-Instruct-v0.3...
[1;36m(EngineCore_DP0 pid=572801)[0;0m INFO 12-04 14:35:01 [gpu_model_runner.py:3259] Starting to load model mistralai/Mistral-7B-Instruct-v0.3...
[1;36m(EngineCore_DP0 pid=572804)[0;0m INFO 12-04 14:35:03 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=572801)[0;0m INFO 12-04 14:35:03 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=572801)[0;0m INFO 12-04 14:35:03 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=572804)[0;0m INFO 12-04 14:35:03 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=572801)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]
[1;36m(EngineCore_DP0 pid=572804)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]
[1;36m(EngineCore_DP0 pid=572804)[0;0m Loading safetensors checkpoint shards:  33% Completed | 1/3 [00:11<00:22, 11.28s/it]
[1;36m(EngineCore_DP0 pid=572801)[0;0m Loading safetensors checkpoint shards:  33% Completed | 1/3 [00:11<00:22, 11.37s/it]
[1;36m(EngineCore_DP0 pid=572804)[0;0m Loading safetensors checkpoint shards:  67% Completed | 2/3 [00:18<00:09,  9.17s/it]
[1;36m(EngineCore_DP0 pid=572801)[0;0m Loading safetensors checkpoint shards:  67% Completed | 2/3 [00:19<00:09,  9.20s/it]
[1;36m(EngineCore_DP0 pid=572804)[0;0m Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:24<00:00,  7.29s/it]
[1;36m(EngineCore_DP0 pid=572801)[0;0m Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:24<00:00,  7.31s/it]
[1;36m(EngineCore_DP0 pid=572804)[0;0m Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:24<00:00,  8.01s/it]
[1;36m(EngineCore_DP0 pid=572804)[0;0m 
[1;36m(EngineCore_DP0 pid=572801)[0;0m Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:24<00:00,  8.04s/it]
[1;36m(EngineCore_DP0 pid=572801)[0;0m 
[1;36m(EngineCore_DP0 pid=572801)[0;0m INFO 12-04 14:35:27 [default_loader.py:314] Loading weights took 24.31 seconds
[1;36m(EngineCore_DP0 pid=572804)[0;0m INFO 12-04 14:35:27 [default_loader.py:314] Loading weights took 24.25 seconds
[1;36m(EngineCore_DP0 pid=572804)[0;0m INFO 12-04 14:35:28 [gpu_model_runner.py:3338] Model loading took 13.5084 GiB memory and 25.479462 seconds
[1;36m(EngineCore_DP0 pid=572801)[0;0m INFO 12-04 14:35:28 [gpu_model_runner.py:3338] Model loading took 13.5084 GiB memory and 25.445512 seconds
[1;36m(EngineCore_DP0 pid=572801)[0;0m INFO 12-04 14:35:38 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/0ef74f8a17/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=572804)[0;0m INFO 12-04 14:35:38 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/0ef74f8a17/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=572801)[0;0m INFO 12-04 14:35:38 [backends.py:647] Dynamo bytecode transform time: 10.00 s
[1;36m(EngineCore_DP0 pid=572804)[0;0m INFO 12-04 14:35:38 [backends.py:647] Dynamo bytecode transform time: 10.01 s
[1;36m(EngineCore_DP0 pid=572804)[0;0m INFO 12-04 14:35:45 [backends.py:210] Directly load the compiled graph(s) for dynamic shape from the cache, took 5.905 s
[1;36m(EngineCore_DP0 pid=572801)[0;0m INFO 12-04 14:35:45 [backends.py:210] Directly load the compiled graph(s) for dynamic shape from the cache, took 5.900 s
[1;36m(EngineCore_DP0 pid=572801)[0;0m INFO 12-04 14:35:47 [monitor.py:34] torch.compile takes 15.90 s in total
[1;36m(EngineCore_DP0 pid=572804)[0;0m INFO 12-04 14:35:47 [monitor.py:34] torch.compile takes 15.92 s in total
[1;36m(EngineCore_DP0 pid=572804)[0;0m INFO 12-04 14:35:50 [gpu_worker.py:359] Available KV cache memory: 10.79 GiB
[1;36m(EngineCore_DP0 pid=572801)[0;0m INFO 12-04 14:35:50 [gpu_worker.py:359] Available KV cache memory: 11.97 GiB
[1;36m(EngineCore_DP0 pid=572804)[0;0m INFO 12-04 14:35:51 [kv_cache_utils.py:1229] GPU KV cache size: 88,416 tokens
[1;36m(EngineCore_DP0 pid=572804)[0;0m INFO 12-04 14:35:51 [kv_cache_utils.py:1234] Maximum concurrency for 32,768 tokens per request: 2.70x
[1;36m(EngineCore_DP0 pid=572801)[0;0m INFO 12-04 14:35:51 [kv_cache_utils.py:1229] GPU KV cache size: 98,016 tokens
[1;36m(EngineCore_DP0 pid=572801)[0;0m INFO 12-04 14:35:51 [kv_cache_utils.py:1234] Maximum concurrency for 32,768 tokens per request: 2.99x
[1;36m(EngineCore_DP0 pid=572801)[0;0m ERROR 12-04 14:35:51 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=572801)[0;0m ERROR 12-04 14:35:51 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=572801)[0;0m ERROR 12-04 14:35:51 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=572801)[0;0m ERROR 12-04 14:35:51 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=572801)[0;0m ERROR 12-04 14:35:51 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=572801)[0;0m ERROR 12-04 14:35:51 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=572801)[0;0m ERROR 12-04 14:35:51 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=572801)[0;0m ERROR 12-04 14:35:51 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=572801)[0;0m ERROR 12-04 14:35:51 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=572801)[0;0m ERROR 12-04 14:35:51 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=572801)[0;0m ERROR 12-04 14:35:51 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 247, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=572801)[0;0m ERROR 12-04 14:35:51 [core.py:842]     self.model_executor.initialize_from_config(kv_cache_configs)
[1;36m(EngineCore_DP0 pid=572801)[0;0m ERROR 12-04 14:35:51 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 115, in initialize_from_config
[1;36m(EngineCore_DP0 pid=572801)[0;0m ERROR 12-04 14:35:51 [core.py:842]     self.collective_rpc("initialize_from_config", args=(kv_cache_configs,))
[1;36m(EngineCore_DP0 pid=572801)[0;0m ERROR 12-04 14:35:51 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=572801)[0;0m ERROR 12-04 14:35:51 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=572801)[0;0m ERROR 12-04 14:35:51 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=572801)[0;0m ERROR 12-04 14:35:51 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=572801)[0;0m ERROR 12-04 14:35:51 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=572801)[0;0m ERROR 12-04 14:35:51 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=572801)[0;0m ERROR 12-04 14:35:51 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 319, in initialize_from_config
[1;36m(EngineCore_DP0 pid=572801)[0;0m ERROR 12-04 14:35:51 [core.py:842]     self.worker.initialize_from_config(kv_cache_config)  # type: ignore
[1;36m(EngineCore_DP0 pid=572801)[0;0m ERROR 12-04 14:35:51 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=572801)[0;0m ERROR 12-04 14:35:51 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 404, in initialize_from_config
[1;36m(EngineCore_DP0 pid=572801)[0;0m ERROR 12-04 14:35:51 [core.py:842]     self.model_runner.initialize_kv_cache(kv_cache_config)
[1;36m(EngineCore_DP0 pid=572801)[0;0m ERROR 12-04 14:35:51 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 5007, in initialize_kv_cache
[1;36m(EngineCore_DP0 pid=572801)[0;0m ERROR 12-04 14:35:51 [core.py:842]     kv_caches = self.initialize_kv_cache_tensors(
[1;36m(EngineCore_DP0 pid=572801)[0;0m ERROR 12-04 14:35:51 [core.py:842]                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=572801)[0;0m ERROR 12-04 14:35:51 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4933, in initialize_kv_cache_tensors
[1;36m(EngineCore_DP0 pid=572801)[0;0m ERROR 12-04 14:35:51 [core.py:842]     kv_cache_raw_tensors = self._allocate_kv_cache_tensors(kv_cache_config)
[1;36m(EngineCore_DP0 pid=572801)[0;0m ERROR 12-04 14:35:51 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=572801)[0;0m ERROR 12-04 14:35:51 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4718, in _allocate_kv_cache_tensors
[1;36m(EngineCore_DP0 pid=572801)[0;0m ERROR 12-04 14:35:51 [core.py:842]     tensor = torch.zeros(
[1;36m(EngineCore_DP0 pid=572801)[0;0m ERROR 12-04 14:35:51 [core.py:842]              ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=572801)[0;0m ERROR 12-04 14:35:51 [core.py:842] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 384.00 MiB. GPU 0 has a total capacity of 44.42 GiB of which 138.50 MiB is free. Process 572804 has 24.73 GiB memory in use. Including non-PyTorch memory, this process has 19.54 GiB memory in use. Of the allocated memory 19.19 GiB is allocated by PyTorch, and 19.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=572801)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=572801)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=572801)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=572801)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=572801)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=572801)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=572801)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=572801)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=572801)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=572801)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=572801)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=572801)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=572801)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=572801)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=572801)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=572801)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=572801)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 247, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=572801)[0;0m     self.model_executor.initialize_from_config(kv_cache_configs)
[1;36m(EngineCore_DP0 pid=572801)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 115, in initialize_from_config
[1;36m(EngineCore_DP0 pid=572801)[0;0m     self.collective_rpc("initialize_from_config", args=(kv_cache_configs,))
[1;36m(EngineCore_DP0 pid=572801)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=572801)[0;0m     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=572801)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=572801)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=572801)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=572801)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=572801)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 319, in initialize_from_config
[1;36m(EngineCore_DP0 pid=572801)[0;0m     self.worker.initialize_from_config(kv_cache_config)  # type: ignore
[1;36m(EngineCore_DP0 pid=572801)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=572801)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 404, in initialize_from_config
[1;36m(EngineCore_DP0 pid=572801)[0;0m     self.model_runner.initialize_kv_cache(kv_cache_config)
[1;36m(EngineCore_DP0 pid=572801)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 5007, in initialize_kv_cache
[1;36m(EngineCore_DP0 pid=572801)[0;0m     kv_caches = self.initialize_kv_cache_tensors(
[1;36m(EngineCore_DP0 pid=572801)[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=572801)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4933, in initialize_kv_cache_tensors
[1;36m(EngineCore_DP0 pid=572801)[0;0m     kv_cache_raw_tensors = self._allocate_kv_cache_tensors(kv_cache_config)
[1;36m(EngineCore_DP0 pid=572801)[0;0m                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=572801)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4718, in _allocate_kv_cache_tensors
[1;36m(EngineCore_DP0 pid=572801)[0;0m     tensor = torch.zeros(
[1;36m(EngineCore_DP0 pid=572801)[0;0m              ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=572801)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 384.00 MiB. GPU 0 has a total capacity of 44.42 GiB of which 138.50 MiB is free. Process 572804 has 24.73 GiB memory in use. Including non-PyTorch memory, this process has 19.54 GiB memory in use. Of the allocated memory 19.19 GiB is allocated by PyTorch, and 19.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=572804)[0;0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/51 [00:00<?, ?it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   2%|▏         | 1/51 [00:00<00:05,  9.64it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   2%|▏         | 1/51 [00:00<00:05,  8.86it/s]
[1;36m(EngineCore_DP0 pid=572804)[0;0m ERROR 12-04 14:35:51 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=572804)[0;0m ERROR 12-04 14:35:51 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=572804)[0;0m ERROR 12-04 14:35:51 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=572804)[0;0m ERROR 12-04 14:35:51 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=572804)[0;0m ERROR 12-04 14:35:51 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=572804)[0;0m ERROR 12-04 14:35:51 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=572804)[0;0m ERROR 12-04 14:35:51 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=572804)[0;0m ERROR 12-04 14:35:51 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=572804)[0;0m ERROR 12-04 14:35:51 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=572804)[0;0m ERROR 12-04 14:35:51 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=572804)[0;0m ERROR 12-04 14:35:51 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 247, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=572804)[0;0m ERROR 12-04 14:35:51 [core.py:842]     self.model_executor.initialize_from_config(kv_cache_configs)
[1;36m(EngineCore_DP0 pid=572804)[0;0m ERROR 12-04 14:35:51 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 116, in initialize_from_config
[1;36m(EngineCore_DP0 pid=572804)[0;0m ERROR 12-04 14:35:51 [core.py:842]     self.collective_rpc("compile_or_warm_up_model")
[1;36m(EngineCore_DP0 pid=572804)[0;0m ERROR 12-04 14:35:51 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=572804)[0;0m ERROR 12-04 14:35:51 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=572804)[0;0m ERROR 12-04 14:35:51 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=572804)[0;0m ERROR 12-04 14:35:51 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=572804)[0;0m ERROR 12-04 14:35:51 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=572804)[0;0m ERROR 12-04 14:35:51 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=572804)[0;0m ERROR 12-04 14:35:51 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 429, in compile_or_warm_up_model
[1;36m(EngineCore_DP0 pid=572804)[0;0m ERROR 12-04 14:35:51 [core.py:842]     cuda_graph_memory_bytes = self.model_runner.capture_model()
[1;36m(EngineCore_DP0 pid=572804)[0;0m ERROR 12-04 14:35:51 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=572804)[0;0m ERROR 12-04 14:35:51 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4201, in capture_model
[1;36m(EngineCore_DP0 pid=572804)[0;0m ERROR 12-04 14:35:51 [core.py:842]     self._capture_cudagraphs(
[1;36m(EngineCore_DP0 pid=572804)[0;0m ERROR 12-04 14:35:51 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4298, in _capture_cudagraphs
[1;36m(EngineCore_DP0 pid=572804)[0;0m ERROR 12-04 14:35:51 [core.py:842]     self._dummy_run(
[1;36m(EngineCore_DP0 pid=572804)[0;0m ERROR 12-04 14:35:51 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=572804)[0;0m ERROR 12-04 14:35:51 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=572804)[0;0m ERROR 12-04 14:35:51 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=572804)[0;0m ERROR 12-04 14:35:51 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=572804)[0;0m ERROR 12-04 14:35:51 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=572804)[0;0m ERROR 12-04 14:35:51 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=572804)[0;0m ERROR 12-04 14:35:51 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=572804)[0;0m ERROR 12-04 14:35:51 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=572804)[0;0m ERROR 12-04 14:35:51 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=572804)[0;0m ERROR 12-04 14:35:51 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=572804)[0;0m ERROR 12-04 14:35:51 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=572804)[0;0m ERROR 12-04 14:35:51 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=572804)[0;0m ERROR 12-04 14:35:51 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=572804)[0;0m ERROR 12-04 14:35:51 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=572804)[0;0m ERROR 12-04 14:35:51 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=572804)[0;0m ERROR 12-04 14:35:51 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=572804)[0;0m ERROR 12-04 14:35:51 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=572804)[0;0m ERROR 12-04 14:35:51 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=572804)[0;0m ERROR 12-04 14:35:51 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 399, in __call__
[1;36m(EngineCore_DP0 pid=572804)[0;0m ERROR 12-04 14:35:51 [core.py:842]     return TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=572804)[0;0m ERROR 12-04 14:35:51 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=572804)[0;0m ERROR 12-04 14:35:51 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 152, in __call__
[1;36m(EngineCore_DP0 pid=572804)[0;0m ERROR 12-04 14:35:51 [core.py:842]     return self.forward(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=572804)[0;0m ERROR 12-04 14:35:51 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=572804)[0;0m ERROR 12-04 14:35:51 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 430, in forward
[1;36m(EngineCore_DP0 pid=572804)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=572804)[0;0m ERROR 12-04 14:35:51 [core.py:842]     def forward(
[1;36m(EngineCore_DP0 pid=572804)[0;0m ERROR 12-04 14:35:51 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
[1;36m(EngineCore_DP0 pid=572804)[0;0m ERROR 12-04 14:35:51 [core.py:842]     return fn(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=572804)[0;0m ERROR 12-04 14:35:51 [core.py:842]            ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=572804)[0;0m ERROR 12-04 14:35:51 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/caching.py", line 53, in __call__
[1;36m(EngineCore_DP0 pid=572804)[0;0m ERROR 12-04 14:35:51 [core.py:842]     return self.optimized_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=572804)[0;0m ERROR 12-04 14:35:51 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=572804)[0;0m ERROR 12-04 14:35:51 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/graph_module.py", line 837, in call_wrapped
[1;36m(EngineCore_DP0 pid=572804)[0;0m ERROR 12-04 14:35:51 [core.py:842]     return self._wrapped_call(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=572804)[0;0m ERROR 12-04 14:35:51 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=572804)[0;0m ERROR 12-04 14:35:51 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/graph_module.py", line 413, in __call__
[1;36m(EngineCore_DP0 pid=572804)[0;0m ERROR 12-04 14:35:51 [core.py:842]     raise e
[1;36m(EngineCore_DP0 pid=572804)[0;0m ERROR 12-04 14:35:51 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/graph_module.py", line 400, in __call__
[1;36m(EngineCore_DP0 pid=572804)[0;0m ERROR 12-04 14:35:51 [core.py:842]     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
[1;36m(EngineCore_DP0 pid=572804)[0;0m ERROR 12-04 14:35:51 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=572804)[0;0m ERROR 12-04 14:35:51 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=572804)[0;0m ERROR 12-04 14:35:51 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=572804)[0;0m ERROR 12-04 14:35:51 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=572804)[0;0m ERROR 12-04 14:35:51 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=572804)[0;0m ERROR 12-04 14:35:51 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=572804)[0;0m ERROR 12-04 14:35:51 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=572804)[0;0m ERROR 12-04 14:35:51 [core.py:842]   File "<eval_with_key>.66", line 209, in forward
[1;36m(EngineCore_DP0 pid=572804)[0;0m ERROR 12-04 14:35:51 [core.py:842]     submod_2 = self.submod_2(getitem_3, s72, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_, getitem_4, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_3 = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_ = getitem_4 = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_ = None
[1;36m(EngineCore_DP0 pid=572804)[0;0m ERROR 12-04 14:35:51 [core.py:842]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=572804)[0;0m ERROR 12-04 14:35:51 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=572804)[0;0m ERROR 12-04 14:35:51 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=572804)[0;0m ERROR 12-04 14:35:51 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=572804)[0;0m ERROR 12-04 14:35:51 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/piecewise_backend.py", line 99, in __call__
[1;36m(EngineCore_DP0 pid=572804)[0;0m ERROR 12-04 14:35:51 [core.py:842]     return self.compiled_graph_for_general_shape(*args)
[1;36m(EngineCore_DP0 pid=572804)[0;0m ERROR 12-04 14:35:51 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=572804)[0;0m ERROR 12-04 14:35:51 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 268, in compiled_graph_wrapper
[1;36m(EngineCore_DP0 pid=572804)[0;0m ERROR 12-04 14:35:51 [core.py:842]     graph_output = inductor_compiled_graph(*args)
[1;36m(EngineCore_DP0 pid=572804)[0;0m ERROR 12-04 14:35:51 [core.py:842]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=572804)[0;0m ERROR 12-04 14:35:51 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
[1;36m(EngineCore_DP0 pid=572804)[0;0m ERROR 12-04 14:35:51 [core.py:842]     return self._compiled_fn(*args)
[1;36m(EngineCore_DP0 pid=572804)[0;0m ERROR 12-04 14:35:51 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=572804)[0;0m ERROR 12-04 14:35:51 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 184, in <lambda>
[1;36m(EngineCore_DP0 pid=572804)[0;0m ERROR 12-04 14:35:51 [core.py:842]     return CompiledArtifact(lambda *args: compiled_fn(list(args)), None)
[1;36m(EngineCore_DP0 pid=572804)[0;0m ERROR 12-04 14:35:51 [core.py:842]                                           ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=572804)[0;0m ERROR 12-04 14:35:51 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
[1;36m(EngineCore_DP0 pid=572804)[0;0m ERROR 12-04 14:35:51 [core.py:842]     all_outs = call_func_at_runtime_with_args(
[1;36m(EngineCore_DP0 pid=572804)[0;0m ERROR 12-04 14:35:51 [core.py:842]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=572804)[0;0m ERROR 12-04 14:35:51 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
[1;36m(EngineCore_DP0 pid=572804)[0;0m ERROR 12-04 14:35:51 [core.py:842]     out = normalize_as_list(f(args))
[1;36m(EngineCore_DP0 pid=572804)[0;0m ERROR 12-04 14:35:51 [core.py:842]                             ^^^^^^^
[1;36m(EngineCore_DP0 pid=572804)[0;0m ERROR 12-04 14:35:51 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
[1;36m(EngineCore_DP0 pid=572804)[0;0m ERROR 12-04 14:35:51 [core.py:842]     return compiled_fn(runtime_args)
[1;36m(EngineCore_DP0 pid=572804)[0;0m ERROR 12-04 14:35:51 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=572804)[0;0m ERROR 12-04 14:35:51 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/output_code.py", line 613, in __call__
[1;36m(EngineCore_DP0 pid=572804)[0;0m ERROR 12-04 14:35:51 [core.py:842]     return self.current_callable(inputs)
[1;36m(EngineCore_DP0 pid=572804)[0;0m ERROR 12-04 14:35:51 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=572804)[0;0m ERROR 12-04 14:35:51 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/utils.py", line 2962, in run
[1;36m(EngineCore_DP0 pid=572804)[0;0m ERROR 12-04 14:35:51 [core.py:842]     out = model(new_inputs)
[1;36m(EngineCore_DP0 pid=572804)[0;0m ERROR 12-04 14:35:51 [core.py:842]           ^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=572804)[0;0m ERROR 12-04 14:35:51 [core.py:842]   File "/tmp/torchinductor_ch269957/e6/ce6cgw7n42hkhqbz26qv76rj3cvq3sj2bzahzg62722bds6daraz.py", line 906, in call
[1;36m(EngineCore_DP0 pid=572804)[0;0m ERROR 12-04 14:35:51 [core.py:842]     buf3 = empty_strided_cuda((s72, 28672), (28672, 1), torch.bfloat16)
[1;36m(EngineCore_DP0 pid=572804)[0;0m ERROR 12-04 14:35:51 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=572804)[0;0m ERROR 12-04 14:35:51 [core.py:842] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 44.42 GiB of which 10.50 MiB is free. Including non-PyTorch memory, this process has 24.86 GiB memory in use. Process 572801 has 19.54 GiB memory in use. Of the allocated memory 24.50 GiB is allocated by PyTorch, with 75.88 MiB allocated in private pools (e.g., CUDA Graphs), and 21.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=572804)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=572804)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=572804)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=572804)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=572804)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=572804)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=572804)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=572804)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=572804)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=572804)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=572804)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=572804)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=572804)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=572804)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=572804)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=572804)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 247, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=572804)[0;0m     self.model_executor.initialize_from_config(kv_cache_configs)
[1;36m(EngineCore_DP0 pid=572804)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 116, in initialize_from_config
[1;36m(EngineCore_DP0 pid=572804)[0;0m     self.collective_rpc("compile_or_warm_up_model")
[1;36m(EngineCore_DP0 pid=572804)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=572804)[0;0m     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=572804)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=572804)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=572804)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=572804)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=572804)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 429, in compile_or_warm_up_model
[1;36m(EngineCore_DP0 pid=572804)[0;0m     cuda_graph_memory_bytes = self.model_runner.capture_model()
[1;36m(EngineCore_DP0 pid=572804)[0;0m                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=572804)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4201, in capture_model
[1;36m(EngineCore_DP0 pid=572804)[0;0m     self._capture_cudagraphs(
[1;36m(EngineCore_DP0 pid=572804)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4298, in _capture_cudagraphs
[1;36m(EngineCore_DP0 pid=572804)[0;0m     self._dummy_run(
[1;36m(EngineCore_DP0 pid=572804)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=572804)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=572804)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=572804)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=572804)[0;0m     outputs = self.model(
[1;36m(EngineCore_DP0 pid=572804)[0;0m               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=572804)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=572804)[0;0m     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=572804)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=572804)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=572804)[0;0m     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=572804)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=572804)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=572804)[0;0m     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=572804)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=572804)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=572804)[0;0m     model_output = self.model(
[1;36m(EngineCore_DP0 pid=572804)[0;0m                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=572804)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 399, in __call__
[1;36m(EngineCore_DP0 pid=572804)[0;0m     return TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=572804)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=572804)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 152, in __call__
[1;36m(EngineCore_DP0 pid=572804)[0;0m     return self.forward(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=572804)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=572804)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 430, in forward
[1;36m(EngineCore_DP0 pid=572804)[0;0m     def forward(
[1;36m(EngineCore_DP0 pid=572804)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
[1;36m(EngineCore_DP0 pid=572804)[0;0m     return fn(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=572804)[0;0m            ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=572804)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/caching.py", line 53, in __call__
[1;36m(EngineCore_DP0 pid=572804)[0;0m     return self.optimized_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=572804)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=572804)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/graph_module.py", line 837, in call_wrapped
[1;36m(EngineCore_DP0 pid=572804)[0;0m     return self._wrapped_call(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=572804)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=572804)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/graph_module.py", line 413, in __call__
[1;36m(EngineCore_DP0 pid=572804)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=572804)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/graph_module.py", line 400, in __call__
[1;36m(EngineCore_DP0 pid=572804)[0;0m     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
[1;36m(EngineCore_DP0 pid=572804)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=572804)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=572804)[0;0m     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=572804)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=572804)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=572804)[0;0m     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=572804)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=572804)[0;0m   File "<eval_with_key>.66", line 209, in forward
[1;36m(EngineCore_DP0 pid=572804)[0;0m     submod_2 = self.submod_2(getitem_3, s72, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_, getitem_4, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_3 = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_ = getitem_4 = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_ = None
[1;36m(EngineCore_DP0 pid=572804)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=572804)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=572804)[0;0m     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=572804)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=572804)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/piecewise_backend.py", line 99, in __call__
[1;36m(EngineCore_DP0 pid=572804)[0;0m     return self.compiled_graph_for_general_shape(*args)
[1;36m(EngineCore_DP0 pid=572804)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=572804)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 268, in compiled_graph_wrapper
[1;36m(EngineCore_DP0 pid=572804)[0;0m     graph_output = inductor_compiled_graph(*args)
[1;36m(EngineCore_DP0 pid=572804)[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=572804)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
[1;36m(EngineCore_DP0 pid=572804)[0;0m     return self._compiled_fn(*args)
[1;36m(EngineCore_DP0 pid=572804)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=572804)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 184, in <lambda>
[1;36m(EngineCore_DP0 pid=572804)[0;0m     return CompiledArtifact(lambda *args: compiled_fn(list(args)), None)
[1;36m(EngineCore_DP0 pid=572804)[0;0m                                           ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=572804)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
[1;36m(EngineCore_DP0 pid=572804)[0;0m     all_outs = call_func_at_runtime_with_args(
[1;36m(EngineCore_DP0 pid=572804)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=572804)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
[1;36m(EngineCore_DP0 pid=572804)[0;0m     out = normalize_as_list(f(args))
[1;36m(EngineCore_DP0 pid=572804)[0;0m                             ^^^^^^^
[1;36m(EngineCore_DP0 pid=572804)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
[1;36m(EngineCore_DP0 pid=572804)[0;0m     return compiled_fn(runtime_args)
[1;36m(EngineCore_DP0 pid=572804)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=572804)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/output_code.py", line 613, in __call__
[1;36m(EngineCore_DP0 pid=572804)[0;0m     return self.current_callable(inputs)
[1;36m(EngineCore_DP0 pid=572804)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=572804)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/utils.py", line 2962, in run
[1;36m(EngineCore_DP0 pid=572804)[0;0m     out = model(new_inputs)
[1;36m(EngineCore_DP0 pid=572804)[0;0m           ^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=572804)[0;0m   File "/tmp/torchinductor_ch269957/e6/ce6cgw7n42hkhqbz26qv76rj3cvq3sj2bzahzg62722bds6daraz.py", line 906, in call
[1;36m(EngineCore_DP0 pid=572804)[0;0m     buf3 = empty_strided_cuda((s72, 28672), (28672, 1), torch.bfloat16)
[1;36m(EngineCore_DP0 pid=572804)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=572804)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 44.42 GiB of which 10.50 MiB is free. Including non-PyTorch memory, this process has 24.86 GiB memory in use. Process 572801 has 19.54 GiB memory in use. Of the allocated memory 24.50 GiB is allocated by PyTorch, with 75.88 MiB allocated in private pools (e.g., CUDA Graphs), and 21.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W1204 14:35:51.218368990 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank0]:[W1204 14:35:52.001931958 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: mistralai/Mistral-7B-Instruct-v0.3 (backend=vllm)
[ModelCache] Using max_model_len=32768 for mistralai/Mistral-7B-Instruct-v0.3
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 14:36:13 [utils.py:253] non-default args: {'max_model_len': 32768, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'mistralai/Mistral-7B-Instruct-v0.3'}
INFO 12-04 14:36:13 [model.py:631] Resolved architecture: MistralForCausalLM
INFO 12-04 14:36:13 [model.py:1745] Using max model len 32768
INFO 12-04 14:36:13 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: mistralai/Mistral-7B-Instruct-v0.3 (backend=vllm)
[ModelCache] Using max_model_len=32768 for mistralai/Mistral-7B-Instruct-v0.3
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 14:36:13 [utils.py:253] non-default args: {'max_model_len': 32768, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'mistralai/Mistral-7B-Instruct-v0.3'}
INFO 12-04 14:36:13 [model.py:631] Resolved architecture: MistralForCausalLM
INFO 12-04 14:36:13 [model.py:1745] Using max model len 32768
INFO 12-04 14:36:13 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=574502)[0;0m INFO 12-04 14:36:26 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='mistralai/Mistral-7B-Instruct-v0.3', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.3', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=mistralai/Mistral-7B-Instruct-v0.3, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=574499)[0;0m INFO 12-04 14:36:26 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='mistralai/Mistral-7B-Instruct-v0.3', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.3', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=mistralai/Mistral-7B-Instruct-v0.3, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=574499)[0;0m INFO 12-04 14:36:28 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.9:47725 backend=nccl
[1;36m(EngineCore_DP0 pid=574502)[0;0m INFO 12-04 14:36:28 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.9:44681 backend=nccl
[W1204 14:36:28.891461864 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-21-2.rc.tch.harvard.edu]:44681 (errno: 97 - Address family not supported by protocol).
[W1204 14:36:28.891736741 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-21-2.rc.tch.harvard.edu]:47725 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=574502)[0;0m INFO 12-04 14:36:28 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=574499)[0;0m INFO 12-04 14:36:28 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=574502)[0;0m INFO 12-04 14:36:28 [gpu_model_runner.py:3259] Starting to load model mistralai/Mistral-7B-Instruct-v0.3...
[1;36m(EngineCore_DP0 pid=574499)[0;0m INFO 12-04 14:36:28 [gpu_model_runner.py:3259] Starting to load model mistralai/Mistral-7B-Instruct-v0.3...
[1;36m(EngineCore_DP0 pid=574502)[0;0m INFO 12-04 14:36:29 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=574502)[0;0m INFO 12-04 14:36:29 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=574499)[0;0m INFO 12-04 14:36:29 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=574499)[0;0m INFO 12-04 14:36:29 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=574502)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]
[1;36m(EngineCore_DP0 pid=574499)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]
[1;36m(EngineCore_DP0 pid=574502)[0;0m Loading safetensors checkpoint shards:  33% Completed | 1/3 [00:01<00:02,  1.27s/it]
[1;36m(EngineCore_DP0 pid=574499)[0;0m Loading safetensors checkpoint shards:  33% Completed | 1/3 [00:01<00:02,  1.24s/it]
[1;36m(EngineCore_DP0 pid=574502)[0;0m Loading safetensors checkpoint shards:  67% Completed | 2/3 [00:11<00:06,  6.81s/it]
[1;36m(EngineCore_DP0 pid=574499)[0;0m Loading safetensors checkpoint shards:  67% Completed | 2/3 [00:11<00:06,  6.74s/it]
[1;36m(EngineCore_DP0 pid=574502)[0;0m Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:29<00:00, 11.66s/it]
[1;36m(EngineCore_DP0 pid=574499)[0;0m Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:29<00:00, 11.63s/it]
[1;36m(EngineCore_DP0 pid=574499)[0;0m Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:29<00:00,  9.76s/it]
[1;36m(EngineCore_DP0 pid=574499)[0;0m 
[1;36m(EngineCore_DP0 pid=574502)[0;0m Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:29<00:00,  9.80s/it]
[1;36m(EngineCore_DP0 pid=574502)[0;0m 
[1;36m(EngineCore_DP0 pid=574502)[0;0m INFO 12-04 14:37:00 [default_loader.py:314] Loading weights took 29.63 seconds
[1;36m(EngineCore_DP0 pid=574499)[0;0m INFO 12-04 14:37:00 [default_loader.py:314] Loading weights took 29.51 seconds
[1;36m(EngineCore_DP0 pid=574502)[0;0m INFO 12-04 14:37:00 [gpu_model_runner.py:3338] Model loading took 13.5084 GiB memory and 30.764100 seconds
[1;36m(EngineCore_DP0 pid=574499)[0;0m INFO 12-04 14:37:00 [gpu_model_runner.py:3338] Model loading took 13.5084 GiB memory and 30.763938 seconds
[1;36m(EngineCore_DP0 pid=574499)[0;0m INFO 12-04 14:37:10 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/0ef74f8a17/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=574502)[0;0m INFO 12-04 14:37:10 [backends.py:631] Using cache directory: /home/ch269957/.cache/vllm/torch_compile_cache/0ef74f8a17/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=574499)[0;0m INFO 12-04 14:37:10 [backends.py:647] Dynamo bytecode transform time: 9.80 s
[1;36m(EngineCore_DP0 pid=574502)[0;0m INFO 12-04 14:37:10 [backends.py:647] Dynamo bytecode transform time: 9.80 s
[1;36m(EngineCore_DP0 pid=574502)[0;0m INFO 12-04 14:37:18 [backends.py:210] Directly load the compiled graph(s) for dynamic shape from the cache, took 6.493 s
[1;36m(EngineCore_DP0 pid=574499)[0;0m INFO 12-04 14:37:18 [backends.py:210] Directly load the compiled graph(s) for dynamic shape from the cache, took 6.512 s
[1;36m(EngineCore_DP0 pid=574502)[0;0m INFO 12-04 14:37:20 [monitor.py:34] torch.compile takes 16.29 s in total
[1;36m(EngineCore_DP0 pid=574499)[0;0m INFO 12-04 14:37:20 [monitor.py:34] torch.compile takes 16.31 s in total
[1;36m(EngineCore_DP0 pid=574499)[0;0m INFO 12-04 14:37:23 [gpu_worker.py:359] Available KV cache memory: 10.80 GiB
[1;36m(EngineCore_DP0 pid=574502)[0;0m INFO 12-04 14:37:23 [gpu_worker.py:359] Available KV cache memory: 11.97 GiB
[1;36m(EngineCore_DP0 pid=574499)[0;0m INFO 12-04 14:37:24 [kv_cache_utils.py:1229] GPU KV cache size: 88,432 tokens
[1;36m(EngineCore_DP0 pid=574499)[0;0m INFO 12-04 14:37:24 [kv_cache_utils.py:1234] Maximum concurrency for 32,768 tokens per request: 2.70x
[1;36m(EngineCore_DP0 pid=574502)[0;0m INFO 12-04 14:37:24 [kv_cache_utils.py:1229] GPU KV cache size: 98,032 tokens
[1;36m(EngineCore_DP0 pid=574502)[0;0m INFO 12-04 14:37:24 [kv_cache_utils.py:1234] Maximum concurrency for 32,768 tokens per request: 2.99x
[1;36m(EngineCore_DP0 pid=574502)[0;0m ERROR 12-04 14:37:24 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=574502)[0;0m ERROR 12-04 14:37:24 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=574502)[0;0m ERROR 12-04 14:37:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=574502)[0;0m ERROR 12-04 14:37:24 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=574502)[0;0m ERROR 12-04 14:37:24 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=574502)[0;0m ERROR 12-04 14:37:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=574502)[0;0m ERROR 12-04 14:37:24 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=574502)[0;0m ERROR 12-04 14:37:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=574502)[0;0m ERROR 12-04 14:37:24 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=574502)[0;0m ERROR 12-04 14:37:24 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=574502)[0;0m ERROR 12-04 14:37:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 247, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=574502)[0;0m ERROR 12-04 14:37:24 [core.py:842]     self.model_executor.initialize_from_config(kv_cache_configs)
[1;36m(EngineCore_DP0 pid=574502)[0;0m ERROR 12-04 14:37:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 115, in initialize_from_config
[1;36m(EngineCore_DP0 pid=574502)[0;0m ERROR 12-04 14:37:24 [core.py:842]     self.collective_rpc("initialize_from_config", args=(kv_cache_configs,))
[1;36m(EngineCore_DP0 pid=574502)[0;0m ERROR 12-04 14:37:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=574502)[0;0m ERROR 12-04 14:37:24 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=574502)[0;0m ERROR 12-04 14:37:24 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=574502)[0;0m ERROR 12-04 14:37:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=574502)[0;0m ERROR 12-04 14:37:24 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=574502)[0;0m ERROR 12-04 14:37:24 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=574502)[0;0m ERROR 12-04 14:37:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 319, in initialize_from_config
[1;36m(EngineCore_DP0 pid=574502)[0;0m ERROR 12-04 14:37:24 [core.py:842]     self.worker.initialize_from_config(kv_cache_config)  # type: ignore
[1;36m(EngineCore_DP0 pid=574502)[0;0m ERROR 12-04 14:37:24 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=574502)[0;0m ERROR 12-04 14:37:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 404, in initialize_from_config
[1;36m(EngineCore_DP0 pid=574502)[0;0m ERROR 12-04 14:37:24 [core.py:842]     self.model_runner.initialize_kv_cache(kv_cache_config)
[1;36m(EngineCore_DP0 pid=574502)[0;0m ERROR 12-04 14:37:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 5007, in initialize_kv_cache
[1;36m(EngineCore_DP0 pid=574502)[0;0m ERROR 12-04 14:37:24 [core.py:842]     kv_caches = self.initialize_kv_cache_tensors(
[1;36m(EngineCore_DP0 pid=574502)[0;0m ERROR 12-04 14:37:24 [core.py:842]                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=574502)[0;0m ERROR 12-04 14:37:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4933, in initialize_kv_cache_tensors
[1;36m(EngineCore_DP0 pid=574502)[0;0m ERROR 12-04 14:37:24 [core.py:842]     kv_cache_raw_tensors = self._allocate_kv_cache_tensors(kv_cache_config)
[1;36m(EngineCore_DP0 pid=574502)[0;0m ERROR 12-04 14:37:24 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=574502)[0;0m ERROR 12-04 14:37:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4718, in _allocate_kv_cache_tensors
[1;36m(EngineCore_DP0 pid=574502)[0;0m ERROR 12-04 14:37:24 [core.py:842]     tensor = torch.zeros(
[1;36m(EngineCore_DP0 pid=574502)[0;0m ERROR 12-04 14:37:24 [core.py:842]              ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=574502)[0;0m ERROR 12-04 14:37:24 [core.py:842] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 384.00 MiB. GPU 0 has a total capacity of 44.42 GiB of which 138.50 MiB is free. Process 574499 has 24.73 GiB memory in use. Including non-PyTorch memory, this process has 19.54 GiB memory in use. Of the allocated memory 19.19 GiB is allocated by PyTorch, and 19.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=574502)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=574502)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=574502)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=574502)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=574502)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=574502)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=574502)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=574502)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=574502)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=574502)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=574502)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=574502)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=574502)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=574502)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=574502)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=574502)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=574502)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 247, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=574502)[0;0m     self.model_executor.initialize_from_config(kv_cache_configs)
[1;36m(EngineCore_DP0 pid=574502)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 115, in initialize_from_config
[1;36m(EngineCore_DP0 pid=574502)[0;0m     self.collective_rpc("initialize_from_config", args=(kv_cache_configs,))
[1;36m(EngineCore_DP0 pid=574502)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=574502)[0;0m     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=574502)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=574502)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=574502)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=574502)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=574502)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 319, in initialize_from_config
[1;36m(EngineCore_DP0 pid=574502)[0;0m     self.worker.initialize_from_config(kv_cache_config)  # type: ignore
[1;36m(EngineCore_DP0 pid=574502)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=574502)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 404, in initialize_from_config
[1;36m(EngineCore_DP0 pid=574502)[0;0m     self.model_runner.initialize_kv_cache(kv_cache_config)
[1;36m(EngineCore_DP0 pid=574502)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 5007, in initialize_kv_cache
[1;36m(EngineCore_DP0 pid=574502)[0;0m     kv_caches = self.initialize_kv_cache_tensors(
[1;36m(EngineCore_DP0 pid=574502)[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=574502)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4933, in initialize_kv_cache_tensors
[1;36m(EngineCore_DP0 pid=574502)[0;0m     kv_cache_raw_tensors = self._allocate_kv_cache_tensors(kv_cache_config)
[1;36m(EngineCore_DP0 pid=574502)[0;0m                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=574502)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4718, in _allocate_kv_cache_tensors
[1;36m(EngineCore_DP0 pid=574502)[0;0m     tensor = torch.zeros(
[1;36m(EngineCore_DP0 pid=574502)[0;0m              ^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=574502)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 384.00 MiB. GPU 0 has a total capacity of 44.42 GiB of which 138.50 MiB is free. Process 574499 has 24.73 GiB memory in use. Including non-PyTorch memory, this process has 19.54 GiB memory in use. Of the allocated memory 19.19 GiB is allocated by PyTorch, and 19.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=574499)[0;0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/51 [00:00<?, ?it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   2%|▏         | 1/51 [00:00<00:05,  9.67it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   2%|▏         | 1/51 [00:00<00:05,  8.91it/s]
[1;36m(EngineCore_DP0 pid=574499)[0;0m ERROR 12-04 14:37:24 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=574499)[0;0m ERROR 12-04 14:37:24 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=574499)[0;0m ERROR 12-04 14:37:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=574499)[0;0m ERROR 12-04 14:37:24 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=574499)[0;0m ERROR 12-04 14:37:24 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=574499)[0;0m ERROR 12-04 14:37:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=574499)[0;0m ERROR 12-04 14:37:24 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=574499)[0;0m ERROR 12-04 14:37:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=574499)[0;0m ERROR 12-04 14:37:24 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=574499)[0;0m ERROR 12-04 14:37:24 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=574499)[0;0m ERROR 12-04 14:37:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 247, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=574499)[0;0m ERROR 12-04 14:37:24 [core.py:842]     self.model_executor.initialize_from_config(kv_cache_configs)
[1;36m(EngineCore_DP0 pid=574499)[0;0m ERROR 12-04 14:37:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 116, in initialize_from_config
[1;36m(EngineCore_DP0 pid=574499)[0;0m ERROR 12-04 14:37:24 [core.py:842]     self.collective_rpc("compile_or_warm_up_model")
[1;36m(EngineCore_DP0 pid=574499)[0;0m ERROR 12-04 14:37:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=574499)[0;0m ERROR 12-04 14:37:24 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=574499)[0;0m ERROR 12-04 14:37:24 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=574499)[0;0m ERROR 12-04 14:37:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=574499)[0;0m ERROR 12-04 14:37:24 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=574499)[0;0m ERROR 12-04 14:37:24 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=574499)[0;0m ERROR 12-04 14:37:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 429, in compile_or_warm_up_model
[1;36m(EngineCore_DP0 pid=574499)[0;0m ERROR 12-04 14:37:24 [core.py:842]     cuda_graph_memory_bytes = self.model_runner.capture_model()
[1;36m(EngineCore_DP0 pid=574499)[0;0m ERROR 12-04 14:37:24 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=574499)[0;0m ERROR 12-04 14:37:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4201, in capture_model
[1;36m(EngineCore_DP0 pid=574499)[0;0m ERROR 12-04 14:37:24 [core.py:842]     self._capture_cudagraphs(
[1;36m(EngineCore_DP0 pid=574499)[0;0m ERROR 12-04 14:37:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4298, in _capture_cudagraphs
[1;36m(EngineCore_DP0 pid=574499)[0;0m ERROR 12-04 14:37:24 [core.py:842]     self._dummy_run(
[1;36m(EngineCore_DP0 pid=574499)[0;0m ERROR 12-04 14:37:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=574499)[0;0m ERROR 12-04 14:37:24 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=574499)[0;0m ERROR 12-04 14:37:24 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=574499)[0;0m ERROR 12-04 14:37:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=574499)[0;0m ERROR 12-04 14:37:24 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=574499)[0;0m ERROR 12-04 14:37:24 [core.py:842]               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=574499)[0;0m ERROR 12-04 14:37:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=574499)[0;0m ERROR 12-04 14:37:24 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=574499)[0;0m ERROR 12-04 14:37:24 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=574499)[0;0m ERROR 12-04 14:37:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=574499)[0;0m ERROR 12-04 14:37:24 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=574499)[0;0m ERROR 12-04 14:37:24 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=574499)[0;0m ERROR 12-04 14:37:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=574499)[0;0m ERROR 12-04 14:37:24 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=574499)[0;0m ERROR 12-04 14:37:24 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=574499)[0;0m ERROR 12-04 14:37:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=574499)[0;0m ERROR 12-04 14:37:24 [core.py:842]     model_output = self.model(
[1;36m(EngineCore_DP0 pid=574499)[0;0m ERROR 12-04 14:37:24 [core.py:842]                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=574499)[0;0m ERROR 12-04 14:37:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 399, in __call__
[1;36m(EngineCore_DP0 pid=574499)[0;0m ERROR 12-04 14:37:24 [core.py:842]     return TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=574499)[0;0m ERROR 12-04 14:37:24 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=574499)[0;0m ERROR 12-04 14:37:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 152, in __call__
[1;36m(EngineCore_DP0 pid=574499)[0;0m ERROR 12-04 14:37:24 [core.py:842]     return self.forward(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=574499)[0;0m ERROR 12-04 14:37:24 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=574499)[0;0m ERROR 12-04 14:37:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 430, in forward
[1;36m(EngineCore_DP0 pid=574499)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=574499)[0;0m ERROR 12-04 14:37:24 [core.py:842]     def forward(
[1;36m(EngineCore_DP0 pid=574499)[0;0m ERROR 12-04 14:37:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
[1;36m(EngineCore_DP0 pid=574499)[0;0m ERROR 12-04 14:37:24 [core.py:842]     return fn(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=574499)[0;0m ERROR 12-04 14:37:24 [core.py:842]            ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=574499)[0;0m ERROR 12-04 14:37:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/caching.py", line 53, in __call__
[1;36m(EngineCore_DP0 pid=574499)[0;0m ERROR 12-04 14:37:24 [core.py:842]     return self.optimized_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=574499)[0;0m ERROR 12-04 14:37:24 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=574499)[0;0m ERROR 12-04 14:37:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/graph_module.py", line 837, in call_wrapped
[1;36m(EngineCore_DP0 pid=574499)[0;0m ERROR 12-04 14:37:24 [core.py:842]     return self._wrapped_call(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=574499)[0;0m ERROR 12-04 14:37:24 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=574499)[0;0m ERROR 12-04 14:37:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/graph_module.py", line 413, in __call__
[1;36m(EngineCore_DP0 pid=574499)[0;0m ERROR 12-04 14:37:24 [core.py:842]     raise e
[1;36m(EngineCore_DP0 pid=574499)[0;0m ERROR 12-04 14:37:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/graph_module.py", line 400, in __call__
[1;36m(EngineCore_DP0 pid=574499)[0;0m ERROR 12-04 14:37:24 [core.py:842]     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
[1;36m(EngineCore_DP0 pid=574499)[0;0m ERROR 12-04 14:37:24 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=574499)[0;0m ERROR 12-04 14:37:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=574499)[0;0m ERROR 12-04 14:37:24 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=574499)[0;0m ERROR 12-04 14:37:24 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=574499)[0;0m ERROR 12-04 14:37:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=574499)[0;0m ERROR 12-04 14:37:24 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=574499)[0;0m ERROR 12-04 14:37:24 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=574499)[0;0m ERROR 12-04 14:37:24 [core.py:842]   File "<eval_with_key>.66", line 209, in forward
[1;36m(EngineCore_DP0 pid=574499)[0;0m ERROR 12-04 14:37:24 [core.py:842]     submod_2 = self.submod_2(getitem_3, s72, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_, getitem_4, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_3 = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_ = getitem_4 = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_ = None
[1;36m(EngineCore_DP0 pid=574499)[0;0m ERROR 12-04 14:37:24 [core.py:842]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=574499)[0;0m ERROR 12-04 14:37:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=574499)[0;0m ERROR 12-04 14:37:24 [core.py:842]     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=574499)[0;0m ERROR 12-04 14:37:24 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=574499)[0;0m ERROR 12-04 14:37:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/piecewise_backend.py", line 99, in __call__
[1;36m(EngineCore_DP0 pid=574499)[0;0m ERROR 12-04 14:37:24 [core.py:842]     return self.compiled_graph_for_general_shape(*args)
[1;36m(EngineCore_DP0 pid=574499)[0;0m ERROR 12-04 14:37:24 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=574499)[0;0m ERROR 12-04 14:37:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 268, in compiled_graph_wrapper
[1;36m(EngineCore_DP0 pid=574499)[0;0m ERROR 12-04 14:37:24 [core.py:842]     graph_output = inductor_compiled_graph(*args)
[1;36m(EngineCore_DP0 pid=574499)[0;0m ERROR 12-04 14:37:24 [core.py:842]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=574499)[0;0m ERROR 12-04 14:37:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
[1;36m(EngineCore_DP0 pid=574499)[0;0m ERROR 12-04 14:37:24 [core.py:842]     return self._compiled_fn(*args)
[1;36m(EngineCore_DP0 pid=574499)[0;0m ERROR 12-04 14:37:24 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=574499)[0;0m ERROR 12-04 14:37:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 184, in <lambda>
[1;36m(EngineCore_DP0 pid=574499)[0;0m ERROR 12-04 14:37:24 [core.py:842]     return CompiledArtifact(lambda *args: compiled_fn(list(args)), None)
[1;36m(EngineCore_DP0 pid=574499)[0;0m ERROR 12-04 14:37:24 [core.py:842]                                           ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=574499)[0;0m ERROR 12-04 14:37:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
[1;36m(EngineCore_DP0 pid=574499)[0;0m ERROR 12-04 14:37:24 [core.py:842]     all_outs = call_func_at_runtime_with_args(
[1;36m(EngineCore_DP0 pid=574499)[0;0m ERROR 12-04 14:37:24 [core.py:842]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=574499)[0;0m ERROR 12-04 14:37:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
[1;36m(EngineCore_DP0 pid=574499)[0;0m ERROR 12-04 14:37:24 [core.py:842]     out = normalize_as_list(f(args))
[1;36m(EngineCore_DP0 pid=574499)[0;0m ERROR 12-04 14:37:24 [core.py:842]                             ^^^^^^^
[1;36m(EngineCore_DP0 pid=574499)[0;0m ERROR 12-04 14:37:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
[1;36m(EngineCore_DP0 pid=574499)[0;0m ERROR 12-04 14:37:24 [core.py:842]     return compiled_fn(runtime_args)
[1;36m(EngineCore_DP0 pid=574499)[0;0m ERROR 12-04 14:37:24 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=574499)[0;0m ERROR 12-04 14:37:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/output_code.py", line 613, in __call__
[1;36m(EngineCore_DP0 pid=574499)[0;0m ERROR 12-04 14:37:24 [core.py:842]     return self.current_callable(inputs)
[1;36m(EngineCore_DP0 pid=574499)[0;0m ERROR 12-04 14:37:24 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=574499)[0;0m ERROR 12-04 14:37:24 [core.py:842]   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/utils.py", line 2962, in run
[1;36m(EngineCore_DP0 pid=574499)[0;0m ERROR 12-04 14:37:24 [core.py:842]     out = model(new_inputs)
[1;36m(EngineCore_DP0 pid=574499)[0;0m ERROR 12-04 14:37:24 [core.py:842]           ^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=574499)[0;0m ERROR 12-04 14:37:24 [core.py:842]   File "/tmp/torchinductor_ch269957/e6/ce6cgw7n42hkhqbz26qv76rj3cvq3sj2bzahzg62722bds6daraz.py", line 906, in call
[1;36m(EngineCore_DP0 pid=574499)[0;0m ERROR 12-04 14:37:24 [core.py:842]     buf3 = empty_strided_cuda((s72, 28672), (28672, 1), torch.bfloat16)
[1;36m(EngineCore_DP0 pid=574499)[0;0m ERROR 12-04 14:37:24 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=574499)[0;0m ERROR 12-04 14:37:24 [core.py:842] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 44.42 GiB of which 10.50 MiB is free. Including non-PyTorch memory, this process has 24.86 GiB memory in use. Process 574502 has 19.54 GiB memory in use. Of the allocated memory 24.50 GiB is allocated by PyTorch, with 75.88 MiB allocated in private pools (e.g., CUDA Graphs), and 21.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W1204 14:37:24.332952573 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[1;36m(EngineCore_DP0 pid=574499)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=574499)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=574499)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=574499)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=574499)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=574499)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=574499)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=574499)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=574499)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=574499)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=574499)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=574499)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=574499)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=574499)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=574499)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=574499)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 247, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=574499)[0;0m     self.model_executor.initialize_from_config(kv_cache_configs)
[1;36m(EngineCore_DP0 pid=574499)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 116, in initialize_from_config
[1;36m(EngineCore_DP0 pid=574499)[0;0m     self.collective_rpc("compile_or_warm_up_model")
[1;36m(EngineCore_DP0 pid=574499)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=574499)[0;0m     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=574499)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=574499)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=574499)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=574499)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=574499)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 429, in compile_or_warm_up_model
[1;36m(EngineCore_DP0 pid=574499)[0;0m     cuda_graph_memory_bytes = self.model_runner.capture_model()
[1;36m(EngineCore_DP0 pid=574499)[0;0m                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=574499)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4201, in capture_model
[1;36m(EngineCore_DP0 pid=574499)[0;0m     self._capture_cudagraphs(
[1;36m(EngineCore_DP0 pid=574499)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4298, in _capture_cudagraphs
[1;36m(EngineCore_DP0 pid=574499)[0;0m     self._dummy_run(
[1;36m(EngineCore_DP0 pid=574499)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=574499)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=574499)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=574499)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=574499)[0;0m     outputs = self.model(
[1;36m(EngineCore_DP0 pid=574499)[0;0m               ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=574499)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=574499)[0;0m     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=574499)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=574499)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=574499)[0;0m     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=574499)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=574499)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=574499)[0;0m     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=574499)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=574499)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
[1;36m(EngineCore_DP0 pid=574499)[0;0m     model_output = self.model(
[1;36m(EngineCore_DP0 pid=574499)[0;0m                    ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=574499)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 399, in __call__
[1;36m(EngineCore_DP0 pid=574499)[0;0m     return TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=574499)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=574499)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 152, in __call__
[1;36m(EngineCore_DP0 pid=574499)[0;0m     return self.forward(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=574499)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=574499)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 430, in forward
[1;36m(EngineCore_DP0 pid=574499)[0;0m     def forward(
[1;36m(EngineCore_DP0 pid=574499)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
[1;36m(EngineCore_DP0 pid=574499)[0;0m     return fn(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=574499)[0;0m            ^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=574499)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/caching.py", line 53, in __call__
[1;36m(EngineCore_DP0 pid=574499)[0;0m     return self.optimized_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=574499)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=574499)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/graph_module.py", line 837, in call_wrapped
[1;36m(EngineCore_DP0 pid=574499)[0;0m     return self._wrapped_call(self, *args, **kwargs)
[1;36m(EngineCore_DP0 pid=574499)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=574499)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/graph_module.py", line 413, in __call__
[1;36m(EngineCore_DP0 pid=574499)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=574499)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/fx/graph_module.py", line 400, in __call__
[1;36m(EngineCore_DP0 pid=574499)[0;0m     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
[1;36m(EngineCore_DP0 pid=574499)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=574499)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=574499)[0;0m     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=574499)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=574499)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=574499)[0;0m     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=574499)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=574499)[0;0m   File "<eval_with_key>.66", line 209, in forward
[1;36m(EngineCore_DP0 pid=574499)[0;0m     submod_2 = self.submod_2(getitem_3, s72, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_, getitem_4, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_3 = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_ = getitem_4 = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_ = None
[1;36m(EngineCore_DP0 pid=574499)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=574499)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
[1;36m(EngineCore_DP0 pid=574499)[0;0m     return self.runnable(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=574499)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=574499)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/piecewise_backend.py", line 99, in __call__
[1;36m(EngineCore_DP0 pid=574499)[0;0m     return self.compiled_graph_for_general_shape(*args)
[1;36m(EngineCore_DP0 pid=574499)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=574499)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py", line 268, in compiled_graph_wrapper
[1;36m(EngineCore_DP0 pid=574499)[0;0m     graph_output = inductor_compiled_graph(*args)
[1;36m(EngineCore_DP0 pid=574499)[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=574499)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
[1;36m(EngineCore_DP0 pid=574499)[0;0m     return self._compiled_fn(*args)
[1;36m(EngineCore_DP0 pid=574499)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=574499)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 184, in <lambda>
[1;36m(EngineCore_DP0 pid=574499)[0;0m     return CompiledArtifact(lambda *args: compiled_fn(list(args)), None)
[1;36m(EngineCore_DP0 pid=574499)[0;0m                                           ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=574499)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
[1;36m(EngineCore_DP0 pid=574499)[0;0m     all_outs = call_func_at_runtime_with_args(
[1;36m(EngineCore_DP0 pid=574499)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=574499)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
[1;36m(EngineCore_DP0 pid=574499)[0;0m     out = normalize_as_list(f(args))
[1;36m(EngineCore_DP0 pid=574499)[0;0m                             ^^^^^^^
[1;36m(EngineCore_DP0 pid=574499)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
[1;36m(EngineCore_DP0 pid=574499)[0;0m     return compiled_fn(runtime_args)
[1;36m(EngineCore_DP0 pid=574499)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=574499)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/output_code.py", line 613, in __call__
[1;36m(EngineCore_DP0 pid=574499)[0;0m     return self.current_callable(inputs)
[1;36m(EngineCore_DP0 pid=574499)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=574499)[0;0m   File "/home/ch269957/.conda/envs/slm/lib/python3.12/site-packages/torch/_inductor/utils.py", line 2962, in run
[1;36m(EngineCore_DP0 pid=574499)[0;0m     out = model(new_inputs)
[1;36m(EngineCore_DP0 pid=574499)[0;0m           ^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=574499)[0;0m   File "/tmp/torchinductor_ch269957/e6/ce6cgw7n42hkhqbz26qv76rj3cvq3sj2bzahzg62722bds6daraz.py", line 906, in call
[1;36m(EngineCore_DP0 pid=574499)[0;0m     buf3 = empty_strided_cuda((s72, 28672), (28672, 1), torch.bfloat16)
[1;36m(EngineCore_DP0 pid=574499)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=574499)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 44.42 GiB of which 10.50 MiB is free. Including non-PyTorch memory, this process has 24.86 GiB memory in use. Process 574502 has 19.54 GiB memory in use. Of the allocated memory 24.50 GiB is allocated by PyTorch, with 75.88 MiB allocated in private pools (e.g., CUDA Graphs), and 21.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W1204 14:37:25.118694335 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: mistralai/Mistral-7B-Instruct-v0.3 (backend=vllm)
[ModelCache] Using max_model_len=32768 for mistralai/Mistral-7B-Instruct-v0.3
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 14:37:46 [utils.py:253] non-default args: {'max_model_len': 32768, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'mistralai/Mistral-7B-Instruct-v0.3'}
INFO 12-04 14:37:46 [model.py:631] Resolved architecture: MistralForCausalLM
INFO 12-04 14:37:46 [model.py:1745] Using max model len 32768
INFO 12-04 14:37:46 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
Error occurred: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Retrying due to an error......
[ModelCache] Loading model: mistralai/Mistral-7B-Instruct-v0.3 (backend=vllm)
[ModelCache] Using max_model_len=32768 for mistralai/Mistral-7B-Instruct-v0.3
[ModelCache] Detected 1 GPU(s), 44.4GB total VRAM
[ModelCache] Using tensor_parallel_size=1
INFO 12-04 14:37:46 [utils.py:253] non-default args: {'max_model_len': 32768, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'model': 'mistralai/Mistral-7B-Instruct-v0.3'}
INFO 12-04 14:37:46 [model.py:631] Resolved architecture: MistralForCausalLM
INFO 12-04 14:37:46 [model.py:1745] Using max model len 32768
INFO 12-04 14:37:46 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=575348)[0;0m INFO 12-04 14:37:59 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='mistralai/Mistral-7B-Instruct-v0.3', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.3', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=mistralai/Mistral-7B-Instruct-v0.3, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=575352)[0;0m INFO 12-04 14:37:59 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='mistralai/Mistral-7B-Instruct-v0.3', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.3', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=mistralai/Mistral-7B-Instruct-v0.3, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=575348)[0;0m INFO 12-04 14:38:00 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.9:46267 backend=nccl
[W1204 14:38:00.833584088 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-21-2.rc.tch.harvard.edu]:46267 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=575348)[0;0m INFO 12-04 14:38:00 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=575352)[0;0m INFO 12-04 14:38:00 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.38.4.9:55491 backend=nccl
[W1204 14:38:00.184284803 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gpu-21-2.rc.tch.harvard.edu]:55491 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=575348)[0;0m INFO 12-04 14:38:00 [gpu_model_runner.py:3259] Starting to load model mistralai/Mistral-7B-Instruct-v0.3...
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=575352)[0;0m INFO 12-04 14:38:00 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=575352)[0;0m INFO 12-04 14:38:01 [gpu_model_runner.py:3259] Starting to load model mistralai/Mistral-7B-Instruct-v0.3...
[1;36m(EngineCore_DP0 pid=575348)[0;0m INFO 12-04 14:38:01 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=575348)[0;0m INFO 12-04 14:38:01 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=575352)[0;0m INFO 12-04 14:38:02 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=575352)[0;0m INFO 12-04 14:38:02 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=575348)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]
[1;36m(EngineCore_DP0 pid=575352)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]
slurmstepd-gpu-21-2: error: *** STEP 12735862.0 ON gpu-21-2 CANCELLED AT 2025-12-04T14:38:17 ***
