# Configuration for multiagent debate experiments

# Default model to use (can be alias or full path)
# Override with --model argument at runtime
model: "deepseek"  # 1.5B - smallest, fastest for testing

# Generation parameters (matching GPT-3.5-turbo-0301 defaults)
generation:
  temperature: 1.0      # High for diverse agent responses
  max_tokens: null      # Let model decide (2048 used as practical limit)
  top_p: 1.0           # Full nucleus sampling
  n: 1                 # One completion per call

# Default experiment configurations (from original study)
experiments:
  math:
    agents: 2
    rounds: 3
    num_problems: 100
    random_seed: 0

  gsm:
    agents: 3
    rounds: 2
    num_problems: 100
    random_seed: 0

  biography:
    agents: 3
    rounds: 2
    num_people: 40
    random_seed: 1

  mmlu:
    agents: 3
    rounds: 2
    num_questions: 100
    random_seed: 0

# Model aliases (for convenience)
models:
  # MLX models (Mac M4 Pro with Apple Silicon)
  deepseek: "valuat/DeepSeek-R1-Distill-Qwen-1.5B-mlx-fp16"
  llama32-3b: "mlx-community/Llama-3.2-3B-Instruct"
  smallthinker: "valuat/SmallThinker-3B-Preview-mlx-fp16"
  qwen25-7b: "valuat/Qwen2.5-7B-Instruct-1M-mlx-fp16"
  llama31-8b: "mlx-community/Meta-Llama-3.1-8B-Instruct-8bit"
  llama31-8b-fp16: "valuat/Meta-Llama-3.1-8B-Instruct-mlx-fp16"
  qwen25-14b: "valuat/Qwen2.5-14B-Instruct-1M-mlx-fp16"
  vibethinker: "valuat/VibeThinker-1.5B-mlx-8Bit"

  # vLLM models (Linux with NVIDIA GPUs - HuggingFace originals)
  vllm-deepseek: "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B"
  vllm-llama32-3b: "meta-llama/Llama-3.2-3B-Instruct"
  vllm-qwen25-7b: "Qwen/Qwen2.5-7B-Instruct"
  vllm-llama31-8b: "meta-llama/Meta-Llama-3.1-8B-Instruct"
  vllm-qwen25-14b: "Qwen/Qwen2.5-14B-Instruct"

  # Ollama models (Cross-platform - GGUF format)
  ollama-deepseek: "deepseek-r1:1.5b"
  ollama-llama32: "llama3.2:3b"
  ollama-qwen25-7b: "qwen2.5:7b"
  ollama-llama31-8b: "llama3.1:8b"
  ollama-qwen25-14b: "qwen2.5:14b"

# Dataset paths (relative to repo root)
datasets:
  gsm: "data/gsm8k/test.jsonl"
  biography: "data/biography/article.json"
  mmlu: "data/mmlu"  # Directory containing *_test.csv files
  math: null  # Generated on-the-fly
