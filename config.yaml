# Configuration for multiagent debate experiments

# Default model to use (can be alias or full path)
# Override with --model argument at runtime
model: "deepseek"  # 1.5B - smallest, fastest for testing

# Generation parameters (matching GPT-3.5-turbo-0301 defaults)
generation:
  temperature: 1.0      # High for diverse agent responses
  max_tokens: null      # Let model decide (2048 used as practical limit)
  top_p: 1.0           # Full nucleus sampling
  n: 1                 # One completion per call

# Default experiment configurations (from original study)
experiments:
  math:
    agents: 2
    rounds: 3
    num_problems: 100
    random_seed: 0

  gsm:
    agents: 3
    rounds: 2
    num_problems: 100
    random_seed: 0

  biography:
    agents: 3
    rounds: 2
    num_people: 40
    random_seed: 1

  mmlu:
    agents: 3
    rounds: 2
    num_questions: 100
    random_seed: 0

# Model aliases (for convenience)
models:
  # MLX models (Mac M4 Pro with Apple Silicon)
  qwen3-0.6b:       "Qwen/Qwen3-0.6B-MLX-8bit"
  vibethinker:      "valuat/VibeThinker-1.5B-mlx-8Bit"
  deepseek:         "valuat/DeepSeek-R1-Distill-Qwen-1.5B-mlx-8Bit"
  qwen3-1.7b:       "Qwen/Qwen3-1.7B-MLX-8bit"
  llama32-3b:       "mlx-community/Llama-3.2-3B-Instruct-8bit"
  smallthinker:     "valuat/SmallThinker-3B-Preview-mlx-8Bit"
  qwen3-4b:         "Qwen/Qwen3-4B-MLX-8bit"
  llama31-8b:       "mlx-community/Meta-Llama-3.1-8B-Instruct-8bit"
  qwen3-8b:         "Qwen/Qwen3-8B-MLX-8bit"
  qwen3-14b:        "Qwen/Qwen3-14B-MLX-8bit"
  oss-gpt-20b:      "mlx-community/gpt-oss-20b-MXFP4-Q8"
  qwen3-embed-600m: "mlx-community/Qwen3-Embedding-0.6B-4bit-DWQ"

  # vLLM models (Linux with NVIDIA GPUs - HuggingFace originals)
  vllm-qwen3-0.6b: "Qwen/Qwen3-0.6B"
  vllm-vibethinker: "WeiboAI/VibeThinker-1.5B"
  vllm-deepseek: "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B"
  vllm-qwen3-1.7b: "Qwen/Qwen3-1.7B"
  vllm-llama32-3b: "meta-llama/Llama-3.2-3B-Instruct"
  vllm-smallthinker: "PowerInfer/SmallThinker-3B-Preview"
  vllm-qwen3-4b: "Qwen/Qwen3-4B-Instruct-2507"
  vllm-llama31-8b: "meta-llama/Llama-3.1-8B-Instruct"
  vllm-qwen3-8b: "Qwen/Qwen3-VL-8B-Instruct"
  vllm-qwen3-14b: "Qwen/Qwen3-14B"
  vllm-oss-20b: "openai/gpt-oss-20b"

  # Ollama models (Cross-platform - GGUF format)
  ollama-deepseek: "deepseek-r1:1.5b"
  ollama-llama32: "llama3.2:3b"
  ollama-qwen25-7b: "qwen2.5:7b"
  ollama-llama31-8b: "llama3.1:8b"
  ollama-qwen25-14b: "qwen2.5:14b"

# Dataset paths (relative to repo root)
datasets:
  gsm: "data/gsm8k/test.jsonl"
  biography: "data/biography/article.json"
  mmlu: "data/mmlu"  # Directory containing *_test.csv files
  math: null  # Generated on-the-fly
