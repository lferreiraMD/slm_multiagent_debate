# Configuration for multiagent debate experiments

# Default model to use (can be alias or full path)
# Override with --model argument at runtime
model: "deepseek"  # 1.5B - smallest, fastest for testing

# Generation parameters (matching GPT-3.5-turbo-0301 defaults)
generation:
  temperature: 1.0      # High for diverse agent responses
  max_tokens: null      # Let model decide (2048 used as practical limit)
  top_p: 1.0           # Full nucleus sampling
  n: 1                 # One completion per call

# Default experiment configurations (from original study)
experiments:
  math:
    agents: 2
    rounds: 3
    num_problems: 100
    random_seed: 0

  gsm:
    agents: 3
    rounds: 2
    num_problems: 100
    random_seed: 0

  biography:
    agents: 3
    rounds: 2
    num_people: 40
    random_seed: 1

  mmlu:
    agents: 3
    rounds: 2
    num_questions: 100
    random_seed: 0

# Model aliases (for convenience)
models:
  # MLX models (already downloaded for M4 Pro)
  deepseek: "valuat/DeepSeek-R1-Distill-Qwen-1.5B-mlx-fp16"
  llama32-3b: "mlx-community/Llama-3.2-3B-Instruct"
  smallthinker: "valuat/SmallThinker-3B-Preview-mlx-fp16"
  qwen25-7b: "valuat/Qwen2.5-7B-Instruct-1M-mlx-fp16"
  llama31-8b: "mlx-community/Meta-Llama-3.1-8B-Instruct-8bit"
  llama31-8b-fp16: "valuat/Meta-Llama-3.1-8B-Instruct-mlx-fp16"
  qwen25-14b: "valuat/Qwen2.5-14B-Instruct-1M-mlx-fp16"

  # Ollama models (for HPC/Windows deployment)
  ollama-llama32: "llama3.2:3b"
  ollama-qwen25: "qwen2.5:7b"

# Dataset paths (relative to repo root)
datasets:
  gsm: "data/gsm8k/test.jsonl"
  biography: "data/biography/article.json"
  mmlu: "data/mmlu"  # Directory containing *_test.csv files
  math: null  # Generated on-the-fly
