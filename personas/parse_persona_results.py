#!/usr/bin/env python3
"""
Parse and aggregate persona optimization results from embedding_search.py

This script scans a directory for pickle files generated by embedding_search.py,
extracts the optimization results, and creates a comprehensive Pandas DataFrame.

Usage:
    python3 parse_persona_results.py --input-dir /path/to/results --output summary.csv

Output:
    - CSV file with aggregated results
    - Summary statistics printed to console
"""

import argparse
import os
import pickle
import re
from pathlib import Path
from typing import Dict, List, Tuple
import pandas as pd
import numpy as np


# Model parameter counts (in billions)
MODEL_PARAMS = {
    "Qwen3-0.6B": 0.6,
    "VibeThinker-1.5B": 1.5,
    "DeepSeek-R1-Distill-Qwen-1.5B": 1.5,
    "Qwen3-1.7B": 1.7,
    "Llama-3.2-3B-Instruct": 3.0,
    "SmallThinker-3B-Preview": 3.0,
    "Qwen3-4B-Instruct-2507": 4.0,
    "Llama-3.1-8B-Instruct": 8.0,
    "Qwen3-8B": 8.0,
    "Qwen3-14B": 14.0,
}


def extract_model_params(model_name: str) -> float:
    """
    Extract parameter count from model name.

    Returns parameter count in billions, or NaN if not found.
    """
    for key, params in MODEL_PARAMS.items():
        if key in model_name:
            return params

    # Fallback: try to extract from name (e.g., "3B" -> 3.0)
    match = re.search(r'(\d+\.?\d*)B', model_name, re.IGNORECASE)
    if match:
        return float(match.group(1))

    return np.nan


def parse_filename(filename: str) -> Dict:
    """
    Parse pickle filename to extract metadata.

    Format: results_{framework}_{N}_agents_{persona_version}_{model}.pk
    Example: results_vllm_3_agents_personas_v1_Llama-3.2-3B-Instruct.pk

    Returns dict with: framework, n_agents, persona_version, model
    """
    # Remove .pk extension
    name = filename.replace('.pk', '')

    # Pattern: results_{framework}_{N}_agents_{persona_version}_{model}
    pattern = r'results_([^_]+)_(\d+)_agents_(personas_v\d+)_(.+)'
    match = re.match(pattern, name)

    if not match:
        raise ValueError(f"Filename doesn't match expected pattern: {filename}")

    framework, n_agents, persona_version, model = match.groups()

    return {
        'framework': framework,
        'n_agents': int(n_agents),
        'persona_version': persona_version.replace('personas_', ''),  # 'v1' or 'v2'
        'model': model
    }


def load_pickle_file(filepath: str) -> Dict:
    """Load pickle file and return contents."""
    with open(filepath, 'rb') as f:
        return pickle.load(f)


def compute_intersection(maxmin_indices: Tuple, maxdet_indices: Tuple) -> int:
    """Count how many personas are in both MaxMin and MaxDet solutions."""
    return len(set(maxmin_indices) & set(maxdet_indices))


def parse_all_results(input_dir: str) -> pd.DataFrame:
    """
    Scan directory for pickle files and aggregate into DataFrame.

    Returns DataFrame with columns:
        - model: Model name
        - params_b: Parameter count in billions
        - framework: 'mlx' or 'vllm'
        - n_agents: Number of agents (2-7)
        - persona_version: 'v1' or 'v2'
        - maxmin_personas: Tuple of selected persona strings (MaxMin)
        - maxmin_indices: Tuple of indices (MaxMin)
        - maxmin_score: MaxMin distance value
        - maxdet_personas: Tuple of selected persona strings (MaxDet)
        - maxdet_indices: Tuple of indices (MaxDet)
        - maxdet_volume: MaxDet determinant value
        - intersection_count: Number of overlapping personas
    """
    input_path = Path(input_dir)

    # Find all pickle files
    pickle_files = list(input_path.glob('results_*.pk'))

    if not pickle_files:
        print(f"Warning: No pickle files found in {input_dir}")
        return pd.DataFrame()

    print(f"Found {len(pickle_files)} pickle files")

    rows = []
    failed_files = []

    for pkl_file in pickle_files:
        try:
            # Parse filename
            metadata = parse_filename(pkl_file.name)

            # Load pickle
            results = load_pickle_file(str(pkl_file))

            # Extract data
            row = {
                'model': metadata['model'],
                'params_b': extract_model_params(metadata['model']),
                'framework': metadata['framework'],
                'n_agents': metadata['n_agents'],
                'persona_version': metadata['persona_version'],

                # MaxMin results
                'maxmin_personas': tuple(results['maxmin_personas']),
                'maxmin_indices': results['best_indices'],
                'maxmin_score': results.get('maxmin_score', np.nan),  # May not exist in old files

                # MaxDet results
                'maxdet_personas': tuple(results['maxdet_personas']),
                'maxdet_indices': results['best_indices_det'],
                'maxdet_volume': results.get('maxdet_volume', np.nan),  # May not exist in old files

                # Comparison
                'intersection_count': compute_intersection(
                    results['best_indices'],
                    results['best_indices_det']
                ),
            }

            rows.append(row)

        except Exception as e:
            failed_files.append((pkl_file.name, str(e)))
            print(f"Error parsing {pkl_file.name}: {e}")

    if failed_files:
        print(f"\nFailed to parse {len(failed_files)} files:")
        for fname, error in failed_files:
            print(f"  - {fname}: {error}")

    # Create DataFrame
    df = pd.DataFrame(rows)

    # Sort by params, n_agents, persona_version
    if not df.empty:
        df = df.sort_values(['params_b', 'n_agents', 'persona_version'])
        df = df.reset_index(drop=True)

    return df


def print_summary(df: pd.DataFrame):
    """Print summary statistics."""
    print("\n" + "="*70)
    print("PERSONA OPTIMIZATION RESULTS SUMMARY")
    print("="*70)

    if df.empty:
        print("No results found.")
        return

    print(f"\nTotal experiments: {len(df)}")
    print(f"\nModels tested: {df['model'].nunique()}")
    print(f"Frameworks: {sorted(df['framework'].unique())}")
    print(f"Persona versions: {sorted(df['persona_version'].unique())}")
    print(f"Agent counts: {sorted(df['n_agents'].unique())}")

    # Parameter distribution
    print(f"\nParameter counts (billions):")
    print(df.groupby('params_b').size().to_string())

    # MaxMin vs MaxDet agreement
    print(f"\nMaxMin/MaxDet Agreement:")
    print(f"  Mean intersection count: {df['intersection_count'].mean():.2f}")
    print(f"  Max intersection: {df['intersection_count'].max()}")
    print(f"  Min intersection: {df['intersection_count'].min()}")

    # Score statistics (if available)
    if not df['maxmin_score'].isna().all():
        print(f"\nMaxMin Scores:")
        print(f"  Mean: {df['maxmin_score'].mean():.4f}")
        print(f"  Min: {df['maxmin_score'].min():.4f}")
        print(f"  Max: {df['maxmin_score'].max():.4f}")

    if not df['maxdet_volume'].isna().all():
        print(f"\nMaxDet Volumes:")
        print(f"  Mean: {df['maxdet_volume'].mean():.6f}")
        print(f"  Min: {df['maxdet_volume'].min():.6f}")
        print(f"  Max: {df['maxdet_volume'].max():.6f}")

    print("\n" + "="*70)


def main():
    parser = argparse.ArgumentParser(
        description="Parse and aggregate persona optimization results"
    )
    parser.add_argument(
        "--input-dir",
        type=str,
        required=True,
        help="Directory containing pickle files from embedding_search.py"
    )
    parser.add_argument(
        "--output",
        type=str,
        default="persona_results_summary.csv",
        help="Output CSV filename (default: persona_results_summary.csv)"
    )
    parser.add_argument(
        "--show-personas",
        action="store_true",
        help="Print selected personas for each experiment (verbose)"
    )

    args = parser.parse_args()

    # Parse all results
    print(f"Scanning directory: {args.input_dir}")
    df = parse_all_results(args.input_dir)

    if df.empty:
        print("No results to save.")
        return

    # Print summary
    print_summary(df)

    # Optionally show personas
    if args.show_personas:
        print("\n" + "="*70)
        print("SELECTED PERSONAS BY EXPERIMENT")
        print("="*70)
        for idx, row in df.iterrows():
            print(f"\n[{idx+1}] {row['model']} | {row['n_agents']} agents | {row['persona_version']}")
            print(f"  MaxMin score: {row['maxmin_score']:.4f}")
            print("  MaxMin personas:")
            for i, persona in enumerate(row['maxmin_personas'], 1):
                print(f"    {i}. {persona}")
            print(f"  MaxDet volume: {row['maxdet_volume']:.6f}")
            print("  MaxDet personas:")
            for i, persona in enumerate(row['maxdet_personas'], 1):
                print(f"    {i}. {persona}")

    # Save to CSV
    output_path = os.path.abspath(args.output)
    df.to_csv(output_path, index=False)
    print(f"\nâœ“ Results saved to: {output_path}")
    print(f"  Rows: {len(df)}")
    print(f"  Columns: {len(df.columns)}")


if __name__ == "__main__":
    main()
